{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GeoAI: Artificial Intelligence for Geospatial Data","text":"<p>A powerful Python package for integrating artificial intelligence with geospatial data analysis and visualization</p>"},{"location":"#introduction","title":"\ud83d\udcd6 Introduction","text":"<p>GeoAI is a comprehensive Python package designed to bridge artificial intelligence (AI) and geospatial data analysis, providing researchers and practitioners with intuitive tools for applying machine learning techniques to geographic data. The package offers a unified framework for processing satellite imagery, aerial photographs, and vector data using state-of-the-art deep learning models. GeoAI integrates popular AI frameworks including PyTorch, Transformers, PyTorch Segmentation Models, and specialized geospatial libraries like torchange, enabling users to perform complex geospatial analyses with minimal code.</p> <p>The package provides five core capabilities:</p> <ol> <li>Interactive and programmatic search and download of remote sensing imagery and geospatial data.</li> <li>Automated dataset preparation with image chips and label generation.</li> <li>Model training for tasks such as classification, detection, and segmentation.</li> <li>Inference pipelines for applying models to new geospatial datasets.</li> <li>Interactive visualization through integration with Leafmap and MapLibre.</li> </ol> <p>GeoAI addresses the growing demand for accessible AI tools in geospatial research by providing high-level APIs that abstract complex machine learning workflows while maintaining flexibility for advanced users. The package supports multiple data formats (GeoTIFF, JPEG2000,GeoJSON, Shapefile, GeoPackage) and includes automatic device management for GPU acceleration when available. With over 10 modules and extensive notebook examples, GeoAI serves as both a research tool and educational resource for the geospatial AI community.</p>"},{"location":"#statement-of-need","title":"\ud83d\udcdd Statement of Need","text":"<p>The integration of artificial intelligence with geospatial data analysis has become increasingly critical across numerous scientific disciplines, from environmental monitoring and urban planning to disaster response and climate research. However, applying AI techniques to geospatial data presents unique challenges including data preprocessing complexities, specialized model architectures, and the need for domain-specific knowledge in both machine learning and geographic information systems.</p> <p>Existing solutions often require researchers to navigate fragmented ecosystems of tools, combining general-purpose machine learning libraries with specialized geospatial packages, leading to steep learning curves and reproducibility challenges. While packages like TorchGeo and TerraTorch provide excellent foundational tools for geospatial deep learning, there remains a gap for comprehensive, high-level interfaces that can democratize access to advanced AI techniques for the broader geospatial community.</p> <p>GeoAI addresses this need by providing a unified, user-friendly interface that abstracts the complexity of integrating multiple AI frameworks with geospatial data processing workflows. It lowers barriers for: (1) geospatial researchers who need accessible AI workflows without deep ML expertise; (2) AI practitioners who want streamlined geospatial preprocessing and domain-specific datasets; and (3) educators seeking reproducible examples and teaching-ready workflows.</p> <p>The package's design philosophy emphasizes simplicity without sacrificing functionality, enabling users to perform sophisticated analyses such as building footprint extraction from satellite imagery, land cover classification, and change detection with just a few lines of code. By integrating cutting-edge AI models and providing seamless access to major geospatial data sources, GeoAI significantly lowers the barrier to entry for geospatial AI applications while maintaining the flexibility needed for advanced research applications.</p>"},{"location":"#citations","title":"Citations","text":"<p>If you find GeoAI useful in your research, please consider citing the following paper to support my work. Thank you for your support.</p> <ul> <li>Wu, Q. (2025). GeoAI: A Python package for integrating artificial intelligence with geospatial data analysis and visualization. Journal of Open Source Software, 9025. https://doi.org/10.21105/joss.09025 (Under Review)</li> </ul>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":""},{"location":"#advanced-geospatial-data-visualization","title":"\ud83d\udcca Advanced Geospatial Data Visualization","text":"<ul> <li>Interactive multi-layer visualization of vector and raster data stored locally or in cloud storage</li> <li>Customizable styling and symbology</li> <li>Time-series data visualization capabilities</li> </ul>"},{"location":"#data-preparation-processing","title":"\ud83d\udee0\ufe0f Data Preparation &amp; Processing","text":"<ul> <li>Streamlined access to satellite and aerial imagery from providers like Sentinel, Landsat, NAIP, and other open datasets</li> <li>Tools for downloading, mosaicking, and preprocessing remote sensing data</li> <li>Automated generation of training datasets with image chips and corresponding labels</li> <li>Vector-to-raster and raster-to-vector conversion utilities optimized for AI workflows</li> <li>Data augmentation techniques specific to geospatial data</li> <li>Support for integrating Overture Maps data and other open datasets for training and validation</li> </ul>"},{"location":"#image-segmentation","title":"\ud83d\uddbc\ufe0f Image Segmentation","text":"<ul> <li>Integration with PyTorch Segmentation Models for automatic feature extraction</li> <li>Specialized segmentation algorithms optimized for satellite and aerial imagery</li> <li>Streamlined workflows for segmenting buildings, water bodies, wetlands,solar panels, etc.</li> <li>Export capabilities to standard geospatial formats (GeoJSON, Shapefile, GeoPackage, GeoParquet)</li> </ul>"},{"location":"#image-classification","title":"\ud83d\udd0d Image Classification","text":"<ul> <li>Pre-trained models for land cover and land use classification</li> <li>Transfer learning utilities for fine-tuning models with your own data</li> <li>Multi-temporal classification support for change detection</li> <li>Accuracy assessment and validation tools</li> </ul>"},{"location":"#additional-capabilities","title":"\ud83c\udf0d Additional Capabilities","text":"<ul> <li>Change detection with AI-enhanced feature extraction</li> <li>Object detection in aerial and satellite imagery</li> <li>Georeferencing utilities for AI model outputs</li> </ul>"},{"location":"#installation","title":"\ud83d\udce6 Installation","text":""},{"location":"#using-pip","title":"Using pip","text":"<pre><code>pip install geoai-py\n</code></pre>"},{"location":"#using-conda","title":"Using conda","text":"<pre><code>conda install -c conda-forge geoai\n</code></pre>"},{"location":"#using-mamba","title":"Using mamba","text":"<pre><code>mamba install -c conda-forge geoai\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udccb Documentation","text":"<p>Comprehensive documentation is available at https://opengeoai.org, including:</p> <ul> <li>Detailed API reference</li> <li>Tutorials and example notebooks</li> <li>Contributing guide</li> </ul>"},{"location":"#video-tutorials","title":"\ud83d\udcfa\u00a0Video Tutorials","text":""},{"location":"#geoai-made-easy-learn-the-python-package-step-by-step-beginner-friendly","title":"GeoAI Made Easy: Learn the Python Package Step-by-Step (Beginner Friendly)","text":""},{"location":"#geoai-workshop-unlocking-the-power-of-geoai-with-python","title":"GeoAI Workshop: Unlocking the Power of GeoAI with Python","text":""},{"location":"#geoai-tutorials-playlist","title":"GeoAI Tutorials Playlist","text":""},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions of all kinds! See our contributing guide for ways to get started.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>GeoAI is free and open source software, licensed under the MIT License.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>We gratefully acknowledge the support of the following organizations:</p> <ul> <li>NASA: This research is partially supported by the National Aeronautics and Space Administration (NASA) through Grant No. 80NSSC22K1742, awarded under the Open Source Tools, Frameworks, and Libraries Program.</li> <li>AmericaView: This work is also partially supported by the U.S. Geological Survey through Grant/Cooperative Agreement No. G23AP00683 (GY23-GY27) in collaboration with AmericaView.</li> </ul>"},{"location":"change_detection/","title":"change_detection module","text":"<p>Change detection module for remote sensing imagery using torchange.</p>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection","title":"<code>ChangeDetection</code>","text":"<p>A class for change detection on geospatial imagery using torchange and SAM.</p> Source code in <code>geoai/change_detection.py</code> <pre><code>class ChangeDetection:\n    \"\"\"A class for change detection on geospatial imagery using torchange and SAM.\"\"\"\n\n    def __init__(self, sam_model_type=\"vit_h\", sam_checkpoint=None):\n        \"\"\"\n        Initialize the ChangeDetection class.\n\n        Args:\n            sam_model_type (str): SAM model type ('vit_h', 'vit_l', 'vit_b')\n            sam_checkpoint (str): Path to SAM checkpoint file\n        \"\"\"\n        self.sam_model_type = sam_model_type\n        self.sam_checkpoint = sam_checkpoint\n        self.model = None\n        self._init_model()\n\n    def _init_model(self):\n        \"\"\"Initialize the AnyChange model.\"\"\"\n        if AnyChange is None:\n            raise ImportError(\n                \"The 'torchange' package is required for change detection. \"\n                \"Please install it using: pip install torchange\\n\"\n                \"Note: torchange requires Python 3.11 or higher.\"\n            )\n\n        if self.sam_checkpoint is None:\n            self.sam_checkpoint = download_checkpoint(self.sam_model_type)\n\n        self.model = AnyChange(self.sam_model_type, sam_checkpoint=self.sam_checkpoint)\n\n        # Set default hyperparameters\n        self.model.make_mask_generator(\n            points_per_side=32,\n            stability_score_thresh=0.95,\n        )\n        self.model.set_hyperparameters(\n            change_confidence_threshold=145,\n            use_normalized_feature=True,\n            bitemporal_match=True,\n        )\n\n    def set_hyperparameters(\n        self,\n        change_confidence_threshold: int = 155,\n        auto_threshold: bool = False,\n        use_normalized_feature: bool = True,\n        area_thresh: float = 0.8,\n        match_hist: bool = False,\n        object_sim_thresh: int = 60,\n        bitemporal_match: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Set hyperparameters for the change detection model.\n\n        Args:\n            change_confidence_threshold (int): Change confidence threshold for SAM\n            auto_threshold (bool): Whether to use auto threshold for SAM\n            use_normalized_feature (bool): Whether to use normalized feature for SAM\n            area_thresh (float): Area threshold for SAM\n            match_hist (bool): Whether to use match hist for SAM\n            object_sim_thresh (int): Object similarity threshold for SAM\n            bitemporal_match (bool): Whether to use bitemporal match for SAM\n            **kwargs: Keyword arguments for model hyperparameters\n        \"\"\"\n        if self.model:\n            self.model.set_hyperparameters(\n                change_confidence_threshold=change_confidence_threshold,\n                auto_threshold=auto_threshold,\n                use_normalized_feature=use_normalized_feature,\n                area_thresh=area_thresh,\n                match_hist=match_hist,\n                object_sim_thresh=object_sim_thresh,\n                bitemporal_match=bitemporal_match,\n                **kwargs,\n            )\n\n    def set_mask_generator_params(\n        self,\n        points_per_side: int = 32,\n        points_per_batch: int = 64,\n        pred_iou_thresh: float = 0.5,\n        stability_score_thresh: float = 0.95,\n        stability_score_offset: float = 1.0,\n        box_nms_thresh: float = 0.7,\n        point_grids: Optional[List] = None,\n        min_mask_region_area: int = 0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Set mask generator parameters.\n\n        Args:\n            points_per_side (int): Number of points per side for SAM\n            points_per_batch (int): Number of points per batch for SAM\n            pred_iou_thresh (float): IoU threshold for SAM\n            stability_score_thresh (float): Stability score threshold for SAM\n            stability_score_offset (float): Stability score offset for SAM\n            box_nms_thresh (float): NMS threshold for SAM\n            point_grids (list): Point grids for SAM\n            min_mask_region_area (int): Minimum mask region area for SAM\n            **kwargs: Keyword arguments for mask generator\n        \"\"\"\n        if self.model:\n            self.model.make_mask_generator(\n                points_per_side=points_per_side,\n                points_per_batch=points_per_batch,\n                pred_iou_thresh=pred_iou_thresh,\n                stability_score_thresh=stability_score_thresh,\n                stability_score_offset=stability_score_offset,\n                box_nms_thresh=box_nms_thresh,\n                point_grids=point_grids,\n                min_mask_region_area=min_mask_region_area,\n                **kwargs,\n            )\n\n    def _read_and_align_images(self, image1_path, image2_path, target_size=1024):\n        \"\"\"\n        Read and align two GeoTIFF images, handling different extents and projections.\n\n        Args:\n            image1_path (str): Path to first image\n            image2_path (str): Path to second image\n            target_size (int): Target size for processing (default 1024 for torchange)\n\n        Returns:\n            tuple: (aligned_img1, aligned_img2, transform, crs, bounds)\n        \"\"\"\n        with rasterio.open(image1_path) as src1, rasterio.open(image2_path) as src2:\n            # Get the intersection of bounds\n            bounds1 = src1.bounds\n            bounds2 = src2.bounds\n\n            # Calculate intersection bounds\n            left = max(bounds1.left, bounds2.left)\n            bottom = max(bounds1.bottom, bounds2.bottom)\n            right = min(bounds1.right, bounds2.right)\n            top = min(bounds1.top, bounds2.top)\n\n            if left &gt;= right or bottom &gt;= top:\n                raise ValueError(\"Images do not overlap\")\n\n            intersection_bounds = (left, bottom, right, top)\n\n            # Read the intersecting area from both images\n            window1 = from_bounds(*intersection_bounds, src1.transform)\n            window2 = from_bounds(*intersection_bounds, src2.transform)\n\n            # Read data\n            img1_data = src1.read(window=window1)\n            img2_data = src2.read(window=window2)\n\n            # Get transform for the intersecting area\n            transform = src1.window_transform(window1)\n            crs = src1.crs\n\n            # Convert from (bands, height, width) to (height, width, bands)\n            img1_data = np.transpose(img1_data, (1, 2, 0))\n            img2_data = np.transpose(img2_data, (1, 2, 0))\n\n            # Use only RGB bands (first 3 channels) for torchange\n            if img1_data.shape[2] &gt;= 3:\n                img1_data = img1_data[:, :, :3]\n            if img2_data.shape[2] &gt;= 3:\n                img2_data = img2_data[:, :, :3]\n\n            # Normalize to 0-255 range if needed\n            if img1_data.dtype != np.uint8:\n                img1_data = (\n                    (img1_data - img1_data.min())\n                    / (img1_data.max() - img1_data.min())\n                    * 255\n                ).astype(np.uint8)\n            if img2_data.dtype != np.uint8:\n                img2_data = (\n                    (img2_data - img2_data.min())\n                    / (img2_data.max() - img2_data.min())\n                    * 255\n                ).astype(np.uint8)\n\n            # Store original size for later use\n            original_shape = img1_data.shape[:2]\n\n            # Resize to target size for torchange processing\n            if img1_data.shape[0] != target_size or img1_data.shape[1] != target_size:\n                img1_resized = resize(\n                    img1_data, (target_size, target_size), preserve_range=True\n                ).astype(np.uint8)\n                img2_resized = resize(\n                    img2_data, (target_size, target_size), preserve_range=True\n                ).astype(np.uint8)\n            else:\n                img1_resized = img1_data\n                img2_resized = img2_data\n\n            return (img1_resized, img2_resized, transform, crs, original_shape)\n\n    def detect_changes(\n        self,\n        image1_path: str,\n        image2_path: str,\n        output_path: Optional[str] = None,\n        target_size: int = 1024,\n        return_results: bool = True,\n        export_probability: bool = False,\n        probability_output_path: Optional[str] = None,\n        export_instance_masks: bool = False,\n        instance_masks_output_path: Optional[str] = None,\n        return_detailed_results: bool = False,\n    ) -&gt; Union[Tuple[Any, np.ndarray, np.ndarray], Dict[str, Any], None]:\n        \"\"\"\n        Detect changes between two GeoTIFF images with instance segmentation.\n\n        Args:\n            image1_path (str): Path to first image\n            image2_path (str): Path to second image\n            output_path (str): Optional path to save binary change mask as GeoTIFF\n            target_size (int): Target size for processing\n            return_results (bool): Whether to return results\n            export_probability (bool): Whether to export probability mask\n            probability_output_path (str): Path to save probability mask (required if export_probability=True)\n            export_instance_masks (bool): Whether to export instance segmentation masks\n            instance_masks_output_path (str): Path to save instance masks (required if export_instance_masks=True)\n            return_detailed_results (bool): Whether to return detailed mask information\n\n        Returns:\n            tuple: (change_masks, img1, img2) if return_results=True\n            dict: Detailed results if return_detailed_results=True\n        \"\"\"\n        # Read and align images\n        (img1, img2, transform, crs, original_shape) = self._read_and_align_images(\n            image1_path, image2_path, target_size\n        )\n\n        # Detect changes\n        change_masks, _, _ = self.model.forward(img1, img2)\n\n        # If output path specified, save binary mask as GeoTIFF\n        if output_path:\n            self._save_change_mask(\n                change_masks, output_path, transform, crs, original_shape, target_size\n            )\n\n        # If probability export requested, save probability mask\n        if export_probability:\n            if probability_output_path is None:\n                raise ValueError(\n                    \"probability_output_path must be specified when export_probability=True\"\n                )\n            self._save_probability_mask(\n                change_masks,\n                probability_output_path,\n                transform,\n                crs,\n                original_shape,\n                target_size,\n            )\n\n        # If instance masks export requested, save instance segmentation masks\n        if export_instance_masks:\n            if instance_masks_output_path is None:\n                raise ValueError(\n                    \"instance_masks_output_path must be specified when export_instance_masks=True\"\n                )\n            num_instances = self._save_instance_segmentation_masks(\n                change_masks,\n                instance_masks_output_path,\n                transform,\n                crs,\n                original_shape,\n                target_size,\n            )\n\n            # Also save instance scores if requested\n            scores_path = instance_masks_output_path.replace(\".tif\", \"_scores.tif\")\n            self._save_instance_scores_mask(\n                change_masks,\n                scores_path,\n                transform,\n                crs,\n                original_shape,\n                target_size,\n            )\n\n        # Return detailed results if requested\n        if return_detailed_results:\n            return self._extract_detailed_results(\n                change_masks, transform, crs, original_shape, target_size\n            )\n\n        if return_results:\n            return change_masks, img1, img2\n\n    def _save_change_mask(\n        self, change_masks, output_path, transform, crs, original_shape, target_size\n    ):\n        \"\"\"\n        Save change masks as a GeoTIFF with proper georeference.\n\n        Args:\n            change_masks: Change detection masks (MaskData object)\n            output_path (str): Output file path\n            transform: Rasterio transform\n            crs: Coordinate reference system\n            original_shape (tuple): Original image shape\n            target_size (int): Processing target size\n        \"\"\"\n        # Convert MaskData to binary mask by decoding RLE masks\n        combined_mask = np.zeros((target_size, target_size), dtype=bool)\n\n        # Extract RLE masks from MaskData object\n        mask_items = dict(change_masks.items())\n        if \"rles\" in mask_items:\n            rles = mask_items[\"rles\"]\n            for rle in rles:\n                if isinstance(rle, dict) and \"size\" in rle and \"counts\" in rle:\n                    try:\n                        # Decode RLE to binary mask\n                        size = rle[\"size\"]\n                        counts = rle[\"counts\"]\n\n                        # Create binary mask from RLE counts\n                        mask = np.zeros(size[0] * size[1], dtype=np.uint8)\n                        pos = 0\n                        value = 0\n\n                        for count in counts:\n                            if pos + count &lt;= len(mask):\n                                if value == 1:\n                                    mask[pos : pos + count] = 1\n                                pos += count\n                                value = 1 - value  # Toggle between 0 and 1\n                            else:\n                                break\n\n                        # RLE is column-major, reshape and transpose\n                        mask = mask.reshape(size).T\n                        if mask.shape == (target_size, target_size):\n                            combined_mask = np.logical_or(\n                                combined_mask, mask.astype(bool)\n                            )\n\n                    except Exception as e:\n                        print(f\"Warning: Failed to decode RLE mask: {e}\")\n                        continue\n\n        # Convert to uint8 first, then resize if needed\n        combined_mask_uint8 = combined_mask.astype(np.uint8) * 255\n\n        # Resize back to original shape if needed\n        if original_shape != (target_size, target_size):\n            # Use precise resize\n            combined_mask_resized = resize(\n                combined_mask_uint8.astype(np.float32),\n                original_shape,\n                preserve_range=True,\n                anti_aliasing=False,\n                order=0,\n            )\n            combined_mask = (combined_mask_resized &gt; 127).astype(np.uint8) * 255\n        else:\n            combined_mask = combined_mask_uint8\n\n        # Save as GeoTIFF\n        with rasterio.open(\n            output_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=combined_mask.shape[0],\n            width=combined_mask.shape[1],\n            count=1,\n            dtype=combined_mask.dtype,\n            crs=crs,\n            transform=transform,\n            compress=\"lzw\",\n        ) as dst:\n            dst.write(combined_mask, 1)\n\n    def _save_probability_mask(\n        self, change_masks, output_path, transform, crs, original_shape, target_size\n    ):\n        \"\"\"\n        Save probability masks as a GeoTIFF with proper georeference.\n\n        Args:\n            change_masks: Change detection masks (MaskData object)\n            output_path (str): Output file path\n            transform: Rasterio transform\n            crs: Coordinate reference system\n            original_shape (tuple): Original image shape\n            target_size (int): Processing target size\n        \"\"\"\n        # Extract mask components for probability calculation\n        mask_items = dict(change_masks.items())\n        rles = mask_items.get(\"rles\", [])\n        iou_preds = mask_items.get(\"iou_preds\", None)\n        stability_scores = mask_items.get(\"stability_score\", None)\n        change_confidence = mask_items.get(\"change_confidence\", None)\n        areas = mask_items.get(\"areas\", None)\n\n        # Convert tensors to numpy if needed\n        if iou_preds is not None:\n            iou_preds = iou_preds.detach().cpu().numpy()\n        if stability_scores is not None:\n            stability_scores = stability_scores.detach().cpu().numpy()\n        if change_confidence is not None:\n            change_confidence = change_confidence.detach().cpu().numpy()\n        if areas is not None:\n            areas = areas.detach().cpu().numpy()\n\n        # Create probability mask\n        probability_mask = np.zeros((target_size, target_size), dtype=np.float32)\n\n        # Process each mask with probability weighting\n        for i, rle in enumerate(rles):\n            if isinstance(rle, dict) and \"size\" in rle and \"counts\" in rle:\n                try:\n                    # Decode RLE to binary mask\n                    size = rle[\"size\"]\n                    counts = rle[\"counts\"]\n\n                    mask = np.zeros(size[0] * size[1], dtype=np.uint8)\n                    pos = 0\n                    value = 0\n\n                    for count in counts:\n                        if pos + count &lt;= len(mask):\n                            if value == 1:\n                                mask[pos : pos + count] = 1\n                            pos += count\n                            value = 1 - value\n                        else:\n                            break\n\n                    mask = mask.reshape(size).T\n                    if mask.shape != (target_size, target_size):\n                        continue\n\n                    mask_bool = mask.astype(bool)\n\n                    # Calculate probability using multiple factors\n                    prob_components = []\n\n                    # IoU prediction (0-1, higher is better)\n                    if iou_preds is not None and i &lt; len(iou_preds):\n                        iou_score = float(iou_preds[i])\n                        prob_components.append((\"iou\", iou_score))\n                    else:\n                        prob_components.append((\"iou\", 0.8))\n\n                    # Stability score (0-1, higher is better)\n                    if stability_scores is not None and i &lt; len(stability_scores):\n                        stability = float(stability_scores[i])\n                        prob_components.append((\"stability\", stability))\n                    else:\n                        prob_components.append((\"stability\", 0.8))\n\n                    # Change confidence (normalize based on threshold)\n                    if change_confidence is not None and i &lt; len(change_confidence):\n                        conf = float(change_confidence[i])\n                        # Normalize confidence: threshold is 145, values above indicate higher confidence\n                        if conf &gt;= 145:\n                            conf_normalized = 0.5 + min(0.5, (conf - 145) / 145)\n                        else:\n                            conf_normalized = max(0.0, conf / 145 * 0.5)\n                        prob_components.append((\"confidence\", conf_normalized))\n                    else:\n                        prob_components.append((\"confidence\", 0.5))\n\n                    # Area-based weight (normalize using log scale)\n                    if areas is not None and i &lt; len(areas):\n                        area = float(areas[i])\n                        area_normalized = 0.2 + 0.8 * min(1.0, np.log(area + 1) / 15.0)\n                        prob_components.append((\"area\", area_normalized))\n                    else:\n                        prob_components.append((\"area\", 0.6))\n\n                    # Calculate weighted probability\n                    weights = {\n                        \"iou\": 0.3,\n                        \"stability\": 0.3,\n                        \"confidence\": 0.35,\n                        \"area\": 0.05,\n                    }\n                    prob_weight = sum(\n                        weights[name] * value for name, value in prob_components\n                    )\n                    prob_weight = np.clip(prob_weight, 0.0, 1.0)\n\n                    # Add to probability mask (take maximum where masks overlap)\n                    current_prob = probability_mask[mask_bool]\n                    new_prob = np.maximum(current_prob, prob_weight)\n                    probability_mask[mask_bool] = new_prob\n\n                except Exception as e:\n                    print(f\"Warning: Failed to process probability mask {i}: {e}\")\n                    continue\n\n        # Resize back to original shape if needed\n        if original_shape != (target_size, target_size):\n            prob_resized = resize(\n                probability_mask,\n                original_shape,\n                preserve_range=True,\n                anti_aliasing=True,\n                order=1,\n            )\n            prob_final = np.clip(prob_resized, 0.0, 1.0)\n        else:\n            prob_final = probability_mask\n\n        # Save as float32 GeoTIFF\n        with rasterio.open(\n            output_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=prob_final.shape[0],\n            width=prob_final.shape[1],\n            count=1,\n            dtype=rasterio.float32,\n            crs=crs,\n            transform=transform,\n            compress=\"lzw\",\n        ) as dst:\n            dst.write(prob_final.astype(np.float32), 1)\n\n    def visualize_changes(\n        self, image1_path: str, image2_path: str, figsize: Tuple[int, int] = (15, 5)\n    ) -&gt; plt.Figure:\n        \"\"\"\n        Visualize change detection results.\n\n        Args:\n            image1_path (str): Path to first image\n            image2_path (str): Path to second image\n            figsize (tuple): Figure size\n\n        Returns:\n            matplotlib.figure.Figure: The figure object\n        \"\"\"\n        if show_change_masks is None:\n            raise ImportError(\n                \"The 'torchange' package is required for change detection visualization. \"\n                \"Please install it using: pip install torchange\\n\"\n                \"Note: torchange requires Python 3.11 or higher.\"\n            )\n\n        change_masks, img1, img2 = self.detect_changes(\n            image1_path, image2_path, return_results=True\n        )\n\n        # Use torchange's visualization function\n        fig, _ = show_change_masks(img1, img2, change_masks)\n        fig.set_size_inches(figsize)\n\n        return fig\n\n    def visualize_results(\n        self,\n        image1_path,\n        image2_path,\n        binary_path,\n        prob_path,\n        title1=\"Earlier Image\",\n        title2=\"Later Image\",\n    ):\n        \"\"\"Create enhanced visualization with probability analysis.\"\"\"\n\n        # Load data\n        with rasterio.open(image1_path) as src:\n            img1 = src.read([1, 2, 3])\n            img1 = np.transpose(img1, (1, 2, 0))\n\n        with rasterio.open(image2_path) as src:\n            img2 = src.read([1, 2, 3])\n            img2 = np.transpose(img2, (1, 2, 0))\n\n        with rasterio.open(binary_path) as src:\n            binary_mask = src.read(1)\n\n        with rasterio.open(prob_path) as src:\n            prob_mask = src.read(1)\n\n        # Create comprehensive visualization\n        fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n\n        # Crop for better visualization\n        h, w = img1.shape[:2]\n        y1, y2 = h // 4, 3 * h // 4\n        x1, x2 = w // 4, 3 * w // 4\n\n        img1_crop = img1[y1:y2, x1:x2]\n        img2_crop = img2[y1:y2, x1:x2]\n        binary_crop = binary_mask[y1:y2, x1:x2]\n        prob_crop = prob_mask[y1:y2, x1:x2]\n\n        # Row 1: Original and overlays\n        axes[0, 0].imshow(img1_crop)\n        axes[0, 0].set_title(title1, fontweight=\"bold\")\n        axes[0, 0].axis(\"off\")\n\n        axes[0, 1].imshow(img2_crop)\n        axes[0, 1].set_title(title2, fontweight=\"bold\")\n        axes[0, 1].axis(\"off\")\n\n        # Binary overlay\n        overlay_binary = img2_crop.copy()\n        overlay_binary[binary_crop &gt; 0] = [255, 0, 0]\n        axes[0, 2].imshow(overlay_binary)\n        axes[0, 2].set_title(\"Binary Changes\\n(Red = Change)\", fontweight=\"bold\")\n        axes[0, 2].axis(\"off\")\n\n        # Probability heatmap\n        im1 = axes[0, 3].imshow(prob_crop, cmap=\"hot\", vmin=0, vmax=1)\n        axes[0, 3].set_title(\n            \"Probability Heatmap\\n(White = High Confidence)\", fontweight=\"bold\"\n        )\n        axes[0, 3].axis(\"off\")\n        plt.colorbar(im1, ax=axes[0, 3], shrink=0.8)\n\n        # Row 2: Detailed probability analysis\n        # Confidence levels overlay\n        overlay_conf = img2_crop.copy()\n        high_conf = prob_crop &gt; 0.7\n        med_conf = (prob_crop &gt; 0.4) &amp; (prob_crop &lt;= 0.7)\n        low_conf = (prob_crop &gt; 0.1) &amp; (prob_crop &lt;= 0.4)\n\n        overlay_conf[high_conf] = [255, 0, 0]  # Red for high\n        overlay_conf[med_conf] = [255, 165, 0]  # Orange for medium\n        overlay_conf[low_conf] = [255, 255, 0]  # Yellow for low\n\n        axes[1, 0].imshow(overlay_conf)\n        axes[1, 0].set_title(\n            \"Confidence Levels\\n(Red&gt;0.7, Orange&gt;0.4, Yellow&gt;0.1)\", fontweight=\"bold\"\n        )\n        axes[1, 0].axis(\"off\")\n\n        # Thresholded probability (&gt;0.5)\n        overlay_thresh = img2_crop.copy()\n        high_prob = prob_crop &gt; 0.5\n        overlay_thresh[high_prob] = [255, 0, 0]\n        axes[1, 1].imshow(overlay_thresh)\n        axes[1, 1].set_title(\n            \"High Confidence Only\\n(Probability &gt; 0.5)\", fontweight=\"bold\"\n        )\n        axes[1, 1].axis(\"off\")\n\n        # Probability histogram\n        prob_values = prob_crop[prob_crop &gt; 0]\n        if len(prob_values) &gt; 0:\n            axes[1, 2].hist(\n                prob_values, bins=50, alpha=0.7, color=\"red\", edgecolor=\"black\"\n            )\n            axes[1, 2].axvline(\n                x=0.5, color=\"blue\", linestyle=\"--\", label=\"0.5 threshold\"\n            )\n            axes[1, 2].axvline(\n                x=0.7, color=\"green\", linestyle=\"--\", label=\"0.7 threshold\"\n            )\n            axes[1, 2].set_xlabel(\"Change Probability\")\n            axes[1, 2].set_ylabel(\"Pixel Count\")\n            axes[1, 2].set_title(\n                f\"Probability Distribution\\n({len(prob_values):,} pixels)\"\n            )\n            axes[1, 2].legend()\n            axes[1, 2].grid(True, alpha=0.3)\n\n        # Statistics text\n        stats_text = f\"\"\"Probability Statistics:\n    Min: {np.min(prob_values):.3f}\n    Max: {np.max(prob_values):.3f}\n    Mean: {np.mean(prob_values):.3f}\n    Median: {np.median(prob_values):.3f}\n\n    Confidence Levels:\n    High (&gt;0.7): {np.sum(prob_crop &gt; 0.7):,}\n    Med (0.4-0.7): {np.sum((prob_crop &gt; 0.4) &amp; (prob_crop &lt;= 0.7)):,}\n    Low (0.1-0.4): {np.sum((prob_crop &gt; 0.1) &amp; (prob_crop &lt;= 0.4)):,}\"\"\"\n\n        axes[1, 3].text(\n            0.05,\n            0.95,\n            stats_text,\n            transform=axes[1, 3].transAxes,\n            fontsize=11,\n            verticalalignment=\"top\",\n            fontfamily=\"monospace\",\n        )\n        axes[1, 3].set_xlim(0, 1)\n        axes[1, 3].set_ylim(0, 1)\n        axes[1, 3].axis(\"off\")\n        axes[1, 3].set_title(\"Statistics Summary\", fontweight=\"bold\")\n\n        plt.tight_layout()\n        plt.suptitle(\n            \"Enhanced Probability-Based Change Detection\",\n            fontsize=16,\n            fontweight=\"bold\",\n            y=0.98,\n        )\n\n        plt.savefig(\"enhanced_probability_results.png\", dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n        print(\"\ud83d\udcbe Enhanced visualization saved as 'enhanced_probability_results.png'\")\n\n    def create_split_comparison(\n        self,\n        image1_path,\n        image2_path,\n        binary_path,\n        prob_path,\n        title1=\"Earlier Image\",\n        title2=\"Later Image\",\n        output_path=\"split_comparison.png\",\n    ):\n        \"\"\"Create a split comparison visualization showing before/after with change overlay.\"\"\"\n\n        # Load data\n        with rasterio.open(image1_path) as src:\n            img1 = src.read([1, 2, 3])\n            img1 = np.transpose(img1, (1, 2, 0))\n            if img1.dtype != np.uint8:\n                img1 = ((img1 - img1.min()) / (img1.max() - img1.min()) * 255).astype(\n                    np.uint8\n                )\n\n        with rasterio.open(image2_path) as src:\n            img2 = src.read([1, 2, 3])\n            img2 = np.transpose(img2, (1, 2, 0))\n            if img2.dtype != np.uint8:\n                img2 = ((img2 - img2.min()) / (img2.max() - img2.min()) * 255).astype(\n                    np.uint8\n                )\n\n        with rasterio.open(prob_path) as src:\n            prob_mask = src.read(1)\n\n        # Ensure all arrays have the same shape\n        h, w = img1.shape[:2]\n        if prob_mask.shape != (h, w):\n            prob_mask = resize(\n                prob_mask, (h, w), preserve_range=True, anti_aliasing=True, order=1\n            )\n\n        # Create split comparison\n        fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n\n        # Create combined image - left half is earlier, right half is later\n        combined_img = np.zeros_like(img1)\n        combined_img[:, : w // 2] = img1[:, : w // 2]\n        combined_img[:, w // 2 :] = img2[:, w // 2 :]\n\n        # Create overlay with changes - ensure prob_mask is 2D and matches image dimensions\n        overlay = combined_img.copy()\n        high_conf_changes = prob_mask &gt; 0.5\n\n        # Apply overlay only where changes are detected\n        if len(overlay.shape) == 3:  # RGB image\n            overlay[high_conf_changes] = [255, 0, 0]  # Red for high confidence changes\n\n        # Blend overlay with original\n        blended = cv2.addWeighted(combined_img, 0.7, overlay, 0.3, 0)\n\n        ax.imshow(blended)\n        ax.axvline(x=w // 2, color=\"white\", linewidth=3, linestyle=\"--\", alpha=0.8)\n        ax.text(\n            w // 4,\n            50,\n            title1,\n            fontsize=20,\n            color=\"white\",\n            ha=\"center\",\n            bbox={\"boxstyle\": \"round,pad=0.3\", \"facecolor\": \"black\", \"alpha\": 0.8},\n        )\n        ax.text(\n            3 * w // 4,\n            50,\n            title2,\n            fontsize=20,\n            color=\"white\",\n            ha=\"center\",\n            bbox={\"boxstyle\": \"round,pad=0.3\", \"facecolor\": \"black\", \"alpha\": 0.8},\n        )\n\n        ax.set_title(\n            \"Split Comparison with Change Detection\\n(Red = High Confidence Changes)\",\n            fontsize=16,\n            fontweight=\"bold\",\n            pad=20,\n        )\n        ax.axis(\"off\")\n\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n        print(f\"\ud83d\udcbe Split comparison saved as '{output_path}'\")\n\n    def analyze_instances(\n        self, instance_mask_path, scores_path, output_path=\"instance_analysis.png\"\n    ):\n        \"\"\"Analyze and visualize instance segmentation results.\"\"\"\n\n        # Load instance mask and scores\n        with rasterio.open(instance_mask_path) as src:\n            instance_mask = src.read(1)\n\n        with rasterio.open(scores_path) as src:\n            scores_mask = src.read(1)\n\n        # Get unique instances (excluding background)\n        unique_instances = np.unique(instance_mask)\n        unique_instances = unique_instances[unique_instances &gt; 0]\n\n        # Calculate statistics for each instance\n        instance_stats = []\n        for instance_id in unique_instances:\n            mask = instance_mask == instance_id\n            area = np.sum(mask)\n            score = np.mean(scores_mask[mask])\n            instance_stats.append({\"id\": instance_id, \"area\": area, \"score\": score})\n\n        # Sort by score\n        instance_stats.sort(key=lambda x: x[\"score\"], reverse=True)\n\n        # Create visualization\n        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n        # 1. Instance segmentation visualization\n        colored_mask = np.zeros((*instance_mask.shape, 3), dtype=np.uint8)\n        colors = plt.cm.Set3(np.linspace(0, 1, len(unique_instances)))\n\n        for i, instance_id in enumerate(unique_instances):\n            mask = instance_mask == instance_id\n            colored_mask[mask] = (colors[i][:3] * 255).astype(np.uint8)\n\n        axes[0, 0].imshow(colored_mask)\n        axes[0, 0].set_title(\n            f\"Instance Segmentation\\n({len(unique_instances)} instances)\",\n            fontweight=\"bold\",\n        )\n        axes[0, 0].axis(\"off\")\n\n        # 2. Scores heatmap\n        im = axes[0, 1].imshow(scores_mask, cmap=\"viridis\", vmin=0, vmax=1)\n        axes[0, 1].set_title(\"Instance Confidence Scores\", fontweight=\"bold\")\n        axes[0, 1].axis(\"off\")\n        plt.colorbar(im, ax=axes[0, 1], shrink=0.8)\n\n        # 3. Score distribution\n        all_scores = [stat[\"score\"] for stat in instance_stats]\n        axes[1, 0].hist(\n            all_scores, bins=20, alpha=0.7, color=\"skyblue\", edgecolor=\"black\"\n        )\n        axes[1, 0].axvline(\n            x=np.mean(all_scores),\n            color=\"red\",\n            linestyle=\"--\",\n            label=f\"Mean: {np.mean(all_scores):.3f}\",\n        )\n        axes[1, 0].set_xlabel(\"Confidence Score\")\n        axes[1, 0].set_ylabel(\"Instance Count\")\n        axes[1, 0].set_title(\"Score Distribution\", fontweight=\"bold\")\n        axes[1, 0].legend()\n        axes[1, 0].grid(True, alpha=0.3)\n\n        # 4. Top instances by score\n        top_instances = instance_stats[:10]\n        instance_ids = [stat[\"id\"] for stat in top_instances]\n        scores = [stat[\"score\"] for stat in top_instances]\n        areas = [stat[\"area\"] for stat in top_instances]\n\n        bars = axes[1, 1].bar(\n            range(len(top_instances)), scores, color=\"coral\", alpha=0.7\n        )\n        axes[1, 1].set_xlabel(\"Top 10 Instances\")\n        axes[1, 1].set_ylabel(\"Confidence Score\")\n        axes[1, 1].set_title(\"Top Instances by Confidence\", fontweight=\"bold\")\n        axes[1, 1].set_xticks(range(len(top_instances)))\n        axes[1, 1].set_xticklabels([f\"#{id}\" for id in instance_ids], rotation=45)\n\n        # Add area info as text on bars\n        for i, (bar, area) in enumerate(zip(bars, areas)):\n            height = bar.get_height()\n            axes[1, 1].text(\n                bar.get_x() + bar.get_width() / 2.0,\n                height,\n                f\"{area}px\",\n                ha=\"center\",\n                va=\"bottom\",\n                fontsize=8,\n            )\n\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n        # Print summary statistics\n        print(f\"\\n\ud83d\udcca Instance Analysis Summary:\")\n        print(f\"   Total instances: {len(unique_instances)}\")\n        print(f\"   Average confidence: {np.mean(all_scores):.3f}\")\n        print(f\"   Score range: {np.min(all_scores):.3f} - {np.max(all_scores):.3f}\")\n        print(f\"   Total change area: {sum(areas):,} pixels\")\n\n        print(f\"\\n\ud83d\udcbe Instance analysis saved as '{output_path}'\")\n\n        return instance_stats\n\n    def create_comprehensive_report(\n        self, results_dict, output_path=\"comprehensive_report.png\"\n    ):\n        \"\"\"Create a comprehensive visualization report from detailed results.\"\"\"\n\n        if not results_dict or \"masks\" not in results_dict:\n            print(\"\u274c No detailed results provided\")\n            return\n\n        masks = results_dict[\"masks\"]\n        stats = results_dict[\"statistics\"]\n\n        # Create comprehensive visualization\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n        # 1. Score distributions\n        if \"iou_predictions\" in stats:\n            iou_scores = [\n                mask[\"iou_pred\"] for mask in masks if mask[\"iou_pred\"] is not None\n            ]\n            axes[0, 0].hist(\n                iou_scores, bins=20, alpha=0.7, color=\"lightblue\", edgecolor=\"black\"\n            )\n            axes[0, 0].axvline(\n                x=stats[\"iou_predictions\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['iou_predictions']['mean']:.3f}\",\n            )\n            axes[0, 0].set_xlabel(\"IoU Score\")\n            axes[0, 0].set_ylabel(\"Count\")\n            axes[0, 0].set_title(\"IoU Predictions Distribution\", fontweight=\"bold\")\n            axes[0, 0].legend()\n            axes[0, 0].grid(True, alpha=0.3)\n\n        # 2. Stability scores\n        if \"stability_scores\" in stats:\n            stability_scores = [\n                mask[\"stability_score\"]\n                for mask in masks\n                if mask[\"stability_score\"] is not None\n            ]\n            axes[0, 1].hist(\n                stability_scores,\n                bins=20,\n                alpha=0.7,\n                color=\"lightgreen\",\n                edgecolor=\"black\",\n            )\n            axes[0, 1].axvline(\n                x=stats[\"stability_scores\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['stability_scores']['mean']:.3f}\",\n            )\n            axes[0, 1].set_xlabel(\"Stability Score\")\n            axes[0, 1].set_ylabel(\"Count\")\n            axes[0, 1].set_title(\"Stability Scores Distribution\", fontweight=\"bold\")\n            axes[0, 1].legend()\n            axes[0, 1].grid(True, alpha=0.3)\n\n        # 3. Change confidence\n        if \"change_confidence\" in stats:\n            change_conf = [\n                mask[\"change_confidence\"]\n                for mask in masks\n                if mask[\"change_confidence\"] is not None\n            ]\n            axes[0, 2].hist(\n                change_conf, bins=20, alpha=0.7, color=\"lightyellow\", edgecolor=\"black\"\n            )\n            axes[0, 2].axvline(\n                x=stats[\"change_confidence\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['change_confidence']['mean']:.1f}\",\n            )\n            axes[0, 2].set_xlabel(\"Change Confidence\")\n            axes[0, 2].set_ylabel(\"Count\")\n            axes[0, 2].set_title(\"Change Confidence Distribution\", fontweight=\"bold\")\n            axes[0, 2].legend()\n            axes[0, 2].grid(True, alpha=0.3)\n\n        # 4. Area distribution\n        if \"areas\" in stats:\n            areas = [mask[\"area\"] for mask in masks if mask[\"area\"] is not None]\n            axes[1, 0].hist(\n                areas, bins=20, alpha=0.7, color=\"lightcoral\", edgecolor=\"black\"\n            )\n            axes[1, 0].axvline(\n                x=stats[\"areas\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['areas']['mean']:.1f}\",\n            )\n            axes[1, 0].set_xlabel(\"Area (pixels)\")\n            axes[1, 0].set_ylabel(\"Count\")\n            axes[1, 0].set_title(\"Area Distribution\", fontweight=\"bold\")\n            axes[1, 0].legend()\n            axes[1, 0].grid(True, alpha=0.3)\n\n        # 5. Combined confidence vs area scatter\n        combined_conf = [\n            mask[\"combined_confidence\"]\n            for mask in masks\n            if \"combined_confidence\" in mask\n        ]\n        areas_for_scatter = [\n            mask[\"area\"]\n            for mask in masks\n            if \"combined_confidence\" in mask and mask[\"area\"] is not None\n        ]\n\n        if combined_conf and areas_for_scatter:\n            scatter = axes[1, 1].scatter(\n                areas_for_scatter,\n                combined_conf,\n                alpha=0.6,\n                c=combined_conf,\n                cmap=\"viridis\",\n                s=50,\n            )\n            axes[1, 1].set_xlabel(\"Area (pixels)\")\n            axes[1, 1].set_ylabel(\"Combined Confidence\")\n            axes[1, 1].set_title(\"Confidence vs Area\", fontweight=\"bold\")\n            axes[1, 1].grid(True, alpha=0.3)\n            plt.colorbar(scatter, ax=axes[1, 1], shrink=0.8)\n\n        # 6. Summary statistics text\n        summary_text = f\"\"\"Detection Summary:\nTotal Instances: {len(masks)}\nProcessing Size: {results_dict['summary']['target_size']}\nOriginal Shape: {results_dict['summary']['original_shape']}\n\nQuality Metrics:\"\"\"\n\n        if \"iou_predictions\" in stats:\n            summary_text += f\"\"\"\nIoU Predictions:\n  Mean: {stats['iou_predictions']['mean']:.3f}\n  Range: {stats['iou_predictions']['min']:.3f} - {stats['iou_predictions']['max']:.3f}\"\"\"\n\n        if \"stability_scores\" in stats:\n            summary_text += f\"\"\"\nStability Scores:\n  Mean: {stats['stability_scores']['mean']:.3f}\n  Range: {stats['stability_scores']['min']:.3f} - {stats['stability_scores']['max']:.3f}\"\"\"\n\n        if \"change_confidence\" in stats:\n            summary_text += f\"\"\"\nChange Confidence:\n  Mean: {stats['change_confidence']['mean']:.1f}\n  Range: {stats['change_confidence']['min']:.1f} - {stats['change_confidence']['max']:.1f}\"\"\"\n\n        if \"areas\" in stats:\n            summary_text += f\"\"\"\nAreas:\n  Mean: {stats['areas']['mean']:.1f}\n  Total: {stats['areas']['total']:,.0f} pixels\"\"\"\n\n        axes[1, 2].text(\n            0.05,\n            0.95,\n            summary_text,\n            transform=axes[1, 2].transAxes,\n            fontsize=10,\n            verticalalignment=\"top\",\n            fontfamily=\"monospace\",\n        )\n        axes[1, 2].set_xlim(0, 1)\n        axes[1, 2].set_ylim(0, 1)\n        axes[1, 2].axis(\"off\")\n        axes[1, 2].set_title(\"Summary Statistics\", fontweight=\"bold\")\n\n        plt.tight_layout()\n        plt.suptitle(\n            \"Comprehensive Change Detection Report\",\n            fontsize=16,\n            fontweight=\"bold\",\n            y=0.98,\n        )\n        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n        print(f\"\ud83d\udcbe Comprehensive report saved as '{output_path}'\")\n\n    def run_complete_analysis(\n        self, image1_path, image2_path, output_dir=\"change_detection_results\"\n    ):\n        \"\"\"Run complete change detection analysis with all outputs and visualizations.\"\"\"\n\n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Define output paths\n        binary_path = os.path.join(output_dir, \"binary_mask.tif\")\n        prob_path = os.path.join(output_dir, \"probability_mask.tif\")\n        instance_path = os.path.join(output_dir, \"instance_masks.tif\")\n\n        print(\"\ud83d\udd0d Running complete change detection analysis...\")\n\n        # Run detection with all outputs\n        results = self.detect_changes(\n            image1_path,\n            image2_path,\n            output_path=binary_path,\n            export_probability=True,\n            probability_output_path=prob_path,\n            export_instance_masks=True,\n            instance_masks_output_path=instance_path,\n            return_detailed_results=True,\n            return_results=False,\n        )\n\n        print(\"\ud83d\udcca Creating visualizations...\")\n\n        # Create all visualizations\n        self.visualize_results(image1_path, image2_path, binary_path, prob_path)\n\n        self.create_split_comparison(\n            image1_path,\n            image2_path,\n            binary_path,\n            prob_path,\n            os.path.join(output_dir, \"split_comparison.png\"),\n        )\n\n        scores_path = instance_path.replace(\".tif\", \"_scores.tif\")\n        self.analyze_instances(\n            instance_path,\n            scores_path,\n            os.path.join(output_dir, \"instance_analysis.png\"),\n        )\n\n        self.create_comprehensive_report(\n            results, os.path.join(output_dir, \"comprehensive_report.png\")\n        )\n\n        print(f\"\u2705 Complete analysis finished! Results saved to: {output_dir}\")\n        return results\n\n    def _save_instance_segmentation_masks(\n        self, change_masks, output_path, transform, crs, original_shape, target_size\n    ):\n        \"\"\"\n        Save instance segmentation masks as a single GeoTIFF where each instance has a unique ID.\n\n        Args:\n            change_masks: Change detection masks (MaskData object)\n            output_path (str): Output path for instance segmentation GeoTIFF\n            transform: Rasterio transform\n            crs: Coordinate reference system\n            original_shape (tuple): Original image shape\n            target_size (int): Processing target size\n        \"\"\"\n        # Extract mask components\n        mask_items = dict(change_masks.items())\n        rles = mask_items.get(\"rles\", [])\n\n        # Create instance segmentation mask (each instance gets unique ID)\n        instance_mask = np.zeros((target_size, target_size), dtype=np.uint16)\n\n        # Process each mask and assign unique instance ID\n        for instance_id, rle in enumerate(rles, start=1):\n            if isinstance(rle, dict) and \"size\" in rle and \"counts\" in rle:\n                try:\n                    # Decode RLE to binary mask\n                    size = rle[\"size\"]\n                    counts = rle[\"counts\"]\n\n                    mask = np.zeros(size[0] * size[1], dtype=np.uint8)\n                    pos = 0\n                    value = 0\n\n                    for count in counts:\n                        if pos + count &lt;= len(mask):\n                            if value == 1:\n                                mask[pos : pos + count] = 1\n                            pos += count\n                            value = 1 - value\n                        else:\n                            break\n\n                    # RLE is column-major, reshape and transpose\n                    mask = mask.reshape(size).T\n                    if mask.shape != (target_size, target_size):\n                        continue\n\n                    # Assign instance ID to this mask\n                    instance_mask[mask.astype(bool)] = instance_id\n\n                except Exception as e:\n                    print(f\"Warning: Failed to process mask {instance_id}: {e}\")\n                    continue\n\n        # Resize back to original shape if needed\n        if original_shape != (target_size, target_size):\n            instance_mask_resized = resize(\n                instance_mask.astype(np.float32),\n                original_shape,\n                preserve_range=True,\n                anti_aliasing=False,\n                order=0,\n            )\n            instance_mask_final = np.round(instance_mask_resized).astype(np.uint16)\n        else:\n            instance_mask_final = instance_mask\n\n        # Save as GeoTIFF\n        with rasterio.open(\n            output_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=instance_mask_final.shape[0],\n            width=instance_mask_final.shape[1],\n            count=1,\n            dtype=instance_mask_final.dtype,\n            crs=crs,\n            transform=transform,\n            compress=\"lzw\",\n        ) as dst:\n            dst.write(instance_mask_final, 1)\n\n            # Add metadata\n            dst.update_tags(\n                description=\"Instance segmentation mask with unique IDs for each change object\",\n                total_instances=str(len(rles)),\n                background_value=\"0\",\n                instance_range=f\"1-{len(rles)}\",\n            )\n\n        print(\n            f\"Saved instance segmentation mask with {len(rles)} instances to {output_path}\"\n        )\n        return len(rles)\n\n    def _save_instance_scores_mask(\n        self, change_masks, output_path, transform, crs, original_shape, target_size\n    ):\n        \"\"\"\n        Save instance scores/probability mask as a GeoTIFF where each instance has its confidence score.\n\n        Args:\n            change_masks: Change detection masks (MaskData object)\n            output_path (str): Output path for instance scores GeoTIFF\n            transform: Rasterio transform\n            crs: Coordinate reference system\n            original_shape (tuple): Original image shape\n            target_size (int): Processing target size\n        \"\"\"\n        # Extract mask components\n        mask_items = dict(change_masks.items())\n        rles = mask_items.get(\"rles\", [])\n        iou_preds = mask_items.get(\"iou_preds\", None)\n        stability_scores = mask_items.get(\"stability_score\", None)\n        change_confidence = mask_items.get(\"change_confidence\", None)\n\n        # Convert tensors to numpy if needed\n        if iou_preds is not None:\n            iou_preds = iou_preds.detach().cpu().numpy()\n        if stability_scores is not None:\n            stability_scores = stability_scores.detach().cpu().numpy()\n        if change_confidence is not None:\n            change_confidence = change_confidence.detach().cpu().numpy()\n\n        # Create instance scores mask\n        scores_mask = np.zeros((target_size, target_size), dtype=np.float32)\n\n        # Process each mask and assign confidence score\n        for instance_id, rle in enumerate(rles):\n            if isinstance(rle, dict) and \"size\" in rle and \"counts\" in rle:\n                try:\n                    # Decode RLE to binary mask\n                    size = rle[\"size\"]\n                    counts = rle[\"counts\"]\n\n                    mask = np.zeros(size[0] * size[1], dtype=np.uint8)\n                    pos = 0\n                    value = 0\n\n                    for count in counts:\n                        if pos + count &lt;= len(mask):\n                            if value == 1:\n                                mask[pos : pos + count] = 1\n                            pos += count\n                            value = 1 - value\n                        else:\n                            break\n\n                    # RLE is column-major, reshape and transpose\n                    mask = mask.reshape(size).T\n                    if mask.shape != (target_size, target_size):\n                        continue\n\n                    # Calculate combined confidence score\n                    confidence_score = 0.5  # Default\n                    if iou_preds is not None and instance_id &lt; len(iou_preds):\n                        iou_score = float(iou_preds[instance_id])\n\n                        if stability_scores is not None and instance_id &lt; len(\n                            stability_scores\n                        ):\n                            stability_score = float(stability_scores[instance_id])\n\n                            if change_confidence is not None and instance_id &lt; len(\n                                change_confidence\n                            ):\n                                change_conf = float(change_confidence[instance_id])\n                                # Normalize change confidence (typically around 145 threshold)\n                                change_conf_norm = max(\n                                    0.0, min(1.0, abs(change_conf) / 200.0)\n                                )\n\n                                # Weighted combination of scores\n                                confidence_score = (\n                                    0.35 * iou_score\n                                    + 0.35 * stability_score\n                                    + 0.3 * change_conf_norm\n                                )\n                            else:\n                                confidence_score = 0.5 * (iou_score + stability_score)\n                        else:\n                            confidence_score = iou_score\n\n                    # Assign confidence score to this mask\n                    scores_mask[mask.astype(bool)] = confidence_score\n\n                except Exception as e:\n                    print(\n                        f\"Warning: Failed to process scores for mask {instance_id}: {e}\"\n                    )\n                    continue\n\n        # Resize back to original shape if needed\n        if original_shape != (target_size, target_size):\n            scores_mask_resized = resize(\n                scores_mask,\n                original_shape,\n                preserve_range=True,\n                anti_aliasing=True,\n                order=1,\n            )\n            scores_mask_final = np.clip(scores_mask_resized, 0.0, 1.0).astype(\n                np.float32\n            )\n        else:\n            scores_mask_final = scores_mask\n\n        # Save as GeoTIFF\n        with rasterio.open(\n            output_path,\n            \"w\",\n            driver=\"GTiff\",\n            height=scores_mask_final.shape[0],\n            width=scores_mask_final.shape[1],\n            count=1,\n            dtype=scores_mask_final.dtype,\n            crs=crs,\n            transform=transform,\n            compress=\"lzw\",\n        ) as dst:\n            dst.write(scores_mask_final, 1)\n\n            # Add metadata\n            dst.update_tags(\n                description=\"Instance scores mask with confidence values for each change object\",\n                total_instances=str(len(rles)),\n                background_value=\"0.0\",\n                score_range=\"0.0-1.0\",\n            )\n\n        print(f\"Saved instance scores mask with {len(rles)} instances to {output_path}\")\n        return len(rles)\n\n    def _extract_detailed_results(\n        self, change_masks, transform, crs, original_shape, target_size\n    ):\n        \"\"\"\n        Extract detailed results from change masks.\n\n        Args:\n            change_masks: Change detection masks (MaskData object)\n            transform: Rasterio transform\n            crs: Coordinate reference system\n            original_shape (tuple): Original image shape\n            target_size (int): Processing target size\n\n        Returns:\n            dict: Detailed results with mask information and statistics\n        \"\"\"\n        # Extract mask components\n        mask_items = dict(change_masks.items())\n        rles = mask_items.get(\"rles\", [])\n        iou_preds = mask_items.get(\"iou_preds\", None)\n        stability_scores = mask_items.get(\"stability_score\", None)\n        change_confidence = mask_items.get(\"change_confidence\", None)\n        areas = mask_items.get(\"areas\", None)\n        boxes = mask_items.get(\"boxes\", None)\n        points = mask_items.get(\"points\", None)\n\n        # Convert tensors to numpy if needed\n        if iou_preds is not None:\n            iou_preds = iou_preds.detach().cpu().numpy()\n        if stability_scores is not None:\n            stability_scores = stability_scores.detach().cpu().numpy()\n        if change_confidence is not None:\n            change_confidence = change_confidence.detach().cpu().numpy()\n        if areas is not None:\n            areas = areas.detach().cpu().numpy()\n        if boxes is not None:\n            boxes = boxes.detach().cpu().numpy()\n        if points is not None:\n            points = points.detach().cpu().numpy()\n\n        # Calculate statistics\n        results = {\n            \"summary\": {\n                \"total_masks\": len(rles),\n                \"target_size\": target_size,\n                \"original_shape\": original_shape,\n                \"crs\": str(crs),\n                \"transform\": transform.to_gdal(),\n            },\n            \"statistics\": {},\n            \"masks\": [],\n        }\n\n        # Calculate statistics for each metric\n        if iou_preds is not None and len(iou_preds) &gt; 0:\n            results[\"statistics\"][\"iou_predictions\"] = {\n                \"mean\": float(np.mean(iou_preds)),\n                \"std\": float(np.std(iou_preds)),\n                \"min\": float(np.min(iou_preds)),\n                \"max\": float(np.max(iou_preds)),\n                \"median\": float(np.median(iou_preds)),\n            }\n\n        if stability_scores is not None and len(stability_scores) &gt; 0:\n            results[\"statistics\"][\"stability_scores\"] = {\n                \"mean\": float(np.mean(stability_scores)),\n                \"std\": float(np.std(stability_scores)),\n                \"min\": float(np.min(stability_scores)),\n                \"max\": float(np.max(stability_scores)),\n                \"median\": float(np.median(stability_scores)),\n            }\n\n        if change_confidence is not None and len(change_confidence) &gt; 0:\n            results[\"statistics\"][\"change_confidence\"] = {\n                \"mean\": float(np.mean(change_confidence)),\n                \"std\": float(np.std(change_confidence)),\n                \"min\": float(np.min(change_confidence)),\n                \"max\": float(np.max(change_confidence)),\n                \"median\": float(np.median(change_confidence)),\n            }\n\n        if areas is not None and len(areas) &gt; 0:\n            results[\"statistics\"][\"areas\"] = {\n                \"mean\": float(np.mean(areas)),\n                \"std\": float(np.std(areas)),\n                \"min\": float(np.min(areas)),\n                \"max\": float(np.max(areas)),\n                \"median\": float(np.median(areas)),\n                \"total\": float(np.sum(areas)),\n            }\n\n        # Extract individual mask details\n        for i in range(len(rles)):\n            mask_info = {\n                \"mask_id\": i,\n                \"iou_pred\": (\n                    float(iou_preds[i])\n                    if iou_preds is not None and i &lt; len(iou_preds)\n                    else None\n                ),\n                \"stability_score\": (\n                    float(stability_scores[i])\n                    if stability_scores is not None and i &lt; len(stability_scores)\n                    else None\n                ),\n                \"change_confidence\": (\n                    float(change_confidence[i])\n                    if change_confidence is not None and i &lt; len(change_confidence)\n                    else None\n                ),\n                \"area\": int(areas[i]) if areas is not None and i &lt; len(areas) else None,\n                \"bbox\": (\n                    boxes[i].tolist() if boxes is not None and i &lt; len(boxes) else None\n                ),\n                \"center_point\": (\n                    points[i].tolist()\n                    if points is not None and i &lt; len(points)\n                    else None\n                ),\n            }\n\n            # Calculate combined confidence score\n            if all(\n                v is not None\n                for v in [\n                    mask_info[\"iou_pred\"],\n                    mask_info[\"stability_score\"],\n                    mask_info[\"change_confidence\"],\n                ]\n            ):\n                # Normalize change confidence (145 is typical threshold)\n                conf_norm = max(0.0, min(1.0, mask_info[\"change_confidence\"] / 145.0))\n                combined_score = (\n                    0.3 * mask_info[\"iou_pred\"]\n                    + 0.3 * mask_info[\"stability_score\"]\n                    + 0.4 * conf_norm\n                )\n                mask_info[\"combined_confidence\"] = float(combined_score)\n\n            results[\"masks\"].append(mask_info)\n\n        # Sort masks by combined confidence if available\n        if results[\"masks\"] and \"combined_confidence\" in results[\"masks\"][0]:\n            results[\"masks\"].sort(key=lambda x: x[\"combined_confidence\"], reverse=True)\n\n        return results\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.__init__","title":"<code>__init__(sam_model_type='vit_h', sam_checkpoint=None)</code>","text":"<p>Initialize the ChangeDetection class.</p> <p>Parameters:</p> Name Type Description Default <code>sam_model_type</code> <code>str</code> <p>SAM model type ('vit_h', 'vit_l', 'vit_b')</p> <code>'vit_h'</code> <code>sam_checkpoint</code> <code>str</code> <p>Path to SAM checkpoint file</p> <code>None</code> Source code in <code>geoai/change_detection.py</code> <pre><code>def __init__(self, sam_model_type=\"vit_h\", sam_checkpoint=None):\n    \"\"\"\n    Initialize the ChangeDetection class.\n\n    Args:\n        sam_model_type (str): SAM model type ('vit_h', 'vit_l', 'vit_b')\n        sam_checkpoint (str): Path to SAM checkpoint file\n    \"\"\"\n    self.sam_model_type = sam_model_type\n    self.sam_checkpoint = sam_checkpoint\n    self.model = None\n    self._init_model()\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.analyze_instances","title":"<code>analyze_instances(instance_mask_path, scores_path, output_path='instance_analysis.png')</code>","text":"<p>Analyze and visualize instance segmentation results.</p> Source code in <code>geoai/change_detection.py</code> <pre><code>def analyze_instances(\n    self, instance_mask_path, scores_path, output_path=\"instance_analysis.png\"\n):\n    \"\"\"Analyze and visualize instance segmentation results.\"\"\"\n\n    # Load instance mask and scores\n    with rasterio.open(instance_mask_path) as src:\n        instance_mask = src.read(1)\n\n    with rasterio.open(scores_path) as src:\n        scores_mask = src.read(1)\n\n    # Get unique instances (excluding background)\n    unique_instances = np.unique(instance_mask)\n    unique_instances = unique_instances[unique_instances &gt; 0]\n\n    # Calculate statistics for each instance\n    instance_stats = []\n    for instance_id in unique_instances:\n        mask = instance_mask == instance_id\n        area = np.sum(mask)\n        score = np.mean(scores_mask[mask])\n        instance_stats.append({\"id\": instance_id, \"area\": area, \"score\": score})\n\n    # Sort by score\n    instance_stats.sort(key=lambda x: x[\"score\"], reverse=True)\n\n    # Create visualization\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\n    # 1. Instance segmentation visualization\n    colored_mask = np.zeros((*instance_mask.shape, 3), dtype=np.uint8)\n    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_instances)))\n\n    for i, instance_id in enumerate(unique_instances):\n        mask = instance_mask == instance_id\n        colored_mask[mask] = (colors[i][:3] * 255).astype(np.uint8)\n\n    axes[0, 0].imshow(colored_mask)\n    axes[0, 0].set_title(\n        f\"Instance Segmentation\\n({len(unique_instances)} instances)\",\n        fontweight=\"bold\",\n    )\n    axes[0, 0].axis(\"off\")\n\n    # 2. Scores heatmap\n    im = axes[0, 1].imshow(scores_mask, cmap=\"viridis\", vmin=0, vmax=1)\n    axes[0, 1].set_title(\"Instance Confidence Scores\", fontweight=\"bold\")\n    axes[0, 1].axis(\"off\")\n    plt.colorbar(im, ax=axes[0, 1], shrink=0.8)\n\n    # 3. Score distribution\n    all_scores = [stat[\"score\"] for stat in instance_stats]\n    axes[1, 0].hist(\n        all_scores, bins=20, alpha=0.7, color=\"skyblue\", edgecolor=\"black\"\n    )\n    axes[1, 0].axvline(\n        x=np.mean(all_scores),\n        color=\"red\",\n        linestyle=\"--\",\n        label=f\"Mean: {np.mean(all_scores):.3f}\",\n    )\n    axes[1, 0].set_xlabel(\"Confidence Score\")\n    axes[1, 0].set_ylabel(\"Instance Count\")\n    axes[1, 0].set_title(\"Score Distribution\", fontweight=\"bold\")\n    axes[1, 0].legend()\n    axes[1, 0].grid(True, alpha=0.3)\n\n    # 4. Top instances by score\n    top_instances = instance_stats[:10]\n    instance_ids = [stat[\"id\"] for stat in top_instances]\n    scores = [stat[\"score\"] for stat in top_instances]\n    areas = [stat[\"area\"] for stat in top_instances]\n\n    bars = axes[1, 1].bar(\n        range(len(top_instances)), scores, color=\"coral\", alpha=0.7\n    )\n    axes[1, 1].set_xlabel(\"Top 10 Instances\")\n    axes[1, 1].set_ylabel(\"Confidence Score\")\n    axes[1, 1].set_title(\"Top Instances by Confidence\", fontweight=\"bold\")\n    axes[1, 1].set_xticks(range(len(top_instances)))\n    axes[1, 1].set_xticklabels([f\"#{id}\" for id in instance_ids], rotation=45)\n\n    # Add area info as text on bars\n    for i, (bar, area) in enumerate(zip(bars, areas)):\n        height = bar.get_height()\n        axes[1, 1].text(\n            bar.get_x() + bar.get_width() / 2.0,\n            height,\n            f\"{area}px\",\n            ha=\"center\",\n            va=\"bottom\",\n            fontsize=8,\n        )\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n\n    # Print summary statistics\n    print(f\"\\n\ud83d\udcca Instance Analysis Summary:\")\n    print(f\"   Total instances: {len(unique_instances)}\")\n    print(f\"   Average confidence: {np.mean(all_scores):.3f}\")\n    print(f\"   Score range: {np.min(all_scores):.3f} - {np.max(all_scores):.3f}\")\n    print(f\"   Total change area: {sum(areas):,} pixels\")\n\n    print(f\"\\n\ud83d\udcbe Instance analysis saved as '{output_path}'\")\n\n    return instance_stats\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.create_comprehensive_report","title":"<code>create_comprehensive_report(results_dict, output_path='comprehensive_report.png')</code>","text":"<p>Create a comprehensive visualization report from detailed results.</p> Source code in <code>geoai/change_detection.py</code> <pre><code>    def create_comprehensive_report(\n        self, results_dict, output_path=\"comprehensive_report.png\"\n    ):\n        \"\"\"Create a comprehensive visualization report from detailed results.\"\"\"\n\n        if not results_dict or \"masks\" not in results_dict:\n            print(\"\u274c No detailed results provided\")\n            return\n\n        masks = results_dict[\"masks\"]\n        stats = results_dict[\"statistics\"]\n\n        # Create comprehensive visualization\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n        # 1. Score distributions\n        if \"iou_predictions\" in stats:\n            iou_scores = [\n                mask[\"iou_pred\"] for mask in masks if mask[\"iou_pred\"] is not None\n            ]\n            axes[0, 0].hist(\n                iou_scores, bins=20, alpha=0.7, color=\"lightblue\", edgecolor=\"black\"\n            )\n            axes[0, 0].axvline(\n                x=stats[\"iou_predictions\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['iou_predictions']['mean']:.3f}\",\n            )\n            axes[0, 0].set_xlabel(\"IoU Score\")\n            axes[0, 0].set_ylabel(\"Count\")\n            axes[0, 0].set_title(\"IoU Predictions Distribution\", fontweight=\"bold\")\n            axes[0, 0].legend()\n            axes[0, 0].grid(True, alpha=0.3)\n\n        # 2. Stability scores\n        if \"stability_scores\" in stats:\n            stability_scores = [\n                mask[\"stability_score\"]\n                for mask in masks\n                if mask[\"stability_score\"] is not None\n            ]\n            axes[0, 1].hist(\n                stability_scores,\n                bins=20,\n                alpha=0.7,\n                color=\"lightgreen\",\n                edgecolor=\"black\",\n            )\n            axes[0, 1].axvline(\n                x=stats[\"stability_scores\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['stability_scores']['mean']:.3f}\",\n            )\n            axes[0, 1].set_xlabel(\"Stability Score\")\n            axes[0, 1].set_ylabel(\"Count\")\n            axes[0, 1].set_title(\"Stability Scores Distribution\", fontweight=\"bold\")\n            axes[0, 1].legend()\n            axes[0, 1].grid(True, alpha=0.3)\n\n        # 3. Change confidence\n        if \"change_confidence\" in stats:\n            change_conf = [\n                mask[\"change_confidence\"]\n                for mask in masks\n                if mask[\"change_confidence\"] is not None\n            ]\n            axes[0, 2].hist(\n                change_conf, bins=20, alpha=0.7, color=\"lightyellow\", edgecolor=\"black\"\n            )\n            axes[0, 2].axvline(\n                x=stats[\"change_confidence\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['change_confidence']['mean']:.1f}\",\n            )\n            axes[0, 2].set_xlabel(\"Change Confidence\")\n            axes[0, 2].set_ylabel(\"Count\")\n            axes[0, 2].set_title(\"Change Confidence Distribution\", fontweight=\"bold\")\n            axes[0, 2].legend()\n            axes[0, 2].grid(True, alpha=0.3)\n\n        # 4. Area distribution\n        if \"areas\" in stats:\n            areas = [mask[\"area\"] for mask in masks if mask[\"area\"] is not None]\n            axes[1, 0].hist(\n                areas, bins=20, alpha=0.7, color=\"lightcoral\", edgecolor=\"black\"\n            )\n            axes[1, 0].axvline(\n                x=stats[\"areas\"][\"mean\"],\n                color=\"red\",\n                linestyle=\"--\",\n                label=f\"Mean: {stats['areas']['mean']:.1f}\",\n            )\n            axes[1, 0].set_xlabel(\"Area (pixels)\")\n            axes[1, 0].set_ylabel(\"Count\")\n            axes[1, 0].set_title(\"Area Distribution\", fontweight=\"bold\")\n            axes[1, 0].legend()\n            axes[1, 0].grid(True, alpha=0.3)\n\n        # 5. Combined confidence vs area scatter\n        combined_conf = [\n            mask[\"combined_confidence\"]\n            for mask in masks\n            if \"combined_confidence\" in mask\n        ]\n        areas_for_scatter = [\n            mask[\"area\"]\n            for mask in masks\n            if \"combined_confidence\" in mask and mask[\"area\"] is not None\n        ]\n\n        if combined_conf and areas_for_scatter:\n            scatter = axes[1, 1].scatter(\n                areas_for_scatter,\n                combined_conf,\n                alpha=0.6,\n                c=combined_conf,\n                cmap=\"viridis\",\n                s=50,\n            )\n            axes[1, 1].set_xlabel(\"Area (pixels)\")\n            axes[1, 1].set_ylabel(\"Combined Confidence\")\n            axes[1, 1].set_title(\"Confidence vs Area\", fontweight=\"bold\")\n            axes[1, 1].grid(True, alpha=0.3)\n            plt.colorbar(scatter, ax=axes[1, 1], shrink=0.8)\n\n        # 6. Summary statistics text\n        summary_text = f\"\"\"Detection Summary:\nTotal Instances: {len(masks)}\nProcessing Size: {results_dict['summary']['target_size']}\nOriginal Shape: {results_dict['summary']['original_shape']}\n\nQuality Metrics:\"\"\"\n\n        if \"iou_predictions\" in stats:\n            summary_text += f\"\"\"\nIoU Predictions:\n  Mean: {stats['iou_predictions']['mean']:.3f}\n  Range: {stats['iou_predictions']['min']:.3f} - {stats['iou_predictions']['max']:.3f}\"\"\"\n\n        if \"stability_scores\" in stats:\n            summary_text += f\"\"\"\nStability Scores:\n  Mean: {stats['stability_scores']['mean']:.3f}\n  Range: {stats['stability_scores']['min']:.3f} - {stats['stability_scores']['max']:.3f}\"\"\"\n\n        if \"change_confidence\" in stats:\n            summary_text += f\"\"\"\nChange Confidence:\n  Mean: {stats['change_confidence']['mean']:.1f}\n  Range: {stats['change_confidence']['min']:.1f} - {stats['change_confidence']['max']:.1f}\"\"\"\n\n        if \"areas\" in stats:\n            summary_text += f\"\"\"\nAreas:\n  Mean: {stats['areas']['mean']:.1f}\n  Total: {stats['areas']['total']:,.0f} pixels\"\"\"\n\n        axes[1, 2].text(\n            0.05,\n            0.95,\n            summary_text,\n            transform=axes[1, 2].transAxes,\n            fontsize=10,\n            verticalalignment=\"top\",\n            fontfamily=\"monospace\",\n        )\n        axes[1, 2].set_xlim(0, 1)\n        axes[1, 2].set_ylim(0, 1)\n        axes[1, 2].axis(\"off\")\n        axes[1, 2].set_title(\"Summary Statistics\", fontweight=\"bold\")\n\n        plt.tight_layout()\n        plt.suptitle(\n            \"Comprehensive Change Detection Report\",\n            fontsize=16,\n            fontweight=\"bold\",\n            y=0.98,\n        )\n        plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n        plt.show()\n\n        print(f\"\ud83d\udcbe Comprehensive report saved as '{output_path}'\")\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.create_split_comparison","title":"<code>create_split_comparison(image1_path, image2_path, binary_path, prob_path, title1='Earlier Image', title2='Later Image', output_path='split_comparison.png')</code>","text":"<p>Create a split comparison visualization showing before/after with change overlay.</p> Source code in <code>geoai/change_detection.py</code> <pre><code>def create_split_comparison(\n    self,\n    image1_path,\n    image2_path,\n    binary_path,\n    prob_path,\n    title1=\"Earlier Image\",\n    title2=\"Later Image\",\n    output_path=\"split_comparison.png\",\n):\n    \"\"\"Create a split comparison visualization showing before/after with change overlay.\"\"\"\n\n    # Load data\n    with rasterio.open(image1_path) as src:\n        img1 = src.read([1, 2, 3])\n        img1 = np.transpose(img1, (1, 2, 0))\n        if img1.dtype != np.uint8:\n            img1 = ((img1 - img1.min()) / (img1.max() - img1.min()) * 255).astype(\n                np.uint8\n            )\n\n    with rasterio.open(image2_path) as src:\n        img2 = src.read([1, 2, 3])\n        img2 = np.transpose(img2, (1, 2, 0))\n        if img2.dtype != np.uint8:\n            img2 = ((img2 - img2.min()) / (img2.max() - img2.min()) * 255).astype(\n                np.uint8\n            )\n\n    with rasterio.open(prob_path) as src:\n        prob_mask = src.read(1)\n\n    # Ensure all arrays have the same shape\n    h, w = img1.shape[:2]\n    if prob_mask.shape != (h, w):\n        prob_mask = resize(\n            prob_mask, (h, w), preserve_range=True, anti_aliasing=True, order=1\n        )\n\n    # Create split comparison\n    fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n\n    # Create combined image - left half is earlier, right half is later\n    combined_img = np.zeros_like(img1)\n    combined_img[:, : w // 2] = img1[:, : w // 2]\n    combined_img[:, w // 2 :] = img2[:, w // 2 :]\n\n    # Create overlay with changes - ensure prob_mask is 2D and matches image dimensions\n    overlay = combined_img.copy()\n    high_conf_changes = prob_mask &gt; 0.5\n\n    # Apply overlay only where changes are detected\n    if len(overlay.shape) == 3:  # RGB image\n        overlay[high_conf_changes] = [255, 0, 0]  # Red for high confidence changes\n\n    # Blend overlay with original\n    blended = cv2.addWeighted(combined_img, 0.7, overlay, 0.3, 0)\n\n    ax.imshow(blended)\n    ax.axvline(x=w // 2, color=\"white\", linewidth=3, linestyle=\"--\", alpha=0.8)\n    ax.text(\n        w // 4,\n        50,\n        title1,\n        fontsize=20,\n        color=\"white\",\n        ha=\"center\",\n        bbox={\"boxstyle\": \"round,pad=0.3\", \"facecolor\": \"black\", \"alpha\": 0.8},\n    )\n    ax.text(\n        3 * w // 4,\n        50,\n        title2,\n        fontsize=20,\n        color=\"white\",\n        ha=\"center\",\n        bbox={\"boxstyle\": \"round,pad=0.3\", \"facecolor\": \"black\", \"alpha\": 0.8},\n    )\n\n    ax.set_title(\n        \"Split Comparison with Change Detection\\n(Red = High Confidence Changes)\",\n        fontsize=16,\n        fontweight=\"bold\",\n        pad=20,\n    )\n    ax.axis(\"off\")\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches=\"tight\")\n    plt.show()\n\n    print(f\"\ud83d\udcbe Split comparison saved as '{output_path}'\")\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.detect_changes","title":"<code>detect_changes(image1_path, image2_path, output_path=None, target_size=1024, return_results=True, export_probability=False, probability_output_path=None, export_instance_masks=False, instance_masks_output_path=None, return_detailed_results=False)</code>","text":"<p>Detect changes between two GeoTIFF images with instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image1_path</code> <code>str</code> <p>Path to first image</p> required <code>image2_path</code> <code>str</code> <p>Path to second image</p> required <code>output_path</code> <code>str</code> <p>Optional path to save binary change mask as GeoTIFF</p> <code>None</code> <code>target_size</code> <code>int</code> <p>Target size for processing</p> <code>1024</code> <code>return_results</code> <code>bool</code> <p>Whether to return results</p> <code>True</code> <code>export_probability</code> <code>bool</code> <p>Whether to export probability mask</p> <code>False</code> <code>probability_output_path</code> <code>str</code> <p>Path to save probability mask (required if export_probability=True)</p> <code>None</code> <code>export_instance_masks</code> <code>bool</code> <p>Whether to export instance segmentation masks</p> <code>False</code> <code>instance_masks_output_path</code> <code>str</code> <p>Path to save instance masks (required if export_instance_masks=True)</p> <code>None</code> <code>return_detailed_results</code> <code>bool</code> <p>Whether to return detailed mask information</p> <code>False</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Union[Tuple[Any, ndarray, ndarray], Dict[str, Any], None]</code> <p>(change_masks, img1, img2) if return_results=True</p> <code>dict</code> <code>Union[Tuple[Any, ndarray, ndarray], Dict[str, Any], None]</code> <p>Detailed results if return_detailed_results=True</p> Source code in <code>geoai/change_detection.py</code> <pre><code>def detect_changes(\n    self,\n    image1_path: str,\n    image2_path: str,\n    output_path: Optional[str] = None,\n    target_size: int = 1024,\n    return_results: bool = True,\n    export_probability: bool = False,\n    probability_output_path: Optional[str] = None,\n    export_instance_masks: bool = False,\n    instance_masks_output_path: Optional[str] = None,\n    return_detailed_results: bool = False,\n) -&gt; Union[Tuple[Any, np.ndarray, np.ndarray], Dict[str, Any], None]:\n    \"\"\"\n    Detect changes between two GeoTIFF images with instance segmentation.\n\n    Args:\n        image1_path (str): Path to first image\n        image2_path (str): Path to second image\n        output_path (str): Optional path to save binary change mask as GeoTIFF\n        target_size (int): Target size for processing\n        return_results (bool): Whether to return results\n        export_probability (bool): Whether to export probability mask\n        probability_output_path (str): Path to save probability mask (required if export_probability=True)\n        export_instance_masks (bool): Whether to export instance segmentation masks\n        instance_masks_output_path (str): Path to save instance masks (required if export_instance_masks=True)\n        return_detailed_results (bool): Whether to return detailed mask information\n\n    Returns:\n        tuple: (change_masks, img1, img2) if return_results=True\n        dict: Detailed results if return_detailed_results=True\n    \"\"\"\n    # Read and align images\n    (img1, img2, transform, crs, original_shape) = self._read_and_align_images(\n        image1_path, image2_path, target_size\n    )\n\n    # Detect changes\n    change_masks, _, _ = self.model.forward(img1, img2)\n\n    # If output path specified, save binary mask as GeoTIFF\n    if output_path:\n        self._save_change_mask(\n            change_masks, output_path, transform, crs, original_shape, target_size\n        )\n\n    # If probability export requested, save probability mask\n    if export_probability:\n        if probability_output_path is None:\n            raise ValueError(\n                \"probability_output_path must be specified when export_probability=True\"\n            )\n        self._save_probability_mask(\n            change_masks,\n            probability_output_path,\n            transform,\n            crs,\n            original_shape,\n            target_size,\n        )\n\n    # If instance masks export requested, save instance segmentation masks\n    if export_instance_masks:\n        if instance_masks_output_path is None:\n            raise ValueError(\n                \"instance_masks_output_path must be specified when export_instance_masks=True\"\n            )\n        num_instances = self._save_instance_segmentation_masks(\n            change_masks,\n            instance_masks_output_path,\n            transform,\n            crs,\n            original_shape,\n            target_size,\n        )\n\n        # Also save instance scores if requested\n        scores_path = instance_masks_output_path.replace(\".tif\", \"_scores.tif\")\n        self._save_instance_scores_mask(\n            change_masks,\n            scores_path,\n            transform,\n            crs,\n            original_shape,\n            target_size,\n        )\n\n    # Return detailed results if requested\n    if return_detailed_results:\n        return self._extract_detailed_results(\n            change_masks, transform, crs, original_shape, target_size\n        )\n\n    if return_results:\n        return change_masks, img1, img2\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.run_complete_analysis","title":"<code>run_complete_analysis(image1_path, image2_path, output_dir='change_detection_results')</code>","text":"<p>Run complete change detection analysis with all outputs and visualizations.</p> Source code in <code>geoai/change_detection.py</code> <pre><code>def run_complete_analysis(\n    self, image1_path, image2_path, output_dir=\"change_detection_results\"\n):\n    \"\"\"Run complete change detection analysis with all outputs and visualizations.\"\"\"\n\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define output paths\n    binary_path = os.path.join(output_dir, \"binary_mask.tif\")\n    prob_path = os.path.join(output_dir, \"probability_mask.tif\")\n    instance_path = os.path.join(output_dir, \"instance_masks.tif\")\n\n    print(\"\ud83d\udd0d Running complete change detection analysis...\")\n\n    # Run detection with all outputs\n    results = self.detect_changes(\n        image1_path,\n        image2_path,\n        output_path=binary_path,\n        export_probability=True,\n        probability_output_path=prob_path,\n        export_instance_masks=True,\n        instance_masks_output_path=instance_path,\n        return_detailed_results=True,\n        return_results=False,\n    )\n\n    print(\"\ud83d\udcca Creating visualizations...\")\n\n    # Create all visualizations\n    self.visualize_results(image1_path, image2_path, binary_path, prob_path)\n\n    self.create_split_comparison(\n        image1_path,\n        image2_path,\n        binary_path,\n        prob_path,\n        os.path.join(output_dir, \"split_comparison.png\"),\n    )\n\n    scores_path = instance_path.replace(\".tif\", \"_scores.tif\")\n    self.analyze_instances(\n        instance_path,\n        scores_path,\n        os.path.join(output_dir, \"instance_analysis.png\"),\n    )\n\n    self.create_comprehensive_report(\n        results, os.path.join(output_dir, \"comprehensive_report.png\")\n    )\n\n    print(f\"\u2705 Complete analysis finished! Results saved to: {output_dir}\")\n    return results\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.set_hyperparameters","title":"<code>set_hyperparameters(change_confidence_threshold=155, auto_threshold=False, use_normalized_feature=True, area_thresh=0.8, match_hist=False, object_sim_thresh=60, bitemporal_match=True, **kwargs)</code>","text":"<p>Set hyperparameters for the change detection model.</p> <p>Parameters:</p> Name Type Description Default <code>change_confidence_threshold</code> <code>int</code> <p>Change confidence threshold for SAM</p> <code>155</code> <code>auto_threshold</code> <code>bool</code> <p>Whether to use auto threshold for SAM</p> <code>False</code> <code>use_normalized_feature</code> <code>bool</code> <p>Whether to use normalized feature for SAM</p> <code>True</code> <code>area_thresh</code> <code>float</code> <p>Area threshold for SAM</p> <code>0.8</code> <code>match_hist</code> <code>bool</code> <p>Whether to use match hist for SAM</p> <code>False</code> <code>object_sim_thresh</code> <code>int</code> <p>Object similarity threshold for SAM</p> <code>60</code> <code>bitemporal_match</code> <code>bool</code> <p>Whether to use bitemporal match for SAM</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for model hyperparameters</p> <code>{}</code> Source code in <code>geoai/change_detection.py</code> <pre><code>def set_hyperparameters(\n    self,\n    change_confidence_threshold: int = 155,\n    auto_threshold: bool = False,\n    use_normalized_feature: bool = True,\n    area_thresh: float = 0.8,\n    match_hist: bool = False,\n    object_sim_thresh: int = 60,\n    bitemporal_match: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Set hyperparameters for the change detection model.\n\n    Args:\n        change_confidence_threshold (int): Change confidence threshold for SAM\n        auto_threshold (bool): Whether to use auto threshold for SAM\n        use_normalized_feature (bool): Whether to use normalized feature for SAM\n        area_thresh (float): Area threshold for SAM\n        match_hist (bool): Whether to use match hist for SAM\n        object_sim_thresh (int): Object similarity threshold for SAM\n        bitemporal_match (bool): Whether to use bitemporal match for SAM\n        **kwargs: Keyword arguments for model hyperparameters\n    \"\"\"\n    if self.model:\n        self.model.set_hyperparameters(\n            change_confidence_threshold=change_confidence_threshold,\n            auto_threshold=auto_threshold,\n            use_normalized_feature=use_normalized_feature,\n            area_thresh=area_thresh,\n            match_hist=match_hist,\n            object_sim_thresh=object_sim_thresh,\n            bitemporal_match=bitemporal_match,\n            **kwargs,\n        )\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.set_mask_generator_params","title":"<code>set_mask_generator_params(points_per_side=32, points_per_batch=64, pred_iou_thresh=0.5, stability_score_thresh=0.95, stability_score_offset=1.0, box_nms_thresh=0.7, point_grids=None, min_mask_region_area=0, **kwargs)</code>","text":"<p>Set mask generator parameters.</p> <p>Parameters:</p> Name Type Description Default <code>points_per_side</code> <code>int</code> <p>Number of points per side for SAM</p> <code>32</code> <code>points_per_batch</code> <code>int</code> <p>Number of points per batch for SAM</p> <code>64</code> <code>pred_iou_thresh</code> <code>float</code> <p>IoU threshold for SAM</p> <code>0.5</code> <code>stability_score_thresh</code> <code>float</code> <p>Stability score threshold for SAM</p> <code>0.95</code> <code>stability_score_offset</code> <code>float</code> <p>Stability score offset for SAM</p> <code>1.0</code> <code>box_nms_thresh</code> <code>float</code> <p>NMS threshold for SAM</p> <code>0.7</code> <code>point_grids</code> <code>list</code> <p>Point grids for SAM</p> <code>None</code> <code>min_mask_region_area</code> <code>int</code> <p>Minimum mask region area for SAM</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for mask generator</p> <code>{}</code> Source code in <code>geoai/change_detection.py</code> <pre><code>def set_mask_generator_params(\n    self,\n    points_per_side: int = 32,\n    points_per_batch: int = 64,\n    pred_iou_thresh: float = 0.5,\n    stability_score_thresh: float = 0.95,\n    stability_score_offset: float = 1.0,\n    box_nms_thresh: float = 0.7,\n    point_grids: Optional[List] = None,\n    min_mask_region_area: int = 0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Set mask generator parameters.\n\n    Args:\n        points_per_side (int): Number of points per side for SAM\n        points_per_batch (int): Number of points per batch for SAM\n        pred_iou_thresh (float): IoU threshold for SAM\n        stability_score_thresh (float): Stability score threshold for SAM\n        stability_score_offset (float): Stability score offset for SAM\n        box_nms_thresh (float): NMS threshold for SAM\n        point_grids (list): Point grids for SAM\n        min_mask_region_area (int): Minimum mask region area for SAM\n        **kwargs: Keyword arguments for mask generator\n    \"\"\"\n    if self.model:\n        self.model.make_mask_generator(\n            points_per_side=points_per_side,\n            points_per_batch=points_per_batch,\n            pred_iou_thresh=pred_iou_thresh,\n            stability_score_thresh=stability_score_thresh,\n            stability_score_offset=stability_score_offset,\n            box_nms_thresh=box_nms_thresh,\n            point_grids=point_grids,\n            min_mask_region_area=min_mask_region_area,\n            **kwargs,\n        )\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.visualize_changes","title":"<code>visualize_changes(image1_path, image2_path, figsize=(15, 5))</code>","text":"<p>Visualize change detection results.</p> <p>Parameters:</p> Name Type Description Default <code>image1_path</code> <code>str</code> <p>Path to first image</p> required <code>image2_path</code> <code>str</code> <p>Path to second image</p> required <code>figsize</code> <code>tuple</code> <p>Figure size</p> <code>(15, 5)</code> <p>Returns:</p> Type Description <code>Figure</code> <p>matplotlib.figure.Figure: The figure object</p> Source code in <code>geoai/change_detection.py</code> <pre><code>def visualize_changes(\n    self, image1_path: str, image2_path: str, figsize: Tuple[int, int] = (15, 5)\n) -&gt; plt.Figure:\n    \"\"\"\n    Visualize change detection results.\n\n    Args:\n        image1_path (str): Path to first image\n        image2_path (str): Path to second image\n        figsize (tuple): Figure size\n\n    Returns:\n        matplotlib.figure.Figure: The figure object\n    \"\"\"\n    if show_change_masks is None:\n        raise ImportError(\n            \"The 'torchange' package is required for change detection visualization. \"\n            \"Please install it using: pip install torchange\\n\"\n            \"Note: torchange requires Python 3.11 or higher.\"\n        )\n\n    change_masks, img1, img2 = self.detect_changes(\n        image1_path, image2_path, return_results=True\n    )\n\n    # Use torchange's visualization function\n    fig, _ = show_change_masks(img1, img2, change_masks)\n    fig.set_size_inches(figsize)\n\n    return fig\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.ChangeDetection.visualize_results","title":"<code>visualize_results(image1_path, image2_path, binary_path, prob_path, title1='Earlier Image', title2='Later Image')</code>","text":"<p>Create enhanced visualization with probability analysis.</p> Source code in <code>geoai/change_detection.py</code> <pre><code>def visualize_results(\n    self,\n    image1_path,\n    image2_path,\n    binary_path,\n    prob_path,\n    title1=\"Earlier Image\",\n    title2=\"Later Image\",\n):\n    \"\"\"Create enhanced visualization with probability analysis.\"\"\"\n\n    # Load data\n    with rasterio.open(image1_path) as src:\n        img1 = src.read([1, 2, 3])\n        img1 = np.transpose(img1, (1, 2, 0))\n\n    with rasterio.open(image2_path) as src:\n        img2 = src.read([1, 2, 3])\n        img2 = np.transpose(img2, (1, 2, 0))\n\n    with rasterio.open(binary_path) as src:\n        binary_mask = src.read(1)\n\n    with rasterio.open(prob_path) as src:\n        prob_mask = src.read(1)\n\n    # Create comprehensive visualization\n    fig, axes = plt.subplots(2, 4, figsize=(24, 12))\n\n    # Crop for better visualization\n    h, w = img1.shape[:2]\n    y1, y2 = h // 4, 3 * h // 4\n    x1, x2 = w // 4, 3 * w // 4\n\n    img1_crop = img1[y1:y2, x1:x2]\n    img2_crop = img2[y1:y2, x1:x2]\n    binary_crop = binary_mask[y1:y2, x1:x2]\n    prob_crop = prob_mask[y1:y2, x1:x2]\n\n    # Row 1: Original and overlays\n    axes[0, 0].imshow(img1_crop)\n    axes[0, 0].set_title(title1, fontweight=\"bold\")\n    axes[0, 0].axis(\"off\")\n\n    axes[0, 1].imshow(img2_crop)\n    axes[0, 1].set_title(title2, fontweight=\"bold\")\n    axes[0, 1].axis(\"off\")\n\n    # Binary overlay\n    overlay_binary = img2_crop.copy()\n    overlay_binary[binary_crop &gt; 0] = [255, 0, 0]\n    axes[0, 2].imshow(overlay_binary)\n    axes[0, 2].set_title(\"Binary Changes\\n(Red = Change)\", fontweight=\"bold\")\n    axes[0, 2].axis(\"off\")\n\n    # Probability heatmap\n    im1 = axes[0, 3].imshow(prob_crop, cmap=\"hot\", vmin=0, vmax=1)\n    axes[0, 3].set_title(\n        \"Probability Heatmap\\n(White = High Confidence)\", fontweight=\"bold\"\n    )\n    axes[0, 3].axis(\"off\")\n    plt.colorbar(im1, ax=axes[0, 3], shrink=0.8)\n\n    # Row 2: Detailed probability analysis\n    # Confidence levels overlay\n    overlay_conf = img2_crop.copy()\n    high_conf = prob_crop &gt; 0.7\n    med_conf = (prob_crop &gt; 0.4) &amp; (prob_crop &lt;= 0.7)\n    low_conf = (prob_crop &gt; 0.1) &amp; (prob_crop &lt;= 0.4)\n\n    overlay_conf[high_conf] = [255, 0, 0]  # Red for high\n    overlay_conf[med_conf] = [255, 165, 0]  # Orange for medium\n    overlay_conf[low_conf] = [255, 255, 0]  # Yellow for low\n\n    axes[1, 0].imshow(overlay_conf)\n    axes[1, 0].set_title(\n        \"Confidence Levels\\n(Red&gt;0.7, Orange&gt;0.4, Yellow&gt;0.1)\", fontweight=\"bold\"\n    )\n    axes[1, 0].axis(\"off\")\n\n    # Thresholded probability (&gt;0.5)\n    overlay_thresh = img2_crop.copy()\n    high_prob = prob_crop &gt; 0.5\n    overlay_thresh[high_prob] = [255, 0, 0]\n    axes[1, 1].imshow(overlay_thresh)\n    axes[1, 1].set_title(\n        \"High Confidence Only\\n(Probability &gt; 0.5)\", fontweight=\"bold\"\n    )\n    axes[1, 1].axis(\"off\")\n\n    # Probability histogram\n    prob_values = prob_crop[prob_crop &gt; 0]\n    if len(prob_values) &gt; 0:\n        axes[1, 2].hist(\n            prob_values, bins=50, alpha=0.7, color=\"red\", edgecolor=\"black\"\n        )\n        axes[1, 2].axvline(\n            x=0.5, color=\"blue\", linestyle=\"--\", label=\"0.5 threshold\"\n        )\n        axes[1, 2].axvline(\n            x=0.7, color=\"green\", linestyle=\"--\", label=\"0.7 threshold\"\n        )\n        axes[1, 2].set_xlabel(\"Change Probability\")\n        axes[1, 2].set_ylabel(\"Pixel Count\")\n        axes[1, 2].set_title(\n            f\"Probability Distribution\\n({len(prob_values):,} pixels)\"\n        )\n        axes[1, 2].legend()\n        axes[1, 2].grid(True, alpha=0.3)\n\n    # Statistics text\n    stats_text = f\"\"\"Probability Statistics:\nMin: {np.min(prob_values):.3f}\nMax: {np.max(prob_values):.3f}\nMean: {np.mean(prob_values):.3f}\nMedian: {np.median(prob_values):.3f}\n\nConfidence Levels:\nHigh (&gt;0.7): {np.sum(prob_crop &gt; 0.7):,}\nMed (0.4-0.7): {np.sum((prob_crop &gt; 0.4) &amp; (prob_crop &lt;= 0.7)):,}\nLow (0.1-0.4): {np.sum((prob_crop &gt; 0.1) &amp; (prob_crop &lt;= 0.4)):,}\"\"\"\n\n    axes[1, 3].text(\n        0.05,\n        0.95,\n        stats_text,\n        transform=axes[1, 3].transAxes,\n        fontsize=11,\n        verticalalignment=\"top\",\n        fontfamily=\"monospace\",\n    )\n    axes[1, 3].set_xlim(0, 1)\n    axes[1, 3].set_ylim(0, 1)\n    axes[1, 3].axis(\"off\")\n    axes[1, 3].set_title(\"Statistics Summary\", fontweight=\"bold\")\n\n    plt.tight_layout()\n    plt.suptitle(\n        \"Enhanced Probability-Based Change Detection\",\n        fontsize=16,\n        fontweight=\"bold\",\n        y=0.98,\n    )\n\n    plt.savefig(\"enhanced_probability_results.png\", dpi=150, bbox_inches=\"tight\")\n    plt.show()\n\n    print(\"\ud83d\udcbe Enhanced visualization saved as 'enhanced_probability_results.png'\")\n</code></pre>"},{"location":"change_detection/#geoai.change_detection.download_checkpoint","title":"<code>download_checkpoint(model_type='vit_h', checkpoint_dir=None)</code>","text":"<p>Download the SAM model checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. Can be one of ['vit_h', 'vit_l', 'vit_b']. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>checkpoint_dir</code> <code>str</code> <p>The checkpoint_dir directory. Defaults to None, which uses \"~/.cache/torch/hub/checkpoints\".</p> <code>None</code> Source code in <code>geoai/change_detection.py</code> <pre><code>def download_checkpoint(\n    model_type: str = \"vit_h\", checkpoint_dir: Optional[str] = None\n) -&gt; str:\n    \"\"\"Download the SAM model checkpoint.\n\n    Args:\n        model_type (str, optional): The model type. Can be one of ['vit_h', 'vit_l', 'vit_b'].\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_dir (str, optional): The checkpoint_dir directory. Defaults to None,\n            which uses \"~/.cache/torch/hub/checkpoints\".\n    \"\"\"\n\n    model_types = {\n        \"vit_h\": {\n            \"name\": \"sam_vit_h_4b8939.pth\",\n            \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n        },\n        \"vit_l\": {\n            \"name\": \"sam_vit_l_0b3195.pth\",\n            \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n        },\n        \"vit_b\": {\n            \"name\": \"sam_vit_b_01ec64.pth\",\n            \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n        },\n    }\n\n    if model_type not in model_types:\n        raise ValueError(\n            f\"Invalid model_type: {model_type}. It must be one of {', '.join(model_types)}\"\n        )\n\n    if checkpoint_dir is None:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    checkpoint = os.path.join(checkpoint_dir, model_types[model_type][\"name\"])\n    if not os.path.exists(checkpoint):\n        print(f\"Model checkpoint for {model_type} not found.\")\n        url = model_types[model_type][\"url\"]\n        if isinstance(url, str):\n            download_file(url, checkpoint)\n\n    return checkpoint\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-aug-11-2023","title":"v0.0.1 - Aug 11, 2023","text":"<p>Initial release</p>"},{"location":"classify/","title":"classify module","text":"<p>The module for training semantic segmentation models for classifying remote sensing imagery.</p>"},{"location":"classify/#geoai.classify.classify_image","title":"<code>classify_image(image_path, model_path, output_path=None, chip_size=1024, overlap=256, batch_size=4, colormap=None, **kwargs)</code>","text":"<p>Classify a geospatial image using a trained semantic segmentation model.</p> <p>This function handles the full image classification pipeline with special attention to edge handling: 1. Process the image in a grid pattern with overlapping tiles 2. Use central regions of tiles for interior parts 3. Special handling for edges to ensure complete coverage 4. Merge results into a single georeferenced output</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input GeoTIFF image.</p> required <code>model_path</code> <code>str</code> <p>Path to the trained model checkpoint.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output classified image.                         Defaults to \"[input_name]_classified.tif\".</p> <code>None</code> <code>chip_size</code> <code>int</code> <p>Size of chips for processing. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap size between adjacent tiles. Defaults to 256.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>colormap</code> <code>dict</code> <p>Colormap to apply to the output image.                        Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for DataLoader.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved classified image.</p> Source code in <code>geoai/classify.py</code> <pre><code>def classify_image(\n    image_path: str,\n    model_path: str,\n    output_path: Optional[str] = None,\n    chip_size: int = 1024,\n    overlap: int = 256,\n    batch_size: int = 4,\n    colormap: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Classify a geospatial image using a trained semantic segmentation model.\n\n    This function handles the full image classification pipeline with special\n    attention to edge handling:\n    1. Process the image in a grid pattern with overlapping tiles\n    2. Use central regions of tiles for interior parts\n    3. Special handling for edges to ensure complete coverage\n    4. Merge results into a single georeferenced output\n\n    Parameters:\n        image_path (str): Path to the input GeoTIFF image.\n        model_path (str): Path to the trained model checkpoint.\n        output_path (str, optional): Path to save the output classified image.\n                                    Defaults to \"[input_name]_classified.tif\".\n        chip_size (int, optional): Size of chips for processing. Defaults to 1024.\n        overlap (int, optional): Overlap size between adjacent tiles. Defaults to 256.\n        batch_size (int, optional): Batch size for inference. Defaults to 4.\n        colormap (dict, optional): Colormap to apply to the output image.\n                                   Defaults to None.\n        **kwargs: Additional keyword arguments for DataLoader.\n\n    Returns:\n        str: Path to the saved classified image.\n    \"\"\"\n    import timeit\n    import warnings\n\n    import rasterio\n    import torch\n    from rasterio.errors import NotGeoreferencedWarning\n    from torchgeo.trainers import SemanticSegmentationTask\n\n    # Disable specific GDAL/rasterio warnings\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio._.*\")\n    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"rasterio\")\n    warnings.filterwarnings(\"ignore\", category=NotGeoreferencedWarning)\n\n    # Also suppress GDAL error reports\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Set default output path if not provided\n    if output_path is None:\n        base_name = os.path.splitext(os.path.basename(image_path))[0]\n        output_path = f\"{base_name}_classified.tif\"\n\n    # Make sure output directory exists\n    output_dir = os.path.dirname(output_path)\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    # Load the model\n    print(f\"Loading model from {model_path}...\")\n    task = SemanticSegmentationTask.load_from_checkpoint(model_path)\n    task.model.eval()\n    task.model.cuda()\n\n    # Process the image using a modified tiling approach\n    with rasterio.open(image_path) as src:\n        # Get image dimensions and metadata\n        height = src.height\n        width = src.width\n        profile = src.profile.copy()\n\n        # Prepare output array for the final result\n        output_image = np.zeros((height, width), dtype=np.uint8)\n        confidence_map = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate number of tiles needed with overlap\n        # Ensure we have tiles that specifically cover the edges\n        effective_stride = chip_size - overlap\n\n        # Calculate x positions ensuring leftmost and rightmost edges are covered\n        x_positions = []\n        # Always include the leftmost position\n        x_positions.append(0)\n        # Add regular grid positions\n        for x in range(effective_stride, width - chip_size, effective_stride):\n            x_positions.append(x)\n        # Always include rightmost position that still fits\n        if width &gt; chip_size and x_positions[-1] + chip_size &lt; width:\n            x_positions.append(width - chip_size)\n\n        # Calculate y positions ensuring top and bottom edges are covered\n        y_positions = []\n        # Always include the topmost position\n        y_positions.append(0)\n        # Add regular grid positions\n        for y in range(effective_stride, height - chip_size, effective_stride):\n            y_positions.append(y)\n        # Always include bottommost position that still fits\n        if height &gt; chip_size and y_positions[-1] + chip_size &lt; height:\n            y_positions.append(height - chip_size)\n\n        # Create list of all tile positions\n        tile_positions = []\n        for y in y_positions:\n            for x in x_positions:\n                y_end = min(y + chip_size, height)\n                x_end = min(x + chip_size, width)\n                tile_positions.append((y, x, y_end, x_end))\n\n        # Print information about the tiling\n        print(\n            f\"Processing {len(tile_positions)} patches covering an image of size {height}x{width}...\"\n        )\n        start_time = timeit.default_timer()\n\n        # Process tiles in batches\n        for batch_start in range(0, len(tile_positions), batch_size):\n            batch_end = min(batch_start + batch_size, len(tile_positions))\n            batch_positions = tile_positions[batch_start:batch_end]\n            batch_data = []\n\n            # Load data for current batch\n            for y_start, x_start, y_end, x_end in batch_positions:\n                # Calculate actual tile size\n                actual_height = y_end - y_start\n                actual_width = x_end - x_start\n\n                # Read the tile data\n                tile_data = src.read(window=((y_start, y_end), (x_start, x_end)))\n\n                # Handle different sized tiles by padding if necessary\n                if tile_data.shape[1] != chip_size or tile_data.shape[2] != chip_size:\n                    padded_data = np.zeros(\n                        (tile_data.shape[0], chip_size, chip_size),\n                        dtype=tile_data.dtype,\n                    )\n                    padded_data[:, : tile_data.shape[1], : tile_data.shape[2]] = (\n                        tile_data\n                    )\n                    tile_data = padded_data\n\n                # Convert to tensor\n\n                tile_tensor = torch.from_numpy(tile_data).float() / 255.0\n                batch_data.append(tile_tensor)\n\n            # Convert batch to tensor\n            batch_tensor = torch.stack(batch_data)\n\n            # Run inference\n            with torch.no_grad():\n                logits = task.model.predict(batch_tensor.cuda())\n                probs = torch.softmax(logits, dim=1)\n                confidence, predictions = torch.max(probs, dim=1)\n                predictions = predictions.cpu().numpy()\n                confidence = confidence.cpu().numpy()\n\n            # Process each prediction\n            for idx, (y_start, x_start, y_end, x_end) in enumerate(batch_positions):\n                pred = predictions[idx]\n                conf = confidence[idx]\n\n                # Calculate actual tile size\n                actual_height = y_end - y_start\n                actual_width = x_end - x_start\n\n                # Get the actual prediction (removing padding if needed)\n                valid_pred = pred[:actual_height, :actual_width]\n                valid_conf = conf[:actual_height, :actual_width]\n\n                # Create confidence weights that favor central parts of tiles\n                # but still allow edge tiles to contribute fully at the image edges\n                is_edge_x = (x_start == 0) or (x_end == width)\n                is_edge_y = (y_start == 0) or (y_end == height)\n\n                # Create a mask that gives higher weight to central regions\n                # but ensures proper edge handling for boundary tiles\n                weight_mask = np.ones((actual_height, actual_width), dtype=np.float32)\n\n                # Only apply central weighting if not at an image edge\n                border = overlap // 2\n                if not is_edge_x and actual_width &gt; 2 * border:\n                    # Apply horizontal edge falloff (linear)\n                    for i in range(border):\n                        # Left edge\n                        weight_mask[:, i] = (i + 1) / (border + 1)\n                        # Right edge (if not at image edge)\n                        if i &lt; actual_width - border:\n                            weight_mask[:, actual_width - i - 1] = (i + 1) / (\n                                border + 1\n                            )\n\n                if not is_edge_y and actual_height &gt; 2 * border:\n                    # Apply vertical edge falloff (linear)\n                    for i in range(border):\n                        # Top edge\n                        weight_mask[i, :] = (i + 1) / (border + 1)\n                        # Bottom edge (if not at image edge)\n                        if i &lt; actual_height - border:\n                            weight_mask[actual_height - i - 1, :] = (i + 1) / (\n                                border + 1\n                            )\n\n                # Combine with prediction confidence\n                final_weight = weight_mask * valid_conf\n\n                # Update the output image based on confidence\n                current_conf = confidence_map[y_start:y_end, x_start:x_end]\n                update_mask = final_weight &gt; current_conf\n\n                if np.any(update_mask):\n                    # Update only pixels where this prediction has higher confidence\n                    output_image[y_start:y_end, x_start:x_end][update_mask] = (\n                        valid_pred[update_mask]\n                    )\n                    confidence_map[y_start:y_end, x_start:x_end][update_mask] = (\n                        final_weight[update_mask]\n                    )\n\n        # Update profile for output\n        profile.update({\"count\": 1, \"dtype\": \"uint8\", \"nodata\": 0})\n\n        # Save the result\n        print(f\"Saving classified image to {output_path}...\")\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            dst.write(output_image[np.newaxis, :, :])\n            if isinstance(colormap, dict):\n                dst.write_colormap(1, colormap)\n\n        # Calculate timing\n        total_time = timeit.default_timer() - start_time\n        print(f\"Total processing time: {total_time:.2f} seconds\")\n        print(f\"Successfully saved classified image to {output_path}\")\n\n    return output_path\n</code></pre>"},{"location":"classify/#geoai.classify.classify_images","title":"<code>classify_images(image_paths, model_path, output_dir=None, chip_size=1024, batch_size=4, colormap=None, file_extension='.tif', **kwargs)</code>","text":"<p>Classify multiple geospatial images using a trained semantic segmentation model.</p> <p>This function accepts either a list of image paths or a directory containing images and applies the classify_image function to each image, saving the results in the specified output directory.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>str or list</code> <p>Either a directory path containing images or a list of paths to input GeoTIFF images.</p> required <code>model_path</code> <code>str</code> <p>Path to the trained model checkpoint.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the output classified images. Defaults to None (same directory as input images for a list, or a new \"classified\" subdirectory for a directory input).</p> <code>None</code> <code>chip_size</code> <code>int</code> <p>Size of chips for processing. Defaults to 1024.</p> <code>1024</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>colormap</code> <code>dict</code> <p>Colormap to apply to the output images. Defaults to None.</p> <code>None</code> <code>file_extension</code> <code>str</code> <p>File extension to filter by when image_paths is a directory. Defaults to \".tif\".</p> <code>'.tif'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the classify_image function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>List of paths to the saved classified images.</p> Source code in <code>geoai/classify.py</code> <pre><code>def classify_images(\n    image_paths: Union[str, List[str]],\n    model_path: str,\n    output_dir: Optional[str] = None,\n    chip_size: int = 1024,\n    batch_size: int = 4,\n    colormap: Optional[Dict] = None,\n    file_extension: str = \".tif\",\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"\n    Classify multiple geospatial images using a trained semantic segmentation model.\n\n    This function accepts either a list of image paths or a directory containing images\n    and applies the classify_image function to each image, saving the results in the\n    specified output directory.\n\n    Parameters:\n        image_paths (str or list): Either a directory path containing images or a list\n            of paths to input GeoTIFF images.\n        model_path (str): Path to the trained model checkpoint.\n        output_dir (str, optional): Directory to save the output classified images.\n            Defaults to None (same directory as input images for a list, or a new\n            \"classified\" subdirectory for a directory input).\n        chip_size (int, optional): Size of chips for processing. Defaults to 1024.\n        batch_size (int, optional): Batch size for inference. Defaults to 4.\n        colormap (dict, optional): Colormap to apply to the output images.\n            Defaults to None.\n        file_extension (str, optional): File extension to filter by when image_paths\n            is a directory. Defaults to \".tif\".\n        **kwargs: Additional keyword arguments for the classify_image function.\n\n    Returns:\n        list: List of paths to the saved classified images.\n    \"\"\"\n    # Import required libraries\n    import glob\n\n    from tqdm import tqdm\n\n    # Process directory input\n    if isinstance(image_paths, str) and os.path.isdir(image_paths):\n        # Set default output directory if not provided\n        if output_dir is None:\n            output_dir = os.path.join(image_paths, \"classified\")\n\n        # Get all images with the specified extension\n        image_path_list = glob.glob(os.path.join(image_paths, f\"*{file_extension}\"))\n\n        # Check if any images were found\n        if not image_path_list:\n            print(f\"No files with extension '{file_extension}' found in {image_paths}\")\n            return []\n\n        print(f\"Found {len(image_path_list)} images in directory {image_paths}\")\n\n    # Process list input\n    elif isinstance(image_paths, list):\n        image_path_list = image_paths\n\n        # Set default output directory if not provided\n        if output_dir is None and len(image_path_list) &gt; 0:\n            output_dir = os.path.dirname(image_path_list[0])\n\n    # Invalid input\n    else:\n        raise ValueError(\n            \"image_paths must be either a directory path or a list of file paths\"\n        )\n\n    # Create output directory if it doesn't exist\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    classified_image_paths = []\n\n    # Create progress bar\n    for image_path in tqdm(image_path_list, desc=\"Classifying images\", unit=\"image\"):\n        try:\n            # Get just the filename without extension\n            base_filename = os.path.splitext(os.path.basename(image_path))[0]\n\n            # Create output path within output_dir\n            output_path = os.path.join(\n                output_dir, f\"{base_filename}_classified{file_extension}\"\n            )\n\n            # Perform classification\n            classified_image_path = classify_image(\n                image_path,\n                model_path,\n                output_path=output_path,\n                chip_size=chip_size,\n                batch_size=batch_size,\n                colormap=colormap,\n                **kwargs,\n            )\n            classified_image_paths.append(classified_image_path)\n        except Exception as e:\n            print(f\"Error processing {image_path}: {str(e)}\")\n\n    print(\n        f\"Classification complete. Processed {len(classified_image_paths)} images successfully.\"\n    )\n    return classified_image_paths\n</code></pre>"},{"location":"classify/#geoai.classify.train_classifier","title":"<code>train_classifier(image_root, label_root, output_dir='output', in_channels=4, num_classes=14, epochs=20, img_size=256, batch_size=8, sample_size=500, model='unet', backbone='resnet50', weights=True, num_filters=3, loss='ce', class_weights=None, ignore_index=None, lr=0.001, patience=10, freeze_backbone=False, freeze_decoder=False, transforms=None, use_augmentation=False, seed=42, train_val_test_split=(0.6, 0.2, 0.2), accelerator='auto', devices='auto', logger=None, callbacks=None, log_every_n_steps=10, use_distributed_sampler=False, monitor_metric='val_loss', mode='min', save_top_k=1, save_last=True, checkpoint_filename='best_model', checkpoint_path=None, every_n_epochs=1, **kwargs)</code>","text":"<p>Train a semantic segmentation model on geospatial imagery.</p> <p>This function sets up datasets, model, trainer, and executes the training process for semantic segmentation tasks using geospatial data. It supports training from scratch or resuming from a checkpoint if available.</p> <p>Parameters:</p> Name Type Description Default <code>image_root</code> <code>str</code> <p>Path to directory containing imagery.</p> required <code>label_root</code> <code>str</code> <p>Path to directory containing land cover labels.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model outputs and checkpoints. Defaults to \"output\".</p> <code>'output'</code> <code>in_channels</code> <code>int</code> <p>Number of input channels in the imagery. Defaults to 4.</p> <code>4</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the segmentation task. Defaults to 14.</p> <code>14</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 20.</p> <code>20</code> <code>img_size</code> <code>int</code> <p>Size of image patches for training. Defaults to 256.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 8.</p> <code>8</code> <code>sample_size</code> <code>int</code> <p>Number of samples per epoch. Defaults to 500.</p> <code>500</code> <code>model</code> <code>str</code> <p>Model architecture to use. Defaults to \"unet\".</p> <code>'unet'</code> <code>backbone</code> <code>str</code> <p>Backbone network for the model. Defaults to \"resnet50\".</p> <code>'resnet50'</code> <code>weights</code> <code>bool</code> <p>Whether to use pretrained weights. Defaults to True.</p> <code>True</code> <code>num_filters</code> <code>int</code> <p>Number of filters for the model. Defaults to 3.</p> <code>3</code> <code>loss</code> <code>str</code> <p>Loss function to use ('ce', 'jaccard', or 'focal'). Defaults to \"ce\".</p> <code>'ce'</code> <code>class_weights</code> <code>list</code> <p>Class weights for loss function. Defaults to None.</p> <code>None</code> <code>ignore_index</code> <code>int</code> <p>Index to ignore in loss calculation. Defaults to None.</p> <code>None</code> <code>lr</code> <code>float</code> <p>Learning rate. Defaults to 0.001.</p> <code>0.001</code> <code>patience</code> <code>int</code> <p>Number of epochs with no improvement after which training will stop. Defaults to 10.</p> <code>10</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze backbone. Defaults to False.</p> <code>False</code> <code>freeze_decoder</code> <code>bool</code> <p>Whether to freeze decoder. Defaults to False.</p> <code>False</code> <code>transforms</code> <code>callable</code> <p>Transforms to apply to the data. Defaults to None.</p> <code>None</code> <code>use_augmentation</code> <code>bool</code> <p>Whether to apply data augmentation. Defaults to False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>train_val_test_split</code> <code>list</code> <p>Proportions for train/val/test split. Defaults to [0.6, 0.2, 0.2].</p> <code>(0.6, 0.2, 0.2)</code> <code>accelerator</code> <code>str</code> <p>Accelerator to use for training ('cpu', 'gpu', etc.). Defaults to \"auto\".</p> <code>'auto'</code> <code>devices</code> <code>str</code> <p>Number of devices to use for training. Defaults to \"auto\".</p> <code>'auto'</code> <code>logger</code> <code>object</code> <p>Logger for tracking training progress. Defaults to None.</p> <code>None</code> <code>callbacks</code> <code>list</code> <p>List of callbacks for the trainer. Defaults to None.</p> <code>None</code> <code>log_every_n_steps</code> <code>int</code> <p>Frequency of logging training progress. Defaults to 10.</p> <code>10</code> <code>use_distributed_sampler</code> <code>bool</code> <p>Whether to use distributed sampling. Defaults to False.</p> <code>False</code> <code>monitor_metric</code> <code>str</code> <p>Metric to monitor for saving best model. Defaults to \"val_loss\".</p> <code>'val_loss'</code> <code>mode</code> <code>str</code> <p>Mode for monitoring metric ('min' or 'max'). Use 'min' for losses and 'max' for metrics like accuracy. Defaults to \"min\".</p> <code>'min'</code> <code>save_top_k</code> <code>int</code> <p>Number of best models to save. Defaults to 1.</p> <code>1</code> <code>save_last</code> <code>bool</code> <p>Whether to save the model from the last epoch. Defaults to True.</p> <code>True</code> <code>checkpoint_filename</code> <code>str</code> <p>Filename pattern for saved checkpoints. Defaults to \"best_model_{epoch:02d}_{val_loss:.4f}\".</p> <code>'best_model'</code> <code>checkpoint_path</code> <code>str</code> <p>Path to a checkpoint file to resume training.</p> <code>None</code> <code>every_n_epochs</code> <code>int</code> <p>Save a checkpoint every N epochs. Defaults to 1.</p> <code>1</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the datasets.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>object</code> <code>Any</code> <p>Trained SemanticSegmentationTask model.</p> Source code in <code>geoai/classify.py</code> <pre><code>def train_classifier(\n    image_root: str,\n    label_root: str,\n    output_dir: str = \"output\",\n    in_channels: int = 4,\n    num_classes: int = 14,\n    epochs: int = 20,\n    img_size: int = 256,\n    batch_size: int = 8,\n    sample_size: int = 500,\n    model: str = \"unet\",\n    backbone: str = \"resnet50\",\n    weights: bool = True,\n    num_filters: int = 3,\n    loss: str = \"ce\",\n    class_weights: Optional[List[float]] = None,\n    ignore_index: Optional[int] = None,\n    lr: float = 0.001,\n    patience: int = 10,\n    freeze_backbone: bool = False,\n    freeze_decoder: bool = False,\n    transforms: Optional[Any] = None,\n    use_augmentation: bool = False,\n    seed: int = 42,\n    train_val_test_split: tuple = (0.6, 0.2, 0.2),\n    accelerator: str = \"auto\",\n    devices: str = \"auto\",\n    logger: Optional[Any] = None,\n    callbacks: Optional[List[Any]] = None,\n    log_every_n_steps: int = 10,\n    use_distributed_sampler: bool = False,\n    monitor_metric: str = \"val_loss\",\n    mode: str = \"min\",\n    save_top_k: int = 1,\n    save_last: bool = True,\n    checkpoint_filename: str = \"best_model\",\n    checkpoint_path: Optional[str] = None,\n    every_n_epochs: int = 1,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Train a semantic segmentation model on geospatial imagery.\n\n    This function sets up datasets, model, trainer, and executes the training process\n    for semantic segmentation tasks using geospatial data. It supports training\n    from scratch or resuming from a checkpoint if available.\n\n    Args:\n        image_root (str): Path to directory containing imagery.\n        label_root (str): Path to directory containing land cover labels.\n        output_dir (str, optional): Directory to save model outputs and checkpoints.\n            Defaults to \"output\".\n        in_channels (int, optional): Number of input channels in the imagery.\n            Defaults to 4.\n        num_classes (int, optional): Number of classes in the segmentation task.\n            Defaults to 14.\n        epochs (int, optional): Number of training epochs. Defaults to 20.\n        img_size (int, optional): Size of image patches for training. Defaults to 256.\n        batch_size (int, optional): Batch size for training. Defaults to 8.\n        sample_size (int, optional): Number of samples per epoch. Defaults to 500.\n        model (str, optional): Model architecture to use. Defaults to \"unet\".\n        backbone (str, optional): Backbone network for the model. Defaults to \"resnet50\".\n        weights (bool, optional): Whether to use pretrained weights. Defaults to True.\n        num_filters (int, optional): Number of filters for the model. Defaults to 3.\n        loss (str, optional): Loss function to use ('ce', 'jaccard', or 'focal').\n            Defaults to \"ce\".\n        class_weights (list, optional): Class weights for loss function. Defaults to None.\n        ignore_index (int, optional): Index to ignore in loss calculation. Defaults to None.\n        lr (float, optional): Learning rate. Defaults to 0.001.\n        patience (int, optional): Number of epochs with no improvement after which\n            training will stop. Defaults to 10.\n        freeze_backbone (bool, optional): Whether to freeze backbone. Defaults to False.\n        freeze_decoder (bool, optional): Whether to freeze decoder. Defaults to False.\n        transforms (callable, optional): Transforms to apply to the data. Defaults to None.\n        use_augmentation (bool, optional): Whether to apply data augmentation.\n            Defaults to False.\n        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n        train_val_test_split (list, optional): Proportions for train/val/test split.\n            Defaults to [0.6, 0.2, 0.2].\n        accelerator (str, optional): Accelerator to use for training ('cpu', 'gpu', etc.).\n            Defaults to \"auto\".\n        devices (str, optional): Number of devices to use for training. Defaults to \"auto\".\n        logger (object, optional): Logger for tracking training progress. Defaults to None.\n        callbacks (list, optional): List of callbacks for the trainer. Defaults to None.\n        log_every_n_steps (int, optional): Frequency of logging training progress.\n            Defaults to 10.\n        use_distributed_sampler (bool, optional): Whether to use distributed sampling.\n            Defaults to False.\n        monitor_metric (str, optional): Metric to monitor for saving best model.\n            Defaults to \"val_loss\".\n        mode (str, optional): Mode for monitoring metric ('min' or 'max').\n            Use 'min' for losses and 'max' for metrics like accuracy.\n            Defaults to \"min\".\n        save_top_k (int, optional): Number of best models to save.\n            Defaults to 1.\n        save_last (bool, optional): Whether to save the model from the last epoch.\n            Defaults to True.\n        checkpoint_filename (str, optional): Filename pattern for saved checkpoints.\n            Defaults to \"best_model_{epoch:02d}_{val_loss:.4f}\".\n        checkpoint_path (str, optional): Path to a checkpoint file to resume training.\n        every_n_epochs (int, optional): Save a checkpoint every N epochs.\n            Defaults to 1.\n        **kwargs: Additional keyword arguments to pass to the datasets.\n\n    Returns:\n        object: Trained SemanticSegmentationTask model.\n    \"\"\"\n    import multiprocessing as mp\n    import timeit\n\n    import albumentations as A\n    import lightning.pytorch as pl\n    import torch\n    from lightning.pytorch.callbacks import ModelCheckpoint\n    from lightning.pytorch.loggers import CSVLogger\n    from torch.utils.data import DataLoader\n    from torchgeo.datamodules import GeoDataModule\n    from torchgeo.datasets import RasterDataset, stack_samples\n    from torchgeo.datasets.splits import random_bbox_assignment\n    from torchgeo.samplers import (\n        GridGeoSampler,\n        RandomBatchGeoSampler,\n        RandomGeoSampler,\n    )\n    from torchgeo.trainers import SemanticSegmentationTask\n\n    # Create a wrapper class for albumentations to work with TorchGeo format\n    class AlbumentationsWrapper:\n        def __init__(self, transform):\n            self.transform = transform\n\n        def __call__(self, sample):\n            # Extract image and mask from TorchGeo sample format\n            if \"image\" not in sample or \"mask\" not in sample:\n                return sample\n\n            image = sample[\"image\"]\n            mask = sample[\"mask\"]\n\n            # Albumentations expects channels last, but TorchGeo uses channels first\n            # Convert (C, H, W) to (H, W, C) for image\n            image_np = image.permute(1, 2, 0).numpy()\n            mask_np = mask.squeeze(0).numpy() if mask.dim() &gt; 2 else mask.numpy()\n\n            # Apply transformation with named arguments\n            transformed = self.transform(image=image_np, mask=mask_np)\n\n            # Convert back to PyTorch tensors with channels first\n            transformed_image = torch.from_numpy(transformed[\"image\"]).permute(2, 0, 1)\n            transformed_mask = torch.from_numpy(transformed[\"mask\"]).unsqueeze(0)\n\n            # Update the sample dictionary\n            result = sample.copy()\n            result[\"image\"] = transformed_image\n            result[\"mask\"] = transformed_mask\n\n            return result\n\n    # Set up data augmentation if requested\n    if use_augmentation:\n        aug_transforms = A.Compose(\n            [\n                A.HorizontalFlip(p=0.5),\n                A.VerticalFlip(p=0.5),\n                A.RandomRotate90(p=0.5),\n                A.ShiftScaleRotate(\n                    p=0.5, shift_limit=0.0625, scale_limit=0.1, rotate_limit=45\n                ),\n                A.RandomBrightnessContrast(\n                    p=0.5, brightness_limit=0.2, contrast_limit=0.2\n                ),\n                A.GaussianBlur(p=0.3),\n                A.GaussNoise(p=0.3),\n                A.CoarseDropout(p=0.3, max_holes=8, max_height=32, max_width=32),\n            ]\n        )\n        # Wrap the albumentations transforms\n        transforms = AlbumentationsWrapper(aug_transforms)\n\n    # # Set up device configuration\n    # device, num_devices = (\n    #     (\"cuda\", torch.cuda.device_count())\n    #     if torch.cuda.is_available()\n    #     else (\"cpu\", mp.cpu_count())\n    # )\n    workers = mp.cpu_count()\n    # print(f\"Running on {num_devices} {device}(s)\")\n\n    # Define datasets\n    class ImageDatasetClass(RasterDataset):\n        filename_glob = \"*.tif\"\n        is_image = True\n        separate_files = False\n\n    class LabelDatasetClass(RasterDataset):\n        filename_glob = \"*.tif\"\n        is_image = False\n        separate_files = False\n\n    # Prepare output directory\n    test_dir = os.path.join(output_dir, \"models\")\n    if not os.path.exists(test_dir):\n        os.makedirs(test_dir)\n\n    # Set up logger and checkpoint callback\n    if logger is None:\n        logger = CSVLogger(test_dir, name=\"lightning_logs\")\n\n    if callbacks is None:\n        checkpoint_callback = ModelCheckpoint(\n            dirpath=test_dir,\n            filename=checkpoint_filename,\n            save_top_k=save_top_k,\n            monitor=monitor_metric,\n            mode=mode,\n            save_last=save_last,\n            every_n_epochs=every_n_epochs,\n            verbose=True,\n        )\n        callbacks = [checkpoint_callback]\n\n    # Initialize the segmentation task\n    task = SemanticSegmentationTask(\n        model=model,\n        backbone=backbone,\n        weights=weights,\n        in_channels=in_channels,\n        num_classes=num_classes,\n        num_filters=num_filters,\n        loss=loss,\n        class_weights=class_weights,\n        ignore_index=ignore_index,\n        lr=lr,\n        patience=patience,\n        freeze_backbone=freeze_backbone,\n        freeze_decoder=freeze_decoder,\n    )\n\n    # Set up trainer\n    trainer = pl.Trainer(\n        accelerator=accelerator,\n        devices=devices,\n        max_epochs=epochs,\n        callbacks=callbacks,\n        logger=logger,\n        log_every_n_steps=log_every_n_steps,\n        use_distributed_sampler=use_distributed_sampler,\n        **kwargs,  # Pass any additional kwargs to the trainer\n    )\n\n    # Load datasets with transforms if augmentation is enabled\n\n    if isinstance(image_root, RasterDataset):\n        images = image_root\n    else:\n        images = ImageDatasetClass(paths=image_root, transforms=transforms, **kwargs)\n\n    if isinstance(label_root, RasterDataset):\n        labels = label_root\n    else:\n        labels = LabelDatasetClass(paths=label_root, **kwargs)\n\n    # Create intersection dataset\n    dataset = images &amp; labels\n\n    # Define custom datamodule for training\n    class CustomGeoDataModule(GeoDataModule):\n        def setup(self, stage: str) -&gt; None:\n            \"\"\"Set up datasets.\n\n            Args:\n                stage: Either 'fit', 'validate', 'test', or 'predict'.\n            \"\"\"\n            self.dataset = self.dataset_class(**self.kwargs)\n\n            generator = torch.Generator().manual_seed(seed)\n            (\n                self.train_dataset,\n                self.val_dataset,\n                self.test_dataset,\n            ) = random_bbox_assignment(dataset, train_val_test_split, generator)\n\n            if stage in [\"fit\"]:\n                self.train_batch_sampler = RandomBatchGeoSampler(\n                    self.train_dataset, self.patch_size, self.batch_size, self.length\n                )\n            if stage in [\"fit\", \"validate\"]:\n                self.val_sampler = GridGeoSampler(\n                    self.val_dataset, self.patch_size, self.patch_size\n                )\n            if stage in [\"test\"]:\n                self.test_sampler = GridGeoSampler(\n                    self.test_dataset, self.patch_size, self.patch_size\n                )\n\n    # Create datamodule\n    datamodule = CustomGeoDataModule(\n        dataset_class=type(dataset),\n        batch_size=batch_size,\n        patch_size=img_size,\n        length=sample_size,\n        num_workers=workers,\n        dataset1=images,\n        dataset2=labels,\n        collate_fn=stack_samples,\n    )\n\n    # Start training timer\n    start = timeit.default_timer()\n\n    # Check for existing checkpoint\n    if checkpoint_path is not None:\n        checkpoint_file = os.path.abspath(checkpoint_path)\n    else:\n        checkpoint_file = os.path.join(test_dir, \"last.ckpt\")\n\n    if os.path.isfile(checkpoint_file):\n        print(\"Resuming training from previous checkpoint...\")\n        trainer.fit(model=task, datamodule=datamodule, ckpt_path=checkpoint_file)\n    else:\n        print(\"Starting training from scratch...\")\n        trainer.fit(\n            model=task,\n            datamodule=datamodule,\n        )\n\n    training_time = timeit.default_timer() - start\n    print(f\"The time taken to train was: {training_time:.2f} seconds\")\n\n    best_model_path = checkpoint_callback.best_model_path\n    print(f\"Best model saved at: {best_model_path}\")\n\n    # Test the model\n    trainer.test(model=task, datamodule=datamodule)\n\n    return task\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/opengeos/geoai/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>geoai could always use more documentation, whether as part of the official geoai docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/opengeos/geoai/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up geoai for local development.</p> <ol> <li> <p>Fork the geoai repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/geoai.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv geoai\n$ cd geoai/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the tests:</p> <pre><code>$ pytest .\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.10 or above. Check https://github.com/opengeos/geoai/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"contributing/#seek-help","title":"Seek Help","text":"<p>If you have any questions, please ask in our GitHub Discussions.</p>"},{"location":"detectron2/","title":"detectron2 module","text":"<p>Detectron2 integration for remote sensing image segmentation. See https://github.com/facebookresearch/detectron2 for more details.</p>"},{"location":"detectron2/#geoai.detectron2.batch_detectron2_segment","title":"<code>batch_detectron2_segment(image_paths, output_dir='.', model_config='COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', model_weights=None, score_threshold=0.5, device=None, save_masks=True, save_probability=True)</code>","text":"<p>Perform batch instance segmentation on multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of paths to input images</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output files</p> <code>'.'</code> <code>model_config</code> <code>str</code> <p>Model configuration file path or name from model zoo</p> <code>'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'</code> <code>model_weights</code> <code>Optional[str]</code> <p>Path to model weights file. If None, uses model zoo weights</p> <code>None</code> <code>score_threshold</code> <code>float</code> <p>Confidence threshold for predictions</p> <code>0.5</code> <code>device</code> <code>Optional[str]</code> <p>Device to use ('cpu', 'cuda', or None for auto-detection)</p> <code>None</code> <code>save_masks</code> <code>bool</code> <p>Whether to save instance masks as GeoTIFF</p> <code>True</code> <code>save_probability</code> <code>bool</code> <p>Whether to save probability masks as GeoTIFF</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of results dictionaries for each image</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def batch_detectron2_segment(\n    image_paths: List[str],\n    output_dir: str = \".\",\n    model_config: str = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\",\n    model_weights: Optional[str] = None,\n    score_threshold: float = 0.5,\n    device: Optional[str] = None,\n    save_masks: bool = True,\n    save_probability: bool = True,\n) -&gt; List[Dict]:\n    \"\"\"\n    Perform batch instance segmentation on multiple images.\n\n    Args:\n        image_paths: List of paths to input images\n        output_dir: Directory to save output files\n        model_config: Model configuration file path or name from model zoo\n        model_weights: Path to model weights file. If None, uses model zoo weights\n        score_threshold: Confidence threshold for predictions\n        device: Device to use ('cpu', 'cuda', or None for auto-detection)\n        save_masks: Whether to save instance masks as GeoTIFF\n        save_probability: Whether to save probability masks as GeoTIFF\n\n    Returns:\n        List of results dictionaries for each image\n    \"\"\"\n    check_detectron2()\n\n    # Load the model once for batch processing\n    predictor = load_detectron2_model(\n        model_config=model_config,\n        model_weights=model_weights,\n        score_threshold=score_threshold,\n        device=device,\n    )\n\n    results = []\n\n    for i, image_path in enumerate(image_paths):\n        try:\n            # Generate unique output prefixes\n            base_name = os.path.splitext(os.path.basename(image_path))[0]\n            mask_prefix = f\"{base_name}_instance_masks\"\n            prob_prefix = f\"{base_name}_probability_mask\"\n\n            # Process image\n            result = detectron2_segment(\n                image_path=image_path,\n                output_dir=output_dir,\n                model_config=model_config,\n                model_weights=model_weights,\n                score_threshold=score_threshold,\n                device=device,\n                save_masks=save_masks,\n                save_probability=save_probability,\n                mask_prefix=mask_prefix,\n                prob_prefix=prob_prefix,\n            )\n\n            result[\"image_path\"] = image_path\n            results.append(result)\n\n            print(f\"Processed {i+1}/{len(image_paths)}: {image_path}\")\n\n        except Exception as e:\n            print(f\"Error processing {image_path}: {str(e)}\")\n            results.append({\"image_path\": image_path, \"error\": str(e)})\n\n    return results\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.check_detectron2","title":"<code>check_detectron2()</code>","text":"<p>Check if detectron2 is available.</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def check_detectron2():\n    \"\"\"Check if detectron2 is available.\"\"\"\n    if not HAS_DETECTRON2:\n        raise ImportError(\n            \"Detectron2 is required. Please install it with: pip install detectron2\"\n        )\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.create_instance_mask","title":"<code>create_instance_mask(masks)</code>","text":"<p>Create an instance mask from individual binary masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>Array of binary masks with shape (num_instances, height, width)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Instance mask with unique ID for each instance</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def create_instance_mask(masks: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Create an instance mask from individual binary masks.\n\n    Args:\n        masks: Array of binary masks with shape (num_instances, height, width)\n\n    Returns:\n        Instance mask with unique ID for each instance\n    \"\"\"\n    if len(masks) == 0:\n        return np.zeros((masks.shape[1], masks.shape[2]), dtype=np.uint16)\n\n    instance_mask = np.zeros((masks.shape[1], masks.shape[2]), dtype=np.uint16)\n\n    for i, mask in enumerate(masks):\n        # Assign unique instance ID (starting from 1)\n        instance_mask[mask] = i + 1\n\n    return instance_mask\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.create_probability_mask","title":"<code>create_probability_mask(masks, scores)</code>","text":"<p>Create a probability mask from individual binary masks and their confidence scores.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>ndarray</code> <p>Array of binary masks with shape (num_instances, height, width)</p> required <code>scores</code> <code>ndarray</code> <p>Array of confidence scores for each mask</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Probability mask with maximum confidence score for each pixel</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def create_probability_mask(masks: np.ndarray, scores: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Create a probability mask from individual binary masks and their confidence scores.\n\n    Args:\n        masks: Array of binary masks with shape (num_instances, height, width)\n        scores: Array of confidence scores for each mask\n\n    Returns:\n        Probability mask with maximum confidence score for each pixel\n    \"\"\"\n    if len(masks) == 0:\n        return np.zeros((masks.shape[1], masks.shape[2]), dtype=np.float32)\n\n    probability_mask = np.zeros((masks.shape[1], masks.shape[2]), dtype=np.float32)\n\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        # Update probability mask with higher confidence scores\n        probability_mask = np.where(\n            mask &amp; (score &gt; probability_mask), score, probability_mask\n        )\n\n    return probability_mask\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.detectron2_segment","title":"<code>detectron2_segment(image_path, output_dir='.', model_config='COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', model_weights=None, score_threshold=0.5, device=None, save_masks=True, save_probability=True, mask_prefix='instance_masks', prob_prefix='probability_mask')</code>","text":"<p>Perform instance segmentation on a remote sensing image using Detectron2.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to input image</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output files</p> <code>'.'</code> <code>model_config</code> <code>str</code> <p>Model configuration file path or name from model zoo</p> <code>'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'</code> <code>model_weights</code> <code>Optional[str]</code> <p>Path to model weights file. If None, uses model zoo weights</p> <code>None</code> <code>score_threshold</code> <code>float</code> <p>Confidence threshold for predictions</p> <code>0.5</code> <code>device</code> <code>Optional[str]</code> <p>Device to use ('cpu', 'cuda', or None for auto-detection)</p> <code>None</code> <code>save_masks</code> <code>bool</code> <p>Whether to save instance masks as GeoTIFF</p> <code>True</code> <code>save_probability</code> <code>bool</code> <p>Whether to save probability masks as GeoTIFF</p> <code>True</code> <code>mask_prefix</code> <code>str</code> <p>Prefix for instance mask output file</p> <code>'instance_masks'</code> <code>prob_prefix</code> <code>str</code> <p>Prefix for probability mask output file</p> <code>'probability_mask'</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dict containing segmentation results and output file paths</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def detectron2_segment(\n    image_path: str,\n    output_dir: str = \".\",\n    model_config: str = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\",\n    model_weights: Optional[str] = None,\n    score_threshold: float = 0.5,\n    device: Optional[str] = None,\n    save_masks: bool = True,\n    save_probability: bool = True,\n    mask_prefix: str = \"instance_masks\",\n    prob_prefix: str = \"probability_mask\",\n) -&gt; Dict:\n    \"\"\"\n    Perform instance segmentation on a remote sensing image using Detectron2.\n\n    Args:\n        image_path: Path to input image\n        output_dir: Directory to save output files\n        model_config: Model configuration file path or name from model zoo\n        model_weights: Path to model weights file. If None, uses model zoo weights\n        score_threshold: Confidence threshold for predictions\n        device: Device to use ('cpu', 'cuda', or None for auto-detection)\n        save_masks: Whether to save instance masks as GeoTIFF\n        save_probability: Whether to save probability masks as GeoTIFF\n        mask_prefix: Prefix for instance mask output file\n        prob_prefix: Prefix for probability mask output file\n\n    Returns:\n        Dict containing segmentation results and output file paths\n    \"\"\"\n    check_detectron2()\n\n    # Load the model\n    predictor = load_detectron2_model(\n        model_config=model_config,\n        model_weights=model_weights,\n        score_threshold=score_threshold,\n        device=device,\n    )\n\n    # Read the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Could not read image from {image_path}\")\n\n    # Run inference\n    outputs = predictor(image)\n\n    # Extract results\n    instances = outputs[\"instances\"].to(\"cpu\")\n    masks = instances.pred_masks.numpy()\n    scores = instances.scores.numpy()\n    classes = instances.pred_classes.numpy()\n    boxes = instances.pred_boxes.tensor.numpy()\n\n    results = {\n        \"masks\": masks,\n        \"scores\": scores,\n        \"classes\": classes,\n        \"boxes\": boxes,\n        \"num_instances\": len(masks),\n    }\n\n    # Get image geospatial information\n    try:\n        with rasterio.open(image_path) as src:\n            transform = src.transform\n            crs = src.crs\n            height, width = src.height, src.width\n    except Exception:\n        # If not a GeoTIFF, create a simple transform\n        height, width = image.shape[:2]\n        transform = from_bounds(0, 0, width, height, width, height)\n        crs = CRS.from_epsg(4326)\n\n    # Save instance masks as GeoTIFF\n    if save_masks and len(masks) &gt; 0:\n        instance_mask_path = os.path.join(output_dir, f\"{mask_prefix}.tif\")\n        instance_mask = create_instance_mask(masks)\n        save_geotiff_mask(\n            instance_mask, instance_mask_path, transform, crs, dtype=\"uint16\"\n        )\n        results[\"instance_mask_path\"] = instance_mask_path\n\n    # Save probability masks as GeoTIFF\n    if save_probability and len(masks) &gt; 0:\n        prob_mask_path = os.path.join(output_dir, f\"{prob_prefix}.tif\")\n        probability_mask = create_probability_mask(masks, scores)\n        save_geotiff_mask(\n            probability_mask, prob_mask_path, transform, crs, dtype=\"float32\"\n        )\n        results[\"probability_mask_path\"] = prob_mask_path\n\n    return results\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.get_class_id_name_mapping","title":"<code>get_class_id_name_mapping(config_path, lazy=False)</code>","text":"<p>Get class ID to name mapping from a Detectron2 model config.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the config file or model_zoo config name.</p> required <code>lazy</code> <code>bool</code> <p>Whether the config is a LazyConfig (i.e., .py).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[int, str]</code> <p>Mapping from class ID (int) to class name (str).</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def get_class_id_name_mapping(config_path: str, lazy: bool = False) -&gt; Dict[int, str]:\n    \"\"\"\n    Get class ID to name mapping from a Detectron2 model config.\n\n    Args:\n        config_path (str): Path to the config file or model_zoo config name.\n        lazy (bool): Whether the config is a LazyConfig (i.e., .py).\n\n    Returns:\n        dict: Mapping from class ID (int) to class name (str).\n    \"\"\"\n    if lazy or config_path.endswith(\".py\"):\n        cfg = LazyConfig.load(\n            model_zoo.get_config_file(config_path)\n            if not os.path.exists(config_path)\n            else config_path\n        )\n        dataset_name = cfg.dataloader.train.mapper.dataset.names[0]\n    else:\n        cfg = get_cfg()\n        cfg.merge_from_file(\n            model_zoo.get_config_file(config_path)\n            if not os.path.exists(config_path)\n            else config_path\n        )\n        dataset_name = cfg.DATASETS.TRAIN[0]\n\n    metadata = MetadataCatalog.get(dataset_name)\n\n    classes = metadata.get(\"thing_classes\", []) or metadata.get(\"stuff_classes\", [])\n    return {i: name for i, name in enumerate(classes)}\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.get_detectron2_models","title":"<code>get_detectron2_models()</code>","text":"<p>Get a list of available Detectron2 models for instance segmentation.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of model configuration names</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def get_detectron2_models() -&gt; List[str]:\n    \"\"\"\n    Get a list of available Detectron2 models for instance segmentation.\n\n    Returns:\n        List of model configuration names\n    \"\"\"\n    from detectron2.model_zoo.model_zoo import _ModelZooUrls\n\n    configs = list(_ModelZooUrls.CONFIG_PATH_TO_URL_SUFFIX.keys())\n    models = [f\"{config}.yaml\" for config in configs]\n    return models\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.load_detectron2_model","title":"<code>load_detectron2_model(model_config='COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml', model_weights=None, score_threshold=0.5, device=None, num_classes=None)</code>","text":"<p>Load a Detectron2 model for instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>str</code> <p>Model configuration file path or name from model zoo</p> <code>'COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml'</code> <code>model_weights</code> <code>Optional[str]</code> <p>Path to model weights file. If None, uses model zoo weights</p> <code>None</code> <code>score_threshold</code> <code>float</code> <p>Confidence threshold for predictions</p> <code>0.5</code> <code>device</code> <code>Optional[str]</code> <p>Device to use ('cpu', 'cuda', or None for auto-detection)</p> <code>None</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for custom models</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DefaultPredictor</code> <code>DefaultPredictor</code> <p>Configured Detectron2 predictor</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def load_detectron2_model(\n    model_config: str = \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\",\n    model_weights: Optional[str] = None,\n    score_threshold: float = 0.5,\n    device: Optional[str] = None,\n    num_classes: Optional[int] = None,\n) -&gt; DefaultPredictor:\n    \"\"\"\n    Load a Detectron2 model for instance segmentation.\n\n    Args:\n        model_config: Model configuration file path or name from model zoo\n        model_weights: Path to model weights file. If None, uses model zoo weights\n        score_threshold: Confidence threshold for predictions\n        device: Device to use ('cpu', 'cuda', or None for auto-detection)\n        num_classes: Number of classes for custom models\n\n    Returns:\n        DefaultPredictor: Configured Detectron2 predictor\n    \"\"\"\n    check_detectron2()\n\n    cfg = get_cfg()\n\n    # Load model configuration\n    if model_config.endswith(\".yaml\"):\n        cfg.merge_from_file(model_zoo.get_config_file(model_config))\n    else:\n        cfg.merge_from_file(model_config)\n\n    # Set model weights\n    if model_weights is None:\n        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_config)\n    else:\n        cfg.MODEL.WEIGHTS = model_weights\n\n    # Set score threshold\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = score_threshold\n\n    # Set device\n    if device is None:\n        device = get_device()\n\n    # Ensure device is a string (detectron2 expects string, not torch.device)\n    if hasattr(device, \"type\"):\n        device = device.type\n    elif not isinstance(device, str):\n        device = str(device)\n\n    cfg.MODEL.DEVICE = device\n\n    # Set number of classes if specified\n    if num_classes is not None:\n        cfg.MODEL.ROI_HEADS.NUM_CLASSES = num_classes\n\n    return DefaultPredictor(cfg)\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.save_geotiff_mask","title":"<code>save_geotiff_mask(mask, output_path, transform, crs, dtype='uint16')</code>","text":"<p>Save a mask as a GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>2D numpy array representing the mask</p> required <code>output_path</code> <code>str</code> <p>Path to save the GeoTIFF file</p> required <code>transform</code> <code>Affine</code> <p>Rasterio transform for georeferencing</p> required <code>crs</code> <code>CRS</code> <p>Coordinate reference system</p> required <code>dtype</code> <code>str</code> <p>Data type for the output file</p> <code>'uint16'</code> Source code in <code>geoai/detectron2.py</code> <pre><code>def save_geotiff_mask(\n    mask: np.ndarray,\n    output_path: str,\n    transform: rasterio.transform.Affine,\n    crs: CRS,\n    dtype: str = \"uint16\",\n) -&gt; None:\n    \"\"\"\n    Save a mask as a GeoTIFF file.\n\n    Args:\n        mask: 2D numpy array representing the mask\n        output_path: Path to save the GeoTIFF file\n        transform: Rasterio transform for georeferencing\n        crs: Coordinate reference system\n        dtype: Data type for the output file\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Determine numpy dtype\n    if dtype == \"uint16\":\n        np_dtype = np.uint16\n    elif dtype == \"float32\":\n        np_dtype = np.float32\n    else:\n        np_dtype = np.uint16\n\n    # Convert mask to appropriate dtype\n    mask = mask.astype(np_dtype)\n\n    # Save as GeoTIFF\n    with rasterio.open(\n        output_path,\n        \"w\",\n        driver=\"GTiff\",\n        height=mask.shape[0],\n        width=mask.shape[1],\n        count=1,\n        dtype=np_dtype,\n        crs=crs,\n        transform=transform,\n        compress=\"lzw\",\n    ) as dst:\n        dst.write(mask, 1)\n</code></pre>"},{"location":"detectron2/#geoai.detectron2.visualize_detectron2_results","title":"<code>visualize_detectron2_results(image_path, results, output_path=None, show_scores=True, show_classes=True)</code>","text":"<p>Visualize Detectron2 segmentation results on the original image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the original image</p> required <code>results</code> <code>Dict</code> <p>Results dictionary from detectron2_segment</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the visualization (optional)</p> <code>None</code> <code>show_scores</code> <code>bool</code> <p>Whether to show confidence scores</p> <code>True</code> <code>show_classes</code> <code>bool</code> <p>Whether to show class labels</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Visualization image as numpy array</p> Source code in <code>geoai/detectron2.py</code> <pre><code>def visualize_detectron2_results(\n    image_path: str,\n    results: Dict,\n    output_path: Optional[str] = None,\n    show_scores: bool = True,\n    show_classes: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"\n    Visualize Detectron2 segmentation results on the original image.\n\n    Args:\n        image_path: Path to the original image\n        results: Results dictionary from detectron2_segment\n        output_path: Path to save the visualization (optional)\n        show_scores: Whether to show confidence scores\n        show_classes: Whether to show class labels\n\n    Returns:\n        Visualization image as numpy array\n    \"\"\"\n    check_detectron2()\n\n    # Load the image\n    image = cv2.imread(image_path)\n    if image is None:\n        raise ValueError(f\"Could not read image from {image_path}\")\n\n    # Convert BGR to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Create visualizer\n    v = Visualizer(image_rgb, scale=1.0)\n\n    # Create instances object for visualization\n    from detectron2.structures import Boxes, Instances\n\n    instances = Instances((image.shape[0], image.shape[1]))\n    instances.pred_masks = torch.from_numpy(results[\"masks\"])\n    instances.pred_boxes = Boxes(torch.from_numpy(results[\"boxes\"]))\n    instances.scores = torch.from_numpy(results[\"scores\"])\n    instances.pred_classes = torch.from_numpy(results[\"classes\"])\n\n    # Draw predictions\n    out = v.draw_instance_predictions(instances)\n    vis_image = out.get_image()\n\n    # Save visualization if path provided\n    if output_path is not None:\n        cv2.imwrite(output_path, cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR))\n\n    return vis_image\n</code></pre>"},{"location":"dinov3/","title":"DINOv3 module","text":"<p>DINOv3 module for patch similarity analysis with GeoTIFF support.</p> <p>This module provides tools for computing patch similarity using DINOv3 features on geospatial imagery stored in GeoTIFF format.</p>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor","title":"<code>DINOv3GeoProcessor</code>","text":"<p>DINOv3 processor with GeoTIFF input/output support. https://github.com/facebookresearch/dinov3</p> Source code in <code>geoai/dinov3.py</code> <pre><code>class DINOv3GeoProcessor:\n    \"\"\"DINOv3 processor with GeoTIFF input/output support.\n    https://github.com/facebookresearch/dinov3\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"dinov3_vitl16\",\n        weights_path: Optional[str] = None,\n        device: Optional[torch.device] = None,\n    ):\n        \"\"\"Initialize DINOv3 processor.\n\n        Args:\n            model_name: Name of the DINOv3 model. Can be \"dinov3_vits16\", \"dinov3_vits16plus\",\n                \"dinov3_vitb16\", \"dinov3_vitl16\", \"dinov3_vith16plus\", \"dinov3_vit7b16\", \"dinov3_convnext_tiny\",\n                \"dinov3_convnext_small\", \"dinov3_convnext_base\", \"dinov3_convnext_large\",\n                \"dinov3dinov3_vitl16\", and \"dinov3_vit7b16\".\n                 See https://github.com/facebookresearch/dinov3 for more details.\n            weights_path: Path to model weights (optional)\n            device: Torch device to use\n        \"\"\"\n\n        dinov3_github_location = \"facebookresearch/dinov3\"\n\n        if os.getenv(\"DINOV3_LOCATION\") is not None:\n            dinov3_location = os.getenv(\"DINOV3_LOCATION\")\n        else:\n            dinov3_location = dinov3_github_location\n\n        self.dinov3_location = dinov3_location\n        self.dinov3_source = (\n            \"local\" if dinov3_location != dinov3_github_location else \"github\"\n        )\n\n        self.device = device or get_device()\n        self.model_name = model_name\n\n        # Add DINOv3 to path if needed\n        if dinov3_location != \"facebookresearch/dinov3\" and (\n            dinov3_location not in sys.path\n        ):\n            sys.path.append(dinov3_location)\n\n        # Load model\n        self.model = self._load_model(weights_path)\n        self.patch_size = self.model.patch_size\n        self.embed_dim = self.model.embed_dim\n\n        # Image transforms - satellite imagery normalization\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=(0.430, 0.411, 0.296),  # SAT-493M normalization\n                    std=(0.213, 0.156, 0.143),\n                ),\n            ]\n        )\n\n    def _download_model_from_hf(\n        self, model_path: Optional[str] = None, repo_id: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"\n        Download the object detection model from Hugging Face.\n\n        Args:\n            model_path: Path to the model file.\n            repo_id: Hugging Face repository ID.\n\n        Returns:\n            Path to the downloaded model file\n        \"\"\"\n        try:\n\n            # Define the repository ID and model filename\n            if repo_id is None:\n                repo_id = \"giswqs/geoai\"\n\n            if model_path is None:\n                model_path = \"dinov3_vitl16_sat493m.pth\"\n\n            # Download the model\n            model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n\n            return model_path\n\n        except Exception as e:\n            print(f\"Error downloading model from Hugging Face: {e}\")\n            print(\"Please specify a local model path or ensure internet connectivity.\")\n            raise\n\n    def _load_model(self, weights_path: Optional[str] = None) -&gt; torch.nn.Module:\n        \"\"\"Load DINOv3 model.\"\"\"\n        try:\n            if weights_path and os.path.exists(weights_path):\n                # Load with custom weights\n                model = torch.hub.load(\n                    repo_or_dir=self.dinov3_location,\n                    model=self.model_name,\n                    source=self.dinov3_source,\n                )\n                # Load state dict manually\n                state_dict = torch.load(weights_path, map_location=self.device)\n                model.load_state_dict(state_dict, strict=False)\n            else:\n                # Download weights and load manually\n                weights_path = self._download_model_from_hf()\n                model = torch.hub.load(\n                    repo_or_dir=self.dinov3_location,\n                    model=self.model_name,\n                    source=self.dinov3_source,\n                    pretrained=False,  # &lt;-- critical: prevents downloading official weights\n                    weights=None,  # &lt;-- be explicit; some hubs honor this\n                    trust_repo=True,  # optional: avoids interactivity prompts\n                    skip_validation=True,  # optional: speeds things up\n                )\n                # Load state dict manually\n                state_dict = torch.load(weights_path, map_location=self.device)\n                model.load_state_dict(state_dict, strict=False)\n\n            model = model.to(self.device)\n            model.eval()\n            return model\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load DINOv3 model: {e}\") from e\n\n    def load_regular_image(\n        self,\n        image_path: str,\n    ) -&gt; Tuple[np.ndarray, dict]:\n        \"\"\"Load regular image file (PNG, JPG, etc.).\n\n        Args:\n            image_path: Path to image file\n\n        Returns:\n            Tuple of (image array, metadata)\n        \"\"\"\n        try:\n            # Load image using PIL\n            image = Image.open(image_path).convert(\"RGB\")\n\n            # Convert to numpy array (H, W, C)\n            img_array = np.array(image)\n\n            # Convert to (C, H, W) format to match GeoTIFF format\n            data = np.transpose(img_array, (2, 0, 1)).astype(np.uint8)\n\n            # Create basic metadata\n            height, width = img_array.shape[:2]\n            metadata = {\n                \"profile\": {\n                    \"driver\": \"PNG\",\n                    \"dtype\": \"uint8\",\n                    \"nodata\": None,\n                    \"width\": width,\n                    \"height\": height,\n                    \"count\": 3,\n                    \"crs\": None,\n                    \"transform\": None,\n                },\n                \"crs\": None,\n                \"transform\": None,\n                \"bounds\": (0, 0, width, height),\n            }\n\n            return data, metadata\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load image {image_path}: {e}\")\n\n    def load_geotiff(\n        self,\n        source: Union[str, DatasetReader],\n        window: Optional[Window] = None,\n        bands: Optional[List[int]] = None,\n    ) -&gt; Tuple[np.ndarray, dict]:\n        \"\"\"Load GeoTIFF file.\n\n        Args:\n            source: Path to GeoTIFF file (str) or an open rasterio.DatasetReader\n            window: Rasterio window for reading subset\n            bands: List of bands to read (1-indexed)\n\n        Returns:\n            Tuple of (image array, metadata)\n        \"\"\"\n        # Flag to determine if we need to close the dataset afterwards\n        should_close = False\n        if isinstance(source, str):\n            src = rasterio.open(source)\n            should_close = True\n        elif isinstance(source, DatasetReader):\n            src = source\n        else:\n            raise TypeError(\"source must be a str path or a rasterio.DatasetReader\")\n\n        try:\n            # Read specified bands or all bands\n            if bands:\n                data = src.read(bands, window=window)\n            else:\n                data = src.read(window=window)\n\n            # Get metadata\n            profile = src.profile.copy()\n            if window:\n                profile.update(\n                    {\n                        \"height\": window.height,\n                        \"width\": window.width,\n                        \"transform\": src.window_transform(window),\n                    }\n                )\n\n            metadata = {\n                \"profile\": profile,\n                \"crs\": src.crs,\n                \"transform\": profile[\"transform\"],\n                \"bounds\": (\n                    src.bounds\n                    if not window\n                    else rasterio.windows.bounds(window, src.transform)\n                ),\n            }\n        finally:\n            if should_close:\n                src.close()\n\n        return data, metadata\n\n    def load_image(\n        self,\n        source: Union[str, DatasetReader],\n        window: Optional[Window] = None,\n        bands: Optional[List[int]] = None,\n    ) -&gt; Tuple[np.ndarray, dict]:\n        \"\"\"Load image file (GeoTIFF or regular image).\n\n        Args:\n            source: Path to image file (str) or an open rasterio.DatasetReader\n            window: Rasterio window for reading subset (only applies to GeoTIFF)\n            bands: List of bands to read (only applies to GeoTIFF)\n\n        Returns:\n            Tuple of (image array, metadata)\n        \"\"\"\n        if isinstance(source, str):\n            # Check if it's a GeoTIFF file\n            try:\n                # Try to open with rasterio first\n                with rasterio.open(source) as src:\n                    # If successful and has CRS, treat as GeoTIFF\n                    if src.crs is not None:\n                        return self.load_geotiff(source, window, bands)\n                    # If no CRS, it might be a regular image opened by rasterio\n                    else:\n                        # Check file extension\n                        file_ext = source.lower().split(\".\")[-1]\n                        if file_ext in [\"tif\", \"tiff\"]:\n                            return self.load_geotiff(source, window, bands)\n                        else:\n                            return self.load_regular_image(source)\n            except (rasterio.RasterioIOError, rasterio.errors.RasterioIOError):\n                # If rasterio fails, try as regular image\n                return self.load_regular_image(source)\n        elif isinstance(source, DatasetReader):\n            # Already opened rasterio dataset\n            return self.load_geotiff(source, window, bands)\n        else:\n            raise TypeError(\"source must be a str path or a rasterio.DatasetReader\")\n\n    def save_geotiff(\n        self, data: np.ndarray, output_path: str, metadata: dict, dtype: str = \"float32\"\n    ) -&gt; None:\n        \"\"\"Save array as GeoTIFF.\n\n        Args:\n            data: Array to save\n            output_path: Output file path\n            metadata: Metadata from original file\n            dtype: Output data type\n        \"\"\"\n        profile = metadata[\"profile\"].copy()\n        profile.update(\n            {\n                \"dtype\": dtype,\n                \"count\": data.shape[0] if data.ndim == 3 else 1,\n                \"height\": data.shape[-2] if data.ndim &gt;= 2 else data.shape[0],\n                \"width\": data.shape[-1] if data.ndim &gt;= 2 else 1,\n            }\n        )\n\n        with rasterio.open(output_path, \"w\", **profile) as dst:\n            if data.ndim == 2:\n                dst.write(data, 1)\n            else:\n                dst.write(data)\n\n    def save_similarity_as_image(\n        self, similarity_data: np.ndarray, output_path: str, colormap: str = \"turbo\"\n    ) -&gt; None:\n        \"\"\"Save similarity array as PNG image with colormap.\n\n        Args:\n            similarity_data: 2D similarity array\n            output_path: Output file path\n            colormap: Matplotlib colormap name\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        # Apply colormap\n        cmap = plt.get_cmap(colormap)\n        colored_data = cmap(similarity_data)\n\n        # Convert to uint8 image (remove alpha channel)\n        img_data = (colored_data[..., :3] * 255).astype(np.uint8)\n\n        # Save as PNG\n        img = Image.fromarray(img_data)\n        img.save(output_path)\n\n    def preprocess_image_for_dinov3(\n        self,\n        data: np.ndarray,\n        target_size: int = 896,\n        normalize_percentile: bool = True,\n    ) -&gt; Image.Image:\n        \"\"\"Preprocess image data for DINOv3.\n\n        Args:\n            data: Input array (C, H, W) or (H, W)\n            target_size: Target size for resizing\n            normalize_percentile: Whether to normalize using percentiles\n\n        Returns:\n            PIL Image ready for DINOv3\n        \"\"\"\n        # Handle different input shapes\n        if data.ndim == 2:\n            data = data[np.newaxis, :, :]  # Add channel dimension\n        elif data.ndim == 3 and data.shape[0] &gt; 3:\n            # Take first 3 bands if more than 3 channels\n            data = data[:3, :, :]\n\n        # Normalize data\n        if normalize_percentile:\n            # Normalize each band using percentiles\n            normalized_data = np.zeros_like(data, dtype=np.float32)\n            for i in range(data.shape[0]):\n                band = data[i]\n                p2, p98 = np.percentile(band, [2, 98])\n                normalized_data[i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n        else:\n            # Simple min-max normalization\n            normalized_data = (data - data.min()) / (data.max() - data.min())\n\n        # Convert to PIL Image\n        if normalized_data.shape[0] == 1:\n            # Grayscale - repeat to 3 channels\n            img_array = np.repeat(normalized_data[0], 3, axis=0)\n        else:\n            img_array = normalized_data\n\n        # Transpose to HWC format and convert to uint8\n        img_array = np.transpose(img_array, (1, 2, 0))\n        img_array = (img_array * 255).astype(np.uint8)\n\n        # Create PIL Image\n        image = Image.fromarray(img_array)\n\n        # Resize to patch-aligned dimensions\n        return self.resize_to_patch_aligned(image, target_size)\n\n    def resize_to_patch_aligned(\n        self, image: Image.Image, target_size: int = 896\n    ) -&gt; Image.Image:\n        \"\"\"Resize image to be aligned with patch grid.\"\"\"\n        w, h = image.size\n\n        # Calculate new dimensions that are multiples of patch_size\n        if w &gt; h:\n            new_h = target_size\n            new_w = int((w * target_size) / h)\n        else:\n            new_w = target_size\n            new_h = int((h * target_size) / w)\n\n        # Round to nearest multiple of patch_size\n        new_h = ((new_h + self.patch_size - 1) // self.patch_size) * self.patch_size\n        new_w = ((new_w + self.patch_size - 1) // self.patch_size) * self.patch_size\n\n        return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n\n    def extract_features(self, image: Image.Image) -&gt; Tuple[torch.Tensor, int, int]:\n        \"\"\"Extract patch features from image.\"\"\"\n\n        if isinstance(image, str):\n            image = Image.open(image)\n\n        if isinstance(image, np.ndarray):\n            image = Image.fromarray(image)\n\n        # Transform image\n        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n\n        with torch.no_grad():\n            # Extract features from last layer\n            features = self.model.get_intermediate_layers(\n                img_tensor, n=1, reshape=True, norm=True\n            )[\n                0\n            ]  # Shape: [1, embed_dim, h_patches, w_patches]\n\n        # Rearrange to [h_patches, w_patches, embed_dim]\n        features = features.squeeze(0).permute(1, 2, 0)\n        h_patches, w_patches = features.shape[:2]\n\n        return features, h_patches, w_patches\n\n    def compute_patch_similarity(\n        self, features: torch.Tensor, patch_x: int, patch_y: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Compute cosine similarity between selected patch and all patches.\"\"\"\n        h_patches, w_patches, embed_dim = features.shape\n\n        # Get query patch feature\n        query_feature = features[patch_y, patch_x]  # Shape: [embed_dim]\n\n        # Reshape features for batch computation\n        all_features = features.view(\n            -1, embed_dim\n        )  # Shape: [h_patches * w_patches, embed_dim]\n\n        # Compute cosine similarity\n        similarities = F.cosine_similarity(\n            query_feature.unsqueeze(0),  # Shape: [1, embed_dim]\n            all_features,  # Shape: [h_patches * w_patches, embed_dim]\n            dim=1,\n        )\n\n        # Reshape back to patch grid\n        similarities = similarities.view(h_patches, w_patches)\n\n        # Normalize to 0-1 range\n        similarities = (similarities + 1) / 2\n\n        return similarities\n\n    def compute_similarity(\n        self,\n        source: str = None,\n        features: torch.Tensor = None,\n        query_coords: Tuple[float, float] = None,\n        output_dir: str = None,\n        window: Optional[Window] = None,\n        bands: Optional[List[int]] = None,\n        target_size: int = 896,\n        save_features: bool = False,\n        coord_crs: str = None,\n        use_interpolation: bool = True,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"Process GeoTIFF for patch similarity analysis.\n\n        Args:\n            source: Path to input GeoTIFF or rasterio dataset\n            features: Pre-extracted features (h_patches, w_patches, embed_dim)\n            query_coords: (x, y) coordinates in image pixel space or (lon, lat) in geographic space\n            output_dir: Output directory for results\n            window: Optional window for reading subset\n            bands: Optional list of bands to use\n            target_size: Target size for processing\n            save_features: Whether to save extracted features\n            coord_crs: Coordinate CRS of the query coordinates\n            use_interpolation: Whether to use interpolation when resizing similarity map\n\n        Returns:\n            Dictionary containing similarity results and metadata\n        \"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n\n        # Load image (GeoTIFF or regular image)\n        data, metadata = self.load_image(source, window, bands)\n        raw_img_w, raw_img_h = data.shape[-1], data.shape[-2]\n\n        # Preprocess for DINOv3\n        image = self.preprocess_image_for_dinov3(data, target_size)\n\n        # Extract features\n        if features is None:\n            features, h_patches, w_patches = self.extract_features(image)\n        else:\n            h_patches, w_patches = features.shape[:2]\n\n        # Convert coordinates to patch space\n        img_w, img_h = image.size\n        if len(query_coords) == 2:\n            # Assume pixel coordinates for now\n            if coord_crs is not None:\n                [query_coords] = coords_to_xy(source, [query_coords], coord_crs)\n\n            new_x = math.floor(query_coords[0] / raw_img_w * img_w)\n            new_y = math.floor(query_coords[1] / raw_img_h * img_h)\n            query_coords = [new_x, new_y]\n\n            x_pixel, y_pixel = query_coords\n            patch_x = math.floor((x_pixel / img_w) * w_patches)\n            patch_y = math.floor((y_pixel / img_h) * h_patches)\n\n            # Clamp to valid range\n            patch_x = max(0, min(w_patches - 1, patch_x))\n            patch_y = max(0, min(h_patches - 1, patch_y))\n\n        # Compute similarity\n        similarities = self.compute_patch_similarity(features, patch_x, patch_y)\n\n        # Prepare results\n        results = {\n            \"similarities\": similarities.cpu().numpy(),\n            \"patch_coords\": (patch_x, patch_y),\n            \"patch_grid_size\": (h_patches, w_patches),\n            \"image_size\": (img_w, img_h),\n            \"metadata\": metadata,\n        }\n\n        # Save similarity as GeoTIFF\n        sim_array = similarities.cpu().numpy()\n\n        # Resize similarity to original data dimensions\n        if use_interpolation:\n            try:\n                from skimage.transform import resize\n\n                sim_resized = resize(\n                    sim_array,\n                    (data.shape[-2], data.shape[-1]),\n                    preserve_range=True,\n                    anti_aliasing=True,\n                )\n            except ImportError:\n                # Fallback to PIL if scikit-image not available\n                from PIL import Image as PILImage\n\n                sim_pil = PILImage.fromarray((sim_array * 255).astype(np.uint8))\n                sim_pil = sim_pil.resize(\n                    (data.shape[-1], data.shape[-2]), PILImage.LANCZOS\n                )\n                sim_resized = np.array(sim_pil, dtype=np.float32) / 255.0\n        else:\n            # Resize without interpolation (nearest neighbor)\n            try:\n                from skimage.transform import resize\n\n                sim_resized = resize(\n                    sim_array,\n                    (data.shape[-2], data.shape[-1]),\n                    preserve_range=True,\n                    anti_aliasing=False,\n                    order=0,  # Nearest neighbor interpolation\n                )\n            except ImportError:\n                # Fallback to PIL with nearest neighbor\n                from PIL import Image as PILImage\n\n                sim_pil = PILImage.fromarray((sim_array * 255).astype(np.uint8))\n                sim_pil = sim_pil.resize(\n                    (data.shape[-1], data.shape[-2]), PILImage.NEAREST\n                )\n                sim_resized = np.array(sim_pil, dtype=np.float32) / 255.0\n\n        # Save similarity map\n        if metadata[\"crs\"] is not None:\n            # Save as GeoTIFF for georeferenced data\n            similarity_path = os.path.join(\n                output_dir, f\"similarity_patch_{patch_x}_{patch_y}.tif\"\n            )\n            self.save_geotiff(\n                sim_resized[np.newaxis, :, :],\n                similarity_path,\n                metadata,\n                dtype=\"float32\",\n            )\n        else:\n            # Save as PNG for regular images\n            similarity_path = os.path.join(\n                output_dir, f\"similarity_patch_{patch_x}_{patch_y}.png\"\n            )\n            self.save_similarity_as_image(sim_resized, similarity_path)\n\n        image_dict = {\n            \"crs\": metadata[\"crs\"],\n            \"bounds\": metadata[\"bounds\"],\n            \"image\": sim_resized[np.newaxis, :, :],\n        }\n        results[\"image_dict\"] = image_dict\n\n        # Save features if requested\n        if save_features:\n            features_np = features.cpu().numpy()\n            features_path = os.path.join(\n                output_dir, f\"features_patch_{patch_x}_{patch_y}.npy\"\n            )\n            np.save(features_path, features_np)\n\n        # Save metadata\n        metadata_dict = {\n            \"input_path\": source,\n            \"query_coords\": query_coords,\n            \"patch_coords\": (patch_x, patch_y),\n            \"patch_grid_size\": (h_patches, w_patches),\n            \"image_size\": (img_w, img_h),\n            \"similarity_stats\": {\n                \"max\": float(sim_array.max()),\n                \"min\": float(sim_array.min()),\n                \"mean\": float(sim_array.mean()),\n                \"std\": float(sim_array.std()),\n            },\n        }\n\n        if save_features:\n            metadata_path = os.path.join(\n                output_dir, f\"metadata_patch_{patch_x}_{patch_y}.json\"\n            )\n            with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n                json.dump(metadata_dict, f, indent=2)\n\n            results[\"output_paths\"] = {\n                \"similarity\": similarity_path,\n                \"metadata\": metadata_path,\n                \"features\": features_path if save_features else None,\n            }\n\n        return results\n\n    def visualize_similarity(\n        self,\n        source: str,\n        similarity_data: np.ndarray,\n        query_coords: Tuple[float, float] = None,\n        patch_coords: Tuple[int, int] = None,\n        figsize: Tuple[int, int] = (15, 6),\n        colormap: str = \"turbo\",\n        alpha: float = 0.7,\n        save_path: str = None,\n        show_query_point: bool = True,\n        overlay: bool = False,\n    ) -&gt; plt.Figure:\n        \"\"\"Visualize original image and similarity map side by side or as overlay.\n\n        Args:\n            source: Path to original image\n            similarity_data: 2D similarity array\n            query_coords: Query coordinates in pixel space (x, y)\n            patch_coords: Patch coordinates (patch_x, patch_y) for marking query patch\n            figsize: Figure size for visualization\n            colormap: Colormap for similarity visualization\n            alpha: Transparency for overlay mode\n            save_path: Optional path to save the visualization\n            show_query_point: Whether to show the query point marker\n            overlay: If True, overlay similarity on original image; if False, show side by side\n\n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        # Load original image\n        data, metadata = self.load_image(source)\n\n        # Convert image data to displayable format\n        if data.ndim == 3:\n            if data.shape[0] &lt;= 3:\n                # Standard RGB/grayscale image (C, H, W)\n                display_img = np.transpose(data, (1, 2, 0))\n            else:\n                # Multi-band image, take first 3 bands\n                display_img = np.transpose(data[:3], (1, 2, 0))\n        else:\n            # Single band image\n            display_img = data\n\n        # Normalize image for display\n        if display_img.dtype != np.uint8:\n            # Normalize using percentiles\n            if display_img.ndim == 3:\n                normalized_img = np.zeros_like(display_img, dtype=np.float32)\n                for i in range(display_img.shape[2]):\n                    band = display_img[:, :, i]\n                    p2, p98 = np.percentile(band, [2, 98])\n                    normalized_img[:, :, i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n            else:\n                p2, p98 = np.percentile(display_img, [2, 98])\n                normalized_img = np.clip((display_img - p2) / (p98 - p2), 0, 1)\n            display_img = normalized_img\n        else:\n            display_img = display_img / 255.0\n\n        # Ensure similarity data matches image dimensions\n        if similarity_data.shape != display_img.shape[:2]:\n            from PIL import Image as PILImage\n\n            sim_pil = PILImage.fromarray((similarity_data * 255).astype(np.uint8))\n            sim_pil = sim_pil.resize(\n                (display_img.shape[1], display_img.shape[0]), PILImage.LANCZOS\n            )\n            similarity_data = np.array(sim_pil, dtype=np.float32) / 255.0\n\n        if overlay:\n            # Single plot with overlay\n            fig, ax = plt.subplots(1, 1, figsize=(figsize[1], figsize[1]))\n\n            # Show original image\n            if display_img.ndim == 2:\n                ax.imshow(display_img, cmap=\"gray\")\n            else:\n                ax.imshow(display_img)\n\n            # Overlay similarity map\n            im_sim = ax.imshow(\n                similarity_data, cmap=colormap, alpha=alpha, vmin=0, vmax=1\n            )\n\n            # Add colorbar for similarity\n            cbar = plt.colorbar(im_sim, ax=ax, fraction=0.046, pad=0.04)\n            cbar.set_label(\"Similarity\", rotation=270, labelpad=20)\n\n            ax.set_title(\"Image with Similarity Overlay\")\n\n        else:\n            # Side-by-side visualization\n            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n\n            # Original image\n            if display_img.ndim == 2:\n                ax1.imshow(display_img, cmap=\"gray\")\n            else:\n                ax1.imshow(display_img)\n            ax1.set_title(\"Original Image\")\n            ax1.axis(\"off\")\n\n            # Similarity map\n            im_sim = ax2.imshow(similarity_data, cmap=colormap, vmin=0, vmax=1)\n            ax2.set_title(\"Similarity Map\")\n            ax2.axis(\"off\")\n\n            # Add colorbar\n            cbar = plt.colorbar(im_sim, ax=ax2, fraction=0.046, pad=0.04)\n            cbar.set_label(\"Similarity\", rotation=270, labelpad=20)\n\n        # Mark query point if provided\n        if show_query_point and query_coords is not None:\n            x, y = query_coords\n            if overlay:\n                ax.plot(\n                    x,\n                    y,\n                    \"r*\",\n                    markersize=15,\n                    markeredgecolor=\"white\",\n                    markeredgewidth=2,\n                )\n                ax.plot(x, y, \"r*\", markersize=12)\n            else:\n                ax1.plot(\n                    x,\n                    y,\n                    \"r*\",\n                    markersize=15,\n                    markeredgecolor=\"white\",\n                    markeredgewidth=2,\n                )\n                ax1.plot(x, y, \"r*\", markersize=12)\n                ax2.plot(\n                    x,\n                    y,\n                    \"r*\",\n                    markersize=15,\n                    markeredgecolor=\"white\",\n                    markeredgewidth=2,\n                )\n                ax2.plot(x, y, \"r*\", markersize=12)\n\n        plt.tight_layout()\n\n        # Save if path provided\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n\n        return fig\n\n    def visualize_patches(\n        self,\n        image: Image.Image,\n        features: torch.Tensor,\n        patch_coords: Tuple[int, int],\n        add_text: bool = False,\n        figsize: Tuple[int, int] = (12, 8),\n        save_path: str = None,\n    ) -&gt; plt.Figure:\n        \"\"\"Visualize image with patch grid and highlight selected patch.\n\n        Args:\n            image: PIL Image\n            features: Feature tensor (h_patches, w_patches, embed_dim)\n            patch_coords: Selected patch coordinates (patch_x, patch_y)\n            add_text: Whether to add text to the patch\n            figsize: Figure size\n            save_path: Optional path to save visualization\n\n        Returns:\n            Matplotlib figure object\n        \"\"\"\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n        # Display image\n        ax.imshow(image)\n        ax.set_title(\"Image with Patch Grid\")\n        ax.axis(\"off\")\n\n        # Get dimensions\n        img_w, img_h = image.size\n        h_patches, w_patches = features.shape[:2]\n        patch_x, patch_y = patch_coords\n\n        # Calculate patch size in pixels\n        patch_w = img_w / w_patches\n        patch_h = img_h / h_patches\n\n        # Draw patch grid\n        for i in range(w_patches + 1):\n            x = i * patch_w\n            ax.axvline(x=x, color=\"white\", alpha=0.3, linewidth=0.5)\n\n        for i in range(h_patches + 1):\n            y = i * patch_h\n            ax.axhline(y=y, color=\"white\", alpha=0.3, linewidth=0.5)\n\n        # Highlight selected patch\n        rect_x = patch_x * patch_w\n        rect_y = patch_y * patch_h\n        rect = patches.Rectangle(\n            (rect_x, rect_y),\n            patch_w,\n            patch_h,\n            linewidth=3,\n            edgecolor=\"red\",\n            facecolor=\"none\",\n        )\n        ax.add_patch(rect)\n\n        # Add patch coordinate text\n        if add_text:\n            ax.text(\n                rect_x + patch_w / 2,\n                rect_y + patch_h / 2,\n                f\"({patch_x}, {patch_y})\",\n                color=\"red\",\n                fontsize=12,\n                ha=\"center\",\n                va=\"center\",\n                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n            )\n\n        plt.tight_layout()\n\n        if save_path:\n            plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n\n        return fig\n\n    def create_similarity_overlay(\n        self,\n        source: str,\n        similarity_data: np.ndarray,\n        colormap: str = \"turbo\",\n        alpha: float = 0.7,\n    ) -&gt; np.ndarray:\n        \"\"\"Create an overlay of similarity map on original image.\n\n        Args:\n            source: Path to original image\n            similarity_data: 2D similarity array\n            colormap: Colormap for similarity visualization\n            alpha: Transparency for overlay\n\n        Returns:\n            RGB overlay image as numpy array\n        \"\"\"\n        # Load original image\n        data, _ = self.load_image(source)\n\n        # Convert to display format\n        if data.ndim == 3:\n            if data.shape[0] &lt;= 3:\n                display_img = np.transpose(data, (1, 2, 0))\n            else:\n                display_img = np.transpose(data[:3], (1, 2, 0))\n        else:\n            display_img = data\n\n        # Normalize image\n        if display_img.dtype != np.uint8:\n            if display_img.ndim == 3:\n                normalized_img = np.zeros_like(display_img, dtype=np.float32)\n                for i in range(display_img.shape[2]):\n                    band = display_img[:, :, i]\n                    p2, p98 = np.percentile(band, [2, 98])\n                    normalized_img[:, :, i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n            else:\n                p2, p98 = np.percentile(display_img, [2, 98])\n                normalized_img = np.clip((display_img - p2) / (p98 - p2), 0, 1)\n            base_img = normalized_img\n        else:\n            base_img = display_img / 255.0\n\n        # Convert grayscale to RGB if needed\n        if base_img.ndim == 2:\n            base_img = np.stack([base_img] * 3, axis=2)\n\n        # Resize similarity data to match image\n        if similarity_data.shape != base_img.shape[:2]:\n            from PIL import Image as PILImage\n\n            sim_pil = PILImage.fromarray((similarity_data * 255).astype(np.uint8))\n            sim_pil = sim_pil.resize(\n                (base_img.shape[1], base_img.shape[0]), PILImage.LANCZOS\n            )\n            similarity_data = np.array(sim_pil, dtype=np.float32) / 255.0\n\n        # Apply colormap to similarity data\n        cmap = plt.get_cmap(colormap)\n        colored_similarity = cmap(similarity_data)[:, :, :3]  # Remove alpha channel\n\n        # Blend images\n        overlay_img = (1 - alpha) * base_img + alpha * colored_similarity\n\n        return np.clip(overlay_img, 0, 1)\n\n    def batch_similarity_analysis(\n        self,\n        input_path: str,\n        query_points: List[Tuple[float, float]],\n        output_dir: str,\n        window: Optional[Window] = None,\n        bands: Optional[List[int]] = None,\n        target_size: int = 896,\n    ) -&gt; List[Dict[str, np.ndarray]]:\n        \"\"\"Process multiple query points for similarity analysis.\n\n        Args:\n            input_path: Path to input GeoTIFF\n            query_points: List of (x, y) coordinates\n            output_dir: Output directory for results\n            window: Optional window for reading subset\n            bands: Optional list of bands to use\n            target_size: Target size for processing\n\n        Returns:\n            List of result dictionaries\n        \"\"\"\n        results = []\n        for i, coords in enumerate(query_points):\n            point_output_dir = os.path.join(output_dir, f\"point_{i}\")\n            result = self.compute_similarity(\n                source=input_path,\n                query_coords=coords,\n                output_dir=point_output_dir,\n                window=window,\n                bands=bands,\n                target_size=target_size,\n            )\n            results.append(result)\n\n        return results\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.__init__","title":"<code>__init__(model_name='dinov3_vitl16', weights_path=None, device=None)</code>","text":"<p>Initialize DINOv3 processor.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the DINOv3 model. Can be \"dinov3_vits16\", \"dinov3_vits16plus\", \"dinov3_vitb16\", \"dinov3_vitl16\", \"dinov3_vith16plus\", \"dinov3_vit7b16\", \"dinov3_convnext_tiny\", \"dinov3_convnext_small\", \"dinov3_convnext_base\", \"dinov3_convnext_large\", \"dinov3dinov3_vitl16\", and \"dinov3_vit7b16\".  See https://github.com/facebookresearch/dinov3 for more details.</p> <code>'dinov3_vitl16'</code> <code>weights_path</code> <code>Optional[str]</code> <p>Path to model weights (optional)</p> <code>None</code> <code>device</code> <code>Optional[device]</code> <p>Torch device to use</p> <code>None</code> Source code in <code>geoai/dinov3.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"dinov3_vitl16\",\n    weights_path: Optional[str] = None,\n    device: Optional[torch.device] = None,\n):\n    \"\"\"Initialize DINOv3 processor.\n\n    Args:\n        model_name: Name of the DINOv3 model. Can be \"dinov3_vits16\", \"dinov3_vits16plus\",\n            \"dinov3_vitb16\", \"dinov3_vitl16\", \"dinov3_vith16plus\", \"dinov3_vit7b16\", \"dinov3_convnext_tiny\",\n            \"dinov3_convnext_small\", \"dinov3_convnext_base\", \"dinov3_convnext_large\",\n            \"dinov3dinov3_vitl16\", and \"dinov3_vit7b16\".\n             See https://github.com/facebookresearch/dinov3 for more details.\n        weights_path: Path to model weights (optional)\n        device: Torch device to use\n    \"\"\"\n\n    dinov3_github_location = \"facebookresearch/dinov3\"\n\n    if os.getenv(\"DINOV3_LOCATION\") is not None:\n        dinov3_location = os.getenv(\"DINOV3_LOCATION\")\n    else:\n        dinov3_location = dinov3_github_location\n\n    self.dinov3_location = dinov3_location\n    self.dinov3_source = (\n        \"local\" if dinov3_location != dinov3_github_location else \"github\"\n    )\n\n    self.device = device or get_device()\n    self.model_name = model_name\n\n    # Add DINOv3 to path if needed\n    if dinov3_location != \"facebookresearch/dinov3\" and (\n        dinov3_location not in sys.path\n    ):\n        sys.path.append(dinov3_location)\n\n    # Load model\n    self.model = self._load_model(weights_path)\n    self.patch_size = self.model.patch_size\n    self.embed_dim = self.model.embed_dim\n\n    # Image transforms - satellite imagery normalization\n    self.transform = transforms.Compose(\n        [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.430, 0.411, 0.296),  # SAT-493M normalization\n                std=(0.213, 0.156, 0.143),\n            ),\n        ]\n    )\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.batch_similarity_analysis","title":"<code>batch_similarity_analysis(input_path, query_points, output_dir, window=None, bands=None, target_size=896)</code>","text":"<p>Process multiple query points for similarity analysis.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input GeoTIFF</p> required <code>query_points</code> <code>List[Tuple[float, float]]</code> <p>List of (x, y) coordinates</p> required <code>output_dir</code> <code>str</code> <p>Output directory for results</p> required <code>window</code> <code>Optional[Window]</code> <p>Optional window for reading subset</p> <code>None</code> <code>bands</code> <code>Optional[List[int]]</code> <p>Optional list of bands to use</p> <code>None</code> <code>target_size</code> <code>int</code> <p>Target size for processing</p> <code>896</code> <p>Returns:</p> Type Description <code>List[Dict[str, ndarray]]</code> <p>List of result dictionaries</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def batch_similarity_analysis(\n    self,\n    input_path: str,\n    query_points: List[Tuple[float, float]],\n    output_dir: str,\n    window: Optional[Window] = None,\n    bands: Optional[List[int]] = None,\n    target_size: int = 896,\n) -&gt; List[Dict[str, np.ndarray]]:\n    \"\"\"Process multiple query points for similarity analysis.\n\n    Args:\n        input_path: Path to input GeoTIFF\n        query_points: List of (x, y) coordinates\n        output_dir: Output directory for results\n        window: Optional window for reading subset\n        bands: Optional list of bands to use\n        target_size: Target size for processing\n\n    Returns:\n        List of result dictionaries\n    \"\"\"\n    results = []\n    for i, coords in enumerate(query_points):\n        point_output_dir = os.path.join(output_dir, f\"point_{i}\")\n        result = self.compute_similarity(\n            source=input_path,\n            query_coords=coords,\n            output_dir=point_output_dir,\n            window=window,\n            bands=bands,\n            target_size=target_size,\n        )\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.compute_patch_similarity","title":"<code>compute_patch_similarity(features, patch_x, patch_y)</code>","text":"<p>Compute cosine similarity between selected patch and all patches.</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def compute_patch_similarity(\n    self, features: torch.Tensor, patch_x: int, patch_y: int\n) -&gt; torch.Tensor:\n    \"\"\"Compute cosine similarity between selected patch and all patches.\"\"\"\n    h_patches, w_patches, embed_dim = features.shape\n\n    # Get query patch feature\n    query_feature = features[patch_y, patch_x]  # Shape: [embed_dim]\n\n    # Reshape features for batch computation\n    all_features = features.view(\n        -1, embed_dim\n    )  # Shape: [h_patches * w_patches, embed_dim]\n\n    # Compute cosine similarity\n    similarities = F.cosine_similarity(\n        query_feature.unsqueeze(0),  # Shape: [1, embed_dim]\n        all_features,  # Shape: [h_patches * w_patches, embed_dim]\n        dim=1,\n    )\n\n    # Reshape back to patch grid\n    similarities = similarities.view(h_patches, w_patches)\n\n    # Normalize to 0-1 range\n    similarities = (similarities + 1) / 2\n\n    return similarities\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.compute_similarity","title":"<code>compute_similarity(source=None, features=None, query_coords=None, output_dir=None, window=None, bands=None, target_size=896, save_features=False, coord_crs=None, use_interpolation=True)</code>","text":"<p>Process GeoTIFF for patch similarity analysis.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to input GeoTIFF or rasterio dataset</p> <code>None</code> <code>features</code> <code>Tensor</code> <p>Pre-extracted features (h_patches, w_patches, embed_dim)</p> <code>None</code> <code>query_coords</code> <code>Tuple[float, float]</code> <p>(x, y) coordinates in image pixel space or (lon, lat) in geographic space</p> <code>None</code> <code>output_dir</code> <code>str</code> <p>Output directory for results</p> <code>None</code> <code>window</code> <code>Optional[Window]</code> <p>Optional window for reading subset</p> <code>None</code> <code>bands</code> <code>Optional[List[int]]</code> <p>Optional list of bands to use</p> <code>None</code> <code>target_size</code> <code>int</code> <p>Target size for processing</p> <code>896</code> <code>save_features</code> <code>bool</code> <p>Whether to save extracted features</p> <code>False</code> <code>coord_crs</code> <code>str</code> <p>Coordinate CRS of the query coordinates</p> <code>None</code> <code>use_interpolation</code> <code>bool</code> <p>Whether to use interpolation when resizing similarity map</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing similarity results and metadata</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def compute_similarity(\n    self,\n    source: str = None,\n    features: torch.Tensor = None,\n    query_coords: Tuple[float, float] = None,\n    output_dir: str = None,\n    window: Optional[Window] = None,\n    bands: Optional[List[int]] = None,\n    target_size: int = 896,\n    save_features: bool = False,\n    coord_crs: str = None,\n    use_interpolation: bool = True,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Process GeoTIFF for patch similarity analysis.\n\n    Args:\n        source: Path to input GeoTIFF or rasterio dataset\n        features: Pre-extracted features (h_patches, w_patches, embed_dim)\n        query_coords: (x, y) coordinates in image pixel space or (lon, lat) in geographic space\n        output_dir: Output directory for results\n        window: Optional window for reading subset\n        bands: Optional list of bands to use\n        target_size: Target size for processing\n        save_features: Whether to save extracted features\n        coord_crs: Coordinate CRS of the query coordinates\n        use_interpolation: Whether to use interpolation when resizing similarity map\n\n    Returns:\n        Dictionary containing similarity results and metadata\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load image (GeoTIFF or regular image)\n    data, metadata = self.load_image(source, window, bands)\n    raw_img_w, raw_img_h = data.shape[-1], data.shape[-2]\n\n    # Preprocess for DINOv3\n    image = self.preprocess_image_for_dinov3(data, target_size)\n\n    # Extract features\n    if features is None:\n        features, h_patches, w_patches = self.extract_features(image)\n    else:\n        h_patches, w_patches = features.shape[:2]\n\n    # Convert coordinates to patch space\n    img_w, img_h = image.size\n    if len(query_coords) == 2:\n        # Assume pixel coordinates for now\n        if coord_crs is not None:\n            [query_coords] = coords_to_xy(source, [query_coords], coord_crs)\n\n        new_x = math.floor(query_coords[0] / raw_img_w * img_w)\n        new_y = math.floor(query_coords[1] / raw_img_h * img_h)\n        query_coords = [new_x, new_y]\n\n        x_pixel, y_pixel = query_coords\n        patch_x = math.floor((x_pixel / img_w) * w_patches)\n        patch_y = math.floor((y_pixel / img_h) * h_patches)\n\n        # Clamp to valid range\n        patch_x = max(0, min(w_patches - 1, patch_x))\n        patch_y = max(0, min(h_patches - 1, patch_y))\n\n    # Compute similarity\n    similarities = self.compute_patch_similarity(features, patch_x, patch_y)\n\n    # Prepare results\n    results = {\n        \"similarities\": similarities.cpu().numpy(),\n        \"patch_coords\": (patch_x, patch_y),\n        \"patch_grid_size\": (h_patches, w_patches),\n        \"image_size\": (img_w, img_h),\n        \"metadata\": metadata,\n    }\n\n    # Save similarity as GeoTIFF\n    sim_array = similarities.cpu().numpy()\n\n    # Resize similarity to original data dimensions\n    if use_interpolation:\n        try:\n            from skimage.transform import resize\n\n            sim_resized = resize(\n                sim_array,\n                (data.shape[-2], data.shape[-1]),\n                preserve_range=True,\n                anti_aliasing=True,\n            )\n        except ImportError:\n            # Fallback to PIL if scikit-image not available\n            from PIL import Image as PILImage\n\n            sim_pil = PILImage.fromarray((sim_array * 255).astype(np.uint8))\n            sim_pil = sim_pil.resize(\n                (data.shape[-1], data.shape[-2]), PILImage.LANCZOS\n            )\n            sim_resized = np.array(sim_pil, dtype=np.float32) / 255.0\n    else:\n        # Resize without interpolation (nearest neighbor)\n        try:\n            from skimage.transform import resize\n\n            sim_resized = resize(\n                sim_array,\n                (data.shape[-2], data.shape[-1]),\n                preserve_range=True,\n                anti_aliasing=False,\n                order=0,  # Nearest neighbor interpolation\n            )\n        except ImportError:\n            # Fallback to PIL with nearest neighbor\n            from PIL import Image as PILImage\n\n            sim_pil = PILImage.fromarray((sim_array * 255).astype(np.uint8))\n            sim_pil = sim_pil.resize(\n                (data.shape[-1], data.shape[-2]), PILImage.NEAREST\n            )\n            sim_resized = np.array(sim_pil, dtype=np.float32) / 255.0\n\n    # Save similarity map\n    if metadata[\"crs\"] is not None:\n        # Save as GeoTIFF for georeferenced data\n        similarity_path = os.path.join(\n            output_dir, f\"similarity_patch_{patch_x}_{patch_y}.tif\"\n        )\n        self.save_geotiff(\n            sim_resized[np.newaxis, :, :],\n            similarity_path,\n            metadata,\n            dtype=\"float32\",\n        )\n    else:\n        # Save as PNG for regular images\n        similarity_path = os.path.join(\n            output_dir, f\"similarity_patch_{patch_x}_{patch_y}.png\"\n        )\n        self.save_similarity_as_image(sim_resized, similarity_path)\n\n    image_dict = {\n        \"crs\": metadata[\"crs\"],\n        \"bounds\": metadata[\"bounds\"],\n        \"image\": sim_resized[np.newaxis, :, :],\n    }\n    results[\"image_dict\"] = image_dict\n\n    # Save features if requested\n    if save_features:\n        features_np = features.cpu().numpy()\n        features_path = os.path.join(\n            output_dir, f\"features_patch_{patch_x}_{patch_y}.npy\"\n        )\n        np.save(features_path, features_np)\n\n    # Save metadata\n    metadata_dict = {\n        \"input_path\": source,\n        \"query_coords\": query_coords,\n        \"patch_coords\": (patch_x, patch_y),\n        \"patch_grid_size\": (h_patches, w_patches),\n        \"image_size\": (img_w, img_h),\n        \"similarity_stats\": {\n            \"max\": float(sim_array.max()),\n            \"min\": float(sim_array.min()),\n            \"mean\": float(sim_array.mean()),\n            \"std\": float(sim_array.std()),\n        },\n    }\n\n    if save_features:\n        metadata_path = os.path.join(\n            output_dir, f\"metadata_patch_{patch_x}_{patch_y}.json\"\n        )\n        with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(metadata_dict, f, indent=2)\n\n        results[\"output_paths\"] = {\n            \"similarity\": similarity_path,\n            \"metadata\": metadata_path,\n            \"features\": features_path if save_features else None,\n        }\n\n    return results\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.create_similarity_overlay","title":"<code>create_similarity_overlay(source, similarity_data, colormap='turbo', alpha=0.7)</code>","text":"<p>Create an overlay of similarity map on original image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to original image</p> required <code>similarity_data</code> <code>ndarray</code> <p>2D similarity array</p> required <code>colormap</code> <code>str</code> <p>Colormap for similarity visualization</p> <code>'turbo'</code> <code>alpha</code> <code>float</code> <p>Transparency for overlay</p> <code>0.7</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>RGB overlay image as numpy array</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def create_similarity_overlay(\n    self,\n    source: str,\n    similarity_data: np.ndarray,\n    colormap: str = \"turbo\",\n    alpha: float = 0.7,\n) -&gt; np.ndarray:\n    \"\"\"Create an overlay of similarity map on original image.\n\n    Args:\n        source: Path to original image\n        similarity_data: 2D similarity array\n        colormap: Colormap for similarity visualization\n        alpha: Transparency for overlay\n\n    Returns:\n        RGB overlay image as numpy array\n    \"\"\"\n    # Load original image\n    data, _ = self.load_image(source)\n\n    # Convert to display format\n    if data.ndim == 3:\n        if data.shape[0] &lt;= 3:\n            display_img = np.transpose(data, (1, 2, 0))\n        else:\n            display_img = np.transpose(data[:3], (1, 2, 0))\n    else:\n        display_img = data\n\n    # Normalize image\n    if display_img.dtype != np.uint8:\n        if display_img.ndim == 3:\n            normalized_img = np.zeros_like(display_img, dtype=np.float32)\n            for i in range(display_img.shape[2]):\n                band = display_img[:, :, i]\n                p2, p98 = np.percentile(band, [2, 98])\n                normalized_img[:, :, i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n        else:\n            p2, p98 = np.percentile(display_img, [2, 98])\n            normalized_img = np.clip((display_img - p2) / (p98 - p2), 0, 1)\n        base_img = normalized_img\n    else:\n        base_img = display_img / 255.0\n\n    # Convert grayscale to RGB if needed\n    if base_img.ndim == 2:\n        base_img = np.stack([base_img] * 3, axis=2)\n\n    # Resize similarity data to match image\n    if similarity_data.shape != base_img.shape[:2]:\n        from PIL import Image as PILImage\n\n        sim_pil = PILImage.fromarray((similarity_data * 255).astype(np.uint8))\n        sim_pil = sim_pil.resize(\n            (base_img.shape[1], base_img.shape[0]), PILImage.LANCZOS\n        )\n        similarity_data = np.array(sim_pil, dtype=np.float32) / 255.0\n\n    # Apply colormap to similarity data\n    cmap = plt.get_cmap(colormap)\n    colored_similarity = cmap(similarity_data)[:, :, :3]  # Remove alpha channel\n\n    # Blend images\n    overlay_img = (1 - alpha) * base_img + alpha * colored_similarity\n\n    return np.clip(overlay_img, 0, 1)\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.extract_features","title":"<code>extract_features(image)</code>","text":"<p>Extract patch features from image.</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def extract_features(self, image: Image.Image) -&gt; Tuple[torch.Tensor, int, int]:\n    \"\"\"Extract patch features from image.\"\"\"\n\n    if isinstance(image, str):\n        image = Image.open(image)\n\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    # Transform image\n    img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n\n    with torch.no_grad():\n        # Extract features from last layer\n        features = self.model.get_intermediate_layers(\n            img_tensor, n=1, reshape=True, norm=True\n        )[\n            0\n        ]  # Shape: [1, embed_dim, h_patches, w_patches]\n\n    # Rearrange to [h_patches, w_patches, embed_dim]\n    features = features.squeeze(0).permute(1, 2, 0)\n    h_patches, w_patches = features.shape[:2]\n\n    return features, h_patches, w_patches\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.load_geotiff","title":"<code>load_geotiff(source, window=None, bands=None)</code>","text":"<p>Load GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, DatasetReader]</code> <p>Path to GeoTIFF file (str) or an open rasterio.DatasetReader</p> required <code>window</code> <code>Optional[Window]</code> <p>Rasterio window for reading subset</p> <code>None</code> <code>bands</code> <code>Optional[List[int]]</code> <p>List of bands to read (1-indexed)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Tuple of (image array, metadata)</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def load_geotiff(\n    self,\n    source: Union[str, DatasetReader],\n    window: Optional[Window] = None,\n    bands: Optional[List[int]] = None,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Load GeoTIFF file.\n\n    Args:\n        source: Path to GeoTIFF file (str) or an open rasterio.DatasetReader\n        window: Rasterio window for reading subset\n        bands: List of bands to read (1-indexed)\n\n    Returns:\n        Tuple of (image array, metadata)\n    \"\"\"\n    # Flag to determine if we need to close the dataset afterwards\n    should_close = False\n    if isinstance(source, str):\n        src = rasterio.open(source)\n        should_close = True\n    elif isinstance(source, DatasetReader):\n        src = source\n    else:\n        raise TypeError(\"source must be a str path or a rasterio.DatasetReader\")\n\n    try:\n        # Read specified bands or all bands\n        if bands:\n            data = src.read(bands, window=window)\n        else:\n            data = src.read(window=window)\n\n        # Get metadata\n        profile = src.profile.copy()\n        if window:\n            profile.update(\n                {\n                    \"height\": window.height,\n                    \"width\": window.width,\n                    \"transform\": src.window_transform(window),\n                }\n            )\n\n        metadata = {\n            \"profile\": profile,\n            \"crs\": src.crs,\n            \"transform\": profile[\"transform\"],\n            \"bounds\": (\n                src.bounds\n                if not window\n                else rasterio.windows.bounds(window, src.transform)\n            ),\n        }\n    finally:\n        if should_close:\n            src.close()\n\n    return data, metadata\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.load_image","title":"<code>load_image(source, window=None, bands=None)</code>","text":"<p>Load image file (GeoTIFF or regular image).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, DatasetReader]</code> <p>Path to image file (str) or an open rasterio.DatasetReader</p> required <code>window</code> <code>Optional[Window]</code> <p>Rasterio window for reading subset (only applies to GeoTIFF)</p> <code>None</code> <code>bands</code> <code>Optional[List[int]]</code> <p>List of bands to read (only applies to GeoTIFF)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Tuple of (image array, metadata)</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def load_image(\n    self,\n    source: Union[str, DatasetReader],\n    window: Optional[Window] = None,\n    bands: Optional[List[int]] = None,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Load image file (GeoTIFF or regular image).\n\n    Args:\n        source: Path to image file (str) or an open rasterio.DatasetReader\n        window: Rasterio window for reading subset (only applies to GeoTIFF)\n        bands: List of bands to read (only applies to GeoTIFF)\n\n    Returns:\n        Tuple of (image array, metadata)\n    \"\"\"\n    if isinstance(source, str):\n        # Check if it's a GeoTIFF file\n        try:\n            # Try to open with rasterio first\n            with rasterio.open(source) as src:\n                # If successful and has CRS, treat as GeoTIFF\n                if src.crs is not None:\n                    return self.load_geotiff(source, window, bands)\n                # If no CRS, it might be a regular image opened by rasterio\n                else:\n                    # Check file extension\n                    file_ext = source.lower().split(\".\")[-1]\n                    if file_ext in [\"tif\", \"tiff\"]:\n                        return self.load_geotiff(source, window, bands)\n                    else:\n                        return self.load_regular_image(source)\n        except (rasterio.RasterioIOError, rasterio.errors.RasterioIOError):\n            # If rasterio fails, try as regular image\n            return self.load_regular_image(source)\n    elif isinstance(source, DatasetReader):\n        # Already opened rasterio dataset\n        return self.load_geotiff(source, window, bands)\n    else:\n        raise TypeError(\"source must be a str path or a rasterio.DatasetReader\")\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.load_regular_image","title":"<code>load_regular_image(image_path)</code>","text":"<p>Load regular image file (PNG, JPG, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to image file</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, dict]</code> <p>Tuple of (image array, metadata)</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def load_regular_image(\n    self,\n    image_path: str,\n) -&gt; Tuple[np.ndarray, dict]:\n    \"\"\"Load regular image file (PNG, JPG, etc.).\n\n    Args:\n        image_path: Path to image file\n\n    Returns:\n        Tuple of (image array, metadata)\n    \"\"\"\n    try:\n        # Load image using PIL\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Convert to numpy array (H, W, C)\n        img_array = np.array(image)\n\n        # Convert to (C, H, W) format to match GeoTIFF format\n        data = np.transpose(img_array, (2, 0, 1)).astype(np.uint8)\n\n        # Create basic metadata\n        height, width = img_array.shape[:2]\n        metadata = {\n            \"profile\": {\n                \"driver\": \"PNG\",\n                \"dtype\": \"uint8\",\n                \"nodata\": None,\n                \"width\": width,\n                \"height\": height,\n                \"count\": 3,\n                \"crs\": None,\n                \"transform\": None,\n            },\n            \"crs\": None,\n            \"transform\": None,\n            \"bounds\": (0, 0, width, height),\n        }\n\n        return data, metadata\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load image {image_path}: {e}\")\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.preprocess_image_for_dinov3","title":"<code>preprocess_image_for_dinov3(data, target_size=896, normalize_percentile=True)</code>","text":"<p>Preprocess image data for DINOv3.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input array (C, H, W) or (H, W)</p> required <code>target_size</code> <code>int</code> <p>Target size for resizing</p> <code>896</code> <code>normalize_percentile</code> <code>bool</code> <p>Whether to normalize using percentiles</p> <code>True</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image ready for DINOv3</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def preprocess_image_for_dinov3(\n    self,\n    data: np.ndarray,\n    target_size: int = 896,\n    normalize_percentile: bool = True,\n) -&gt; Image.Image:\n    \"\"\"Preprocess image data for DINOv3.\n\n    Args:\n        data: Input array (C, H, W) or (H, W)\n        target_size: Target size for resizing\n        normalize_percentile: Whether to normalize using percentiles\n\n    Returns:\n        PIL Image ready for DINOv3\n    \"\"\"\n    # Handle different input shapes\n    if data.ndim == 2:\n        data = data[np.newaxis, :, :]  # Add channel dimension\n    elif data.ndim == 3 and data.shape[0] &gt; 3:\n        # Take first 3 bands if more than 3 channels\n        data = data[:3, :, :]\n\n    # Normalize data\n    if normalize_percentile:\n        # Normalize each band using percentiles\n        normalized_data = np.zeros_like(data, dtype=np.float32)\n        for i in range(data.shape[0]):\n            band = data[i]\n            p2, p98 = np.percentile(band, [2, 98])\n            normalized_data[i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n    else:\n        # Simple min-max normalization\n        normalized_data = (data - data.min()) / (data.max() - data.min())\n\n    # Convert to PIL Image\n    if normalized_data.shape[0] == 1:\n        # Grayscale - repeat to 3 channels\n        img_array = np.repeat(normalized_data[0], 3, axis=0)\n    else:\n        img_array = normalized_data\n\n    # Transpose to HWC format and convert to uint8\n    img_array = np.transpose(img_array, (1, 2, 0))\n    img_array = (img_array * 255).astype(np.uint8)\n\n    # Create PIL Image\n    image = Image.fromarray(img_array)\n\n    # Resize to patch-aligned dimensions\n    return self.resize_to_patch_aligned(image, target_size)\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.resize_to_patch_aligned","title":"<code>resize_to_patch_aligned(image, target_size=896)</code>","text":"<p>Resize image to be aligned with patch grid.</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def resize_to_patch_aligned(\n    self, image: Image.Image, target_size: int = 896\n) -&gt; Image.Image:\n    \"\"\"Resize image to be aligned with patch grid.\"\"\"\n    w, h = image.size\n\n    # Calculate new dimensions that are multiples of patch_size\n    if w &gt; h:\n        new_h = target_size\n        new_w = int((w * target_size) / h)\n    else:\n        new_w = target_size\n        new_h = int((h * target_size) / w)\n\n    # Round to nearest multiple of patch_size\n    new_h = ((new_h + self.patch_size - 1) // self.patch_size) * self.patch_size\n    new_w = ((new_w + self.patch_size - 1) // self.patch_size) * self.patch_size\n\n    return image.resize((new_w, new_h), Image.Resampling.LANCZOS)\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.save_geotiff","title":"<code>save_geotiff(data, output_path, metadata, dtype='float32')</code>","text":"<p>Save array as GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Array to save</p> required <code>output_path</code> <code>str</code> <p>Output file path</p> required <code>metadata</code> <code>dict</code> <p>Metadata from original file</p> required <code>dtype</code> <code>str</code> <p>Output data type</p> <code>'float32'</code> Source code in <code>geoai/dinov3.py</code> <pre><code>def save_geotiff(\n    self, data: np.ndarray, output_path: str, metadata: dict, dtype: str = \"float32\"\n) -&gt; None:\n    \"\"\"Save array as GeoTIFF.\n\n    Args:\n        data: Array to save\n        output_path: Output file path\n        metadata: Metadata from original file\n        dtype: Output data type\n    \"\"\"\n    profile = metadata[\"profile\"].copy()\n    profile.update(\n        {\n            \"dtype\": dtype,\n            \"count\": data.shape[0] if data.ndim == 3 else 1,\n            \"height\": data.shape[-2] if data.ndim &gt;= 2 else data.shape[0],\n            \"width\": data.shape[-1] if data.ndim &gt;= 2 else 1,\n        }\n    )\n\n    with rasterio.open(output_path, \"w\", **profile) as dst:\n        if data.ndim == 2:\n            dst.write(data, 1)\n        else:\n            dst.write(data)\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.save_similarity_as_image","title":"<code>save_similarity_as_image(similarity_data, output_path, colormap='turbo')</code>","text":"<p>Save similarity array as PNG image with colormap.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_data</code> <code>ndarray</code> <p>2D similarity array</p> required <code>output_path</code> <code>str</code> <p>Output file path</p> required <code>colormap</code> <code>str</code> <p>Matplotlib colormap name</p> <code>'turbo'</code> Source code in <code>geoai/dinov3.py</code> <pre><code>def save_similarity_as_image(\n    self, similarity_data: np.ndarray, output_path: str, colormap: str = \"turbo\"\n) -&gt; None:\n    \"\"\"Save similarity array as PNG image with colormap.\n\n    Args:\n        similarity_data: 2D similarity array\n        output_path: Output file path\n        colormap: Matplotlib colormap name\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Apply colormap\n    cmap = plt.get_cmap(colormap)\n    colored_data = cmap(similarity_data)\n\n    # Convert to uint8 image (remove alpha channel)\n    img_data = (colored_data[..., :3] * 255).astype(np.uint8)\n\n    # Save as PNG\n    img = Image.fromarray(img_data)\n    img.save(output_path)\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.visualize_patches","title":"<code>visualize_patches(image, features, patch_coords, add_text=False, figsize=(12, 8), save_path=None)</code>","text":"<p>Visualize image with patch grid and highlight selected patch.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>PIL Image</p> required <code>features</code> <code>Tensor</code> <p>Feature tensor (h_patches, w_patches, embed_dim)</p> required <code>patch_coords</code> <code>Tuple[int, int]</code> <p>Selected patch coordinates (patch_x, patch_y)</p> required <code>add_text</code> <code>bool</code> <p>Whether to add text to the patch</p> <code>False</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size</p> <code>(12, 8)</code> <code>save_path</code> <code>str</code> <p>Optional path to save visualization</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure object</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def visualize_patches(\n    self,\n    image: Image.Image,\n    features: torch.Tensor,\n    patch_coords: Tuple[int, int],\n    add_text: bool = False,\n    figsize: Tuple[int, int] = (12, 8),\n    save_path: str = None,\n) -&gt; plt.Figure:\n    \"\"\"Visualize image with patch grid and highlight selected patch.\n\n    Args:\n        image: PIL Image\n        features: Feature tensor (h_patches, w_patches, embed_dim)\n        patch_coords: Selected patch coordinates (patch_x, patch_y)\n        add_text: Whether to add text to the patch\n        figsize: Figure size\n        save_path: Optional path to save visualization\n\n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    # Display image\n    ax.imshow(image)\n    ax.set_title(\"Image with Patch Grid\")\n    ax.axis(\"off\")\n\n    # Get dimensions\n    img_w, img_h = image.size\n    h_patches, w_patches = features.shape[:2]\n    patch_x, patch_y = patch_coords\n\n    # Calculate patch size in pixels\n    patch_w = img_w / w_patches\n    patch_h = img_h / h_patches\n\n    # Draw patch grid\n    for i in range(w_patches + 1):\n        x = i * patch_w\n        ax.axvline(x=x, color=\"white\", alpha=0.3, linewidth=0.5)\n\n    for i in range(h_patches + 1):\n        y = i * patch_h\n        ax.axhline(y=y, color=\"white\", alpha=0.3, linewidth=0.5)\n\n    # Highlight selected patch\n    rect_x = patch_x * patch_w\n    rect_y = patch_y * patch_h\n    rect = patches.Rectangle(\n        (rect_x, rect_y),\n        patch_w,\n        patch_h,\n        linewidth=3,\n        edgecolor=\"red\",\n        facecolor=\"none\",\n    )\n    ax.add_patch(rect)\n\n    # Add patch coordinate text\n    if add_text:\n        ax.text(\n            rect_x + patch_w / 2,\n            rect_y + patch_h / 2,\n            f\"({patch_x}, {patch_y})\",\n            color=\"red\",\n            fontsize=12,\n            ha=\"center\",\n            va=\"center\",\n            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8),\n        )\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n\n    return fig\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.DINOv3GeoProcessor.visualize_similarity","title":"<code>visualize_similarity(source, similarity_data, query_coords=None, patch_coords=None, figsize=(15, 6), colormap='turbo', alpha=0.7, save_path=None, show_query_point=True, overlay=False)</code>","text":"<p>Visualize original image and similarity map side by side or as overlay.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to original image</p> required <code>similarity_data</code> <code>ndarray</code> <p>2D similarity array</p> required <code>query_coords</code> <code>Tuple[float, float]</code> <p>Query coordinates in pixel space (x, y)</p> <code>None</code> <code>patch_coords</code> <code>Tuple[int, int]</code> <p>Patch coordinates (patch_x, patch_y) for marking query patch</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for visualization</p> <code>(15, 6)</code> <code>colormap</code> <code>str</code> <p>Colormap for similarity visualization</p> <code>'turbo'</code> <code>alpha</code> <code>float</code> <p>Transparency for overlay mode</p> <code>0.7</code> <code>save_path</code> <code>str</code> <p>Optional path to save the visualization</p> <code>None</code> <code>show_query_point</code> <code>bool</code> <p>Whether to show the query point marker</p> <code>True</code> <code>overlay</code> <code>bool</code> <p>If True, overlay similarity on original image; if False, show side by side</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Matplotlib figure object</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def visualize_similarity(\n    self,\n    source: str,\n    similarity_data: np.ndarray,\n    query_coords: Tuple[float, float] = None,\n    patch_coords: Tuple[int, int] = None,\n    figsize: Tuple[int, int] = (15, 6),\n    colormap: str = \"turbo\",\n    alpha: float = 0.7,\n    save_path: str = None,\n    show_query_point: bool = True,\n    overlay: bool = False,\n) -&gt; plt.Figure:\n    \"\"\"Visualize original image and similarity map side by side or as overlay.\n\n    Args:\n        source: Path to original image\n        similarity_data: 2D similarity array\n        query_coords: Query coordinates in pixel space (x, y)\n        patch_coords: Patch coordinates (patch_x, patch_y) for marking query patch\n        figsize: Figure size for visualization\n        colormap: Colormap for similarity visualization\n        alpha: Transparency for overlay mode\n        save_path: Optional path to save the visualization\n        show_query_point: Whether to show the query point marker\n        overlay: If True, overlay similarity on original image; if False, show side by side\n\n    Returns:\n        Matplotlib figure object\n    \"\"\"\n    # Load original image\n    data, metadata = self.load_image(source)\n\n    # Convert image data to displayable format\n    if data.ndim == 3:\n        if data.shape[0] &lt;= 3:\n            # Standard RGB/grayscale image (C, H, W)\n            display_img = np.transpose(data, (1, 2, 0))\n        else:\n            # Multi-band image, take first 3 bands\n            display_img = np.transpose(data[:3], (1, 2, 0))\n    else:\n        # Single band image\n        display_img = data\n\n    # Normalize image for display\n    if display_img.dtype != np.uint8:\n        # Normalize using percentiles\n        if display_img.ndim == 3:\n            normalized_img = np.zeros_like(display_img, dtype=np.float32)\n            for i in range(display_img.shape[2]):\n                band = display_img[:, :, i]\n                p2, p98 = np.percentile(band, [2, 98])\n                normalized_img[:, :, i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n        else:\n            p2, p98 = np.percentile(display_img, [2, 98])\n            normalized_img = np.clip((display_img - p2) / (p98 - p2), 0, 1)\n        display_img = normalized_img\n    else:\n        display_img = display_img / 255.0\n\n    # Ensure similarity data matches image dimensions\n    if similarity_data.shape != display_img.shape[:2]:\n        from PIL import Image as PILImage\n\n        sim_pil = PILImage.fromarray((similarity_data * 255).astype(np.uint8))\n        sim_pil = sim_pil.resize(\n            (display_img.shape[1], display_img.shape[0]), PILImage.LANCZOS\n        )\n        similarity_data = np.array(sim_pil, dtype=np.float32) / 255.0\n\n    if overlay:\n        # Single plot with overlay\n        fig, ax = plt.subplots(1, 1, figsize=(figsize[1], figsize[1]))\n\n        # Show original image\n        if display_img.ndim == 2:\n            ax.imshow(display_img, cmap=\"gray\")\n        else:\n            ax.imshow(display_img)\n\n        # Overlay similarity map\n        im_sim = ax.imshow(\n            similarity_data, cmap=colormap, alpha=alpha, vmin=0, vmax=1\n        )\n\n        # Add colorbar for similarity\n        cbar = plt.colorbar(im_sim, ax=ax, fraction=0.046, pad=0.04)\n        cbar.set_label(\"Similarity\", rotation=270, labelpad=20)\n\n        ax.set_title(\"Image with Similarity Overlay\")\n\n    else:\n        # Side-by-side visualization\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n\n        # Original image\n        if display_img.ndim == 2:\n            ax1.imshow(display_img, cmap=\"gray\")\n        else:\n            ax1.imshow(display_img)\n        ax1.set_title(\"Original Image\")\n        ax1.axis(\"off\")\n\n        # Similarity map\n        im_sim = ax2.imshow(similarity_data, cmap=colormap, vmin=0, vmax=1)\n        ax2.set_title(\"Similarity Map\")\n        ax2.axis(\"off\")\n\n        # Add colorbar\n        cbar = plt.colorbar(im_sim, ax=ax2, fraction=0.046, pad=0.04)\n        cbar.set_label(\"Similarity\", rotation=270, labelpad=20)\n\n    # Mark query point if provided\n    if show_query_point and query_coords is not None:\n        x, y = query_coords\n        if overlay:\n            ax.plot(\n                x,\n                y,\n                \"r*\",\n                markersize=15,\n                markeredgecolor=\"white\",\n                markeredgewidth=2,\n            )\n            ax.plot(x, y, \"r*\", markersize=12)\n        else:\n            ax1.plot(\n                x,\n                y,\n                \"r*\",\n                markersize=15,\n                markeredgecolor=\"white\",\n                markeredgewidth=2,\n            )\n            ax1.plot(x, y, \"r*\", markersize=12)\n            ax2.plot(\n                x,\n                y,\n                \"r*\",\n                markersize=15,\n                markeredgecolor=\"white\",\n                markeredgewidth=2,\n            )\n            ax2.plot(x, y, \"r*\", markersize=12)\n\n    plt.tight_layout()\n\n    # Save if path provided\n    if save_path:\n        plt.savefig(save_path, dpi=150, bbox_inches=\"tight\")\n\n    return fig\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.analyze_image_patches","title":"<code>analyze_image_patches(input_image, query_points, output_dir, model_name='dinov3_vitl16', weights_path=None)</code>","text":"<p>Analyze multiple patches in an image file.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>str</code> <p>Path to input image file (GeoTIFF, PNG, JPG, etc.)</p> required <code>query_points</code> <code>List[Tuple[float, float]]</code> <p>List of query coordinates</p> required <code>output_dir</code> <code>str</code> <p>Output directory</p> required <code>model_name</code> <code>str</code> <p>DINOv3 model name</p> <code>'dinov3_vitl16'</code> <code>weights_path</code> <code>Optional[str]</code> <p>Optional path to model weights</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, ndarray]]</code> <p>List of result dictionaries</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def analyze_image_patches(\n    input_image: str,\n    query_points: List[Tuple[float, float]],\n    output_dir: str,\n    model_name: str = \"dinov3_vitl16\",\n    weights_path: Optional[str] = None,\n) -&gt; List[Dict[str, np.ndarray]]:\n    \"\"\"Analyze multiple patches in an image file.\n\n    Args:\n        input_image: Path to input image file (GeoTIFF, PNG, JPG, etc.)\n        query_points: List of query coordinates\n        output_dir: Output directory\n        model_name: DINOv3 model name\n        weights_path: Optional path to model weights\n\n    Returns:\n        List of result dictionaries\n    \"\"\"\n    processor = DINOv3GeoProcessor(model_name=model_name, weights_path=weights_path)\n\n    return processor.batch_similarity_analysis(input_image, query_points, output_dir)\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.create_similarity_map","title":"<code>create_similarity_map(input_image, query_coords, output_dir, model_name='dinov3_vitl16', weights_path=None, window=None, bands=None, target_size=896, save_features=False, coord_crs=None, use_interpolation=True)</code>","text":"<p>Convenience function to create similarity map from image file.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>str</code> <p>Path to input image file (GeoTIFF, PNG, JPG, etc.)</p> required <code>query_coords</code> <code>Tuple[float, float]</code> <p>Query coordinates (x, y) in pixel space</p> required <code>output_dir</code> <code>str</code> <p>Output directory</p> required <code>model_name</code> <code>str</code> <p>DINOv3 model name</p> <code>'dinov3_vitl16'</code> <code>weights_path</code> <code>Optional[str]</code> <p>Optional path to model weights</p> <code>None</code> <code>window</code> <code>Optional[Window]</code> <p>Optional rasterio window (only applies to GeoTIFF)</p> <code>None</code> <code>bands</code> <code>Optional[List[int]]</code> <p>Optional list of bands to use (only applies to GeoTIFF)</p> <code>None</code> <code>target_size</code> <code>int</code> <p>Target size for processing</p> <code>896</code> <code>save_features</code> <code>bool</code> <p>Whether to save extracted features</p> <code>False</code> <code>coord_crs</code> <code>str</code> <p>Coordinate CRS of the query coordinates (only applies to GeoTIFF)</p> <code>None</code> <code>use_interpolation</code> <code>bool</code> <p>Whether to use interpolation when resizing similarity map</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing results</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def create_similarity_map(\n    input_image: str,\n    query_coords: Tuple[float, float],\n    output_dir: str,\n    model_name: str = \"dinov3_vitl16\",\n    weights_path: Optional[str] = None,\n    window: Optional[Window] = None,\n    bands: Optional[List[int]] = None,\n    target_size: int = 896,\n    save_features: bool = False,\n    coord_crs: str = None,\n    use_interpolation: bool = True,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"Convenience function to create similarity map from image file.\n\n    Args:\n        input_image: Path to input image file (GeoTIFF, PNG, JPG, etc.)\n        query_coords: Query coordinates (x, y) in pixel space\n        output_dir: Output directory\n        model_name: DINOv3 model name\n        weights_path: Optional path to model weights\n        window: Optional rasterio window (only applies to GeoTIFF)\n        bands: Optional list of bands to use (only applies to GeoTIFF)\n        target_size: Target size for processing\n        save_features: Whether to save extracted features\n        coord_crs: Coordinate CRS of the query coordinates (only applies to GeoTIFF)\n        use_interpolation: Whether to use interpolation when resizing similarity map\n\n    Returns:\n        Dictionary containing results\n    \"\"\"\n    processor = DINOv3GeoProcessor(model_name=model_name, weights_path=weights_path)\n\n    return processor.compute_similarity(\n        source=input_image,\n        query_coords=query_coords,\n        output_dir=output_dir,\n        window=window,\n        bands=bands,\n        target_size=target_size,\n        save_features=save_features,\n        coord_crs=coord_crs,\n        use_interpolation=use_interpolation,\n    )\n</code></pre>"},{"location":"dinov3/#geoai.dinov3.visualize_similarity_results","title":"<code>visualize_similarity_results(input_image, query_coords, output_dir=None, model_name='dinov3_vitl16', weights_path=None, figsize=(15, 6), colormap='turbo', alpha=0.7, save_path=None, show_query_point=True, overlay=False, target_size=896, coord_crs=None, use_interpolation=True)</code>","text":"<p>Create similarity map and visualize results in one function.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>str</code> <p>Path to input image file (GeoTIFF, PNG, JPG, etc.)</p> required <code>query_coords</code> <code>Tuple[float, float]</code> <p>Query coordinates (x, y) in pixel space</p> required <code>output_dir</code> <code>str</code> <p>Output directory for similarity map files (optional)</p> <code>None</code> <code>model_name</code> <code>str</code> <p>DINOv3 model name</p> <code>'dinov3_vitl16'</code> <code>weights_path</code> <code>Optional[str]</code> <p>Optional path to model weights</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size for visualization</p> <code>(15, 6)</code> <code>colormap</code> <code>str</code> <p>Colormap for similarity visualization</p> <code>'turbo'</code> <code>alpha</code> <code>float</code> <p>Transparency for overlay mode</p> <code>0.7</code> <code>save_path</code> <code>str</code> <p>Optional path to save the visualization</p> <code>None</code> <code>show_query_point</code> <code>bool</code> <p>Whether to show the query point marker</p> <code>True</code> <code>overlay</code> <code>bool</code> <p>If True, overlay similarity on original image; if False, show side by side</p> <code>False</code> <code>target_size</code> <code>int</code> <p>Target size for processing</p> <code>896</code> <code>coord_crs</code> <code>str</code> <p>Coordinate CRS of the query coordinates</p> <code>None</code> <code>use_interpolation</code> <code>bool</code> <p>Whether to use interpolation when resizing similarity map</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary containing similarity results, metadata, and matplotlib figure</p> Source code in <code>geoai/dinov3.py</code> <pre><code>def visualize_similarity_results(\n    input_image: str,\n    query_coords: Tuple[float, float],\n    output_dir: str = None,\n    model_name: str = \"dinov3_vitl16\",\n    weights_path: Optional[str] = None,\n    figsize: Tuple[int, int] = (15, 6),\n    colormap: str = \"turbo\",\n    alpha: float = 0.7,\n    save_path: str = None,\n    show_query_point: bool = True,\n    overlay: bool = False,\n    target_size: int = 896,\n    coord_crs: str = None,\n    use_interpolation: bool = True,\n) -&gt; Dict:\n    \"\"\"Create similarity map and visualize results in one function.\n\n    Args:\n        input_image: Path to input image file (GeoTIFF, PNG, JPG, etc.)\n        query_coords: Query coordinates (x, y) in pixel space\n        output_dir: Output directory for similarity map files (optional)\n        model_name: DINOv3 model name\n        weights_path: Optional path to model weights\n        figsize: Figure size for visualization\n        colormap: Colormap for similarity visualization\n        alpha: Transparency for overlay mode\n        save_path: Optional path to save the visualization\n        show_query_point: Whether to show the query point marker\n        overlay: If True, overlay similarity on original image; if False, show side by side\n        target_size: Target size for processing\n        coord_crs: Coordinate CRS of the query coordinates\n        use_interpolation: Whether to use interpolation when resizing similarity map\n\n    Returns:\n        Dictionary containing similarity results, metadata, and matplotlib figure\n    \"\"\"\n    processor = DINOv3GeoProcessor(model_name=model_name, weights_path=weights_path)\n\n    # Create temporary output directory if not provided\n    if output_dir is None:\n        import tempfile\n\n        output_dir = tempfile.mkdtemp(prefix=\"dinov3_similarity_\")\n\n    # Compute similarity\n    results = processor.compute_similarity(\n        source=input_image,\n        query_coords=query_coords,\n        output_dir=output_dir,\n        target_size=target_size,\n        coord_crs=coord_crs,\n        use_interpolation=use_interpolation,\n    )\n\n    # Get similarity data from results\n    similarity_data = results[\"image_dict\"][\"image\"][0]  # Remove channel dimension\n\n    # Create visualization\n    fig = processor.visualize_similarity(\n        source=input_image,\n        similarity_data=similarity_data,\n        query_coords=query_coords,\n        patch_coords=results[\"patch_coords\"],\n        figsize=figsize,\n        colormap=colormap,\n        alpha=alpha,\n        save_path=save_path,\n        show_query_point=show_query_point,\n        overlay=overlay,\n    )\n\n    # Add figure to results\n    results[\"visualization\"] = fig\n\n    return results\n</code></pre>"},{"location":"download/","title":"download module","text":"<p>This module provides functions to download data, including NAIP imagery and building data from Overture Maps.</p>"},{"location":"download/#geoai.download.convert_vector_format","title":"<code>convert_vector_format(input_file, output_format='geojson', filter_expression=None)</code>","text":"<p>Convert the downloaded data to a different format or filter it.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the input file.</p> required <code>output_format</code> <code>str</code> <p>Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".</p> <code>'geojson'</code> <code>filter_expression</code> <code>Optional[str]</code> <p>Optional GeoDataFrame query expression to filter the data.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the converted file.</p> Source code in <code>geoai/download.py</code> <pre><code>def convert_vector_format(\n    input_file: str,\n    output_format: str = \"geojson\",\n    filter_expression: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Convert the downloaded data to a different format or filter it.\n\n    Args:\n        input_file: Path to the input file.\n        output_format: Format to convert to, one of \"geojson\", \"parquet\", \"shapefile\", \"csv\".\n        filter_expression: Optional GeoDataFrame query expression to filter the data.\n\n    Returns:\n        Path to the converted file.\n    \"\"\"\n    try:\n        # Read the input file\n        logger.info(f\"Reading {input_file}\")\n        gdf = gpd.read_file(input_file)\n\n        # Apply filter if specified\n        if filter_expression:\n            logger.info(f\"Filtering data using expression: {filter_expression}\")\n            gdf = gdf.query(filter_expression)\n            logger.info(f\"After filtering: {len(gdf)} features\")\n\n        # Define output file path\n        base_path = os.path.splitext(input_file)[0]\n\n        if output_format == \"geojson\":\n            output_file = f\"{base_path}.geojson\"\n            logger.info(f\"Converting to GeoJSON: {output_file}\")\n            gdf.to_file(output_file, driver=\"GeoJSON\")\n        elif output_format == \"parquet\":\n            output_file = f\"{base_path}.parquet\"\n            logger.info(f\"Converting to Parquet: {output_file}\")\n            gdf.to_parquet(output_file)\n        elif output_format == \"shapefile\":\n            output_file = f\"{base_path}.shp\"\n            logger.info(f\"Converting to Shapefile: {output_file}\")\n            gdf.to_file(output_file)\n        elif output_format == \"csv\":\n            output_file = f\"{base_path}.csv\"\n            logger.info(f\"Converting to CSV: {output_file}\")\n\n            # For CSV, we need to convert geometry to WKT\n            gdf[\"geometry_wkt\"] = gdf.geometry.apply(lambda g: g.wkt)\n\n            # Save to CSV with geometry as WKT\n            gdf.drop(columns=[\"geometry\"]).to_csv(output_file, index=False)\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        return output_file\n\n    except Exception as e:\n        logger.error(f\"Error converting data: {str(e)}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.download_naip","title":"<code>download_naip(bbox, output_dir, year=None, max_items=10, overwrite=False, preview=False, **kwargs)</code>","text":"<p>Download NAIP imagery from Planetary Computer based on a bounding box.</p> <p>This function searches for NAIP (National Agriculture Imagery Program) imagery from Microsoft's Planetary Computer that intersects with the specified bounding box. It downloads the imagery and saves it as GeoTIFF files.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save the downloaded imagery.</p> required <code>year</code> <code>Optional[int]</code> <p>Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.</p> <code>None</code> <code>max_items</code> <code>int</code> <p>Maximum number of items to download.</p> <code>10</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing files with the same name.</p> <code>False</code> <code>preview</code> <code>bool</code> <p>If True, display a preview of the downloaded imagery.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of downloaded file paths.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is an error downloading or saving the imagery.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_naip(\n    bbox: Tuple[float, float, float, float],\n    output_dir: str,\n    year: Optional[int] = None,\n    max_items: int = 10,\n    overwrite: bool = False,\n    preview: bool = False,\n    **kwargs: Any,\n) -&gt; List[str]:\n    \"\"\"Download NAIP imagery from Planetary Computer based on a bounding box.\n\n    This function searches for NAIP (National Agriculture Imagery Program) imagery\n    from Microsoft's Planetary Computer that intersects with the specified bounding box.\n    It downloads the imagery and saves it as GeoTIFF files.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output_dir: Directory to save the downloaded imagery.\n        year: Specific year of NAIP imagery to download (e.g., 2020). If None, returns imagery from all available years.\n        max_items: Maximum number of items to download.\n        overwrite: If True, overwrite existing files with the same name.\n        preview: If True, display a preview of the downloaded imagery.\n\n    Returns:\n        List of downloaded file paths.\n\n    Raises:\n        Exception: If there is an error downloading or saving the imagery.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Create a geometry from the bounding box\n    geometry = box(*bbox)\n\n    # Connect to Planetary Computer STAC API\n    catalog = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n\n    # Build query for NAIP data\n    search_params = {\n        \"collections\": [\"naip\"],\n        \"intersects\": geometry,\n        \"limit\": max_items,\n    }\n\n    # Add year filter if specified\n    if year:\n        search_params[\"query\"] = {\"naip:year\": {\"eq\": year}}\n\n    for key, value in kwargs.items():\n        search_params[key] = value\n\n    # Search for NAIP imagery\n    search_results = catalog.search(**search_params)\n    items = list(search_results.items())\n\n    if len(items) &gt; max_items:\n        items = items[:max_items]\n\n    if not items:\n        print(\"No NAIP imagery found for the specified region and parameters.\")\n        return []\n\n    print(f\"Found {len(items)} NAIP items.\")\n\n    # Download and save each item\n    downloaded_files = []\n    for i, item in enumerate(items):\n        # Sign the assets (required for Planetary Computer)\n        signed_item = pc.sign(item)\n\n        # Get the RGB asset URL\n        rgb_asset = signed_item.assets.get(\"image\")\n        if not rgb_asset:\n            print(f\"No RGB asset found for item {i+1}\")\n            continue\n\n        # Use the original filename from the asset\n        original_filename = os.path.basename(\n            rgb_asset.href.split(\"?\")[0]\n        )  # Remove query parameters\n        output_path = os.path.join(output_dir, original_filename)\n        if not overwrite and os.path.exists(output_path):\n            print(f\"Skipping existing file: {output_path}\")\n            downloaded_files.append(output_path)\n            continue\n\n        print(f\"Downloading item {i+1}/{len(items)}: {original_filename}\")\n\n        try:\n            # Open and save the data with progress bar\n            # For direct file download with progress bar\n            if rgb_asset.href.startswith(\"http\"):\n                download_with_progress(rgb_asset.href, output_path)\n                #\n            else:\n                # Fallback to direct rioxarray opening (less common case)\n                data = rxr.open_rasterio(rgb_asset.href)\n                data.rio.to_raster(output_path)\n\n            downloaded_files.append(output_path)\n            print(f\"Successfully saved to {output_path}\")\n\n            # Optional: Display a preview (uncomment if needed)\n            if preview:\n                data = rxr.open_rasterio(output_path)\n                preview_raster(data)\n\n        except Exception as e:\n            print(f\"Error downloading item {i+1}: {str(e)}\")\n\n    return downloaded_files\n</code></pre>"},{"location":"download/#geoai.download.download_overture_buildings","title":"<code>download_overture_buildings(bbox, output, overture_type='building', **kwargs)</code>","text":"<p>Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.</p> required <code>output</code> <code>str</code> <p>Path to save the output file.</p> required <code>overture_type</code> <code>str</code> <p>The Overture Maps data type to download (building, place, etc.).</p> <code>'building'</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_overture_buildings(\n    bbox: Tuple[float, float, float, float],\n    output: str,\n    overture_type: str = \"building\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Download building data from Overture Maps for a given bounding box using the overturemaps CLI tool.\n\n    Args:\n        bbox: Bounding box in the format (min_lon, min_lat, max_lon, max_lat) in WGS84 coordinates.\n        output: Path to save the output file.\n        overture_type: The Overture Maps data type to download (building, place, etc.).\n\n    Returns:\n        Path to the output file.\n    \"\"\"\n\n    return get_overture_data(\n        overture_type=overture_type, bbox=bbox, output=output, **kwargs\n    )\n</code></pre>"},{"location":"download/#geoai.download.download_pc_stac_item","title":"<code>download_pc_stac_item(item_url, bands=None, output_dir=None, show_progress=True, merge_bands=False, merged_filename=None, overwrite=False, cell_size=None)</code>","text":"<p>Downloads a STAC item from Microsoft Planetary Computer with specified bands.</p> <p>This function fetches a STAC item by URL, signs the assets using Planetary Computer credentials, and downloads the specified bands with a progress bar. Can optionally merge bands into a single multi-band GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>item_url</code> <code>str</code> <p>The URL of the STAC item to download.</p> required <code>bands</code> <code>list</code> <p>List of specific bands to download (e.g., ['B01', 'B02']).                    If None, all available bands will be downloaded.</p> <code>None</code> <code>output_dir</code> <code>str</code> <p>Directory to save downloaded bands. If None,                        bands are returned as xarray DataArrays.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to display a progress bar. Default is True.</p> <code>True</code> <code>merge_bands</code> <code>bool</code> <p>Whether to merge downloaded bands into a single                          multi-band GeoTIFF file. Default is False.</p> <code>False</code> <code>merged_filename</code> <code>str</code> <p>Filename for the merged bands. If None and                             merge_bands is True, uses \"{item_id}_merged.tif\".</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing files. Default is False.</p> <code>False</code> <code>cell_size</code> <code>float</code> <p>Resolution in meters for the merged output. If None,                         uses the resolution of the first band.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary mapping band names to their corresponding xarray DataArrays   or file paths if output_dir is provided. If merge_bands is True, also   includes a 'merged' key with the path to the merged file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the item cannot be retrieved or a requested band is not available.</p> Source code in <code>geoai/download.py</code> <pre><code>def download_pc_stac_item(\n    item_url: str,\n    bands: Optional[List[str]] = None,\n    output_dir: Optional[str] = None,\n    show_progress: bool = True,\n    merge_bands: bool = False,\n    merged_filename: Optional[str] = None,\n    overwrite: bool = False,\n    cell_size: Optional[float] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Downloads a STAC item from Microsoft Planetary Computer with specified bands.\n\n    This function fetches a STAC item by URL, signs the assets using Planetary Computer\n    credentials, and downloads the specified bands with a progress bar. Can optionally\n    merge bands into a single multi-band GeoTIFF.\n\n    Args:\n        item_url (str): The URL of the STAC item to download.\n        bands (list, optional): List of specific bands to download (e.g., ['B01', 'B02']).\n                               If None, all available bands will be downloaded.\n        output_dir (str, optional): Directory to save downloaded bands. If None,\n                                   bands are returned as xarray DataArrays.\n        show_progress (bool, optional): Whether to display a progress bar. Default is True.\n        merge_bands (bool, optional): Whether to merge downloaded bands into a single\n                                     multi-band GeoTIFF file. Default is False.\n        merged_filename (str, optional): Filename for the merged bands. If None and\n                                        merge_bands is True, uses \"{item_id}_merged.tif\".\n        overwrite (bool, optional): Whether to overwrite existing files. Default is False.\n        cell_size (float, optional): Resolution in meters for the merged output. If None,\n                                    uses the resolution of the first band.\n\n    Returns:\n        dict: Dictionary mapping band names to their corresponding xarray DataArrays\n              or file paths if output_dir is provided. If merge_bands is True, also\n              includes a 'merged' key with the path to the merged file.\n\n    Raises:\n        ValueError: If the item cannot be retrieved or a requested band is not available.\n    \"\"\"\n    from rasterio.enums import Resampling\n\n    # Get the item ID from the URL\n    item_id = item_url.split(\"/\")[-1]\n    collection = item_url.split(\"/collections/\")[1].split(\"/items/\")[0]\n\n    # Connect to the Planetary Computer STAC API\n    catalog = Client.open(\n        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        modifier=pc.sign_inplace,\n    )\n\n    # Search for the specific item\n    search = catalog.search(collections=[collection], ids=[item_id])\n\n    # Get the first item from the search results\n    items = list(search.get_items())\n    if not items:\n        raise ValueError(f\"Item with ID {item_id} not found\")\n\n    item = items[0]\n\n    # Determine which bands to download\n    available_assets = list(item.assets.keys())\n\n    if bands is None:\n        # If no bands specified, download all band assets\n        bands_to_download = [\n            asset for asset in available_assets if asset.startswith(\"B\")\n        ]\n    else:\n        # Verify all requested bands exist\n        missing_bands = [band for band in bands if band not in available_assets]\n        if missing_bands:\n            raise ValueError(\n                f\"The following bands are not available: {missing_bands}. \"\n                f\"Available assets are: {available_assets}\"\n            )\n        bands_to_download = bands\n\n    # Create output directory if specified and doesn't exist\n    if output_dir and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    result = {}\n    band_data_arrays = []\n    resampled_arrays = []\n    band_names = []  # Track band names in order\n\n    # Set up progress bar\n    progress_iter = (\n        tqdm(bands_to_download, desc=\"Downloading bands\")\n        if show_progress\n        else bands_to_download\n    )\n\n    # Download each requested band\n    for band in progress_iter:\n        if band not in item.assets:\n            if show_progress and not isinstance(progress_iter, list):\n                progress_iter.write(\n                    f\"Warning: Band {band} not found in assets, skipping.\"\n                )\n            continue\n\n        band_url = item.assets[band].href\n\n        if output_dir:\n            file_path = os.path.join(output_dir, f\"{item.id}_{band}.tif\")\n\n            # Check if file exists and skip if overwrite is False\n            if os.path.exists(file_path) and not overwrite:\n                if show_progress and not isinstance(progress_iter, list):\n                    progress_iter.write(\n                        f\"File {file_path} already exists, skipping (use overwrite=True to force download).\"\n                    )\n                # Still need to open the file to get the data for merging\n                if merge_bands:\n                    band_data = rxr.open_rasterio(file_path)\n                    band_data_arrays.append((band, band_data))\n                    band_names.append(band)\n                result[band] = file_path\n                continue\n\n        if show_progress and not isinstance(progress_iter, list):\n            progress_iter.set_description(f\"Downloading {band}\")\n\n        band_data = rxr.open_rasterio(band_url)\n\n        # Store the data array for potential merging later\n        if merge_bands:\n            band_data_arrays.append((band, band_data))\n            band_names.append(band)\n\n        if output_dir:\n            file_path = os.path.join(output_dir, f\"{item.id}_{band}.tif\")\n            band_data.rio.to_raster(file_path)\n            result[band] = file_path\n        else:\n            result[band] = band_data\n\n    # Merge bands if requested\n    if merge_bands and output_dir:\n        if merged_filename is None:\n            merged_filename = f\"{item.id}_merged.tif\"\n\n        merged_path = os.path.join(output_dir, merged_filename)\n\n        # Check if merged file exists and skip if overwrite is False\n        if os.path.exists(merged_path) and not overwrite:\n            if show_progress:\n                print(\n                    f\"Merged file {merged_path} already exists, skipping (use overwrite=True to force creation).\"\n                )\n            result[\"merged\"] = merged_path\n        else:\n            if show_progress:\n                print(\"Resampling and merging bands...\")\n\n            # Determine target cell size if not provided\n            if cell_size is None and band_data_arrays:\n                # Use the resolution of the first band (usually 10m for B02, B03, B04, B08)\n                # Get the affine transform (containing resolution info)\n                first_band_data = band_data_arrays[0][1]\n                # Extract resolution from transform\n                cell_size = abs(first_band_data.rio.transform()[0])\n                if show_progress:\n                    print(f\"Using detected resolution: {cell_size}m\")\n            elif cell_size is None:\n                # Default to 10m if no bands are available\n                cell_size = 10\n                if show_progress:\n                    print(f\"Using default resolution: {cell_size}m\")\n\n            # Process bands in memory-efficient way\n            for i, (band_name, data_array) in enumerate(band_data_arrays):\n                if show_progress:\n                    print(f\"Processing band: {band_name}\")\n\n                # Get current resolution\n                current_res = abs(data_array.rio.transform()[0])\n\n                # Resample if needed\n                if (\n                    abs(current_res - cell_size) &gt; 0.01\n                ):  # Small tolerance for floating point comparison\n                    if show_progress:\n                        print(\n                            f\"Resampling {band_name} from {current_res}m to {cell_size}m\"\n                        )\n\n                    # Use bilinear for downsampling (higher to lower resolution)\n                    # Use nearest for upsampling (lower to higher resolution)\n                    resampling_method = (\n                        Resampling.bilinear\n                        if current_res &lt; cell_size\n                        else Resampling.nearest\n                    )\n\n                    resampled = data_array.rio.reproject(\n                        data_array.rio.crs,\n                        resolution=(cell_size, cell_size),\n                        resampling=resampling_method,\n                    )\n                    resampled_arrays.append(resampled)\n                else:\n                    resampled_arrays.append(data_array)\n\n            if show_progress:\n                print(\"Stacking bands...\")\n\n            # Concatenate all resampled arrays along the band dimension\n            try:\n                merged_data = xr.concat(resampled_arrays, dim=\"band\")\n\n                if show_progress:\n                    print(f\"Writing merged data to {merged_path}...\")\n\n                # Add description metadata\n                merged_data.attrs[\"description\"] = (\n                    f\"Multi-band image containing {', '.join(band_names)}\"\n                )\n\n                # Create a dictionary mapping band indices to band names\n                band_descriptions = {}\n                for i, name in enumerate(band_names):\n                    band_descriptions[i + 1] = name\n\n                # Write the merged data to file with band descriptions\n                merged_data.rio.to_raster(\n                    merged_path,\n                    tags={\"BAND_NAMES\": \",\".join(band_names)},\n                    descriptions=band_names,\n                )\n\n                result[\"merged\"] = merged_path\n\n                if show_progress:\n                    print(f\"Merged bands saved to: {merged_path}\")\n                    print(f\"Band order in merged file: {', '.join(band_names)}\")\n            except Exception as e:\n                if show_progress:\n                    print(f\"Error during merging: {str(e)}\")\n                    print(f\"Error details: {type(e).__name__}: {str(e)}\")\n                raise\n\n    return result\n</code></pre>"},{"location":"download/#geoai.download.download_with_progress","title":"<code>download_with_progress(url, output_path)</code>","text":"<p>Download a file with a progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>Path where the file will be saved.</p> required Source code in <code>geoai/download.py</code> <pre><code>def download_with_progress(url: str, output_path: str) -&gt; None:\n    \"\"\"Download a file with a progress bar.\n\n    Args:\n        url: URL of the file to download.\n        output_path: Path where the file will be saved.\n    \"\"\"\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get(\"content-length\", 0))\n    block_size = 1024  # 1 Kibibyte\n\n    with (\n        open(output_path, \"wb\") as file,\n        tqdm(\n            desc=os.path.basename(output_path),\n            total=total_size,\n            unit=\"iB\",\n            unit_scale=True,\n            unit_divisor=1024,\n        ) as bar,\n    ):\n        for data in response.iter_content(block_size):\n            size = file.write(data)\n            bar.update(size)\n</code></pre>"},{"location":"download/#geoai.download.extract_building_stats","title":"<code>extract_building_stats(data)</code>","text":"<p>Extract statistics from the building data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Path to the GeoJSON file or GeoDataFrame containing building data.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with statistics.</p> Source code in <code>geoai/download.py</code> <pre><code>def extract_building_stats(data: str) -&gt; Dict[str, Any]:\n    \"\"\"Extract statistics from the building data.\n\n    Args:\n        data: Path to the GeoJSON file or GeoDataFrame containing building data.\n\n    Returns:\n        Dictionary with statistics.\n    \"\"\"\n    try:\n        # Read the GeoJSON file\n\n        if isinstance(data, gpd.GeoDataFrame):\n            gdf = data\n        else:\n\n            gdf = gpd.read_file(data)\n\n        # Calculate statistics\n        bbox = gdf.total_bounds.tolist()\n        # Convert numpy values to Python native types\n        bbox = [float(x) for x in bbox]\n\n        stats = {\n            \"total_buildings\": int(len(gdf)),\n            \"has_height\": (\n                int(gdf[\"height\"].notna().sum()) if \"height\" in gdf.columns else 0\n            ),\n            \"has_name\": (\n                int(gdf[\"names.common.value\"].notna().sum())\n                if \"names.common.value\" in gdf.columns\n                else 0\n            ),\n            \"bbox\": bbox,\n        }\n\n        return stats\n\n    except Exception as e:\n        logger.error(f\"Error extracting statistics: {str(e)}\")\n        return {\"error\": str(e)}\n</code></pre>"},{"location":"download/#geoai.download.get_all_overture_types","title":"<code>get_all_overture_types()</code>","text":"<p>Get a list of all available Overture Maps data types.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>List of available Overture Maps data types.</p> Source code in <code>geoai/download.py</code> <pre><code>def get_all_overture_types() -&gt; List[str]:\n    \"\"\"Get a list of all available Overture Maps data types.\n\n    Returns:\n        list: List of available Overture Maps data types.\n    \"\"\"\n    from overturemaps import core\n\n    return core.get_all_overture_types()\n</code></pre>"},{"location":"download/#geoai.download.get_overture_data","title":"<code>get_overture_data(overture_type, bbox=None, columns=None, output=None, **kwargs)</code>","text":"<p>Fetches overture data and returns it as a GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>overture_type</code> <code>str</code> <p>The type of overture data to fetch.It can be one of the following: address|building|building_part|division|division_area|division_boundary|place| segment|connector|infrastructure|land|land_cover|land_use|water</p> required <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>The bounding box to filter the data. Defaults to None.</p> <code>None</code> <code>columns</code> <code>List[str]</code> <p>The columns to include in the output. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The file path to save the output GeoDataFrame. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: The fetched overture data as a GeoDataFrame.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the overture package is not installed.</p> Source code in <code>geoai/download.py</code> <pre><code>def get_overture_data(\n    overture_type: str,\n    bbox: Tuple[float, float, float, float] = None,\n    columns: List[str] = None,\n    output: str = None,\n    **kwargs: Any,\n) -&gt; \"gpd.GeoDataFrame\":\n    \"\"\"Fetches overture data and returns it as a GeoDataFrame.\n\n    Args:\n        overture_type (str): The type of overture data to fetch.It can be one of the following:\n            address|building|building_part|division|division_area|division_boundary|place|\n            segment|connector|infrastructure|land|land_cover|land_use|water\n        bbox (Tuple[float, float, float, float], optional): The bounding box to\n            filter the data. Defaults to None.\n        columns (List[str], optional): The columns to include in the output.\n            Defaults to None.\n        output (str, optional): The file path to save the output GeoDataFrame.\n            Defaults to None.\n\n    Returns:\n        gpd.GeoDataFrame: The fetched overture data as a GeoDataFrame.\n\n    Raises:\n        ImportError: If the overture package is not installed.\n    \"\"\"\n\n    try:\n        from overturemaps import core\n    except ImportError:\n        raise ImportError(\"The overturemaps package is required to use this function\")\n\n    gdf = core.geodataframe(overture_type, bbox=bbox)\n    if columns is not None:\n        gdf = gdf[columns]\n\n    gdf.crs = \"EPSG:4326\"\n\n    out_dir = os.path.dirname(os.path.abspath(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    if output is not None:\n        gdf.to_file(output, **kwargs)\n\n    return gdf\n</code></pre>"},{"location":"download/#geoai.download.get_overture_latest_release","title":"<code>get_overture_latest_release(patch=True)</code>","text":"<p>Retrieves the value of the 'latest' key from the Overture Maps release JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>patch</code> <code>bool</code> <p>If True, returns the full version string (e.g., \"2025-02-19.0\").</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The value of the 'latest' key from the releases.json file.</p> <p>Raises:</p> Type Description <code>RequestException</code> <p>If there's an issue with the HTTP request.</p> <code>KeyError</code> <p>If the 'latest' key is not found in the JSON data.</p> <code>JSONDecodeError</code> <p>If the response cannot be parsed as JSON.</p> Source code in <code>geoai/download.py</code> <pre><code>def get_overture_latest_release(patch=True) -&gt; str:\n    \"\"\"\n    Retrieves the value of the 'latest' key from the Overture Maps release JSON file.\n\n    Args:\n        patch (bool): If True, returns the full version string (e.g., \"2025-02-19.0\").\n\n    Returns:\n        str: The value of the 'latest' key from the releases.json file.\n\n    Raises:\n        requests.RequestException: If there's an issue with the HTTP request.\n        KeyError: If the 'latest' key is not found in the JSON data.\n        json.JSONDecodeError: If the response cannot be parsed as JSON.\n    \"\"\"\n    url = \"https://labs.overturemaps.org/data/releases.json\"\n\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        data = response.json()\n        if patch:\n            latest_release = data.get(\"latest\")\n        else:\n            latest_release = data.get(\"latest\").split(\".\")[\n                0\n            ]  # Extract the version number\n\n        if latest_release is None:\n            raise KeyError(\"The 'latest' key was not found in the releases.json file\")\n\n        return latest_release\n\n    except requests.RequestException as e:\n        print(f\"Error making the request: {e}\")\n        raise\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON response: {e}\")\n        raise\n    except KeyError as e:\n        print(f\"Key error: {e}\")\n        raise\n</code></pre>"},{"location":"download/#geoai.download.json_serializable","title":"<code>json_serializable(obj)</code>","text":"<p>Convert NumPy types to native Python types for JSON serialization.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Any object to convert.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>JSON serializable version of the object.</p> Source code in <code>geoai/download.py</code> <pre><code>def json_serializable(obj: Any) -&gt; Any:\n    \"\"\"Convert NumPy types to native Python types for JSON serialization.\n\n    Args:\n        obj: Any object to convert.\n\n    Returns:\n        JSON serializable version of the object.\n    \"\"\"\n    if isinstance(obj, np.integer):\n        return int(obj)\n    elif isinstance(obj, np.floating):\n        return float(obj)\n    elif isinstance(obj, np.ndarray):\n        return obj.tolist()\n    else:\n        return obj\n</code></pre>"},{"location":"download/#geoai.download.pc_collection_list","title":"<code>pc_collection_list(endpoint='https://planetarycomputer.microsoft.com/api/stac/v1', detailed=False, filter_by=None, sort_by='id')</code>","text":"<p>Retrieves and displays the list of available collections from Planetary Computer.</p> <p>This function connects to the Planetary Computer STAC API and retrieves the list of all available collections, with options to filter and sort the results.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>STAC API endpoint URL. Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".</p> <code>'https://planetarycomputer.microsoft.com/api/stac/v1'</code> <code>detailed</code> <code>bool</code> <p>Whether to return detailed information for each collection. If False, returns only basic info. Defaults to False.</p> <code>False</code> <code>filter_by</code> <code>dict</code> <p>Dictionary of field:value pairs to filter collections. For example, {\"license\": \"CC-BY-4.0\"}. Defaults to None.</p> <code>None</code> <code>sort_by</code> <code>str</code> <p>Field to sort the collections by. Defaults to \"id\".</p> <code>'id'</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>pandas.DataFrame: DataFrame containing collection information.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If there's an issue connecting to the API.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_collection_list(\n    endpoint: str = \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    detailed: bool = False,\n    filter_by: Optional[str] = None,\n    sort_by: str = \"id\",\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieves and displays the list of available collections from Planetary Computer.\n\n    This function connects to the Planetary Computer STAC API and retrieves the\n    list of all available collections, with options to filter and sort the results.\n\n    Args:\n        endpoint (str, optional): STAC API endpoint URL.\n            Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".\n        detailed (bool, optional): Whether to return detailed information for each\n            collection. If False, returns only basic info. Defaults to False.\n        filter_by (dict, optional): Dictionary of field:value pairs to filter\n            collections. For example, {\"license\": \"CC-BY-4.0\"}. Defaults to None.\n        sort_by (str, optional): Field to sort the collections by.\n            Defaults to \"id\".\n\n    Returns:\n        pandas.DataFrame: DataFrame containing collection information.\n\n    Raises:\n        ConnectionError: If there's an issue connecting to the API.\n    \"\"\"\n    # Initialize the STAC client\n    try:\n        catalog = Client.open(endpoint)\n    except Exception as e:\n        raise ConnectionError(f\"Failed to connect to STAC API at {endpoint}: {str(e)}\")\n\n    # Get all collections\n    try:\n        collections = list(catalog.get_collections())\n    except Exception as e:\n        raise Exception(f\"Error retrieving collections: {str(e)}\")\n\n    # Basic info to extract from all collections\n    collection_info = []\n\n    # Extract information based on detail level\n    for collection in collections:\n        # Basic information always included\n        info = {\n            \"id\": collection.id,\n            \"title\": collection.title or \"No title\",\n            \"description\": (\n                collection.description[:100] + \"...\"\n                if collection.description and len(collection.description) &gt; 100\n                else collection.description\n            ),\n        }\n\n        # Add detailed information if requested\n        if detailed:\n            # Get temporal extent if available\n            temporal_extent = \"Unknown\"\n            if collection.extent and collection.extent.temporal:\n                interval = (\n                    collection.extent.temporal.intervals[0]\n                    if collection.extent.temporal.intervals\n                    else None\n                )\n                if interval:\n                    start = interval[0] or \"Unknown Start\"\n                    end = interval[1] or \"Present\"\n                    if isinstance(start, datetime.datetime):\n                        start = start.strftime(\"%Y-%m-%d\")\n                    if isinstance(end, datetime.datetime):\n                        end = end.strftime(\"%Y-%m-%d\")\n                    temporal_extent = f\"{start} to {end}\"\n\n            # Add additional details\n            info.update(\n                {\n                    \"license\": collection.license or \"Unknown\",\n                    \"keywords\": (\n                        \", \".join(collection.keywords)\n                        if collection.keywords\n                        else \"None\"\n                    ),\n                    \"temporal_extent\": temporal_extent,\n                    \"asset_count\": len(collection.assets) if collection.assets else 0,\n                    \"providers\": (\n                        \", \".join([p.name for p in collection.providers])\n                        if collection.providers\n                        else \"Unknown\"\n                    ),\n                }\n            )\n\n            # Add spatial extent if available\n            if collection.extent and collection.extent.spatial:\n                info[\"bbox\"] = (\n                    str(collection.extent.spatial.bboxes[0])\n                    if collection.extent.spatial.bboxes\n                    else \"Unknown\"\n                )\n\n        collection_info.append(info)\n\n    # Convert to DataFrame for easier filtering and sorting\n    df = pd.DataFrame(collection_info)\n\n    # Apply filtering if specified\n    if filter_by:\n        for field, value in filter_by.items():\n            if field in df.columns:\n                df = df[df[field].astype(str).str.contains(value, case=False, na=False)]\n\n    # Apply sorting\n    if sort_by in df.columns:\n        df = df.sort_values(by=sort_by)\n\n    print(f\"Retrieved {len(df)} collections from Planetary Computer\")\n\n    # # Print a nicely formatted table\n    # if not df.empty:\n    #     print(\"\\nAvailable collections:\")\n    #     print(tabulate(df, headers=\"keys\", tablefmt=\"grid\", showindex=False))\n\n    return df\n</code></pre>"},{"location":"download/#geoai.download.pc_item_asset_list","title":"<code>pc_item_asset_list(item)</code>","text":"<p>Retrieve the list of asset keys from a STAC item in the Planetary Computer catalog.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>The URL of the STAC item.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>A list of asset keys available in the signed STAC item.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_item_asset_list(item: Union[str, \"pystac.Item\"]) -&gt; List[str]:\n    \"\"\"\n    Retrieve the list of asset keys from a STAC item in the Planetary Computer catalog.\n\n    Args:\n        item (str): The URL of the STAC item.\n\n    Returns:\n        list: A list of asset keys available in the signed STAC item.\n    \"\"\"\n    if isinstance(item, str):\n        item = pystac.Item.from_file(item)\n\n    if not isinstance(item, pystac.Item):\n        raise ValueError(\"item_url must be a string (URL) or a pystac.Item object\")\n\n    return list(item.assets.keys())\n</code></pre>"},{"location":"download/#geoai.download.pc_stac_download","title":"<code>pc_stac_download(items, output_dir='.', assets=None, max_workers=1, skip_existing=True)</code>","text":"<p>Download assets from STAC items retrieved from the Planetary Computer.</p> <p>This function downloads specified assets from a list of STAC items to the specified output directory. It supports parallel downloads and can skip already downloaded files.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list or Item</code> <p>STAC Item object or list of STAC Item objects.</p> required <code>output_dir</code> <code>str</code> <p>Directory where assets will be saved. Defaults to current directory.</p> <code>'.'</code> <code>assets</code> <code>list</code> <p>List of asset keys to download. If None, downloads all available assets. Defaults to None.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent download threads. Defaults to 1.</p> <code>1</code> <code>skip_existing</code> <code>bool</code> <p>Skip download if the file already exists. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Dict[str, str]]</code> <p>Dictionary mapping STAC item IDs to dictionaries of their downloaded assets {asset_key: file_path}.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If items is not a STAC Item or list of STAC Items.</p> <code>IOError</code> <p>If there's an error writing the downloaded assets to disk.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_stac_download(\n    items: Union[\"pystac.Item\", List[\"pystac.Item\"]],\n    output_dir: str = \".\",\n    assets: Optional[List[str]] = None,\n    max_workers: int = 1,\n    skip_existing: bool = True,\n) -&gt; Dict[str, Dict[str, str]]:\n    \"\"\"\n    Download assets from STAC items retrieved from the Planetary Computer.\n\n    This function downloads specified assets from a list of STAC items to the\n    specified output directory. It supports parallel downloads and can skip\n    already downloaded files.\n\n    Args:\n        items (list or pystac.Item): STAC Item object or list of STAC Item objects.\n        output_dir (str, optional): Directory where assets will be saved.\n            Defaults to current directory.\n        assets (list, optional): List of asset keys to download. If None,\n            downloads all available assets. Defaults to None.\n        max_workers (int, optional): Maximum number of concurrent download threads.\n            Defaults to 1.\n        skip_existing (bool, optional): Skip download if the file already exists.\n            Defaults to True.\n\n    Returns:\n        dict: Dictionary mapping STAC item IDs to dictionaries of their downloaded\n            assets {asset_key: file_path}.\n\n    Raises:\n        TypeError: If items is not a STAC Item or list of STAC Items.\n        IOError: If there's an error writing the downloaded assets to disk.\n    \"\"\"\n\n    from concurrent.futures import ThreadPoolExecutor, as_completed\n\n    # Handle single item case\n    if isinstance(items, pystac.Item) or isinstance(items, str):\n        items = [items]\n    elif not isinstance(items, list):\n        raise TypeError(\"items must be a STAC Item or list of STAC Items\")\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Function to download a single asset\n    def download_asset(\n        item: \"pystac.Item\", asset_key: str, asset: \"pystac.Asset\"\n    ) -&gt; str:\n        item = pc.sign(item)\n        item_id = item.id\n\n        # Get the asset URL and sign it if needed\n        asset_url = item.assets[asset_key].href\n        # Determine output filename\n        if asset.media_type:\n            # Use appropriate file extension based on media type\n            if \"tiff\" in asset.media_type or \"geotiff\" in asset.media_type:\n                ext = \".tif\"\n            elif \"jpeg\" in asset.media_type:\n                ext = \".jpg\"\n            elif \"png\" in asset.media_type:\n                ext = \".png\"\n            elif \"json\" in asset.media_type:\n                ext = \".json\"\n            else:\n                # Default extension based on the original URL\n                ext = os.path.splitext(asset_url.split(\"?\")[0])[1] or \".data\"\n        else:\n            # Default extension based on the original URL\n            ext = os.path.splitext(asset_url.split(\"?\")[0])[1] or \".data\"\n\n        output_path = os.path.join(output_dir, f\"{item_id}_{asset_key}{ext}\")\n\n        # Skip if file exists and skip_existing is True\n        if skip_existing and os.path.exists(output_path):\n            print(f\"Skipping existing asset: {asset_key} -&gt; {output_path}\")\n            return asset_key, output_path\n\n        try:\n            # Download the asset with progress bar\n            with requests.get(asset_url, stream=True) as r:\n                r.raise_for_status()\n                total_size = int(r.headers.get(\"content-length\", 0))\n                with open(output_path, \"wb\") as f:\n                    with tqdm(\n                        total=total_size,\n                        unit=\"B\",\n                        unit_scale=True,\n                        unit_divisor=1024,\n                        desc=f\"Downloading {item_id}_{asset_key}\",\n                        ncols=100,\n                    ) as pbar:\n                        for chunk in r.iter_content(chunk_size=8192):\n                            f.write(chunk)\n                            pbar.update(len(chunk))\n\n            return asset_key, output_path\n        except Exception as e:\n            print(f\"Error downloading {asset_key} for item {item_id}: {str(e)}\")\n            if os.path.exists(output_path):\n                os.remove(output_path)  # Clean up partial download\n            return asset_key, None\n\n    # Process all items and their assets\n    results = {}\n\n    for item in items:\n        item_assets = {}\n        if isinstance(item, str):\n            item = pystac.Item.from_file(item)\n        item_id = item.id\n        print(f\"Processing STAC item: {item_id}\")\n\n        # Determine which assets to download\n        if assets:\n            assets_to_download = {k: v for k, v in item.assets.items() if k in assets}\n            if not assets_to_download:\n                print(\n                    f\"Warning: None of the specified asset keys {assets} found in item {item_id}\"\n                )\n                print(f\"Available asset keys: {list(item.assets.keys())}\")\n                continue\n        else:\n            assets_to_download = item.assets\n\n        # Download assets concurrently\n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all download tasks\n            future_to_asset = {\n                executor.submit(download_asset, item, asset_key, asset): (\n                    asset_key,\n                    asset,\n                )\n                for asset_key, asset in assets_to_download.items()\n            }\n\n            # Process results as they complete\n            for future in as_completed(future_to_asset):\n                asset_key, asset = future_to_asset[future]\n                try:\n                    key, path = future.result()\n                    if path:\n                        item_assets[key] = path\n                except Exception as e:\n                    print(\n                        f\"Error processing asset {asset_key} for item {item_id}: {str(e)}\"\n                    )\n\n        results[item_id] = item_assets\n\n    # Count total downloaded assets\n    total_assets = sum(len(assets) for assets in results.values())\n    print(f\"\\nDownloaded {total_assets} assets for {len(results)} items\")\n\n    return results\n</code></pre>"},{"location":"download/#geoai.download.pc_stac_search","title":"<code>pc_stac_search(collection, bbox=None, time_range=None, query=None, limit=10, max_items=None, quiet=False, endpoint='https://planetarycomputer.microsoft.com/api/stac/v1')</code>","text":"<p>Search for STAC items in the Planetary Computer catalog.</p> <p>This function queries the Planetary Computer STAC API to find items matching the specified criteria, including collection, bounding box, time range, and additional query parameters.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>The STAC collection ID to search within.</p> required <code>bbox</code> <code>list</code> <p>Bounding box coordinates [west, south, east, north]. Defaults to None.</p> <code>None</code> <code>time_range</code> <code>str or tuple</code> <p>Time range as a string \"start/end\" or a tuple of (start, end) datetime objects. Defaults to None.</p> <code>None</code> <code>query</code> <code>dict</code> <p>Additional query parameters for filtering. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Number of items to return per page. Defaults to 10.</p> <code>10</code> <code>max_items</code> <code>int</code> <p>Maximum total number of items to return. Defaults to None (returns all matching items).</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>bool, optional): Whether to suppress print statements. Defaults to False.</p> <code>False</code> <code>endpoint</code> <code>str</code> <p>STAC API endpoint URL. Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".</p> <code>'https://planetarycomputer.microsoft.com/api/stac/v1'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[Item]</code> <p>List of STAC Item objects matching the search criteria.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid parameters are provided.</p> <code>ConnectionError</code> <p>If there's an issue connecting to the API.</p> Source code in <code>geoai/download.py</code> <pre><code>def pc_stac_search(\n    collection: str,\n    bbox: Optional[List[float]] = None,\n    time_range: Optional[str] = None,\n    query: Optional[Dict[str, Any]] = None,\n    limit: int = 10,\n    max_items: Optional[int] = None,\n    quiet: bool = False,\n    endpoint: str = \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n) -&gt; List[\"pystac.Item\"]:\n    \"\"\"\n    Search for STAC items in the Planetary Computer catalog.\n\n    This function queries the Planetary Computer STAC API to find items matching\n    the specified criteria, including collection, bounding box, time range, and\n    additional query parameters.\n\n    Args:\n        collection (str): The STAC collection ID to search within.\n        bbox (list, optional): Bounding box coordinates [west, south, east, north].\n            Defaults to None.\n        time_range (str or tuple, optional): Time range as a string \"start/end\" or\n            a tuple of (start, end) datetime objects. Defaults to None.\n        query (dict, optional): Additional query parameters for filtering.\n            Defaults to None.\n        limit (int, optional): Number of items to return per page. Defaults to 10.\n        max_items (int, optional): Maximum total number of items to return.\n            Defaults to None (returns all matching items).\n        quiet: bool, optional): Whether to suppress print statements. Defaults to False.\n        endpoint (str, optional): STAC API endpoint URL.\n            Defaults to \"https://planetarycomputer.microsoft.com/api/stac/v1\".\n\n    Returns:\n        list: List of STAC Item objects matching the search criteria.\n\n    Raises:\n        ValueError: If invalid parameters are provided.\n        ConnectionError: If there's an issue connecting to the API.\n    \"\"\"\n    import datetime\n\n    # Initialize the STAC client\n    try:\n        catalog = Client.open(endpoint)\n    except Exception as e:\n        raise ConnectionError(f\"Failed to connect to STAC API at {endpoint}: {str(e)}\")\n\n    # Process time_range if provided\n    if time_range:\n        if isinstance(time_range, tuple) and len(time_range) == 2:\n            # Convert datetime objects to ISO format strings\n            start, end = time_range\n            if isinstance(start, datetime.datetime):\n                start = start.isoformat()\n            if isinstance(end, datetime.datetime):\n                end = end.isoformat()\n            time_str = f\"{start}/{end}\"\n        elif isinstance(time_range, str):\n            time_str = time_range\n        else:\n            raise ValueError(\n                \"time_range must be a 'start/end' string or tuple of (start, end)\"\n            )\n    else:\n        time_str = None\n\n    # Create the search object\n    search = catalog.search(\n        collections=[collection], bbox=bbox, datetime=time_str, query=query, limit=limit\n    )\n\n    # Collect the items\n    items = []\n    try:\n        # Use max_items if specified, otherwise get all items\n        if max_items:\n            items_gen = search.get_items()\n            for item in items_gen:\n                items.append(item)\n                if len(items) &gt;= max_items:\n                    break\n        else:\n            items = list(search.get_items())\n    except Exception as e:\n        raise Exception(f\"Error retrieving search results: {str(e)}\")\n\n    if not quiet:\n        print(f\"Found {len(items)} items matching search criteria\")\n\n    return items\n</code></pre>"},{"location":"download/#geoai.download.preview_raster","title":"<code>preview_raster(data, title=None)</code>","text":"<p>Display a preview of the downloaded imagery.</p> <p>This function creates a visualization of the downloaded NAIP imagery by converting it to an RGB array and displaying it with matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>The raster data as a rioxarray object.</p> required <code>title</code> <code>str</code> <p>The title for the preview plot.</p> <code>None</code> Source code in <code>geoai/download.py</code> <pre><code>def preview_raster(data: Any, title: str = None) -&gt; None:\n    \"\"\"Display a preview of the downloaded imagery.\n\n    This function creates a visualization of the downloaded NAIP imagery\n    by converting it to an RGB array and displaying it with matplotlib.\n\n    Args:\n        data: The raster data as a rioxarray object.\n        title: The title for the preview plot.\n    \"\"\"\n    # Convert to 8-bit RGB for display\n    rgb_data = data.transpose(\"y\", \"x\", \"band\").values[:, :, 0:3]\n    rgb_data = np.where(rgb_data &gt; 255, 255, rgb_data).astype(np.uint8)\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_data)\n    if title is not None:\n        plt.title(title)\n    plt.axis(\"off\")\n    plt.show()\n</code></pre>"},{"location":"download/#geoai.download.read_pc_item_asset","title":"<code>read_pc_item_asset(item, asset, output=None, as_cog=True, **kwargs)</code>","text":"<p>Read a specific asset from a STAC item in the Planetary Computer catalog.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>str</code> <p>The URL of the STAC item.</p> required <code>asset</code> <code>str</code> <p>The key of the asset to read.</p> required <code>output</code> <code>str</code> <p>If specified, the path to save the asset as a raster file.</p> <code>None</code> <code>as_cog</code> <code>bool</code> <p>If True, save the asset as a Cloud Optimized GeoTIFF (COG).</p> <code>True</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>xarray.DataArray: The data array for the specified asset.</p> Source code in <code>geoai/download.py</code> <pre><code>def read_pc_item_asset(\n    item: Union[str, \"pystac.Item\"],\n    asset: str,\n    output: Optional[str] = None,\n    as_cog: bool = True,\n    **kwargs: Any,\n) -&gt; \"xr.Dataset\":\n    \"\"\"\n    Read a specific asset from a STAC item in the Planetary Computer catalog.\n\n    Args:\n        item (str): The URL of the STAC item.\n        asset (str): The key of the asset to read.\n        output (str, optional): If specified, the path to save the asset as a raster file.\n        as_cog (bool, optional): If True, save the asset as a Cloud Optimized GeoTIFF (COG).\n\n    Returns:\n        xarray.DataArray: The data array for the specified asset.\n    \"\"\"\n    if isinstance(item, str):\n        item = pystac.Item.from_file(item)\n\n    if not isinstance(item, pystac.Item):\n        raise ValueError(\"item must be a string (URL) or a pystac.Item object\")\n\n    signed_item = pc.sign(item)\n\n    if asset not in signed_item.assets:\n        raise ValueError(\n            f\"Asset '{asset}' not found in item '{item.id}'. It has available assets: {list(signed_item.assets.keys())}\"\n        )\n\n    asset_url = signed_item.assets[asset].href\n    ds = rxr.open_rasterio(asset_url)\n\n    if as_cog:\n        kwargs[\"driver\"] = \"COG\"  # Ensure the output is a Cloud Optimized GeoTIFF\n\n    if output:\n        print(f\"Saving asset '{asset}' to {output}...\")\n        ds.rio.to_raster(output, **kwargs)\n        print(f\"Asset '{asset}' saved successfully.\")\n    return ds\n</code></pre>"},{"location":"download/#geoai.download.view_pc_item","title":"<code>view_pc_item(url=None, collection=None, item=None, assets=None, bands=None, titiler_endpoint=None, name='STAC Item', attribution='Planetary Computer', opacity=1.0, shown=True, fit_bounds=True, layer_index=None, backend='folium', basemap=None, map_args=None, **kwargs)</code>","text":"<p>Visualize a STAC item from the Planetary Computer on an interactive map.</p> <p>This function allows users to display a STAC item on a map using either the <code>folium</code> or <code>ipyleaflet</code> backend. It supports adding basemaps, configuring map options, and customizing the visualization of the STAC item.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the STAC item to visualize. Defaults to None.</p> <code>None</code> <code>collection</code> <code>str</code> <p>The collection ID of the STAC item. Defaults to None.</p> <code>None</code> <code>item</code> <code>str or Item</code> <p>The STAC item or its ID. Defaults to None.</p> <code>None</code> <code>assets</code> <code>list</code> <p>List of asset keys to visualize. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of specific bands to visualize. Defaults to None.</p> <code>None</code> <code>titiler_endpoint</code> <code>str</code> <p>URL of the Titiler endpoint for rendering. Defaults to None.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the layer to display on the map. Defaults to \"STAC Item\".</p> <code>'STAC Item'</code> <code>attribution</code> <code>str</code> <p>Attribution text for the layer. Defaults to \"Planetary Computer\".</p> <code>'Planetary Computer'</code> <code>opacity</code> <code>float</code> <p>Opacity of the layer (0.0 to 1.0). Defaults to 1.0.</p> <code>1.0</code> <code>shown</code> <code>bool</code> <p>Whether the layer is visible by default. Defaults to True.</p> <code>True</code> <code>fit_bounds</code> <code>bool</code> <p>Whether to fit the map bounds to the layer. Defaults to True.</p> <code>True</code> <code>layer_index</code> <code>int</code> <p>Index of the layer in the map's layer stack. Defaults to None.</p> <code>None</code> <code>backend</code> <code>str</code> <p>Map backend to use ('folium' or 'ipyleaflet'). Defaults to \"folium\".</p> <code>'folium'</code> <code>basemap</code> <code>str</code> <p>Name of the basemap to add to the map. Defaults to None.</p> <code>None</code> <code>map_args</code> <code>dict</code> <p>Additional arguments for configuring the map. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the <code>add_stac_layer</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>leafmap.Map: An interactive map with the STAC item visualized.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p> Source code in <code>geoai/download.py</code> <pre><code>def view_pc_item(\n    url: Optional[str] = None,\n    collection: Optional[str] = None,\n    item: Optional[str] = None,\n    assets: Optional[Union[str, List[str]]] = None,\n    bands: Optional[List[str]] = None,\n    titiler_endpoint: Optional[str] = None,\n    name: str = \"STAC Item\",\n    attribution: str = \"Planetary Computer\",\n    opacity: float = 1.0,\n    shown: bool = True,\n    fit_bounds: bool = True,\n    layer_index: Optional[int] = None,\n    backend: str = \"folium\",\n    basemap: Optional[str] = None,\n    map_args: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize a STAC item from the Planetary Computer on an interactive map.\n\n    This function allows users to display a STAC item on a map using either the\n    `folium` or `ipyleaflet` backend. It supports adding basemaps, configuring\n    map options, and customizing the visualization of the STAC item.\n\n    Args:\n        url (str, optional): URL of the STAC item to visualize. Defaults to None.\n        collection (str, optional): The collection ID of the STAC item. Defaults to None.\n        item (str or pystac.Item, optional): The STAC item or its ID. Defaults to None.\n        assets (list, optional): List of asset keys to visualize. Defaults to None.\n        bands (list, optional): List of specific bands to visualize. Defaults to None.\n        titiler_endpoint (str, optional): URL of the Titiler endpoint for rendering. Defaults to None.\n        name (str, optional): Name of the layer to display on the map. Defaults to \"STAC Item\".\n        attribution (str, optional): Attribution text for the layer. Defaults to \"Planetary Computer\".\n        opacity (float, optional): Opacity of the layer (0.0 to 1.0). Defaults to 1.0.\n        shown (bool, optional): Whether the layer is visible by default. Defaults to True.\n        fit_bounds (bool, optional): Whether to fit the map bounds to the layer. Defaults to True.\n        layer_index (int, optional): Index of the layer in the map's layer stack. Defaults to None.\n        backend (str, optional): Map backend to use ('folium' or 'ipyleaflet'). Defaults to \"folium\".\n        basemap (str, optional): Name of the basemap to add to the map. Defaults to None.\n        map_args (dict, optional): Additional arguments for configuring the map. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the `add_stac_layer` method.\n\n    Returns:\n        leafmap.Map: An interactive map with the STAC item visualized.\n\n    Raises:\n        ValueError: If an unsupported backend is specified.\n    \"\"\"\n    if backend == \"folium\":\n        import leafmap.foliumap as leafmap\n\n    elif backend == \"ipyleaflet\":\n        import leafmap.leafmap as leafmap\n\n    else:\n        raise ValueError(\n            f\"Unsupported backend: {backend}. Supported backends are 'folium' and 'ipyleaflet'.\"\n        )\n\n    if map_args is None:\n        map_args = {}\n\n    if \"draw_control\" not in map_args:\n        map_args[\"draw_control\"] = False\n\n    if url is not None:\n\n        item = pystac.Item.from_file(url)\n\n    if isinstance(item, pystac.Item):\n        collection = item.collection_id\n        if assets is None:\n            assets = [list(item.assets.keys())[0]]\n        item = item.id\n\n    m = leafmap.Map(**map_args)\n    if basemap is not None:\n        m.add_basemap(basemap)\n    m.add_stac_layer(\n        collection=collection,\n        item=item,\n        assets=assets,\n        bands=bands,\n        titiler_endpoint=titiler_endpoint,\n        name=name,\n        attribution=attribution,\n        opacity=opacity,\n        shown=shown,\n        fit_bounds=fit_bounds,\n        layer_index=layer_index,\n        **kwargs,\n    )\n    return m\n</code></pre>"},{"location":"download/#geoai.download.view_pc_items","title":"<code>view_pc_items(urls=None, collection=None, items=None, assets=None, bands=None, titiler_endpoint=None, attribution='Planetary Computer', opacity=1.0, shown=True, fit_bounds=True, layer_index=None, backend='folium', basemap=None, map_args=None, **kwargs)</code>","text":"<p>Visualize multiple STAC items from the Planetary Computer on an interactive map.</p> <p>This function allows users to display multiple STAC items on a map using either the <code>folium</code> or <code>ipyleaflet</code> backend. It supports adding basemaps, configuring map options, and customizing the visualization of the STAC items.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>List of URLs of the STAC items to visualize. Defaults to None.</p> <code>None</code> <code>collection</code> <code>str</code> <p>The collection ID of the STAC items. Defaults to None.</p> <code>None</code> <code>items</code> <code>list or str</code> <p>List of STAC items or their IDs. Defaults to None.</p> <code>None</code> <code>assets</code> <code>list</code> <p>List of asset keys to visualize. Defaults to None.</p> <code>None</code> <code>bands</code> <code>list</code> <p>List of specific bands to visualize. Defaults to None.</p> <code>None</code> <code>titiler_endpoint</code> <code>str</code> <p>URL of the Titiler endpoint for rendering. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>str</code> <p>Attribution text for the layers. Defaults to \"Planetary Computer\".</p> <code>'Planetary Computer'</code> <code>opacity</code> <code>float</code> <p>Opacity of the layers (0.0 to 1.0). Defaults to 1.0.</p> <code>1.0</code> <code>shown</code> <code>bool</code> <p>Whether the layers are visible by default. Defaults to True.</p> <code>True</code> <code>fit_bounds</code> <code>bool</code> <p>Whether to fit the map bounds to the layers. Defaults to True.</p> <code>True</code> <code>layer_index</code> <code>int</code> <p>Index of the layers in the map's layer stack. Defaults to None.</p> <code>None</code> <code>backend</code> <code>str</code> <p>Map backend to use ('folium' or 'ipyleaflet'). Defaults to \"folium\".</p> <code>'folium'</code> <code>basemap</code> <code>str</code> <p>Name of the basemap to add to the map. Defaults to None.</p> <code>None</code> <code>map_args</code> <code>dict</code> <p>Additional arguments for configuring the map. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the <code>add_stac_layer</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>leafmap.Map: An interactive map with the STAC items visualized.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported backend is specified.</p> Source code in <code>geoai/download.py</code> <pre><code>def view_pc_items(\n    urls: Optional[List[str]] = None,\n    collection: Optional[str] = None,\n    items: Optional[List[str]] = None,\n    assets: Optional[Union[str, List[str]]] = None,\n    bands: Optional[List[str]] = None,\n    titiler_endpoint: Optional[str] = None,\n    attribution: str = \"Planetary Computer\",\n    opacity: float = 1.0,\n    shown: bool = True,\n    fit_bounds: bool = True,\n    layer_index: Optional[int] = None,\n    backend: str = \"folium\",\n    basemap: Optional[str] = None,\n    map_args: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize multiple STAC items from the Planetary Computer on an interactive map.\n\n    This function allows users to display multiple STAC items on a map using either\n    the `folium` or `ipyleaflet` backend. It supports adding basemaps, configuring\n    map options, and customizing the visualization of the STAC items.\n\n    Args:\n        urls (list, optional): List of URLs of the STAC items to visualize. Defaults to None.\n        collection (str, optional): The collection ID of the STAC items. Defaults to None.\n        items (list or str, optional): List of STAC items or their IDs. Defaults to None.\n        assets (list, optional): List of asset keys to visualize. Defaults to None.\n        bands (list, optional): List of specific bands to visualize. Defaults to None.\n        titiler_endpoint (str, optional): URL of the Titiler endpoint for rendering. Defaults to None.\n        attribution (str, optional): Attribution text for the layers. Defaults to \"Planetary Computer\".\n        opacity (float, optional): Opacity of the layers (0.0 to 1.0). Defaults to 1.0.\n        shown (bool, optional): Whether the layers are visible by default. Defaults to True.\n        fit_bounds (bool, optional): Whether to fit the map bounds to the layers. Defaults to True.\n        layer_index (int, optional): Index of the layers in the map's layer stack. Defaults to None.\n        backend (str, optional): Map backend to use ('folium' or 'ipyleaflet'). Defaults to \"folium\".\n        basemap (str, optional): Name of the basemap to add to the map. Defaults to None.\n        map_args (dict, optional): Additional arguments for configuring the map. Defaults to None.\n        **kwargs: Additional keyword arguments passed to the `add_stac_layer` method.\n\n    Returns:\n        leafmap.Map: An interactive map with the STAC items visualized.\n\n    Raises:\n        ValueError: If an unsupported backend is specified.\n    \"\"\"\n    if backend == \"folium\":\n        import leafmap.foliumap as leafmap\n\n    elif backend == \"ipyleaflet\":\n        import leafmap.leafmap as leafmap\n\n    else:\n        raise ValueError(\n            f\"Unsupported backend: {backend}. Supported backends are 'folium' and 'ipyleaflet'.\"\n        )\n\n    if map_args is None:\n        map_args = {}\n\n    if \"draw_control\" not in map_args:\n        map_args[\"draw_control\"] = False\n\n    if urls is not None:\n        items = [pystac.Item.from_file(url) for url in urls]\n\n    if isinstance(items, str):\n        items = [pystac.Item.from_file(items)]\n    m = leafmap.Map(**map_args)\n\n    if basemap is not None:\n        m.add_basemap(basemap)\n    if isinstance(items, list):\n\n        for item in items:\n\n            if isinstance(item, pystac.Item):\n                collection = item.collection_id\n                if assets is None:\n                    assets = [list(item.assets.keys())[0]]\n                item = item.id\n\n                m.add_stac_layer(\n                    collection=collection,\n                    item=item,\n                    assets=assets,\n                    bands=bands,\n                    titiler_endpoint=titiler_endpoint,\n                    name=item,\n                    attribution=attribution,\n                    opacity=opacity,\n                    shown=shown,\n                    fit_bounds=fit_bounds,\n                    layer_index=layer_index,\n                    **kwargs,\n                )\n    return m\n</code></pre>"},{"location":"extract/","title":"extract module","text":"<p>This module provides a dataset class for object extraction from raster data</p>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator","title":"<code>AgricultureFieldDelineator</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Agricultural field boundary delineation using a pre-trained Mask R-CNN model.</p> <p>This class extends the ObjectDetector class to specifically handle Sentinel-2 imagery with 12 spectral bands for agricultural field boundary detection.</p> <p>Attributes:</p> Name Type Description <code>band_selection</code> <p>List of band indices to use for prediction (default: RGB)</p> <code>sentinel_band_stats</code> <p>Per-band statistics for Sentinel-2 data</p> <code>use_ndvi</code> <p>Whether to calculate and include NDVI as an additional channel</p> Source code in <code>geoai/extract.py</code> <pre><code>class AgricultureFieldDelineator(ObjectDetector):\n    \"\"\"\n    Agricultural field boundary delineation using a pre-trained Mask R-CNN model.\n\n    This class extends the ObjectDetector class to specifically handle Sentinel-2\n    imagery with 12 spectral bands for agricultural field boundary detection.\n\n    Attributes:\n        band_selection: List of band indices to use for prediction (default: RGB)\n        sentinel_band_stats: Per-band statistics for Sentinel-2 data\n        use_ndvi: Whether to calculate and include NDVI as an additional channel\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"field_boundary_detector.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n        band_selection: Optional[List[int]] = None,\n        use_ndvi: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the field boundary delineator.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n            band_selection: List of Sentinel-2 band indices to use (None = adapt based on model)\n            use_ndvi: Whether to calculate and include NDVI as an additional channel\n        \"\"\"\n        # Save parameters before calling parent constructor\n        self.custom_band_selection = band_selection\n        self.use_ndvi = use_ndvi\n\n        # Set device (copied from parent init to ensure it's set before initialize_model)\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Initialize model differently for multi-spectral input\n        model = self.initialize_sentinel2_model(model)\n\n        # Call parent but with our custom model\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n        # Default Sentinel-2 band statistics (can be overridden with actual stats)\n        # Band order: [B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12]\n        self.sentinel_band_stats = {\n            \"means\": [\n                0.0975,\n                0.0476,\n                0.0598,\n                0.0697,\n                0.1077,\n                0.1859,\n                0.2378,\n                0.2061,\n                0.2598,\n                0.4120,\n                0.1956,\n                0.1410,\n            ],\n            \"stds\": [\n                0.0551,\n                0.0290,\n                0.0298,\n                0.0479,\n                0.0506,\n                0.0505,\n                0.0747,\n                0.0642,\n                0.0782,\n                0.1187,\n                0.0651,\n                0.0679,\n            ],\n        }\n\n        # Set default band selection (RGB - typically B4, B3, B2 for Sentinel-2)\n        self.band_selection = (\n            self.custom_band_selection\n            if self.custom_band_selection is not None\n            else [3, 2, 1]\n        )  # R, G, B bands\n\n        # Customize parameters for field delineation\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.overlap = 0.5  # Higher overlap for field boundary detection\n        self.min_object_area = 1000  # Minimum area in pixels for field detection\n        self.simplify_tolerance = 2.0  # Higher tolerance for field boundaries\n\n    def initialize_sentinel2_model(self, model: Optional[Any] = None) -&gt; Any:\n        \"\"\"\n        Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.\n\n        Args:\n            model: Pre-initialized model (optional)\n\n        Returns:\n            Modified model with appropriate input channels\n        \"\"\"\n        import torchvision\n        from torchvision.models.detection import maskrcnn_resnet50_fpn\n        from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n        if model is not None:\n            return model\n\n        # Determine number of input channels based on band selection and NDVI\n        num_input_channels = (\n            len(self.custom_band_selection)\n            if self.custom_band_selection is not None\n            else 3\n        )\n        if self.use_ndvi:\n            num_input_channels += 1\n\n        print(f\"Initializing Mask R-CNN model with {num_input_channels} input channels\")\n\n        # Create a ResNet50 backbone with modified input channels\n        backbone = resnet_fpn_backbone(\"resnet50\", weights=None)\n\n        # Replace the first conv layer to accept multi-spectral input\n        original_conv = backbone.body.conv1\n        backbone.body.conv1 = torch.nn.Conv2d(\n            num_input_channels,\n            original_conv.out_channels,\n            kernel_size=original_conv.kernel_size,\n            stride=original_conv.stride,\n            padding=original_conv.padding,\n            bias=original_conv.bias is not None,\n        )\n\n        # Create Mask R-CNN with our modified backbone\n        model = maskrcnn_resnet50_fpn(\n            backbone=backbone,\n            num_classes=2,  # Background + field\n            image_mean=[0.485] * num_input_channels,  # Extend mean to all channels\n            image_std=[0.229] * num_input_channels,  # Extend std to all channels\n        )\n\n        model.to(self.device)\n        return model\n\n    def preprocess_sentinel_bands(\n        self,\n        image_data: np.ndarray,\n        band_selection: Optional[List[int]] = None,\n        use_ndvi: Optional[bool] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Preprocess Sentinel-2 band data for model input.\n\n        Args:\n            image_data: Raw Sentinel-2 image data as numpy array [bands, height, width]\n            band_selection: List of band indices to use (overrides instance default if provided)\n            use_ndvi: Whether to include NDVI (overrides instance default if provided)\n\n        Returns:\n            Processed tensor ready for model input\n        \"\"\"\n        # Use instance defaults if not specified\n        band_selection = (\n            band_selection if band_selection is not None else self.band_selection\n        )\n        use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n        # Select bands\n        selected_bands = image_data[band_selection]\n\n        # Calculate NDVI if requested (using B8 and B4 which are indices 7 and 3)\n        if (\n            use_ndvi\n            and 7 in range(image_data.shape[0])\n            and 3 in range(image_data.shape[0])\n        ):\n            nir = image_data[7].astype(np.float32)  # B8 (NIR)\n            red = image_data[3].astype(np.float32)  # B4 (Red)\n\n            # Avoid division by zero\n            denominator = nir + red\n            ndvi = np.zeros_like(nir)\n            valid_mask = denominator &gt; 0\n            ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / denominator[\n                valid_mask\n            ]\n\n            # Rescale NDVI from [-1, 1] to [0, 1]\n            ndvi = (ndvi + 1) / 2\n\n            # Add NDVI as an additional channel\n            selected_bands = np.vstack([selected_bands, ndvi[np.newaxis, :, :]])\n\n        # Convert to tensor\n        image_tensor = torch.from_numpy(selected_bands).float()\n\n        # Normalize using band statistics\n        for i, band_idx in enumerate(band_selection):\n            # Make sure band_idx is within range of our statistics\n            if band_idx &lt; len(self.sentinel_band_stats[\"means\"]):\n                mean = self.sentinel_band_stats[\"means\"][band_idx]\n                std = self.sentinel_band_stats[\"stds\"][band_idx]\n                image_tensor[i] = (image_tensor[i] - mean) / std\n\n        # If NDVI was added, normalize it too (last channel)\n        if use_ndvi:\n            # NDVI is already roughly in [0,1] range, just standardize it slightly\n            image_tensor[-1] = (image_tensor[-1] - 0.5) / 0.5\n\n        return image_tensor\n\n    def update_band_stats(\n        self,\n        raster_path: str,\n        band_selection: Optional[List[int]] = None,\n        sample_size: int = 1000,\n    ) -&gt; Dict[str, List[float]]:\n        \"\"\"\n        Update band statistics from the input Sentinel-2 raster.\n\n        Args:\n            raster_path: Path to the Sentinel-2 raster file\n            band_selection: Specific bands to update (None = update all available)\n            sample_size: Number of random pixels to sample for statistics calculation\n\n        Returns:\n            Updated band statistics dictionary\n        \"\"\"\n        with rasterio.open(raster_path) as src:\n            # Check if this is likely a Sentinel-2 product\n            band_count = src.count\n            if band_count &lt; 3:\n                print(\n                    f\"Warning: Raster has only {band_count} bands, may not be Sentinel-2 data\"\n                )\n\n            # Get dimensions\n            height, width = src.height, src.width\n\n            # Determine which bands to analyze\n            if band_selection is None:\n                band_selection = list(range(1, band_count + 1))  # 1-indexed\n\n            # Initialize arrays for band statistics\n            means = []\n            stds = []\n\n            # Sample random pixels\n            np.random.seed(42)  # For reproducibility\n            sample_rows = np.random.randint(0, height, sample_size)\n            sample_cols = np.random.randint(0, width, sample_size)\n\n            # Calculate statistics for each band\n            for band in band_selection:\n                # Read band data\n                band_data = src.read(band)\n\n                # Sample values\n                sample_values = band_data[sample_rows, sample_cols]\n\n                # Remove invalid values (e.g., nodata)\n                valid_samples = sample_values[np.isfinite(sample_values)]\n\n                # Calculate statistics\n                mean = float(np.mean(valid_samples))\n                std = float(np.std(valid_samples))\n\n                # Store results\n                means.append(mean)\n                stds.append(std)\n\n                print(f\"Band {band}: mean={mean:.4f}, std={std:.4f}\")\n\n            # Update instance variables\n            self.sentinel_band_stats = {\"means\": means, \"stds\": stds}\n\n            return self.sentinel_band_stats\n\n    def process_sentinel_raster(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        batch_size: int = 4,\n        band_selection: Optional[List[int]] = None,\n        use_ndvi: Optional[bool] = None,\n        filter_edges: bool = True,\n        edge_buffer: int = 20,\n        **kwargs: Any,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Process a Sentinel-2 raster to extract field boundaries.\n\n        Args:\n            raster_path: Path to Sentinel-2 raster file\n            output_path: Path to output GeoJSON or Parquet file (optional)\n            batch_size: Batch size for processing\n            band_selection: List of bands to use (None = use instance default)\n            use_ndvi: Whether to include NDVI (None = use instance default)\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects\n            **kwargs: Additional parameters for processing\n\n        Returns:\n            GeoDataFrame with field boundaries\n        \"\"\"\n        # Use instance defaults if not specified\n        band_selection = (\n            band_selection if band_selection is not None else self.band_selection\n        )\n        use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Update band statistics if not already done\n        if kwargs.get(\"update_stats\", True):\n            self.update_band_stats(raster_path, band_selection)\n\n        print(f\"Processing with parameters:\")\n        print(f\"- Using bands: {band_selection}\")\n        print(f\"- Include NDVI: {use_ndvi}\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n\n        # Create a custom Sentinel-2 dataset class\n        class Sentinel2Dataset(torch.utils.data.Dataset):\n            def __init__(\n                self,\n                raster_path: str,\n                chip_size: Tuple[int, int],\n                stride_x: int,\n                stride_y: int,\n                band_selection: List[int],\n                use_ndvi: bool,\n                field_delineator: Any,\n            ) -&gt; None:\n                self.raster_path = raster_path\n                self.chip_size = chip_size\n                self.stride_x = stride_x\n                self.stride_y = stride_y\n                self.band_selection = band_selection\n                self.use_ndvi = use_ndvi\n                self.field_delineator = field_delineator\n\n                with rasterio.open(self.raster_path) as src:\n                    self.height = src.height\n                    self.width = src.width\n                    self.count = src.count\n                    self.crs = src.crs\n                    self.transform = src.transform\n\n                    # Calculate row_starts and col_starts\n                    self.row_starts = []\n                    self.col_starts = []\n\n                    # Normal row starts using stride\n                    for r in range((self.height - 1) // self.stride_y):\n                        self.row_starts.append(r * self.stride_y)\n\n                    # Add a special last row that ensures we reach the bottom edge\n                    if self.height &gt; self.chip_size[0]:\n                        self.row_starts.append(max(0, self.height - self.chip_size[0]))\n                    else:\n                        # If the image is smaller than chip size, just start at 0\n                        if not self.row_starts:\n                            self.row_starts.append(0)\n\n                    # Normal column starts using stride\n                    for c in range((self.width - 1) // self.stride_x):\n                        self.col_starts.append(c * self.stride_x)\n\n                    # Add a special last column that ensures we reach the right edge\n                    if self.width &gt; self.chip_size[1]:\n                        self.col_starts.append(max(0, self.width - self.chip_size[1]))\n                    else:\n                        # If the image is smaller than chip size, just start at 0\n                        if not self.col_starts:\n                            self.col_starts.append(0)\n\n                # Calculate number of tiles\n                self.rows = len(self.row_starts)\n                self.cols = len(self.col_starts)\n\n                print(\n                    f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n                )\n                print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n                print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n\n            def __len__(self) -&gt; int:\n                return self.rows * self.cols\n\n            def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n                # Convert flat index to grid position\n                row = idx // self.cols\n                col = idx % self.cols\n\n                # Get pre-calculated starting positions\n                j = self.row_starts[row]\n                i = self.col_starts[col]\n\n                # Read window from raster\n                with rasterio.open(self.raster_path) as src:\n                    # Make sure we don't read outside the image\n                    width = min(self.chip_size[1], self.width - i)\n                    height = min(self.chip_size[0], self.height - j)\n\n                    window = Window(i, j, width, height)\n\n                    # Read all bands\n                    image = src.read(window=window)\n\n                    # Handle partial windows at edges by padding\n                    if (\n                        image.shape[1] != self.chip_size[0]\n                        or image.shape[2] != self.chip_size[1]\n                    ):\n                        temp = np.zeros(\n                            (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                            dtype=image.dtype,\n                        )\n                        temp[:, : image.shape[1], : image.shape[2]] = image\n                        image = temp\n\n                # Preprocess bands for the model\n                image_tensor = self.field_delineator.preprocess_sentinel_bands(\n                    image, self.band_selection, self.use_ndvi\n                )\n\n                # Get geographic bounds for the window\n                with rasterio.open(self.raster_path) as src:\n                    window_transform = src.window_transform(window)\n                    minx, miny = window_transform * (0, height)\n                    maxx, maxy = window_transform * (width, 0)\n                    bbox = [minx, miny, maxx, maxy]\n\n                return {\n                    \"image\": image_tensor,\n                    \"bbox\": bbox,\n                    \"coords\": torch.tensor([i, j], dtype=torch.long),\n                    \"window_size\": torch.tensor([width, height], dtype=torch.long),\n                }\n\n        # Calculate stride based on overlap\n        stride_x = int(chip_size[1] * (1 - overlap))\n        stride_y = int(chip_size[0] * (1 - overlap))\n\n        # Create dataset\n        dataset = Sentinel2Dataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            stride_x=stride_x,\n            stride_y=stride_y,\n            band_selection=band_selection,\n            use_ndvi=use_ndvi,\n            field_delineator=self,\n        )\n\n        # Define custom collate function\n        def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate bbox objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches (call the parent class's process_raster method)\n        # We'll adapt the process_raster method to work with our Sentinel2Dataset\n        results = super().process_raster(\n            raster_path=raster_path,\n            output_path=output_path,\n            batch_size=batch_size,\n            filter_edges=filter_edges,\n            edge_buffer=edge_buffer,\n            confidence_threshold=confidence_threshold,\n            overlap=overlap,\n            chip_size=chip_size,\n            nms_iou_threshold=nms_iou_threshold,\n            mask_threshold=mask_threshold,\n            min_object_area=min_object_area,\n            simplify_tolerance=simplify_tolerance,\n        )\n\n        return results\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.__init__","title":"<code>__init__(model_path='field_boundary_detector.pth', repo_id=None, model=None, device=None, band_selection=None, use_ndvi=False)</code>","text":"<p>Initialize the field boundary delineator.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'field_boundary_detector.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> <code>band_selection</code> <code>Optional[List[int]]</code> <p>List of Sentinel-2 band indices to use (None = adapt based on model)</p> <code>None</code> <code>use_ndvi</code> <code>bool</code> <p>Whether to calculate and include NDVI as an additional channel</p> <code>False</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"field_boundary_detector.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n    band_selection: Optional[List[int]] = None,\n    use_ndvi: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the field boundary delineator.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        band_selection: List of Sentinel-2 band indices to use (None = adapt based on model)\n        use_ndvi: Whether to calculate and include NDVI as an additional channel\n    \"\"\"\n    # Save parameters before calling parent constructor\n    self.custom_band_selection = band_selection\n    self.use_ndvi = use_ndvi\n\n    # Set device (copied from parent init to ensure it's set before initialize_model)\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Initialize model differently for multi-spectral input\n    model = self.initialize_sentinel2_model(model)\n\n    # Call parent but with our custom model\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n\n    # Default Sentinel-2 band statistics (can be overridden with actual stats)\n    # Band order: [B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12]\n    self.sentinel_band_stats = {\n        \"means\": [\n            0.0975,\n            0.0476,\n            0.0598,\n            0.0697,\n            0.1077,\n            0.1859,\n            0.2378,\n            0.2061,\n            0.2598,\n            0.4120,\n            0.1956,\n            0.1410,\n        ],\n        \"stds\": [\n            0.0551,\n            0.0290,\n            0.0298,\n            0.0479,\n            0.0506,\n            0.0505,\n            0.0747,\n            0.0642,\n            0.0782,\n            0.1187,\n            0.0651,\n            0.0679,\n        ],\n    }\n\n    # Set default band selection (RGB - typically B4, B3, B2 for Sentinel-2)\n    self.band_selection = (\n        self.custom_band_selection\n        if self.custom_band_selection is not None\n        else [3, 2, 1]\n    )  # R, G, B bands\n\n    # Customize parameters for field delineation\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.overlap = 0.5  # Higher overlap for field boundary detection\n    self.min_object_area = 1000  # Minimum area in pixels for field detection\n    self.simplify_tolerance = 2.0  # Higher tolerance for field boundaries\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.initialize_sentinel2_model","title":"<code>initialize_sentinel2_model(model=None)</code>","text":"<p>Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Any]</code> <p>Pre-initialized model (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Modified model with appropriate input channels</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_sentinel2_model(self, model: Optional[Any] = None) -&gt; Any:\n    \"\"\"\n    Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.\n\n    Args:\n        model: Pre-initialized model (optional)\n\n    Returns:\n        Modified model with appropriate input channels\n    \"\"\"\n    import torchvision\n    from torchvision.models.detection import maskrcnn_resnet50_fpn\n    from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n    if model is not None:\n        return model\n\n    # Determine number of input channels based on band selection and NDVI\n    num_input_channels = (\n        len(self.custom_band_selection)\n        if self.custom_band_selection is not None\n        else 3\n    )\n    if self.use_ndvi:\n        num_input_channels += 1\n\n    print(f\"Initializing Mask R-CNN model with {num_input_channels} input channels\")\n\n    # Create a ResNet50 backbone with modified input channels\n    backbone = resnet_fpn_backbone(\"resnet50\", weights=None)\n\n    # Replace the first conv layer to accept multi-spectral input\n    original_conv = backbone.body.conv1\n    backbone.body.conv1 = torch.nn.Conv2d(\n        num_input_channels,\n        original_conv.out_channels,\n        kernel_size=original_conv.kernel_size,\n        stride=original_conv.stride,\n        padding=original_conv.padding,\n        bias=original_conv.bias is not None,\n    )\n\n    # Create Mask R-CNN with our modified backbone\n    model = maskrcnn_resnet50_fpn(\n        backbone=backbone,\n        num_classes=2,  # Background + field\n        image_mean=[0.485] * num_input_channels,  # Extend mean to all channels\n        image_std=[0.229] * num_input_channels,  # Extend std to all channels\n    )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.preprocess_sentinel_bands","title":"<code>preprocess_sentinel_bands(image_data, band_selection=None, use_ndvi=None)</code>","text":"<p>Preprocess Sentinel-2 band data for model input.</p> <p>Parameters:</p> Name Type Description Default <code>image_data</code> <code>ndarray</code> <p>Raw Sentinel-2 image data as numpy array [bands, height, width]</p> required <code>band_selection</code> <code>Optional[List[int]]</code> <p>List of band indices to use (overrides instance default if provided)</p> <code>None</code> <code>use_ndvi</code> <code>Optional[bool]</code> <p>Whether to include NDVI (overrides instance default if provided)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Processed tensor ready for model input</p> Source code in <code>geoai/extract.py</code> <pre><code>def preprocess_sentinel_bands(\n    self,\n    image_data: np.ndarray,\n    band_selection: Optional[List[int]] = None,\n    use_ndvi: Optional[bool] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocess Sentinel-2 band data for model input.\n\n    Args:\n        image_data: Raw Sentinel-2 image data as numpy array [bands, height, width]\n        band_selection: List of band indices to use (overrides instance default if provided)\n        use_ndvi: Whether to include NDVI (overrides instance default if provided)\n\n    Returns:\n        Processed tensor ready for model input\n    \"\"\"\n    # Use instance defaults if not specified\n    band_selection = (\n        band_selection if band_selection is not None else self.band_selection\n    )\n    use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n    # Select bands\n    selected_bands = image_data[band_selection]\n\n    # Calculate NDVI if requested (using B8 and B4 which are indices 7 and 3)\n    if (\n        use_ndvi\n        and 7 in range(image_data.shape[0])\n        and 3 in range(image_data.shape[0])\n    ):\n        nir = image_data[7].astype(np.float32)  # B8 (NIR)\n        red = image_data[3].astype(np.float32)  # B4 (Red)\n\n        # Avoid division by zero\n        denominator = nir + red\n        ndvi = np.zeros_like(nir)\n        valid_mask = denominator &gt; 0\n        ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / denominator[\n            valid_mask\n        ]\n\n        # Rescale NDVI from [-1, 1] to [0, 1]\n        ndvi = (ndvi + 1) / 2\n\n        # Add NDVI as an additional channel\n        selected_bands = np.vstack([selected_bands, ndvi[np.newaxis, :, :]])\n\n    # Convert to tensor\n    image_tensor = torch.from_numpy(selected_bands).float()\n\n    # Normalize using band statistics\n    for i, band_idx in enumerate(band_selection):\n        # Make sure band_idx is within range of our statistics\n        if band_idx &lt; len(self.sentinel_band_stats[\"means\"]):\n            mean = self.sentinel_band_stats[\"means\"][band_idx]\n            std = self.sentinel_band_stats[\"stds\"][band_idx]\n            image_tensor[i] = (image_tensor[i] - mean) / std\n\n    # If NDVI was added, normalize it too (last channel)\n    if use_ndvi:\n        # NDVI is already roughly in [0,1] range, just standardize it slightly\n        image_tensor[-1] = (image_tensor[-1] - 0.5) / 0.5\n\n    return image_tensor\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.process_sentinel_raster","title":"<code>process_sentinel_raster(raster_path, output_path=None, batch_size=4, band_selection=None, use_ndvi=None, filter_edges=True, edge_buffer=20, **kwargs)</code>","text":"<p>Process a Sentinel-2 raster to extract field boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to Sentinel-2 raster file</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to output GeoJSON or Parquet file (optional)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>band_selection</code> <code>Optional[List[int]]</code> <p>List of bands to use (None = use instance default)</p> <code>None</code> <code>use_ndvi</code> <code>Optional[bool]</code> <p>Whether to include NDVI (None = use instance default)</p> <code>None</code> <code>filter_edges</code> <code>bool</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <code>int</code> <p>Size of edge buffer in pixels to filter out objects</p> <code>20</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for processing</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with field boundaries</p> Source code in <code>geoai/extract.py</code> <pre><code>def process_sentinel_raster(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    batch_size: int = 4,\n    band_selection: Optional[List[int]] = None,\n    use_ndvi: Optional[bool] = None,\n    filter_edges: bool = True,\n    edge_buffer: int = 20,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Process a Sentinel-2 raster to extract field boundaries.\n\n    Args:\n        raster_path: Path to Sentinel-2 raster file\n        output_path: Path to output GeoJSON or Parquet file (optional)\n        batch_size: Batch size for processing\n        band_selection: List of bands to use (None = use instance default)\n        use_ndvi: Whether to include NDVI (None = use instance default)\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects\n        **kwargs: Additional parameters for processing\n\n    Returns:\n        GeoDataFrame with field boundaries\n    \"\"\"\n    # Use instance defaults if not specified\n    band_selection = (\n        band_selection if band_selection is not None else self.band_selection\n    )\n    use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Update band statistics if not already done\n    if kwargs.get(\"update_stats\", True):\n        self.update_band_stats(raster_path, band_selection)\n\n    print(f\"Processing with parameters:\")\n    print(f\"- Using bands: {band_selection}\")\n    print(f\"- Include NDVI: {use_ndvi}\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n\n    # Create a custom Sentinel-2 dataset class\n    class Sentinel2Dataset(torch.utils.data.Dataset):\n        def __init__(\n            self,\n            raster_path: str,\n            chip_size: Tuple[int, int],\n            stride_x: int,\n            stride_y: int,\n            band_selection: List[int],\n            use_ndvi: bool,\n            field_delineator: Any,\n        ) -&gt; None:\n            self.raster_path = raster_path\n            self.chip_size = chip_size\n            self.stride_x = stride_x\n            self.stride_y = stride_y\n            self.band_selection = band_selection\n            self.use_ndvi = use_ndvi\n            self.field_delineator = field_delineator\n\n            with rasterio.open(self.raster_path) as src:\n                self.height = src.height\n                self.width = src.width\n                self.count = src.count\n                self.crs = src.crs\n                self.transform = src.transform\n\n                # Calculate row_starts and col_starts\n                self.row_starts = []\n                self.col_starts = []\n\n                # Normal row starts using stride\n                for r in range((self.height - 1) // self.stride_y):\n                    self.row_starts.append(r * self.stride_y)\n\n                # Add a special last row that ensures we reach the bottom edge\n                if self.height &gt; self.chip_size[0]:\n                    self.row_starts.append(max(0, self.height - self.chip_size[0]))\n                else:\n                    # If the image is smaller than chip size, just start at 0\n                    if not self.row_starts:\n                        self.row_starts.append(0)\n\n                # Normal column starts using stride\n                for c in range((self.width - 1) // self.stride_x):\n                    self.col_starts.append(c * self.stride_x)\n\n                # Add a special last column that ensures we reach the right edge\n                if self.width &gt; self.chip_size[1]:\n                    self.col_starts.append(max(0, self.width - self.chip_size[1]))\n                else:\n                    # If the image is smaller than chip size, just start at 0\n                    if not self.col_starts:\n                        self.col_starts.append(0)\n\n            # Calculate number of tiles\n            self.rows = len(self.row_starts)\n            self.cols = len(self.col_starts)\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n\n        def __len__(self) -&gt; int:\n            return self.rows * self.cols\n\n        def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n            # Convert flat index to grid position\n            row = idx // self.cols\n            col = idx % self.cols\n\n            # Get pre-calculated starting positions\n            j = self.row_starts[row]\n            i = self.col_starts[col]\n\n            # Read window from raster\n            with rasterio.open(self.raster_path) as src:\n                # Make sure we don't read outside the image\n                width = min(self.chip_size[1], self.width - i)\n                height = min(self.chip_size[0], self.height - j)\n\n                window = Window(i, j, width, height)\n\n                # Read all bands\n                image = src.read(window=window)\n\n                # Handle partial windows at edges by padding\n                if (\n                    image.shape[1] != self.chip_size[0]\n                    or image.shape[2] != self.chip_size[1]\n                ):\n                    temp = np.zeros(\n                        (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                        dtype=image.dtype,\n                    )\n                    temp[:, : image.shape[1], : image.shape[2]] = image\n                    image = temp\n\n            # Preprocess bands for the model\n            image_tensor = self.field_delineator.preprocess_sentinel_bands(\n                image, self.band_selection, self.use_ndvi\n            )\n\n            # Get geographic bounds for the window\n            with rasterio.open(self.raster_path) as src:\n                window_transform = src.window_transform(window)\n                minx, miny = window_transform * (0, height)\n                maxx, maxy = window_transform * (width, 0)\n                bbox = [minx, miny, maxx, maxy]\n\n            return {\n                \"image\": image_tensor,\n                \"bbox\": bbox,\n                \"coords\": torch.tensor([i, j], dtype=torch.long),\n                \"window_size\": torch.tensor([width, height], dtype=torch.long),\n            }\n\n    # Calculate stride based on overlap\n    stride_x = int(chip_size[1] * (1 - overlap))\n    stride_y = int(chip_size[0] * (1 - overlap))\n\n    # Create dataset\n    dataset = Sentinel2Dataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        stride_x=stride_x,\n        stride_y=stride_y,\n        band_selection=band_selection,\n        use_ndvi=use_ndvi,\n        field_delineator=self,\n    )\n\n    # Define custom collate function\n    def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate bbox objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches (call the parent class's process_raster method)\n    # We'll adapt the process_raster method to work with our Sentinel2Dataset\n    results = super().process_raster(\n        raster_path=raster_path,\n        output_path=output_path,\n        batch_size=batch_size,\n        filter_edges=filter_edges,\n        edge_buffer=edge_buffer,\n        confidence_threshold=confidence_threshold,\n        overlap=overlap,\n        chip_size=chip_size,\n        nms_iou_threshold=nms_iou_threshold,\n        mask_threshold=mask_threshold,\n        min_object_area=min_object_area,\n        simplify_tolerance=simplify_tolerance,\n    )\n\n    return results\n</code></pre>"},{"location":"extract/#geoai.extract.AgricultureFieldDelineator.update_band_stats","title":"<code>update_band_stats(raster_path, band_selection=None, sample_size=1000)</code>","text":"<p>Update band statistics from the input Sentinel-2 raster.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the Sentinel-2 raster file</p> required <code>band_selection</code> <code>Optional[List[int]]</code> <p>Specific bands to update (None = update all available)</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of random pixels to sample for statistics calculation</p> <code>1000</code> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>Updated band statistics dictionary</p> Source code in <code>geoai/extract.py</code> <pre><code>def update_band_stats(\n    self,\n    raster_path: str,\n    band_selection: Optional[List[int]] = None,\n    sample_size: int = 1000,\n) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Update band statistics from the input Sentinel-2 raster.\n\n    Args:\n        raster_path: Path to the Sentinel-2 raster file\n        band_selection: Specific bands to update (None = update all available)\n        sample_size: Number of random pixels to sample for statistics calculation\n\n    Returns:\n        Updated band statistics dictionary\n    \"\"\"\n    with rasterio.open(raster_path) as src:\n        # Check if this is likely a Sentinel-2 product\n        band_count = src.count\n        if band_count &lt; 3:\n            print(\n                f\"Warning: Raster has only {band_count} bands, may not be Sentinel-2 data\"\n            )\n\n        # Get dimensions\n        height, width = src.height, src.width\n\n        # Determine which bands to analyze\n        if band_selection is None:\n            band_selection = list(range(1, band_count + 1))  # 1-indexed\n\n        # Initialize arrays for band statistics\n        means = []\n        stds = []\n\n        # Sample random pixels\n        np.random.seed(42)  # For reproducibility\n        sample_rows = np.random.randint(0, height, sample_size)\n        sample_cols = np.random.randint(0, width, sample_size)\n\n        # Calculate statistics for each band\n        for band in band_selection:\n            # Read band data\n            band_data = src.read(band)\n\n            # Sample values\n            sample_values = band_data[sample_rows, sample_cols]\n\n            # Remove invalid values (e.g., nodata)\n            valid_samples = sample_values[np.isfinite(sample_values)]\n\n            # Calculate statistics\n            mean = float(np.mean(valid_samples))\n            std = float(np.std(valid_samples))\n\n            # Store results\n            means.append(mean)\n            stds.append(std)\n\n            print(f\"Band {band}: mean={mean:.4f}, std={std:.4f}\")\n\n        # Update instance variables\n        self.sentinel_band_stats = {\"means\": means, \"stds\": stds}\n\n        return self.sentinel_band_stats\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor","title":"<code>BuildingFootprintExtractor</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Building footprint extraction using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for building footprint extraction.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class BuildingFootprintExtractor(ObjectDetector):\n    \"\"\"\n    Building footprint extraction using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for building footprint extraction.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"building_footprints_usa.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n    def regularize_buildings(\n        self,\n        gdf: gpd.GeoDataFrame,\n        min_area: int = 10,\n        angle_threshold: int = 15,\n        orthogonality_threshold: float = 0.3,\n        rectangularity_threshold: float = 0.7,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Regularize building footprints to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with building footprints\n            min_area: Minimum area in square units to keep a building\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n            rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized building footprints\n        \"\"\"\n        return self.regularize_objects(\n            gdf,\n            min_area=min_area,\n            angle_threshold=angle_threshold,\n            orthogonality_threshold=orthogonality_threshold,\n            rectangularity_threshold=rectangularity_threshold,\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.__init__","title":"<code>__init__(model_path='building_footprints_usa.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'building_footprints_usa.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"building_footprints_usa.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.BuildingFootprintExtractor.regularize_buildings","title":"<code>regularize_buildings(gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize building footprints to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with building footprints</p> required <code>min_area</code> <code>int</code> <p>Minimum area in square units to keep a building</p> <code>10</code> <code>angle_threshold</code> <code>int</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <code>float</code> <p>Percentage of angles that must be orthogonal for a building to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <code>float</code> <p>Minimum area ratio to building's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with regularized building footprints</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_buildings(\n    self,\n    gdf: gpd.GeoDataFrame,\n    min_area: int = 10,\n    angle_threshold: int = 15,\n    orthogonality_threshold: float = 0.3,\n    rectangularity_threshold: float = 0.7,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Regularize building footprints to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with building footprints\n        min_area: Minimum area in square units to keep a building\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n        rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized building footprints\n    \"\"\"\n    return self.regularize_objects(\n        gdf,\n        min_area=min_area,\n        angle_threshold=angle_threshold,\n        orthogonality_threshold=orthogonality_threshold,\n        rectangularity_threshold=rectangularity_threshold,\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.CarDetector","title":"<code>CarDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CarDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the `ObjectDetector` class with additional methods for car detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"car_detection_usa.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.CarDetector.__init__","title":"<code>__init__(model_path='car_detection_usa.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'car_detection_usa.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"car_detection_usa.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>A TorchGeo dataset for object extraction with overlapping tiles support.</p> <p>This dataset class creates overlapping image tiles for object detection, ensuring complete coverage of the input raster including right and bottom edges. It inherits from NonGeoDataset to avoid spatial indexing issues.</p> <p>Attributes:</p> Name Type Description <code>raster_path</code> <p>Path to the input raster file.</p> <code>chip_size</code> <p>Size of image chips to extract (height, width).</p> <code>overlap</code> <p>Amount of overlap between adjacent tiles (0.0-1.0).</p> <code>transforms</code> <p>Transforms to apply to the image.</p> <code>verbose</code> <p>Whether to print detailed processing information.</p> <code>stride_x</code> <p>Horizontal stride between tiles based on overlap.</p> <code>stride_y</code> <p>Vertical stride between tiles based on overlap.</p> <code>row_starts</code> <p>Starting Y positions for each row of tiles.</p> <code>col_starts</code> <p>Starting X positions for each column of tiles.</p> <code>crs</code> <p>Coordinate reference system of the raster.</p> <code>transform</code> <p>Affine transform of the raster.</p> <code>height</code> <p>Height of the raster in pixels.</p> <code>width</code> <p>Width of the raster in pixels.</p> <code>count</code> <p>Number of bands in the raster.</p> <code>bounds</code> <p>Geographic bounds of the raster (west, south, east, north).</p> <code>roi</code> <p>Shapely box representing the region of interest.</p> <code>rows</code> <p>Number of rows of tiles.</p> <code>cols</code> <p>Number of columns of tiles.</p> <code>raster_stats</code> <p>Statistics of the raster.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CustomDataset(NonGeoDataset):\n    \"\"\"\n    A TorchGeo dataset for object extraction with overlapping tiles support.\n\n    This dataset class creates overlapping image tiles for object detection,\n    ensuring complete coverage of the input raster including right and bottom edges.\n    It inherits from NonGeoDataset to avoid spatial indexing issues.\n\n    Attributes:\n        raster_path: Path to the input raster file.\n        chip_size: Size of image chips to extract (height, width).\n        overlap: Amount of overlap between adjacent tiles (0.0-1.0).\n        transforms: Transforms to apply to the image.\n        verbose: Whether to print detailed processing information.\n        stride_x: Horizontal stride between tiles based on overlap.\n        stride_y: Vertical stride between tiles based on overlap.\n        row_starts: Starting Y positions for each row of tiles.\n        col_starts: Starting X positions for each column of tiles.\n        crs: Coordinate reference system of the raster.\n        transform: Affine transform of the raster.\n        height: Height of the raster in pixels.\n        width: Width of the raster in pixels.\n        count: Number of bands in the raster.\n        bounds: Geographic bounds of the raster (west, south, east, north).\n        roi: Shapely box representing the region of interest.\n        rows: Number of rows of tiles.\n        cols: Number of columns of tiles.\n        raster_stats: Statistics of the raster.\n    \"\"\"\n\n    def __init__(\n        self,\n        raster_path: str,\n        chip_size: Tuple[int, int] = (512, 512),\n        overlap: float = 0.5,\n        transforms: Optional[Any] = None,\n        band_indexes: Optional[List[int]] = None,\n        verbose: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the dataset with overlapping tiles.\n\n        Args:\n            raster_path: Path to the input raster file.\n            chip_size: Size of image chips to extract (height, width). Default is (512, 512).\n            overlap: Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).\n            transforms: Transforms to apply to the image. Default is None.\n            band_indexes: List of band indexes to use. Default is None (use all bands).\n            verbose: Whether to print detailed processing information. Default is False.\n\n        Raises:\n            ValueError: If overlap is too high resulting in non-positive stride.\n        \"\"\"\n        super().__init__()\n\n        # Initialize parameters\n        self.raster_path = raster_path\n        self.chip_size = chip_size\n        self.overlap = overlap\n        self.transforms = transforms\n        self.band_indexes = band_indexes\n        self.verbose = verbose\n        self.warned_about_bands = False\n\n        # Calculate stride based on overlap\n        self.stride_x = int(chip_size[1] * (1 - overlap))\n        self.stride_y = int(chip_size[0] * (1 - overlap))\n\n        if self.stride_x &lt;= 0 or self.stride_y &lt;= 0:\n            raise ValueError(\n                f\"Overlap {overlap} is too high, resulting in non-positive stride\"\n            )\n\n        with rasterio.open(self.raster_path) as src:\n            self.crs = src.crs\n            self.transform = src.transform\n            self.height = src.height\n            self.width = src.width\n            self.count = src.count\n\n            # Define the bounds of the dataset\n            west, south, east, north = src.bounds\n            self.bounds = (west, south, east, north)\n            self.roi = box(*self.bounds)\n\n            # Calculate starting positions for each tile\n            self.row_starts = []\n            self.col_starts = []\n\n            # Normal row starts using stride\n            for r in range((self.height - 1) // self.stride_y):\n                self.row_starts.append(r * self.stride_y)\n\n            # Add a special last row that ensures we reach the bottom edge\n            if self.height &gt; self.chip_size[0]:\n                self.row_starts.append(max(0, self.height - self.chip_size[0]))\n            else:\n                # If the image is smaller than chip size, just start at 0\n                if not self.row_starts:\n                    self.row_starts.append(0)\n\n            # Normal column starts using stride\n            for c in range((self.width - 1) // self.stride_x):\n                self.col_starts.append(c * self.stride_x)\n\n            # Add a special last column that ensures we reach the right edge\n            if self.width &gt; self.chip_size[1]:\n                self.col_starts.append(max(0, self.width - self.chip_size[1]))\n            else:\n                # If the image is smaller than chip size, just start at 0\n                if not self.col_starts:\n                    self.col_starts.append(0)\n\n            # Update rows and cols based on actual starting positions\n            self.rows = len(self.row_starts)\n            self.cols = len(self.col_starts)\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n            print(\n                f\"Overlap: {overlap*100}% (stride_x={self.stride_x}, stride_y={self.stride_y})\"\n            )\n            if src.crs:\n                print(f\"CRS: {src.crs}\")\n\n        # Get raster stats\n        self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get an image chip from the dataset by index.\n\n        Retrieves an image tile with the specified overlap pattern, ensuring\n        proper coverage of the entire raster including edges.\n\n        Args:\n            idx: Index of the chip to retrieve.\n\n        Returns:\n            dict: Dictionary containing:\n                - image: Image tensor.\n                - bbox: Geographic bounding box for the window.\n                - coords: Pixel coordinates as tensor [i, j].\n                - window_size: Window size as tensor [width, height].\n        \"\"\"\n        # Convert flat index to grid position\n        row = idx // self.cols\n        col = idx % self.cols\n\n        # Get pre-calculated starting positions\n        j = self.row_starts[row]\n        i = self.col_starts[col]\n\n        # Read window from raster\n        with rasterio.open(self.raster_path) as src:\n            # Make sure we don't read outside the image\n            width = min(self.chip_size[1], self.width - i)\n            height = min(self.chip_size[0], self.height - j)\n\n            window = Window(i, j, width, height)\n            image = src.read(window=window)\n\n            # Handle RGBA or multispectral images - keep only first 3 bands\n            if image.shape[0] &gt; 3:\n                if not self.warned_about_bands and self.verbose:\n                    print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                    self.warned_about_bands = True\n                if self.band_indexes is not None:\n                    image = image[self.band_indexes]\n                else:\n                    image = image[:3]\n            elif image.shape[0] &lt; 3:\n                # If image has fewer than 3 bands, duplicate the last band to make 3\n                if not self.warned_about_bands and self.verbose:\n                    print(\n                        f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                    )\n                    self.warned_about_bands = True\n                temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n                for c in range(3):\n                    temp[c] = image[min(c, image.shape[0] - 1)]\n                image = temp\n\n            # Handle partial windows at edges by padding\n            if (\n                image.shape[1] != self.chip_size[0]\n                or image.shape[2] != self.chip_size[1]\n            ):\n                temp = np.zeros(\n                    (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                    dtype=image.dtype,\n                )\n                temp[:, : image.shape[1], : image.shape[2]] = image\n                image = temp\n\n        # Convert to format expected by model (C,H,W)\n        image = torch.from_numpy(image).float()\n\n        # Normalize to [0, 1]\n        if image.max() &gt; 1:\n            image = image / 255.0\n\n        # Apply transforms if any\n        if self.transforms is not None:\n            image = self.transforms(image)\n\n        # Create geographic bounding box for the window\n        minx, miny = self.transform * (i, j + height)\n        maxx, maxy = self.transform * (i + width, j)\n        bbox = box(minx, miny, maxx, maxy)\n\n        return {\n            \"image\": image,\n            \"bbox\": bbox,\n            \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n            \"window_size\": torch.tensor(\n                [width, height], dtype=torch.long\n            ),  # Consistent format\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the number of samples in the dataset.\n\n        Returns:\n            int: Total number of tiles in the dataset.\n        \"\"\"\n        return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an image chip from the dataset by index.</p> <p>Retrieves an image tile with the specified overlap pattern, ensuring proper coverage of the entire raster including edges.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the chip to retrieve.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing: - image: Image tensor. - bbox: Geographic bounding box for the window. - coords: Pixel coordinates as tensor [i, j]. - window_size: Window size as tensor [width, height].</p> Source code in <code>geoai/extract.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get an image chip from the dataset by index.\n\n    Retrieves an image tile with the specified overlap pattern, ensuring\n    proper coverage of the entire raster including edges.\n\n    Args:\n        idx: Index of the chip to retrieve.\n\n    Returns:\n        dict: Dictionary containing:\n            - image: Image tensor.\n            - bbox: Geographic bounding box for the window.\n            - coords: Pixel coordinates as tensor [i, j].\n            - window_size: Window size as tensor [width, height].\n    \"\"\"\n    # Convert flat index to grid position\n    row = idx // self.cols\n    col = idx % self.cols\n\n    # Get pre-calculated starting positions\n    j = self.row_starts[row]\n    i = self.col_starts[col]\n\n    # Read window from raster\n    with rasterio.open(self.raster_path) as src:\n        # Make sure we don't read outside the image\n        width = min(self.chip_size[1], self.width - i)\n        height = min(self.chip_size[0], self.height - j)\n\n        window = Window(i, j, width, height)\n        image = src.read(window=window)\n\n        # Handle RGBA or multispectral images - keep only first 3 bands\n        if image.shape[0] &gt; 3:\n            if not self.warned_about_bands and self.verbose:\n                print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                self.warned_about_bands = True\n            if self.band_indexes is not None:\n                image = image[self.band_indexes]\n            else:\n                image = image[:3]\n        elif image.shape[0] &lt; 3:\n            # If image has fewer than 3 bands, duplicate the last band to make 3\n            if not self.warned_about_bands and self.verbose:\n                print(\n                    f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                )\n                self.warned_about_bands = True\n            temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n            for c in range(3):\n                temp[c] = image[min(c, image.shape[0] - 1)]\n            image = temp\n\n        # Handle partial windows at edges by padding\n        if (\n            image.shape[1] != self.chip_size[0]\n            or image.shape[2] != self.chip_size[1]\n        ):\n            temp = np.zeros(\n                (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                dtype=image.dtype,\n            )\n            temp[:, : image.shape[1], : image.shape[2]] = image\n            image = temp\n\n    # Convert to format expected by model (C,H,W)\n    image = torch.from_numpy(image).float()\n\n    # Normalize to [0, 1]\n    if image.max() &gt; 1:\n        image = image / 255.0\n\n    # Apply transforms if any\n    if self.transforms is not None:\n        image = self.transforms(image)\n\n    # Create geographic bounding box for the window\n    minx, miny = self.transform * (i, j + height)\n    maxx, maxy = self.transform * (i + width, j)\n    bbox = box(minx, miny, maxx, maxy)\n\n    return {\n        \"image\": image,\n        \"bbox\": bbox,\n        \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n        \"window_size\": torch.tensor(\n            [width, height], dtype=torch.long\n        ),  # Consistent format\n    }\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__init__","title":"<code>__init__(raster_path, chip_size=(512, 512), overlap=0.5, transforms=None, band_indexes=None, verbose=False)</code>","text":"<p>Initialize the dataset with overlapping tiles.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file.</p> required <code>chip_size</code> <code>Tuple[int, int]</code> <p>Size of image chips to extract (height, width). Default is (512, 512).</p> <code>(512, 512)</code> <code>overlap</code> <code>float</code> <p>Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).</p> <code>0.5</code> <code>transforms</code> <code>Optional[Any]</code> <p>Transforms to apply to the image. Default is None.</p> <code>None</code> <code>band_indexes</code> <code>Optional[List[int]]</code> <p>List of band indexes to use. Default is None (use all bands).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed processing information. Default is False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If overlap is too high resulting in non-positive stride.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    raster_path: str,\n    chip_size: Tuple[int, int] = (512, 512),\n    overlap: float = 0.5,\n    transforms: Optional[Any] = None,\n    band_indexes: Optional[List[int]] = None,\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the dataset with overlapping tiles.\n\n    Args:\n        raster_path: Path to the input raster file.\n        chip_size: Size of image chips to extract (height, width). Default is (512, 512).\n        overlap: Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).\n        transforms: Transforms to apply to the image. Default is None.\n        band_indexes: List of band indexes to use. Default is None (use all bands).\n        verbose: Whether to print detailed processing information. Default is False.\n\n    Raises:\n        ValueError: If overlap is too high resulting in non-positive stride.\n    \"\"\"\n    super().__init__()\n\n    # Initialize parameters\n    self.raster_path = raster_path\n    self.chip_size = chip_size\n    self.overlap = overlap\n    self.transforms = transforms\n    self.band_indexes = band_indexes\n    self.verbose = verbose\n    self.warned_about_bands = False\n\n    # Calculate stride based on overlap\n    self.stride_x = int(chip_size[1] * (1 - overlap))\n    self.stride_y = int(chip_size[0] * (1 - overlap))\n\n    if self.stride_x &lt;= 0 or self.stride_y &lt;= 0:\n        raise ValueError(\n            f\"Overlap {overlap} is too high, resulting in non-positive stride\"\n        )\n\n    with rasterio.open(self.raster_path) as src:\n        self.crs = src.crs\n        self.transform = src.transform\n        self.height = src.height\n        self.width = src.width\n        self.count = src.count\n\n        # Define the bounds of the dataset\n        west, south, east, north = src.bounds\n        self.bounds = (west, south, east, north)\n        self.roi = box(*self.bounds)\n\n        # Calculate starting positions for each tile\n        self.row_starts = []\n        self.col_starts = []\n\n        # Normal row starts using stride\n        for r in range((self.height - 1) // self.stride_y):\n            self.row_starts.append(r * self.stride_y)\n\n        # Add a special last row that ensures we reach the bottom edge\n        if self.height &gt; self.chip_size[0]:\n            self.row_starts.append(max(0, self.height - self.chip_size[0]))\n        else:\n            # If the image is smaller than chip size, just start at 0\n            if not self.row_starts:\n                self.row_starts.append(0)\n\n        # Normal column starts using stride\n        for c in range((self.width - 1) // self.stride_x):\n            self.col_starts.append(c * self.stride_x)\n\n        # Add a special last column that ensures we reach the right edge\n        if self.width &gt; self.chip_size[1]:\n            self.col_starts.append(max(0, self.width - self.chip_size[1]))\n        else:\n            # If the image is smaller than chip size, just start at 0\n            if not self.col_starts:\n                self.col_starts.append(0)\n\n        # Update rows and cols based on actual starting positions\n        self.rows = len(self.row_starts)\n        self.cols = len(self.col_starts)\n\n        print(\n            f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n        )\n        print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n        print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n        print(\n            f\"Overlap: {overlap*100}% (stride_x={self.stride_x}, stride_y={self.stride_y})\"\n        )\n        if src.crs:\n            print(f\"CRS: {src.crs}\")\n\n    # Get raster stats\n    self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n</code></pre>"},{"location":"extract/#geoai.extract.CustomDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of tiles in the dataset.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of samples in the dataset.\n\n    Returns:\n        int: Total number of tiles in the dataset.\n    \"\"\"\n    return self.rows * self.cols\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector","title":"<code>ObjectDetector</code>","text":"<p>Object extraction using Mask R-CNN with TorchGeo.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ObjectDetector:\n    \"\"\"\n    Object extraction using Mask R-CNN with TorchGeo.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: Optional[str] = None,\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        num_classes: int = 2,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Hugging Face repository ID for model download.\n            model: Pre-initialized model object (optional).\n            num_classes: Number of classes for detection (default: 2).\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        # Set device\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Default parameters for object detection - these can be overridden in process_raster\n        self.chip_size = (512, 512)  # Size of image chips for processing\n        self.overlap = 0.25  # Default overlap between tiles\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n        self.min_object_area = 100  # Minimum area in pixels to keep an object\n        self.max_object_area = None  # Maximum area in pixels to keep an object\n        self.mask_threshold = 0.5  # Threshold for mask binarization\n        self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n        # Initialize model\n        self.model = self.initialize_model(model, num_classes=num_classes)\n\n        # Download model if needed\n        if model_path is None or (not os.path.exists(model_path)):\n            model_path = self.download_model_from_hf(model_path, repo_id)\n\n        # Load model weights\n        self.load_weights(model_path)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n    def download_model_from_hf(\n        self, model_path: Optional[str] = None, repo_id: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"\n        Download the object detection model from Hugging Face.\n\n        Args:\n            model_path: Path to the model file.\n            repo_id: Hugging Face repository ID.\n\n        Returns:\n            Path to the downloaded model file\n        \"\"\"\n        try:\n\n            print(\"Model path not specified, downloading from Hugging Face...\")\n\n            # Define the repository ID and model filename\n            if repo_id is None:\n                repo_id = \"giswqs/geoai\"\n\n            if model_path is None:\n                model_path = \"building_footprints_usa.pth\"\n\n            # Download the model\n            model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n            print(f\"Model downloaded to: {model_path}\")\n\n            return model_path\n\n        except Exception as e:\n            print(f\"Error downloading model from Hugging Face: {e}\")\n            print(\"Please specify a local model path or ensure internet connectivity.\")\n            raise\n\n    def initialize_model(self, model: Optional[Any], num_classes: int = 2) -&gt; Any:\n        \"\"\"Initialize a deep learning model for object detection.\n\n        Args:\n            model (torch.nn.Module): A pre-initialized model object.\n            num_classes (int): Number of classes for detection.\n\n        Returns:\n            torch.nn.Module: A deep learning model for object detection.\n        \"\"\"\n\n        if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n            # Standard image mean and std for pre-trained models\n            image_mean = [0.485, 0.456, 0.406]\n            image_std = [0.229, 0.224, 0.225]\n\n            # Create model with explicit normalization parameters\n            model = maskrcnn_resnet50_fpn(\n                weights=None,\n                progress=False,\n                num_classes=num_classes,  # Background + object\n                weights_backbone=None,\n                # These parameters ensure consistent normalization\n                image_mean=image_mean,\n                image_std=image_std,\n            )\n\n        model.to(self.device)\n        return model\n\n    def load_weights(self, model_path: str) -&gt; None:\n        \"\"\"\n        Load weights from file with error handling for different formats.\n\n        Args:\n            model_path: Path to model weights\n        \"\"\"\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n        try:\n            state_dict = torch.load(model_path, map_location=self.device)\n\n            # Handle different state dict formats\n            if isinstance(state_dict, dict):\n                if \"model\" in state_dict:\n                    state_dict = state_dict[\"model\"]\n                elif \"state_dict\" in state_dict:\n                    state_dict = state_dict[\"state_dict\"]\n\n            # Try to load state dict\n            try:\n                self.model.load_state_dict(state_dict)\n                print(\"Model loaded successfully\")\n            except Exception as e:\n                print(f\"Error loading model: {e}\")\n                print(\"Attempting to fix state_dict keys...\")\n\n                # Try to fix state_dict keys (remove module prefix if needed)\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith(\"module.\"):\n                        new_state_dict[k[7:]] = v\n                    else:\n                        new_state_dict[k] = v\n\n                self.model.load_state_dict(new_state_dict)\n                print(\"Model loaded successfully after key fixing\")\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model: {e}\")\n\n    def mask_to_polygons(self, mask: np.ndarray, **kwargs: Any) -&gt; List[Polygon]:\n        \"\"\"\n        Convert binary mask to polygon contours using OpenCV.\n\n        Args:\n            mask: Binary mask as numpy array\n            **kwargs: Optional parameters:\n                simplify_tolerance: Tolerance for polygon simplification\n                mask_threshold: Threshold for mask binarization\n                min_object_area: Minimum area in pixels to keep an object\n                max_object_area: Maximum area in pixels to keep an object\n\n        Returns:\n            List of polygons as lists of (x, y) coordinates\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n        # Ensure binary mask\n        mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n        # Optional: apply morphological operations to improve mask quality\n        kernel = np.ones((3, 3), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Convert to list of [x, y] coordinates\n        polygons = []\n        for contour in contours:\n            # Filter out too small contours\n            if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n                continue\n\n            # Filter out too large contours\n            if (\n                max_object_area is not None\n                and cv2.contourArea(contour) &gt; max_object_area\n            ):\n                continue\n\n            # Simplify contour if it has many points\n            if contour.shape[0] &gt; 50:\n                epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                contour = cv2.approxPolyDP(contour, epsilon, True)\n\n            # Convert to list of [x, y] coordinates\n            polygon = contour.reshape(-1, 2).tolist()\n            polygons.append(polygon)\n\n        return polygons\n\n    def filter_overlapping_polygons(\n        self, gdf: gpd.GeoDataFrame, **kwargs: Any\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Filter overlapping polygons using non-maximum suppression.\n\n        Args:\n            gdf: GeoDataFrame with polygons\n            **kwargs: Optional parameters:\n                nms_iou_threshold: IoU threshold for filtering\n\n        Returns:\n            Filtered GeoDataFrame\n        \"\"\"\n        if len(gdf) &lt;= 1:\n            return gdf\n\n        # Get parameters from kwargs or use instance defaults\n        iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n        # Sort by confidence\n        gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n        # Fix any invalid geometries\n        gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n        )\n\n        keep_indices = []\n        polygons = gdf.geometry.values\n\n        for i in range(len(polygons)):\n            if i in keep_indices:\n                continue\n\n            keep = True\n            for j in keep_indices:\n                # Skip invalid geometries\n                if not polygons[i].is_valid or not polygons[j].is_valid:\n                    continue\n\n                # Calculate IoU\n                try:\n                    intersection = polygons[i].intersection(polygons[j]).area\n                    union = polygons[i].area + polygons[j].area - intersection\n                    iou = intersection / union if union &gt; 0 else 0\n\n                    if iou &gt; iou_threshold:\n                        keep = False\n                        break\n                except Exception:\n                    # Skip on topology exceptions\n                    continue\n\n            if keep:\n                keep_indices.append(i)\n\n        return gdf.iloc[keep_indices]\n\n    def filter_edge_objects(\n        self, gdf: gpd.GeoDataFrame, raster_path: str, edge_buffer: int = 10\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Filter out object detections that fall in padding/edge areas of the image.\n\n        Args:\n            gdf: GeoDataFrame with object detections\n            raster_path: Path to the original raster file\n            edge_buffer: Buffer in pixels to consider as edge region\n\n        Returns:\n            GeoDataFrame with filtered objects\n        \"\"\"\n        import rasterio\n        from shapely.geometry import box\n\n        # If no objects detected, return empty GeoDataFrame\n        if gdf is None or len(gdf) == 0:\n            return gdf\n\n        print(f\"Objects before filtering: {len(gdf)}\")\n\n        with rasterio.open(raster_path) as src:\n            # Get raster bounds\n            raster_bounds = src.bounds\n            raster_width = src.width\n            raster_height = src.height\n\n            # Convert edge buffer from pixels to geographic units\n            # We need the smallest dimension of a pixel in geographic units\n            pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n            pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n            buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n            # Create a slightly smaller bounding box to exclude edge regions\n            inner_bounds = (\n                raster_bounds[0] + buffer_size,  # min x (west)\n                raster_bounds[1] + buffer_size,  # min y (south)\n                raster_bounds[2] - buffer_size,  # max x (east)\n                raster_bounds[3] - buffer_size,  # max y (north)\n            )\n\n            # Check that inner bounds are valid\n            if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n                print(\"Warning: Edge buffer too large, using original bounds\")\n                inner_box = box(*raster_bounds)\n            else:\n                inner_box = box(*inner_bounds)\n\n            # Filter out objects that intersect with the edge of the image\n            filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n            # Additional check for objects that have &gt;50% of their area outside the valid region\n            valid_objects = []\n            for idx, row in filtered_gdf.iterrows():\n                if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                    valid_objects.append(idx)\n\n            filtered_gdf = filtered_gdf.loc[valid_objects]\n\n            print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n            return filtered_gdf\n\n    def masks_to_vector(\n        self,\n        mask_path: str,\n        output_path: Optional[str] = None,\n        simplify_tolerance: Optional[float] = None,\n        mask_threshold: Optional[float] = None,\n        min_object_area: Optional[int] = None,\n        max_object_area: Optional[int] = None,\n        nms_iou_threshold: Optional[float] = None,\n        regularize: bool = True,\n        angle_threshold: int = 15,\n        rectangularity_threshold: float = 0.7,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n        Args:\n            mask_path: Path to the object masks GeoTIFF\n            output_path: Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)\n            simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n            mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n            min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n            max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n            regularize: Whether to regularize objects to right angles (default: True)\n            angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n            rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Use class defaults if parameters not provided\n        simplify_tolerance = (\n            simplify_tolerance\n            if simplify_tolerance is not None\n            else self.simplify_tolerance\n        )\n        mask_threshold = (\n            mask_threshold if mask_threshold is not None else self.mask_threshold\n        )\n        min_object_area = (\n            min_object_area if min_object_area is not None else self.min_object_area\n        )\n        max_object_area = (\n            max_object_area if max_object_area is not None else self.max_object_area\n        )\n        nms_iou_threshold = (\n            nms_iou_threshold\n            if nms_iou_threshold is not None\n            else self.nms_iou_threshold\n        )\n\n        # Set default output path if not provided\n        # if output_path is None:\n        #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n        print(f\"Converting mask to GeoJSON with parameters:\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Regularize objects: {regularize}\")\n        if regularize:\n            print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n            print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n        # Open the mask raster\n        with rasterio.open(mask_path) as src:\n            # Read the mask data\n            mask_data = src.read(1)\n            transform = src.transform\n            crs = src.crs\n\n            # Print mask statistics\n            print(f\"Mask dimensions: {mask_data.shape}\")\n            print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n            # Prepare for connected component analysis\n            # Binarize the mask based on threshold\n            binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n            # Apply morphological operations for better results (optional)\n            kernel = np.ones((3, 3), np.uint8)\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n            # Find connected components\n            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask, connectivity=8\n            )\n\n            print(\n                f\"Found {num_labels-1} potential objects\"\n            )  # Subtract 1 for background\n\n            # Create list to store polygons and confidence values\n            all_polygons = []\n            all_confidences = []\n\n            # Process each component (skip the first one which is background)\n            for i in tqdm(range(1, num_labels)):\n                # Extract this object\n                area = stats[i, cv2.CC_STAT_AREA]\n\n                # Skip if too small\n                if area &lt; min_object_area:\n                    continue\n\n                # Skip if too large\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Create a mask for this object\n                object_mask = (labels == i).astype(np.uint8)\n\n                # Find contours\n                contours, _ = cv2.findContours(\n                    object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                )\n\n                # Process each contour\n                for contour in contours:\n                    # Skip if too few points\n                    if contour.shape[0] &lt; 3:\n                        continue\n\n                    # Simplify contour if it has many points\n                    if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                        epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                        contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                    # Convert to list of (x, y) coordinates\n                    polygon_points = contour.reshape(-1, 2)\n\n                    # Convert pixel coordinates to geographic coordinates\n                    geo_points = []\n                    for x, y in polygon_points:\n                        gx, gy = transform * (x, y)\n                        geo_points.append((gx, gy))\n\n                    # Create Shapely polygon\n                    if len(geo_points) &gt;= 3:\n                        try:\n                            shapely_poly = Polygon(geo_points)\n                            if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                all_polygons.append(shapely_poly)\n\n                                # Calculate \"confidence\" as normalized size\n                                # This is a proxy since we don't have model confidence scores\n                                normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                                all_confidences.append(normalized_size)\n                        except Exception as e:\n                            print(f\"Error creating polygon: {e}\")\n\n            print(f\"Created {len(all_polygons)} valid polygons\")\n\n            # Create GeoDataFrame\n            if not all_polygons:\n                print(\"No valid polygons found\")\n                return None\n\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": all_polygons,\n                    \"confidence\": all_confidences,\n                    \"class\": 1,  # Object class\n                },\n                crs=crs,\n            )\n\n            # Apply non-maximum suppression to remove overlapping polygons\n            gdf = self.filter_overlapping_polygons(\n                gdf, nms_iou_threshold=nms_iou_threshold\n            )\n\n            print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n            # Apply regularization if requested\n            if regularize and len(gdf) &gt; 0:\n                # Convert pixel area to geographic units for min_area parameter\n                # Estimate pixel size in geographic units\n                with rasterio.open(mask_path) as src:\n                    pixel_size_x = src.transform[\n                        0\n                    ]  # width of a pixel in geographic units\n                    pixel_size_y = abs(\n                        src.transform[4]\n                    )  # height of a pixel in geographic units\n                    avg_pixel_area = pixel_size_x * pixel_size_y\n\n                # Use 10 pixels as minimum area in geographic units\n                min_geo_area = 10 * avg_pixel_area\n\n                # Regularize objects\n                gdf = self.regularize_objects(\n                    gdf,\n                    min_area=min_geo_area,\n                    angle_threshold=angle_threshold,\n                    rectangularity_threshold=rectangularity_threshold,\n                )\n\n            # Save to file\n            if output_path:\n                if output_path.endswith(\".parquet\"):\n                    gdf.to_parquet(output_path)\n                else:\n                    gdf.to_file(output_path)\n                print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n            return gdf\n\n    @torch.no_grad()\n    def process_raster(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        batch_size: int = 4,\n        filter_edges: bool = True,\n        edge_buffer: int = 20,\n        band_indexes: Optional[List[int]] = None,\n        **kwargs: Any,\n    ) -&gt; \"gpd.GeoDataFrame\":\n        \"\"\"\n        Process a raster file to extract objects with customizable parameters.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoJSON or Parquet file (optional)\n            batch_size: Batch size for processing\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n            band_indexes: List of band indexes to use (if None, use all bands)\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                overlap: Overlap between adjacent tiles (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n                min_object_area: Minimum area in pixels to keep an object\n                simplify_tolerance: Tolerance for polygon simplification\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Print parameters being used\n        print(f\"Processing with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n        if filter_edges:\n            print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            band_indexes=band_indexes,\n        )\n        self.raster_stats = dataset.raster_stats\n\n        # Custom collate function to handle Shapely objects\n        def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with simple indexing and custom collate\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        all_polygons = []\n        all_scores = []\n\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n            bboxes = batch[\n                \"bbox\"\n            ]  # Geographic bounding boxes - now a list, not a tensor\n\n            # Run inference\n            predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n                labels = prediction[\"labels\"].cpu().numpy()\n\n                # Skip if no predictions\n                if len(scores) == 0:\n                    continue\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n                labels = labels[valid_indices]\n\n                # Skip if no valid predictions\n                if len(scores) == 0:\n                    continue\n\n                # Get window coordinates\n                # The coords might be in different formats depending on batch handling\n                if isinstance(coords, list):\n                    # If coords is a list of tuples\n                    coord_item = coords[idx]\n                    if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                        i, j = coord_item\n                    elif isinstance(coord_item, torch.Tensor):\n                        i, j = coord_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords format: {type(coord_item)}\")\n                        continue\n                elif isinstance(coords, torch.Tensor):\n                    # If coords is a tensor of shape [batch_size, 2]\n                    i, j = coords[idx].cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords type: {type(coords)}\")\n                    continue\n\n                # Get window size\n                if isinstance(batch[\"window_size\"], list):\n                    window_item = batch[\"window_size\"][idx]\n                    if isinstance(window_item, tuple) and len(window_item) == 2:\n                        window_width, window_height = window_item\n                    elif isinstance(window_item, torch.Tensor):\n                        window_width, window_height = window_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected window_size format: {type(window_item)}\")\n                        continue\n                elif isinstance(batch[\"window_size\"], torch.Tensor):\n                    window_width, window_height = (\n                        batch[\"window_size\"][idx].cpu().numpy().tolist()\n                    )\n                else:\n                    print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                    continue\n\n                # Process masks to polygons\n                for mask_idx, mask in enumerate(masks):\n                    # Get binary mask\n                    binary_mask = mask[0]  # Get binary mask\n\n                    # Convert mask to polygon with custom parameters\n                    contours = self.mask_to_polygons(\n                        binary_mask,\n                        simplify_tolerance=simplify_tolerance,\n                        mask_threshold=mask_threshold,\n                        min_object_area=min_object_area,\n                        max_object_area=max_object_area,\n                    )\n\n                    # Skip if no valid polygons\n                    if not contours:\n                        continue\n\n                    # Transform polygons to geographic coordinates\n                    with rasterio.open(raster_path) as src:\n                        transform = src.transform\n\n                        for contour in contours:\n                            # Convert polygon to global coordinates\n                            global_polygon = []\n                            for x, y in contour:\n                                # Adjust coordinates based on window position\n                                gx, gy = transform * (i + x, j + y)\n                                global_polygon.append((gx, gy))\n\n                            # Create Shapely polygon\n                            if len(global_polygon) &gt;= 3:\n                                try:\n                                    shapely_poly = Polygon(global_polygon)\n                                    if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                        all_polygons.append(shapely_poly)\n                                        all_scores.append(float(scores[mask_idx]))\n                                except Exception as e:\n                                    print(f\"Error creating polygon: {e}\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_scores,\n                \"class\": 1,  # Object class\n            },\n            crs=dataset.crs,\n        )\n\n        # Remove overlapping polygons with custom threshold\n        gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        # Filter edge objects if requested\n        if filter_edges:\n            gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n        # Save to file if requested\n        if output_path:\n            if output_path.endswith(\".parquet\"):\n                gdf.to_parquet(output_path)\n            else:\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n\n    def save_masks_as_geotiff(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        batch_size: int = 4,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Process a raster file to extract object masks and save as GeoTIFF.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n            batch_size: Batch size for processing\n            verbose: Whether to print detailed processing information\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n        Returns:\n            Path to the saved GeoTIFF file\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        overlap = kwargs.get(\"overlap\", self.overlap)\n\n        # Set default output path if not provided\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n        # Print parameters being used\n        print(f\"Processing masks with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            verbose=verbose,\n        )\n\n        # Store a flag to avoid repetitive messages\n        self.raster_stats = dataset.raster_stats\n        seen_warnings = {\n            \"bands\": False,\n            \"resize\": {},  # Dictionary to track resize warnings by shape\n        }\n\n        # Open original raster to get metadata\n        with rasterio.open(raster_path) as src:\n            # Create output binary mask raster with same dimensions as input\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=1,  # Single band for object mask\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Create output mask raster\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                # Initialize mask with zeros\n                mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n                # Custom collate function to handle Shapely objects\n                def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n                    \"\"\"Custom collate function for DataLoader\"\"\"\n                    elem = batch[0]\n                    if isinstance(elem, dict):\n                        result = {}\n                        for key in elem:\n                            if key == \"bbox\":\n                                # Don't collate shapely objects, keep as list\n                                result[key] = [d[key] for d in batch]\n                            else:\n                                # For tensors and other collatable types\n                                try:\n                                    result[key] = (\n                                        torch.utils.data._utils.collate.default_collate(\n                                            [d[key] for d in batch]\n                                        )\n                                    )\n                                except TypeError:\n                                    # Fall back to list for non-collatable types\n                                    result[key] = [d[key] for d in batch]\n                        return result\n                    else:\n                        # Default collate for non-dict types\n                        return torch.utils.data._utils.collate.default_collate(batch)\n\n                # Create dataloader\n                dataloader = torch.utils.data.DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=0,\n                    collate_fn=custom_collate,\n                )\n\n                # Process batches\n                print(f\"Processing raster with {len(dataloader)} batches\")\n                for batch in tqdm(dataloader):\n                    # Move images to device\n                    images = batch[\"image\"].to(self.device)\n                    coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                    # Run inference\n                    with torch.no_grad():\n                        predictions = self.model(images)\n\n                    # Process predictions\n                    for idx, prediction in enumerate(predictions):\n                        masks = prediction[\"masks\"].cpu().numpy()\n                        scores = prediction[\"scores\"].cpu().numpy()\n\n                        # Skip if no predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Filter by confidence threshold\n                        valid_indices = scores &gt;= confidence_threshold\n                        masks = masks[valid_indices]\n                        scores = scores[valid_indices]\n\n                        # Skip if no valid predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Get window coordinates\n                        if isinstance(coords, list):\n                            coord_item = coords[idx]\n                            if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                                i, j = coord_item\n                            elif isinstance(coord_item, torch.Tensor):\n                                i, j = coord_item.cpu().numpy().tolist()\n                            else:\n                                print(f\"Unexpected coords format: {type(coord_item)}\")\n                                continue\n                        elif isinstance(coords, torch.Tensor):\n                            i, j = coords[idx].cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords type: {type(coords)}\")\n                            continue\n\n                        # Get window size\n                        if isinstance(batch[\"window_size\"], list):\n                            window_item = batch[\"window_size\"][idx]\n                            if isinstance(window_item, tuple) and len(window_item) == 2:\n                                window_width, window_height = window_item\n                            elif isinstance(window_item, torch.Tensor):\n                                window_width, window_height = (\n                                    window_item.cpu().numpy().tolist()\n                                )\n                            else:\n                                print(\n                                    f\"Unexpected window_size format: {type(window_item)}\"\n                                )\n                                continue\n                        elif isinstance(batch[\"window_size\"], torch.Tensor):\n                            window_width, window_height = (\n                                batch[\"window_size\"][idx].cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                            )\n                            continue\n\n                        # Combine all masks for this window\n                        combined_mask = np.zeros(\n                            (window_height, window_width), dtype=np.uint8\n                        )\n\n                        for mask in masks:\n                            # Get the binary mask\n                            binary_mask = (mask[0] &gt; mask_threshold).astype(\n                                np.uint8\n                            ) * 255\n\n                            # Handle size mismatch - resize binary_mask if needed\n                            mask_h, mask_w = binary_mask.shape\n                            if mask_h != window_height or mask_w != window_width:\n                                resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                                if resize_key not in seen_warnings[\"resize\"]:\n                                    if verbose:\n                                        print(\n                                            f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                        )\n                                    else:\n                                        if not seen_warnings[\n                                            \"resize\"\n                                        ]:  # If this is the first resize warning\n                                            print(\n                                                f\"Resizing masks at image edges (set verbose=True for details)\"\n                                            )\n                                    seen_warnings[\"resize\"][resize_key] = True\n\n                                # Crop or pad the binary mask to match window size\n                                resized_mask = np.zeros(\n                                    (window_height, window_width), dtype=np.uint8\n                                )\n                                copy_h = min(mask_h, window_height)\n                                copy_w = min(mask_w, window_width)\n                                resized_mask[:copy_h, :copy_w] = binary_mask[\n                                    :copy_h, :copy_w\n                                ]\n                                binary_mask = resized_mask\n\n                            # Update combined mask (taking maximum where masks overlap)\n                            combined_mask = np.maximum(combined_mask, binary_mask)\n\n                        # Write combined mask to output array\n                        # Handle edge cases where window might be smaller than chip size\n                        h, w = combined_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                combined_mask[:valid_h, :valid_w],\n                            )\n\n                # Write the final mask to the output file\n                dst.write(mask_array, 1)\n\n        print(f\"Object masks saved to {output_path}\")\n        return output_path\n\n    def regularize_objects(\n        self,\n        gdf: gpd.GeoDataFrame,\n        min_area: int = 10,\n        angle_threshold: int = 15,\n        orthogonality_threshold: float = 0.3,\n        rectangularity_threshold: float = 0.7,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Regularize objects to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with objects\n            min_area: Minimum area in square units to keep an object\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n            rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized objects\n        \"\"\"\n        import math\n\n        import cv2\n        import geopandas as gpd\n        import numpy as np\n        from shapely.affinity import rotate, translate\n        from shapely.geometry import MultiPolygon, Polygon, box\n        from tqdm import tqdm\n\n        def get_angle(\n            p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]\n        ) -&gt; float:\n            \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n            a = np.array(p1)\n            b = np.array(p2)\n            c = np.array(p3)\n\n            ba = a - b\n            bc = c - b\n\n            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n            # Handle numerical errors that could push cosine outside [-1, 1]\n            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n            angle = np.degrees(np.arccos(cosine_angle))\n\n            return angle\n\n        def is_orthogonal(angle: float, threshold: int = angle_threshold) -&gt; bool:\n            \"\"\"Check if angle is close to 90 degrees\"\"\"\n            return abs(angle - 90) &lt;= threshold\n\n        def calculate_dominant_direction(polygon: Polygon) -&gt; float:\n            \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n            # Extract coordinates\n            coords = np.array(polygon.exterior.coords)\n\n            # Mean center the coordinates\n            mean = np.mean(coords, axis=0)\n            centered_coords = coords - mean\n\n            # Calculate covariance matrix and its eigenvalues/eigenvectors\n            cov_matrix = np.cov(centered_coords.T)\n            eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n            # Get the index of the largest eigenvalue\n            largest_idx = np.argmax(eigenvalues)\n\n            # Get the corresponding eigenvector (principal axis)\n            principal_axis = eigenvectors[:, largest_idx]\n\n            # Calculate the angle in degrees\n            angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n            angle_deg = np.degrees(angle_rad)\n\n            # Normalize to range 0-180\n            if angle_deg &lt; 0:\n                angle_deg += 180\n\n            return angle_deg\n\n        def create_oriented_envelope(polygon: Polygon, angle_deg: float) -&gt; Polygon:\n            \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n            # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n            coords = np.array(polygon.exterior.coords)[:-1].astype(\n                np.float32\n            )  # Skip the last point (same as first)\n\n            # Use OpenCV's minAreaRect\n            rect = cv2.minAreaRect(coords)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to shapely polygon\n            oriented_box = Polygon(box_points)\n\n            return oriented_box\n\n        def get_rectangularity(polygon: Polygon, oriented_box: Polygon) -&gt; float:\n            \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n            if oriented_box.area == 0:\n                return 0\n            return polygon.area / oriented_box.area\n\n        def check_orthogonality(polygon: Polygon) -&gt; float:\n            \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n            coords = list(polygon.exterior.coords)\n            if len(coords) &lt;= 4:  # Triangle or point\n                return 0\n\n            # Remove last point (same as first)\n            coords = coords[:-1]\n\n            orthogonal_count = 0\n            total_angles = len(coords)\n\n            for i in range(total_angles):\n                p1 = coords[i]\n                p2 = coords[(i + 1) % total_angles]\n                p3 = coords[(i + 2) % total_angles]\n\n                angle = get_angle(p1, p2, p3)\n                if is_orthogonal(angle):\n                    orthogonal_count += 1\n\n            return orthogonal_count / total_angles\n\n        def simplify_to_rectangle(polygon: Polygon) -&gt; Polygon:\n            \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n            # Get dominant direction\n            angle = calculate_dominant_direction(polygon)\n\n            # Create oriented envelope\n            rect = create_oriented_envelope(polygon, angle)\n\n            return rect\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No Objects to regularize\")\n            return gdf\n\n        print(f\"Regularizing {len(gdf)} objects...\")\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n        print(\n            f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n        )\n\n        # Create a copy to avoid modifying the original\n        result_gdf = gdf.copy()\n\n        # Track statistics\n        total_objects = len(gdf)\n        regularized_count = 0\n        rectangularized_count = 0\n\n        # Process each Object\n        for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n            geom = row.geometry\n\n            # Skip invalid or empty geometries\n            if geom is None or geom.is_empty:\n                continue\n\n            # Handle MultiPolygons by processing the largest part\n            if isinstance(geom, MultiPolygon):\n                areas = [p.area for p in geom.geoms]\n                if not areas:\n                    continue\n                geom = list(geom.geoms)[np.argmax(areas)]\n\n            # Filter out tiny Objects\n            if geom.area &lt; min_area:\n                continue\n\n            # Check orthogonality\n            orthogonality = check_orthogonality(geom)\n\n            # Create oriented envelope\n            oriented_box = create_oriented_envelope(\n                geom, calculate_dominant_direction(geom)\n            )\n\n            # Check rectangularity\n            rectangularity = get_rectangularity(geom, oriented_box)\n\n            # Decide how to regularize\n            if rectangularity &gt;= rectangularity_threshold:\n                # Object is already quite rectangular, simplify to a rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n                rectangularized_count += 1\n            elif orthogonality &gt;= orthogonality_threshold:\n                # Object has many orthogonal angles but isn't rectangular\n                # Could implement more sophisticated regularization here\n                # For now, we'll still use the oriented rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n                regularized_count += 1\n            else:\n                # Object doesn't have clear orthogonal structure\n                # Keep original but flag as unmodified\n                result_gdf.at[idx, \"regularized\"] = \"original\"\n\n        # Report statistics\n        print(f\"Regularization completed:\")\n        print(f\"- Total objects: {total_objects}\")\n        print(\n            f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n        )\n\n        return result_gdf\n\n    def visualize_results(\n        self,\n        raster_path: str,\n        gdf: Optional[gpd.GeoDataFrame] = None,\n        output_path: Optional[str] = None,\n        figsize: Tuple[int, int] = (12, 12),\n    ) -&gt; bool:\n        \"\"\"\n        Visualize object detection results with proper coordinate transformation.\n\n        This function displays objects on top of the raster image,\n        ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n        Args:\n            raster_path: Path to input raster\n            gdf: GeoDataFrame with object polygons (optional)\n            output_path: Path to save visualization (optional)\n            figsize: Figure size (width, height) in inches\n\n        Returns:\n            bool: True if visualization was successful\n        \"\"\"\n        # Check if raster file exists\n        if not os.path.exists(raster_path):\n            print(f\"Error: Raster file '{raster_path}' not found.\")\n            return False\n\n        # Process raster if GeoDataFrame not provided\n        if gdf is None:\n            gdf = self.process_raster(raster_path)\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No objects to visualize\")\n            return False\n\n        # Check if confidence column exists in the GeoDataFrame\n        has_confidence = False\n        if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n            # Try to access a confidence value to confirm it works\n            try:\n                if len(gdf) &gt; 0:\n                    # Try getitem access\n                    conf_val = gdf[\"confidence\"].iloc[0]\n                    has_confidence = True\n                    print(\n                        f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                    )\n            except Exception as e:\n                print(f\"Confidence column exists but couldn't access values: {e}\")\n                has_confidence = False\n        else:\n            print(\"No confidence column found in GeoDataFrame\")\n            has_confidence = False\n\n        # Read raster for visualization\n        with rasterio.open(raster_path) as src:\n            # Read the entire image or a subset if it's very large\n            if src.height &gt; 2000 or src.width &gt; 2000:\n                # Calculate scale factor to reduce size\n                scale = min(2000 / src.height, 2000 / src.width)\n                out_shape = (\n                    int(src.count),\n                    int(src.height * scale),\n                    int(src.width * scale),\n                )\n\n                # Read and resample\n                image = src.read(\n                    out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n                )\n\n                # Create a scaled transform for the resampled image\n                # Calculate scaling factors\n                x_scale = src.width / out_shape[2]\n                y_scale = src.height / out_shape[1]\n\n                # Get the original transform\n                orig_transform = src.transform\n\n                # Create a scaled transform\n                scaled_transform = rasterio.transform.Affine(\n                    orig_transform.a * x_scale,\n                    orig_transform.b,\n                    orig_transform.c,\n                    orig_transform.d,\n                    orig_transform.e * y_scale,\n                    orig_transform.f,\n                )\n            else:\n                image = src.read()\n                scaled_transform = src.transform\n\n            # Convert to RGB for display\n            if image.shape[0] &gt; 3:\n                image = image[:3]\n            elif image.shape[0] == 1:\n                image = np.repeat(image, 3, axis=0)\n\n            # Normalize image for display\n            image = image.transpose(1, 2, 0)  # CHW to HWC\n            image = image.astype(np.float32)\n\n            if image.max() &gt; 10:  # Likely 0-255 range\n                image = image / 255.0\n\n            image = np.clip(image, 0, 1)\n\n            # Get image bounds\n            bounds = src.bounds\n            crs = src.crs\n\n        # Create figure with appropriate aspect ratio\n        aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n        plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n        ax = plt.gca()\n\n        # Display image\n        ax.imshow(image)\n\n        # Make sure the GeoDataFrame has the same CRS as the raster\n        if gdf.crs != crs:\n            print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n            gdf = gdf.to_crs(crs)\n\n        # Set up colors for confidence visualization\n        if has_confidence:\n            try:\n                import matplotlib.cm as cm\n                from matplotlib.colors import Normalize\n\n                # Get min/max confidence values\n                min_conf = gdf[\"confidence\"].min()\n                max_conf = gdf[\"confidence\"].max()\n\n                # Set up normalization and colormap\n                norm = Normalize(vmin=min_conf, vmax=max_conf)\n                cmap = cm.viridis\n\n                # Create scalar mappable for colorbar\n                sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                sm.set_array([])\n\n                # Add colorbar\n                cbar = plt.colorbar(\n                    sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n                )\n                cbar.set_label(\"Confidence Score\")\n            except Exception as e:\n                print(f\"Error setting up confidence visualization: {e}\")\n                has_confidence = False\n\n        # Function to convert coordinates\n        def geo_to_pixel(\n            geometry: Any, transform: Any\n        ) -&gt; Optional[Tuple[List[float], List[float]]]:\n            \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n            if geometry.is_empty:\n                return None\n\n            if geometry.geom_type == \"Polygon\":\n                # Get exterior coordinates\n                exterior_coords = list(geometry.exterior.coords)\n\n                # Convert to pixel coordinates\n                pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n                # Split into x and y lists\n                pixel_x = [coord[0] for coord in pixel_coords]\n                pixel_y = [coord[1] for coord in pixel_coords]\n\n                return pixel_x, pixel_y\n            else:\n                print(f\"Unsupported geometry type: {geometry.geom_type}\")\n                return None\n\n        # Plot each object\n        for idx, row in gdf.iterrows():\n            try:\n                # Convert polygon to pixel coordinates\n                coords = geo_to_pixel(row.geometry, scaled_transform)\n\n                if coords:\n                    pixel_x, pixel_y = coords\n\n                    if has_confidence:\n                        try:\n                            # Get confidence value using different methods\n                            # Method 1: Try direct attribute access\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            # Method 2: Try dictionary-style access\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            # Method 3: Try accessing by index from the GeoDataFrame\n                            if confidence is None:\n                                try:\n                                    confidence = gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1,\n                                    alpha=0.8,\n                                )\n                            else:\n                                # Fall back to red if confidence value couldn't be accessed\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence value for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    else:\n                        # No confidence data, just plot outlines in red\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n            except Exception as e:\n                print(f\"Error plotting polygon {idx}: {e}\")\n\n        # Remove axes\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n        # Save if requested\n        if output_path:\n            plt.tight_layout()\n            plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n            print(f\"Visualization saved to {output_path}\")\n\n        plt.close()\n\n        # Create a simpler visualization focused just on a subset of objects\n        if len(gdf) &gt; 0:\n            plt.figure(figsize=figsize)\n            ax = plt.gca()\n\n            # Choose a subset of the image to show\n            with rasterio.open(raster_path) as src:\n                # Get centroid of first object\n                sample_geom = gdf.iloc[0].geometry\n                centroid = sample_geom.centroid\n\n                # Convert to pixel coordinates\n                center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n                # Define a window around this object\n                window_size = 500  # pixels\n                window = rasterio.windows.Window(\n                    max(0, int(center_x - window_size / 2)),\n                    max(0, int(center_y - window_size / 2)),\n                    min(window_size, src.width - int(center_x - window_size / 2)),\n                    min(window_size, src.height - int(center_y - window_size / 2)),\n                )\n\n                # Read this window\n                sample_image = src.read(window=window)\n\n                # Convert to RGB for display\n                if sample_image.shape[0] &gt; 3:\n                    sample_image = sample_image[:3]\n                elif sample_image.shape[0] == 1:\n                    sample_image = np.repeat(sample_image, 3, axis=0)\n\n                # Normalize image for display\n                sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n                sample_image = sample_image.astype(np.float32)\n\n                if sample_image.max() &gt; 10:  # Likely 0-255 range\n                    sample_image = sample_image / 255.0\n\n                sample_image = np.clip(sample_image, 0, 1)\n\n                # Display sample image\n                ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n                # Get the correct transform for this window\n                window_transform = src.window_transform(window)\n\n                # Calculate bounds of the window\n                window_bounds = rasterio.windows.bounds(window, src.transform)\n                window_box = box(*window_bounds)\n\n                # Filter objects that intersect with this window\n                visible_gdf = gdf[gdf.intersects(window_box)]\n\n                # Set up colors for sample view if confidence data exists\n                if has_confidence:\n                    try:\n                        # Reuse the same normalization and colormap from main view\n                        sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                        sample_sm.set_array([])\n\n                        # Add colorbar to sample view\n                        sample_cbar = plt.colorbar(\n                            sample_sm,\n                            ax=ax,\n                            orientation=\"vertical\",\n                            shrink=0.7,\n                            pad=0.01,\n                        )\n                        sample_cbar.set_label(\"Confidence Score\")\n                    except Exception as e:\n                        print(f\"Error setting up sample confidence visualization: {e}\")\n\n                # Plot objects in sample view\n                for idx, row in visible_gdf.iterrows():\n                    try:\n                        # Get window-relative pixel coordinates\n                        geom = row.geometry\n\n                        # Skip empty geometries\n                        if geom.is_empty:\n                            continue\n\n                        # Get exterior coordinates\n                        exterior_coords = list(geom.exterior.coords)\n\n                        # Convert to pixel coordinates relative to window origin\n                        pixel_coords = []\n                        for x, y in exterior_coords:\n                            px, py = ~src.transform * (x, y)  # Convert to image pixels\n                            # Make coordinates relative to window\n                            px = px - window.col_off\n                            py = py - window.row_off\n                            pixel_coords.append((px, py))\n\n                        # Extract x and y coordinates\n                        pixel_x = [coord[0] for coord in pixel_coords]\n                        pixel_y = [coord[1] for coord in pixel_coords]\n\n                        # Use confidence colors if available\n                        if has_confidence:\n                            try:\n                                # Try different methods to access confidence\n                                confidence = None\n                                try:\n                                    confidence = row.confidence\n                                except:\n                                    pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = row[\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is not None:\n                                    color = cmap(norm(confidence))\n                                    # Fill polygon with semi-transparent color\n                                    ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                    # Draw border\n                                    ax.plot(\n                                        pixel_x,\n                                        pixel_y,\n                                        color=color,\n                                        linewidth=1.5,\n                                        alpha=0.8,\n                                    )\n                                else:\n                                    ax.plot(\n                                        pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                    )\n                            except Exception as e:\n                                print(\n                                    f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                                )\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                        else:\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    except Exception as e:\n                        print(f\"Error plotting polygon in sample view: {e}\")\n\n                # Set title\n                ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n                # Remove axes\n                ax.set_xticks([])\n                ax.set_yticks([])\n\n                # Save if requested\n                if output_path:\n                    sample_output = (\n                        os.path.splitext(output_path)[0]\n                        + \"_sample\"\n                        + os.path.splitext(output_path)[1]\n                    )\n                    plt.tight_layout()\n                    plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                    print(f\"Sample visualization saved to {sample_output}\")\n\n    def generate_masks(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        confidence_threshold: Optional[float] = None,\n        mask_threshold: Optional[float] = None,\n        min_object_area: int = 10,\n        max_object_area: float = float(\"inf\"),\n        overlap: float = 0.25,\n        batch_size: int = 4,\n        band_indexes: Optional[List[int]] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Save masks with confidence values as a multi-band GeoTIFF.\n\n        Objects with area smaller than min_object_area or larger than max_object_area\n        will be filtered out.\n\n        Args:\n            raster_path: Path to input raster\n            output_path: Path for output GeoTIFF\n            confidence_threshold: Minimum confidence score (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area (in pixels) for an object to be included\n            max_object_area: Maximum area (in pixels) for an object to be included\n            overlap: Overlap between tiles (0.0-1.0)\n            batch_size: Batch size for processing\n            band_indexes: List of band indexes to use (default: all bands)\n            verbose: Whether to print detailed processing information\n\n        Returns:\n            Path to the saved GeoTIFF\n        \"\"\"\n        # Use provided thresholds or fall back to instance defaults\n        if confidence_threshold is None:\n            confidence_threshold = self.confidence_threshold\n        if mask_threshold is None:\n            mask_threshold = self.mask_threshold\n\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n\n        # Default output path\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks_conf.tif\"\n\n        # Process the raster to get individual masks with confidence\n        with rasterio.open(raster_path) as src:\n            # Create dataset with the specified overlap\n            dataset = CustomDataset(\n                raster_path=raster_path,\n                chip_size=chip_size,\n                overlap=overlap,\n                band_indexes=band_indexes,\n                verbose=verbose,\n            )\n\n            # Create output profile\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=2,  # Two bands: mask and confidence\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Initialize mask and confidence arrays\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n            conf_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Define custom collate function to handle Shapely objects\n            def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n                \"\"\"\n                Custom collate function that handles Shapely geometries\n                by keeping them as Python objects rather than trying to collate them.\n                \"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader with custom collate function\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # Tensor of shape [batch_size, 2]\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(masks) == 0:\n                        continue\n\n                    # Get window coordinates\n                    i, j = coords[idx].cpu().numpy()\n\n                    # Process each mask\n                    for mask_idx, mask in enumerate(masks):\n                        # Convert to binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(np.uint8) * 255\n\n                        # Check object area - calculate number of pixels in the mask\n                        object_area = np.sum(binary_mask &gt; 0)\n\n                        # Skip objects that don't meet area criteria\n                        if (\n                            object_area &lt; min_object_area\n                            or object_area &gt; max_object_area\n                        ):\n                            if verbose:\n                                print(\n                                    f\"Filtering out object with area {object_area} pixels\"\n                                )\n                            continue\n\n                        conf_value = int(scores[mask_idx] * 255)  # Scale to 0-255\n\n                        # Update the mask and confidence arrays\n                        h, w = binary_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            # Use maximum for overlapping regions in the mask\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                binary_mask[:valid_h, :valid_w],\n                            )\n\n                            # For confidence, only update where mask is positive\n                            # and confidence is higher than existing\n                            mask_region = binary_mask[:valid_h, :valid_w] &gt; 0\n                            if np.any(mask_region):\n                                # Only update where mask is positive and new confidence is higher\n                                current_conf = conf_array[\n                                    j : j + valid_h, i : i + valid_w\n                                ]\n\n                                # Where to update confidence (mask positive &amp; higher confidence)\n                                update_mask = np.logical_and(\n                                    mask_region,\n                                    np.logical_or(\n                                        current_conf == 0, current_conf &lt; conf_value\n                                    ),\n                                )\n\n                                if np.any(update_mask):\n                                    conf_array[j : j + valid_h, i : i + valid_w][\n                                        update_mask\n                                    ] = conf_value\n\n            # Write to GeoTIFF\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                dst.write(mask_array, 1)\n                dst.write(conf_array, 2)\n\n            print(f\"Masks with confidence values saved to {output_path}\")\n            return output_path\n\n    def vectorize_masks(\n        self,\n        masks_path: str,\n        output_path: Optional[str] = None,\n        confidence_threshold: float = 0.5,\n        min_object_area: int = 100,\n        max_object_area: Optional[int] = None,\n        n_workers: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert masks with confidence to vector polygons.\n\n        Args:\n            masks_path: Path to masks GeoTIFF with confidence band.\n            output_path: Path for output GeoJSON.\n            confidence_threshold: Minimum confidence score (0.0-1.0). Default: 0.5\n            min_object_area: Minimum area in pixels to keep an object. Default: 100\n            max_object_area: Maximum area in pixels to keep an object. Default: None\n            n_workers: int, default=None\n                The number of worker threads to use.\n                \"None\" means single-threaded processing.\n                \"-1\"   means using all available CPU processors.\n                Positive integer means using that specific number of threads.\n            **kwargs: Additional parameters\n\n        Returns:\n            GeoDataFrame with car detections and confidence values\n        \"\"\"\n\n        def _process_single_component(\n            component_mask: np.ndarray,\n            conf_data: np.ndarray,\n            transform: Any,\n            confidence_threshold: float,\n            min_object_area: int,\n            max_object_area: Optional[int],\n        ) -&gt; Optional[Dict[str, Any]]:\n            # Get confidence value\n            conf_region = conf_data[component_mask &gt; 0]\n            if len(conf_region) &gt; 0:\n                confidence = np.mean(conf_region) / 255.0\n            else:\n                confidence = 0.0\n\n            # Skip if confidence is below threshold\n            if confidence &lt; confidence_threshold:\n                return None\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            results = []\n\n            for contour in contours:\n                # Filter by size\n                area = cv2.contourArea(contour)\n                if area &lt; min_object_area:\n                    continue\n\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Get minimum area rectangle\n                rect = cv2.minAreaRect(contour)\n                box_points = cv2.boxPoints(rect)\n\n                # Convert to geographic coordinates\n                geo_points = []\n                for x, y in box_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create polygon\n                poly = Polygon(geo_points)\n                results.append((poly, confidence, area))\n\n            return results\n\n        import concurrent.futures\n        from functools import partial\n\n        def process_component(\n            args: Tuple[int, np.ndarray, np.ndarray, Any, float, int, Optional[int]],\n        ) -&gt; Optional[Dict[str, Any]]:\n            \"\"\"\n            Helper function to process a single component\n            \"\"\"\n            (\n                label,\n                labeled_mask,\n                conf_data,\n                transform,\n                confidence_threshold,\n                min_object_area,\n                max_object_area,\n            ) = args\n\n            # Create mask for this component\n            component_mask = (labeled_mask == label).astype(np.uint8)\n\n            return _process_single_component(\n                component_mask,\n                conf_data,\n                transform,\n                confidence_threshold,\n                min_object_area,\n                max_object_area,\n            )\n\n        start_time = time.time()\n        print(f\"Processing masks from: {masks_path}\")\n\n        if n_workers == -1:\n            n_workers = os.cpu_count()\n\n        with rasterio.open(masks_path) as src:\n            # Read mask and confidence bands\n            mask_data = src.read(1)\n            conf_data = src.read(2)\n            transform = src.transform\n            crs = src.crs\n\n            # Convert to binary mask\n            binary_mask = mask_data &gt; 0\n\n            # Find connected components\n            labeled_mask, num_features = ndimage.label(binary_mask)\n            print(f\"Found {num_features} connected components\")\n\n            # Process each component\n            polygons = []\n            confidences = []\n            pixels = []\n\n            if n_workers is None or n_workers == 1:\n                print(\n                    \"Using single-threaded processing, you can speed up processing by setting n_workers &gt; 1\"\n                )\n                # Add progress bar\n                for label in tqdm(\n                    range(1, num_features + 1), desc=\"Processing components\"\n                ):\n                    # Create mask for this component\n                    component_mask = (labeled_mask == label).astype(np.uint8)\n\n                    result = _process_single_component(\n                        component_mask,\n                        conf_data,\n                        transform,\n                        confidence_threshold,\n                        min_object_area,\n                        max_object_area,\n                    )\n\n                    if result:\n                        for poly, confidence, area in result:\n                            # Add to lists\n                            polygons.append(poly)\n                            confidences.append(confidence)\n                            pixels.append(area)\n\n            else:\n                # Process components in parallel\n                print(f\"Using {n_workers} workers for parallel processing\")\n\n                process_args = [\n                    (\n                        label,\n                        labeled_mask,\n                        conf_data,\n                        transform,\n                        confidence_threshold,\n                        min_object_area,\n                        max_object_area,\n                    )\n                    for label in range(1, num_features + 1)\n                ]\n\n                with concurrent.futures.ThreadPoolExecutor(\n                    max_workers=n_workers\n                ) as executor:\n                    results = list(\n                        tqdm(\n                            executor.map(process_component, process_args),\n                            total=num_features,\n                            desc=\"Processing components\",\n                        )\n                    )\n\n                    for result in results:\n                        if result:\n                            for poly, confidence, area in result:\n                                # Add to lists\n                                polygons.append(poly)\n                                confidences.append(confidence)\n                                pixels.append(area)\n\n            # Create GeoDataFrame\n            if polygons:\n                gdf = gpd.GeoDataFrame(\n                    {\n                        \"geometry\": polygons,\n                        \"confidence\": confidences,\n                        \"class\": [1] * len(polygons),\n                        \"pixels\": pixels,\n                    },\n                    crs=crs,\n                )\n\n                # Save to file if requested\n                if output_path:\n                    gdf.to_file(output_path, driver=\"GeoJSON\")\n                    print(f\"Saved {len(gdf)} objects with confidence to {output_path}\")\n\n                end_time = time.time()\n                print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n                return gdf\n            else:\n                end_time = time.time()\n                print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n                print(\"No valid polygons found\")\n                return None\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.__init__","title":"<code>__init__(model_path=None, repo_id=None, model=None, num_classes=2, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Optional[str]</code> <p>Path to the .pth model file.</p> <code>None</code> <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID for model download.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Pre-initialized model object (optional).</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes for detection (default: 2).</p> <code>2</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: Optional[str] = None,\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    num_classes: int = 2,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Hugging Face repository ID for model download.\n        model: Pre-initialized model object (optional).\n        num_classes: Number of classes for detection (default: 2).\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    # Set device\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Default parameters for object detection - these can be overridden in process_raster\n    self.chip_size = (512, 512)  # Size of image chips for processing\n    self.overlap = 0.25  # Default overlap between tiles\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n    self.min_object_area = 100  # Minimum area in pixels to keep an object\n    self.max_object_area = None  # Maximum area in pixels to keep an object\n    self.mask_threshold = 0.5  # Threshold for mask binarization\n    self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n    # Initialize model\n    self.model = self.initialize_model(model, num_classes=num_classes)\n\n    # Download model if needed\n    if model_path is None or (not os.path.exists(model_path)):\n        model_path = self.download_model_from_hf(model_path, repo_id)\n\n    # Load model weights\n    self.load_weights(model_path)\n\n    # Set model to evaluation mode\n    self.model.eval()\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.download_model_from_hf","title":"<code>download_model_from_hf(model_path=None, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Optional[str]</code> <p>Path to the model file.</p> <code>None</code> <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded model file</p> Source code in <code>geoai/extract.py</code> <pre><code>def download_model_from_hf(\n    self, model_path: Optional[str] = None, repo_id: Optional[str] = None\n) -&gt; str:\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    try:\n\n        print(\"Model path not specified, downloading from Hugging Face...\")\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            repo_id = \"giswqs/geoai\"\n\n        if model_path is None:\n            model_path = \"building_footprints_usa.pth\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.filter_edge_objects","title":"<code>filter_edge_objects(gdf, raster_path, edge_buffer=10)</code>","text":"<p>Filter out object detections that fall in padding/edge areas of the image.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with object detections</p> required <code>raster_path</code> <code>str</code> <p>Path to the original raster file</p> required <code>edge_buffer</code> <code>int</code> <p>Buffer in pixels to consider as edge region</p> <code>10</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with filtered objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_edge_objects(\n    self, gdf: gpd.GeoDataFrame, raster_path: str, edge_buffer: int = 10\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Filter out object detections that fall in padding/edge areas of the image.\n\n    Args:\n        gdf: GeoDataFrame with object detections\n        raster_path: Path to the original raster file\n        edge_buffer: Buffer in pixels to consider as edge region\n\n    Returns:\n        GeoDataFrame with filtered objects\n    \"\"\"\n    import rasterio\n    from shapely.geometry import box\n\n    # If no objects detected, return empty GeoDataFrame\n    if gdf is None or len(gdf) == 0:\n        return gdf\n\n    print(f\"Objects before filtering: {len(gdf)}\")\n\n    with rasterio.open(raster_path) as src:\n        # Get raster bounds\n        raster_bounds = src.bounds\n        raster_width = src.width\n        raster_height = src.height\n\n        # Convert edge buffer from pixels to geographic units\n        # We need the smallest dimension of a pixel in geographic units\n        pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n        pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n        buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n        # Create a slightly smaller bounding box to exclude edge regions\n        inner_bounds = (\n            raster_bounds[0] + buffer_size,  # min x (west)\n            raster_bounds[1] + buffer_size,  # min y (south)\n            raster_bounds[2] - buffer_size,  # max x (east)\n            raster_bounds[3] - buffer_size,  # max y (north)\n        )\n\n        # Check that inner bounds are valid\n        if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n            print(\"Warning: Edge buffer too large, using original bounds\")\n            inner_box = box(*raster_bounds)\n        else:\n            inner_box = box(*inner_bounds)\n\n        # Filter out objects that intersect with the edge of the image\n        filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n        # Additional check for objects that have &gt;50% of their area outside the valid region\n        valid_objects = []\n        for idx, row in filtered_gdf.iterrows():\n            if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                valid_objects.append(idx)\n\n        filtered_gdf = filtered_gdf.loc[valid_objects]\n\n        print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n        return filtered_gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.filter_overlapping_polygons","title":"<code>filter_overlapping_polygons(gdf, **kwargs)</code>","text":"<p>Filter overlapping polygons using non-maximum suppression.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with polygons</p> required <code>**kwargs</code> <code>Any</code> <p>Optional parameters: nms_iou_threshold: IoU threshold for filtering</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Filtered GeoDataFrame</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_overlapping_polygons(\n    self, gdf: gpd.GeoDataFrame, **kwargs: Any\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Filter overlapping polygons using non-maximum suppression.\n\n    Args:\n        gdf: GeoDataFrame with polygons\n        **kwargs: Optional parameters:\n            nms_iou_threshold: IoU threshold for filtering\n\n    Returns:\n        Filtered GeoDataFrame\n    \"\"\"\n    if len(gdf) &lt;= 1:\n        return gdf\n\n    # Get parameters from kwargs or use instance defaults\n    iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n    # Sort by confidence\n    gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n    # Fix any invalid geometries\n    gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n        lambda geom: geom.buffer(0) if not geom.is_valid else geom\n    )\n\n    keep_indices = []\n    polygons = gdf.geometry.values\n\n    for i in range(len(polygons)):\n        if i in keep_indices:\n            continue\n\n        keep = True\n        for j in keep_indices:\n            # Skip invalid geometries\n            if not polygons[i].is_valid or not polygons[j].is_valid:\n                continue\n\n            # Calculate IoU\n            try:\n                intersection = polygons[i].intersection(polygons[j]).area\n                union = polygons[i].area + polygons[j].area - intersection\n                iou = intersection / union if union &gt; 0 else 0\n\n                if iou &gt; iou_threshold:\n                    keep = False\n                    break\n            except Exception:\n                # Skip on topology exceptions\n                continue\n\n        if keep:\n            keep_indices.append(i)\n\n    return gdf.iloc[keep_indices]\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.generate_masks","title":"<code>generate_masks(raster_path, output_path=None, confidence_threshold=None, mask_threshold=None, min_object_area=10, max_object_area=float('inf'), overlap=0.25, batch_size=4, band_indexes=None, verbose=False, **kwargs)</code>","text":"<p>Save masks with confidence values as a multi-band GeoTIFF.</p> <p>Objects with area smaller than min_object_area or larger than max_object_area will be filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path for output GeoTIFF</p> <code>None</code> <code>confidence_threshold</code> <code>Optional[float]</code> <p>Minimum confidence score (0.0-1.0)</p> <code>None</code> <code>mask_threshold</code> <code>Optional[float]</code> <p>Threshold for mask binarization (0.0-1.0)</p> <code>None</code> <code>min_object_area</code> <code>int</code> <p>Minimum area (in pixels) for an object to be included</p> <code>10</code> <code>max_object_area</code> <code>float</code> <p>Maximum area (in pixels) for an object to be included</p> <code>float('inf')</code> <code>overlap</code> <code>float</code> <p>Overlap between tiles (0.0-1.0)</p> <code>0.25</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>band_indexes</code> <code>Optional[List[int]]</code> <p>List of band indexes to use (default: all bands)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed processing information</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved GeoTIFF</p> Source code in <code>geoai/extract.py</code> <pre><code>def generate_masks(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    confidence_threshold: Optional[float] = None,\n    mask_threshold: Optional[float] = None,\n    min_object_area: int = 10,\n    max_object_area: float = float(\"inf\"),\n    overlap: float = 0.25,\n    batch_size: int = 4,\n    band_indexes: Optional[List[int]] = None,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Save masks with confidence values as a multi-band GeoTIFF.\n\n    Objects with area smaller than min_object_area or larger than max_object_area\n    will be filtered out.\n\n    Args:\n        raster_path: Path to input raster\n        output_path: Path for output GeoTIFF\n        confidence_threshold: Minimum confidence score (0.0-1.0)\n        mask_threshold: Threshold for mask binarization (0.0-1.0)\n        min_object_area: Minimum area (in pixels) for an object to be included\n        max_object_area: Maximum area (in pixels) for an object to be included\n        overlap: Overlap between tiles (0.0-1.0)\n        batch_size: Batch size for processing\n        band_indexes: List of band indexes to use (default: all bands)\n        verbose: Whether to print detailed processing information\n\n    Returns:\n        Path to the saved GeoTIFF\n    \"\"\"\n    # Use provided thresholds or fall back to instance defaults\n    if confidence_threshold is None:\n        confidence_threshold = self.confidence_threshold\n    if mask_threshold is None:\n        mask_threshold = self.mask_threshold\n\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n\n    # Default output path\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks_conf.tif\"\n\n    # Process the raster to get individual masks with confidence\n    with rasterio.open(raster_path) as src:\n        # Create dataset with the specified overlap\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            band_indexes=band_indexes,\n            verbose=verbose,\n        )\n\n        # Create output profile\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=2,  # Two bands: mask and confidence\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Initialize mask and confidence arrays\n        mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n        conf_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n        # Define custom collate function to handle Shapely objects\n        def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with custom collate function\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # Tensor of shape [batch_size, 2]\n\n            # Run inference\n            with torch.no_grad():\n                predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n\n                # Skip if no valid predictions\n                if len(masks) == 0:\n                    continue\n\n                # Get window coordinates\n                i, j = coords[idx].cpu().numpy()\n\n                # Process each mask\n                for mask_idx, mask in enumerate(masks):\n                    # Convert to binary mask\n                    binary_mask = (mask[0] &gt; mask_threshold).astype(np.uint8) * 255\n\n                    # Check object area - calculate number of pixels in the mask\n                    object_area = np.sum(binary_mask &gt; 0)\n\n                    # Skip objects that don't meet area criteria\n                    if (\n                        object_area &lt; min_object_area\n                        or object_area &gt; max_object_area\n                    ):\n                        if verbose:\n                            print(\n                                f\"Filtering out object with area {object_area} pixels\"\n                            )\n                        continue\n\n                    conf_value = int(scores[mask_idx] * 255)  # Scale to 0-255\n\n                    # Update the mask and confidence arrays\n                    h, w = binary_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        # Use maximum for overlapping regions in the mask\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            binary_mask[:valid_h, :valid_w],\n                        )\n\n                        # For confidence, only update where mask is positive\n                        # and confidence is higher than existing\n                        mask_region = binary_mask[:valid_h, :valid_w] &gt; 0\n                        if np.any(mask_region):\n                            # Only update where mask is positive and new confidence is higher\n                            current_conf = conf_array[\n                                j : j + valid_h, i : i + valid_w\n                            ]\n\n                            # Where to update confidence (mask positive &amp; higher confidence)\n                            update_mask = np.logical_and(\n                                mask_region,\n                                np.logical_or(\n                                    current_conf == 0, current_conf &lt; conf_value\n                                ),\n                            )\n\n                            if np.any(update_mask):\n                                conf_array[j : j + valid_h, i : i + valid_w][\n                                    update_mask\n                                ] = conf_value\n\n        # Write to GeoTIFF\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            dst.write(mask_array, 1)\n            dst.write(conf_array, 2)\n\n        print(f\"Masks with confidence values saved to {output_path}\")\n        return output_path\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.initialize_model","title":"<code>initialize_model(model, num_classes=2)</code>","text":"<p>Initialize a deep learning model for object detection.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>A pre-initialized model object.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for detection.</p> <code>2</code> <p>Returns:</p> Type Description <code>Any</code> <p>torch.nn.Module: A deep learning model for object detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_model(self, model: Optional[Any], num_classes: int = 2) -&gt; Any:\n    \"\"\"Initialize a deep learning model for object detection.\n\n    Args:\n        model (torch.nn.Module): A pre-initialized model object.\n        num_classes (int): Number of classes for detection.\n\n    Returns:\n        torch.nn.Module: A deep learning model for object detection.\n    \"\"\"\n\n    if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n        # Standard image mean and std for pre-trained models\n        image_mean = [0.485, 0.456, 0.406]\n        image_std = [0.229, 0.224, 0.225]\n\n        # Create model with explicit normalization parameters\n        model = maskrcnn_resnet50_fpn(\n            weights=None,\n            progress=False,\n            num_classes=num_classes,  # Background + object\n            weights_backbone=None,\n            # These parameters ensure consistent normalization\n            image_mean=image_mean,\n            image_std=image_std,\n        )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.load_weights","title":"<code>load_weights(model_path)</code>","text":"<p>Load weights from file with error handling for different formats.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to model weights</p> required Source code in <code>geoai/extract.py</code> <pre><code>def load_weights(self, model_path: str) -&gt; None:\n    \"\"\"\n    Load weights from file with error handling for different formats.\n\n    Args:\n        model_path: Path to model weights\n    \"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    try:\n        state_dict = torch.load(model_path, map_location=self.device)\n\n        # Handle different state dict formats\n        if isinstance(state_dict, dict):\n            if \"model\" in state_dict:\n                state_dict = state_dict[\"model\"]\n            elif \"state_dict\" in state_dict:\n                state_dict = state_dict[\"state_dict\"]\n\n        # Try to load state dict\n        try:\n            self.model.load_state_dict(state_dict)\n            print(\"Model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Attempting to fix state_dict keys...\")\n\n            # Try to fix state_dict keys (remove module prefix if needed)\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                if k.startswith(\"module.\"):\n                    new_state_dict[k[7:]] = v\n                else:\n                    new_state_dict[k] = v\n\n            self.model.load_state_dict(new_state_dict)\n            print(\"Model loaded successfully after key fixing\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model: {e}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.mask_to_polygons","title":"<code>mask_to_polygons(mask, **kwargs)</code>","text":"<p>Convert binary mask to polygon contours using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Binary mask as numpy array</p> required <code>**kwargs</code> <code>Any</code> <p>Optional parameters: simplify_tolerance: Tolerance for polygon simplification mask_threshold: Threshold for mask binarization min_object_area: Minimum area in pixels to keep an object max_object_area: Maximum area in pixels to keep an object</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Polygon]</code> <p>List of polygons as lists of (x, y) coordinates</p> Source code in <code>geoai/extract.py</code> <pre><code>def mask_to_polygons(self, mask: np.ndarray, **kwargs: Any) -&gt; List[Polygon]:\n    \"\"\"\n    Convert binary mask to polygon contours using OpenCV.\n\n    Args:\n        mask: Binary mask as numpy array\n        **kwargs: Optional parameters:\n            simplify_tolerance: Tolerance for polygon simplification\n            mask_threshold: Threshold for mask binarization\n            min_object_area: Minimum area in pixels to keep an object\n            max_object_area: Maximum area in pixels to keep an object\n\n    Returns:\n        List of polygons as lists of (x, y) coordinates\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n    # Ensure binary mask\n    mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n    # Optional: apply morphological operations to improve mask quality\n    kernel = np.ones((3, 3), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    # Find contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Convert to list of [x, y] coordinates\n    polygons = []\n    for contour in contours:\n        # Filter out too small contours\n        if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n            continue\n\n        # Filter out too large contours\n        if (\n            max_object_area is not None\n            and cv2.contourArea(contour) &gt; max_object_area\n        ):\n            continue\n\n        # Simplify contour if it has many points\n        if contour.shape[0] &gt; 50:\n            epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n            contour = cv2.approxPolyDP(contour, epsilon, True)\n\n        # Convert to list of [x, y] coordinates\n        polygon = contour.reshape(-1, 2).tolist()\n        polygons.append(polygon)\n\n    return polygons\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=None, mask_threshold=None, min_object_area=None, max_object_area=None, nms_iou_threshold=None, regularize=True, angle_threshold=15, rectangularity_threshold=0.7)</code>","text":"<p>Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <code>str</code> <p>Path to the object masks GeoTIFF</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <code>Optional[float]</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>None</code> <code>mask_threshold</code> <code>Optional[float]</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>None</code> <code>min_object_area</code> <code>Optional[int]</code> <p>Minimum area in pixels to keep an object (default: self.min_object_area)</p> <code>None</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Minimum area in pixels to keep an object (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <code>Optional[float]</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>None</code> <code>regularize</code> <code>bool</code> <p>Whether to regularize objects to right angles (default: True)</p> <code>True</code> <code>angle_threshold</code> <code>int</code> <p>Maximum deviation from 90 degrees for regularization (default: 15)</p> <code>15</code> <code>rectangularity_threshold</code> <code>float</code> <p>Threshold for rectangle simplification (default: 0.7)</p> <code>0.7</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def masks_to_vector(\n    self,\n    mask_path: str,\n    output_path: Optional[str] = None,\n    simplify_tolerance: Optional[float] = None,\n    mask_threshold: Optional[float] = None,\n    min_object_area: Optional[int] = None,\n    max_object_area: Optional[int] = None,\n    nms_iou_threshold: Optional[float] = None,\n    regularize: bool = True,\n    angle_threshold: int = 15,\n    rectangularity_threshold: float = 0.7,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n    Args:\n        mask_path: Path to the object masks GeoTIFF\n        output_path: Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n        max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n        regularize: Whether to regularize objects to right angles (default: True)\n        angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n        rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Use class defaults if parameters not provided\n    simplify_tolerance = (\n        simplify_tolerance\n        if simplify_tolerance is not None\n        else self.simplify_tolerance\n    )\n    mask_threshold = (\n        mask_threshold if mask_threshold is not None else self.mask_threshold\n    )\n    min_object_area = (\n        min_object_area if min_object_area is not None else self.min_object_area\n    )\n    max_object_area = (\n        max_object_area if max_object_area is not None else self.max_object_area\n    )\n    nms_iou_threshold = (\n        nms_iou_threshold\n        if nms_iou_threshold is not None\n        else self.nms_iou_threshold\n    )\n\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Regularize objects: {regularize}\")\n    if regularize:\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(\n            f\"Found {num_labels-1} potential objects\"\n        )  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this object\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this object\n            object_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Object class\n            },\n            crs=crs,\n        )\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = self.filter_overlapping_polygons(\n            gdf, nms_iou_threshold=nms_iou_threshold\n        )\n\n        print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n        # Apply regularization if requested\n        if regularize and len(gdf) &gt; 0:\n            # Convert pixel area to geographic units for min_area parameter\n            # Estimate pixel size in geographic units\n            with rasterio.open(mask_path) as src:\n                pixel_size_x = src.transform[\n                    0\n                ]  # width of a pixel in geographic units\n                pixel_size_y = abs(\n                    src.transform[4]\n                )  # height of a pixel in geographic units\n                avg_pixel_area = pixel_size_x * pixel_size_y\n\n            # Use 10 pixels as minimum area in geographic units\n            min_geo_area = 10 * avg_pixel_area\n\n            # Regularize objects\n            gdf = self.regularize_objects(\n                gdf,\n                min_area=min_geo_area,\n                angle_threshold=angle_threshold,\n                rectangularity_threshold=rectangularity_threshold,\n            )\n\n        # Save to file\n        if output_path:\n            if output_path.endswith(\".parquet\"):\n                gdf.to_parquet(output_path)\n            else:\n                gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.process_raster","title":"<code>process_raster(raster_path, output_path=None, batch_size=4, filter_edges=True, edge_buffer=20, band_indexes=None, **kwargs)</code>","text":"<p>Process a raster file to extract objects with customizable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster file</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to output GeoJSON or Parquet file (optional)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>filter_edges</code> <code>bool</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <code>int</code> <p>Size of edge buffer in pixels to filter out objects (if filter_edges=True)</p> <code>20</code> <code>band_indexes</code> <code>Optional[List[int]]</code> <p>List of band indexes to use (if None, use all bands)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) overlap: Overlap between adjacent tiles (0.0-1.0) chip_size: Size of image chips for processing (height, width) nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0) mask_threshold: Threshold for mask binarization (0.0-1.0) min_object_area: Minimum area in pixels to keep an object simplify_tolerance: Tolerance for polygon simplification</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>@torch.no_grad()\ndef process_raster(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    batch_size: int = 4,\n    filter_edges: bool = True,\n    edge_buffer: int = 20,\n    band_indexes: Optional[List[int]] = None,\n    **kwargs: Any,\n) -&gt; \"gpd.GeoDataFrame\":\n    \"\"\"\n    Process a raster file to extract objects with customizable parameters.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoJSON or Parquet file (optional)\n        batch_size: Batch size for processing\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n        band_indexes: List of band indexes to use (if None, use all bands)\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            overlap: Overlap between adjacent tiles (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area in pixels to keep an object\n            simplify_tolerance: Tolerance for polygon simplification\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Print parameters being used\n    print(f\"Processing with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n    if filter_edges:\n        print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        overlap=overlap,\n        band_indexes=band_indexes,\n    )\n    self.raster_stats = dataset.raster_stats\n\n    # Custom collate function to handle Shapely objects\n    def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Custom collate function that handles Shapely geometries\n        by keeping them as Python objects rather than trying to collate them.\n        \"\"\"\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate shapely objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader with simple indexing and custom collate\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches\n    all_polygons = []\n    all_scores = []\n\n    print(f\"Processing raster with {len(dataloader)} batches\")\n    for batch in tqdm(dataloader):\n        # Move images to device\n        images = batch[\"image\"].to(self.device)\n        coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n        bboxes = batch[\n            \"bbox\"\n        ]  # Geographic bounding boxes - now a list, not a tensor\n\n        # Run inference\n        predictions = self.model(images)\n\n        # Process predictions\n        for idx, prediction in enumerate(predictions):\n            masks = prediction[\"masks\"].cpu().numpy()\n            scores = prediction[\"scores\"].cpu().numpy()\n            labels = prediction[\"labels\"].cpu().numpy()\n\n            # Skip if no predictions\n            if len(scores) == 0:\n                continue\n\n            # Filter by confidence threshold\n            valid_indices = scores &gt;= confidence_threshold\n            masks = masks[valid_indices]\n            scores = scores[valid_indices]\n            labels = labels[valid_indices]\n\n            # Skip if no valid predictions\n            if len(scores) == 0:\n                continue\n\n            # Get window coordinates\n            # The coords might be in different formats depending on batch handling\n            if isinstance(coords, list):\n                # If coords is a list of tuples\n                coord_item = coords[idx]\n                if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                    i, j = coord_item\n                elif isinstance(coord_item, torch.Tensor):\n                    i, j = coord_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords format: {type(coord_item)}\")\n                    continue\n            elif isinstance(coords, torch.Tensor):\n                # If coords is a tensor of shape [batch_size, 2]\n                i, j = coords[idx].cpu().numpy().tolist()\n            else:\n                print(f\"Unexpected coords type: {type(coords)}\")\n                continue\n\n            # Get window size\n            if isinstance(batch[\"window_size\"], list):\n                window_item = batch[\"window_size\"][idx]\n                if isinstance(window_item, tuple) and len(window_item) == 2:\n                    window_width, window_height = window_item\n                elif isinstance(window_item, torch.Tensor):\n                    window_width, window_height = window_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected window_size format: {type(window_item)}\")\n                    continue\n            elif isinstance(batch[\"window_size\"], torch.Tensor):\n                window_width, window_height = (\n                    batch[\"window_size\"][idx].cpu().numpy().tolist()\n                )\n            else:\n                print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                continue\n\n            # Process masks to polygons\n            for mask_idx, mask in enumerate(masks):\n                # Get binary mask\n                binary_mask = mask[0]  # Get binary mask\n\n                # Convert mask to polygon with custom parameters\n                contours = self.mask_to_polygons(\n                    binary_mask,\n                    simplify_tolerance=simplify_tolerance,\n                    mask_threshold=mask_threshold,\n                    min_object_area=min_object_area,\n                    max_object_area=max_object_area,\n                )\n\n                # Skip if no valid polygons\n                if not contours:\n                    continue\n\n                # Transform polygons to geographic coordinates\n                with rasterio.open(raster_path) as src:\n                    transform = src.transform\n\n                    for contour in contours:\n                        # Convert polygon to global coordinates\n                        global_polygon = []\n                        for x, y in contour:\n                            # Adjust coordinates based on window position\n                            gx, gy = transform * (i + x, j + y)\n                            global_polygon.append((gx, gy))\n\n                        # Create Shapely polygon\n                        if len(global_polygon) &gt;= 3:\n                            try:\n                                shapely_poly = Polygon(global_polygon)\n                                if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                    all_polygons.append(shapely_poly)\n                                    all_scores.append(float(scores[mask_idx]))\n                            except Exception as e:\n                                print(f\"Error creating polygon: {e}\")\n\n    # Create GeoDataFrame\n    if not all_polygons:\n        print(\"No valid polygons found\")\n        return None\n\n    gdf = gpd.GeoDataFrame(\n        {\n            \"geometry\": all_polygons,\n            \"confidence\": all_scores,\n            \"class\": 1,  # Object class\n        },\n        crs=dataset.crs,\n    )\n\n    # Remove overlapping polygons with custom threshold\n    gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n    # Filter edge objects if requested\n    if filter_edges:\n        gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n    # Save to file if requested\n    if output_path:\n        if output_path.endswith(\".parquet\"):\n            gdf.to_parquet(output_path)\n        else:\n            gdf.to_file(output_path, driver=\"GeoJSON\")\n        print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n    return gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.regularize_objects","title":"<code>regularize_objects(gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize objects to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with objects</p> required <code>min_area</code> <code>int</code> <p>Minimum area in square units to keep an object</p> <code>10</code> <code>angle_threshold</code> <code>int</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <code>float</code> <p>Percentage of angles that must be orthogonal for an object to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <code>float</code> <p>Minimum area ratio to Object's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with regularized objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_objects(\n    self,\n    gdf: gpd.GeoDataFrame,\n    min_area: int = 10,\n    angle_threshold: int = 15,\n    orthogonality_threshold: float = 0.3,\n    rectangularity_threshold: float = 0.7,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Regularize objects to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with objects\n        min_area: Minimum area in square units to keep an object\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n        rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized objects\n    \"\"\"\n    import math\n\n    import cv2\n    import geopandas as gpd\n    import numpy as np\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import MultiPolygon, Polygon, box\n    from tqdm import tqdm\n\n    def get_angle(\n        p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]\n    ) -&gt; float:\n        \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n        a = np.array(p1)\n        b = np.array(p2)\n        c = np.array(p3)\n\n        ba = a - b\n        bc = c - b\n\n        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n        # Handle numerical errors that could push cosine outside [-1, 1]\n        cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n        angle = np.degrees(np.arccos(cosine_angle))\n\n        return angle\n\n    def is_orthogonal(angle: float, threshold: int = angle_threshold) -&gt; bool:\n        \"\"\"Check if angle is close to 90 degrees\"\"\"\n        return abs(angle - 90) &lt;= threshold\n\n    def calculate_dominant_direction(polygon: Polygon) -&gt; float:\n        \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n        # Extract coordinates\n        coords = np.array(polygon.exterior.coords)\n\n        # Mean center the coordinates\n        mean = np.mean(coords, axis=0)\n        centered_coords = coords - mean\n\n        # Calculate covariance matrix and its eigenvalues/eigenvectors\n        cov_matrix = np.cov(centered_coords.T)\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n        # Get the index of the largest eigenvalue\n        largest_idx = np.argmax(eigenvalues)\n\n        # Get the corresponding eigenvector (principal axis)\n        principal_axis = eigenvectors[:, largest_idx]\n\n        # Calculate the angle in degrees\n        angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n        angle_deg = np.degrees(angle_rad)\n\n        # Normalize to range 0-180\n        if angle_deg &lt; 0:\n            angle_deg += 180\n\n        return angle_deg\n\n    def create_oriented_envelope(polygon: Polygon, angle_deg: float) -&gt; Polygon:\n        \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n        # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n        coords = np.array(polygon.exterior.coords)[:-1].astype(\n            np.float32\n        )  # Skip the last point (same as first)\n\n        # Use OpenCV's minAreaRect\n        rect = cv2.minAreaRect(coords)\n        box_points = cv2.boxPoints(rect)\n\n        # Convert to shapely polygon\n        oriented_box = Polygon(box_points)\n\n        return oriented_box\n\n    def get_rectangularity(polygon: Polygon, oriented_box: Polygon) -&gt; float:\n        \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n        if oriented_box.area == 0:\n            return 0\n        return polygon.area / oriented_box.area\n\n    def check_orthogonality(polygon: Polygon) -&gt; float:\n        \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n        coords = list(polygon.exterior.coords)\n        if len(coords) &lt;= 4:  # Triangle or point\n            return 0\n\n        # Remove last point (same as first)\n        coords = coords[:-1]\n\n        orthogonal_count = 0\n        total_angles = len(coords)\n\n        for i in range(total_angles):\n            p1 = coords[i]\n            p2 = coords[(i + 1) % total_angles]\n            p3 = coords[(i + 2) % total_angles]\n\n            angle = get_angle(p1, p2, p3)\n            if is_orthogonal(angle):\n                orthogonal_count += 1\n\n        return orthogonal_count / total_angles\n\n    def simplify_to_rectangle(polygon: Polygon) -&gt; Polygon:\n        \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n        # Get dominant direction\n        angle = calculate_dominant_direction(polygon)\n\n        # Create oriented envelope\n        rect = create_oriented_envelope(polygon, angle)\n\n        return rect\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No Objects to regularize\")\n        return gdf\n\n    print(f\"Regularizing {len(gdf)} objects...\")\n    print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n    print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n    print(\n        f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n    )\n\n    # Create a copy to avoid modifying the original\n    result_gdf = gdf.copy()\n\n    # Track statistics\n    total_objects = len(gdf)\n    regularized_count = 0\n    rectangularized_count = 0\n\n    # Process each Object\n    for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n        geom = row.geometry\n\n        # Skip invalid or empty geometries\n        if geom is None or geom.is_empty:\n            continue\n\n        # Handle MultiPolygons by processing the largest part\n        if isinstance(geom, MultiPolygon):\n            areas = [p.area for p in geom.geoms]\n            if not areas:\n                continue\n            geom = list(geom.geoms)[np.argmax(areas)]\n\n        # Filter out tiny Objects\n        if geom.area &lt; min_area:\n            continue\n\n        # Check orthogonality\n        orthogonality = check_orthogonality(geom)\n\n        # Create oriented envelope\n        oriented_box = create_oriented_envelope(\n            geom, calculate_dominant_direction(geom)\n        )\n\n        # Check rectangularity\n        rectangularity = get_rectangularity(geom, oriented_box)\n\n        # Decide how to regularize\n        if rectangularity &gt;= rectangularity_threshold:\n            # Object is already quite rectangular, simplify to a rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n            rectangularized_count += 1\n        elif orthogonality &gt;= orthogonality_threshold:\n            # Object has many orthogonal angles but isn't rectangular\n            # Could implement more sophisticated regularization here\n            # For now, we'll still use the oriented rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n            regularized_count += 1\n        else:\n            # Object doesn't have clear orthogonal structure\n            # Keep original but flag as unmodified\n            result_gdf.at[idx, \"regularized\"] = \"original\"\n\n    # Report statistics\n    print(f\"Regularization completed:\")\n    print(f\"- Total objects: {total_objects}\")\n    print(\n        f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n    )\n\n    return result_gdf\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.save_masks_as_geotiff","title":"<code>save_masks_as_geotiff(raster_path, output_path=None, batch_size=4, verbose=False, **kwargs)</code>","text":"<p>Process a raster file to extract object masks and save as GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster file</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to output GeoTIFF file (optional, default: input_masks.tif)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed processing information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) chip_size: Size of image chips for processing (height, width) mask_threshold: Threshold for mask binarization (0.0-1.0)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved GeoTIFF file</p> Source code in <code>geoai/extract.py</code> <pre><code>def save_masks_as_geotiff(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    batch_size: int = 4,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Process a raster file to extract object masks and save as GeoTIFF.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n        batch_size: Batch size for processing\n        verbose: Whether to print detailed processing information\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n    Returns:\n        Path to the saved GeoTIFF file\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    overlap = kwargs.get(\"overlap\", self.overlap)\n\n    # Set default output path if not provided\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n    # Print parameters being used\n    print(f\"Processing masks with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        overlap=overlap,\n        verbose=verbose,\n    )\n\n    # Store a flag to avoid repetitive messages\n    self.raster_stats = dataset.raster_stats\n    seen_warnings = {\n        \"bands\": False,\n        \"resize\": {},  # Dictionary to track resize warnings by shape\n    }\n\n    # Open original raster to get metadata\n    with rasterio.open(raster_path) as src:\n        # Create output binary mask raster with same dimensions as input\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=1,  # Single band for object mask\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Create output mask raster\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            # Initialize mask with zeros\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Custom collate function to handle Shapely objects\n            def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n                \"\"\"Custom collate function for DataLoader\"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Skip if no predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Get window coordinates\n                    if isinstance(coords, list):\n                        coord_item = coords[idx]\n                        if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                            i, j = coord_item\n                        elif isinstance(coord_item, torch.Tensor):\n                            i, j = coord_item.cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords format: {type(coord_item)}\")\n                            continue\n                    elif isinstance(coords, torch.Tensor):\n                        i, j = coords[idx].cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords type: {type(coords)}\")\n                        continue\n\n                    # Get window size\n                    if isinstance(batch[\"window_size\"], list):\n                        window_item = batch[\"window_size\"][idx]\n                        if isinstance(window_item, tuple) and len(window_item) == 2:\n                            window_width, window_height = window_item\n                        elif isinstance(window_item, torch.Tensor):\n                            window_width, window_height = (\n                                window_item.cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size format: {type(window_item)}\"\n                            )\n                            continue\n                    elif isinstance(batch[\"window_size\"], torch.Tensor):\n                        window_width, window_height = (\n                            batch[\"window_size\"][idx].cpu().numpy().tolist()\n                        )\n                    else:\n                        print(\n                            f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                        )\n                        continue\n\n                    # Combine all masks for this window\n                    combined_mask = np.zeros(\n                        (window_height, window_width), dtype=np.uint8\n                    )\n\n                    for mask in masks:\n                        # Get the binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(\n                            np.uint8\n                        ) * 255\n\n                        # Handle size mismatch - resize binary_mask if needed\n                        mask_h, mask_w = binary_mask.shape\n                        if mask_h != window_height or mask_w != window_width:\n                            resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                            if resize_key not in seen_warnings[\"resize\"]:\n                                if verbose:\n                                    print(\n                                        f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                    )\n                                else:\n                                    if not seen_warnings[\n                                        \"resize\"\n                                    ]:  # If this is the first resize warning\n                                        print(\n                                            f\"Resizing masks at image edges (set verbose=True for details)\"\n                                        )\n                                seen_warnings[\"resize\"][resize_key] = True\n\n                            # Crop or pad the binary mask to match window size\n                            resized_mask = np.zeros(\n                                (window_height, window_width), dtype=np.uint8\n                            )\n                            copy_h = min(mask_h, window_height)\n                            copy_w = min(mask_w, window_width)\n                            resized_mask[:copy_h, :copy_w] = binary_mask[\n                                :copy_h, :copy_w\n                            ]\n                            binary_mask = resized_mask\n\n                        # Update combined mask (taking maximum where masks overlap)\n                        combined_mask = np.maximum(combined_mask, binary_mask)\n\n                    # Write combined mask to output array\n                    # Handle edge cases where window might be smaller than chip size\n                    h, w = combined_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            combined_mask[:valid_h, :valid_w],\n                        )\n\n            # Write the final mask to the output file\n            dst.write(mask_array, 1)\n\n    print(f\"Object masks saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.vectorize_masks","title":"<code>vectorize_masks(masks_path, output_path=None, confidence_threshold=0.5, min_object_area=100, max_object_area=None, n_workers=None, **kwargs)</code>","text":"<p>Convert masks with confidence to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>masks_path</code> <code>str</code> <p>Path to masks GeoTIFF with confidence band.</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path for output GeoJSON.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence score (0.0-1.0). Default: 0.5</p> <code>0.5</code> <code>min_object_area</code> <code>int</code> <p>Minimum area in pixels to keep an object. Default: 100</p> <code>100</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Maximum area in pixels to keep an object. Default: None</p> <code>None</code> <code>n_workers</code> <code>Optional[int]</code> <p>int, default=None The number of worker threads to use. \"None\" means single-threaded processing. \"-1\"   means using all available CPU processors. Positive integer means using that specific number of threads.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with car detections and confidence values</p> Source code in <code>geoai/extract.py</code> <pre><code>def vectorize_masks(\n    self,\n    masks_path: str,\n    output_path: Optional[str] = None,\n    confidence_threshold: float = 0.5,\n    min_object_area: int = 100,\n    max_object_area: Optional[int] = None,\n    n_workers: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert masks with confidence to vector polygons.\n\n    Args:\n        masks_path: Path to masks GeoTIFF with confidence band.\n        output_path: Path for output GeoJSON.\n        confidence_threshold: Minimum confidence score (0.0-1.0). Default: 0.5\n        min_object_area: Minimum area in pixels to keep an object. Default: 100\n        max_object_area: Maximum area in pixels to keep an object. Default: None\n        n_workers: int, default=None\n            The number of worker threads to use.\n            \"None\" means single-threaded processing.\n            \"-1\"   means using all available CPU processors.\n            Positive integer means using that specific number of threads.\n        **kwargs: Additional parameters\n\n    Returns:\n        GeoDataFrame with car detections and confidence values\n    \"\"\"\n\n    def _process_single_component(\n        component_mask: np.ndarray,\n        conf_data: np.ndarray,\n        transform: Any,\n        confidence_threshold: float,\n        min_object_area: int,\n        max_object_area: Optional[int],\n    ) -&gt; Optional[Dict[str, Any]]:\n        # Get confidence value\n        conf_region = conf_data[component_mask &gt; 0]\n        if len(conf_region) &gt; 0:\n            confidence = np.mean(conf_region) / 255.0\n        else:\n            confidence = 0.0\n\n        # Skip if confidence is below threshold\n        if confidence &lt; confidence_threshold:\n            return None\n\n        # Find contours\n        contours, _ = cv2.findContours(\n            component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        results = []\n\n        for contour in contours:\n            # Filter by size\n            area = cv2.contourArea(contour)\n            if area &lt; min_object_area:\n                continue\n\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Get minimum area rectangle\n            rect = cv2.minAreaRect(contour)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to geographic coordinates\n            geo_points = []\n            for x, y in box_points:\n                gx, gy = transform * (x, y)\n                geo_points.append((gx, gy))\n\n            # Create polygon\n            poly = Polygon(geo_points)\n            results.append((poly, confidence, area))\n\n        return results\n\n    import concurrent.futures\n    from functools import partial\n\n    def process_component(\n        args: Tuple[int, np.ndarray, np.ndarray, Any, float, int, Optional[int]],\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Helper function to process a single component\n        \"\"\"\n        (\n            label,\n            labeled_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        ) = args\n\n        # Create mask for this component\n        component_mask = (labeled_mask == label).astype(np.uint8)\n\n        return _process_single_component(\n            component_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        )\n\n    start_time = time.time()\n    print(f\"Processing masks from: {masks_path}\")\n\n    if n_workers == -1:\n        n_workers = os.cpu_count()\n\n    with rasterio.open(masks_path) as src:\n        # Read mask and confidence bands\n        mask_data = src.read(1)\n        conf_data = src.read(2)\n        transform = src.transform\n        crs = src.crs\n\n        # Convert to binary mask\n        binary_mask = mask_data &gt; 0\n\n        # Find connected components\n        labeled_mask, num_features = ndimage.label(binary_mask)\n        print(f\"Found {num_features} connected components\")\n\n        # Process each component\n        polygons = []\n        confidences = []\n        pixels = []\n\n        if n_workers is None or n_workers == 1:\n            print(\n                \"Using single-threaded processing, you can speed up processing by setting n_workers &gt; 1\"\n            )\n            # Add progress bar\n            for label in tqdm(\n                range(1, num_features + 1), desc=\"Processing components\"\n            ):\n                # Create mask for this component\n                component_mask = (labeled_mask == label).astype(np.uint8)\n\n                result = _process_single_component(\n                    component_mask,\n                    conf_data,\n                    transform,\n                    confidence_threshold,\n                    min_object_area,\n                    max_object_area,\n                )\n\n                if result:\n                    for poly, confidence, area in result:\n                        # Add to lists\n                        polygons.append(poly)\n                        confidences.append(confidence)\n                        pixels.append(area)\n\n        else:\n            # Process components in parallel\n            print(f\"Using {n_workers} workers for parallel processing\")\n\n            process_args = [\n                (\n                    label,\n                    labeled_mask,\n                    conf_data,\n                    transform,\n                    confidence_threshold,\n                    min_object_area,\n                    max_object_area,\n                )\n                for label in range(1, num_features + 1)\n            ]\n\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=n_workers\n            ) as executor:\n                results = list(\n                    tqdm(\n                        executor.map(process_component, process_args),\n                        total=num_features,\n                        desc=\"Processing components\",\n                    )\n                )\n\n                for result in results:\n                    if result:\n                        for poly, confidence, area in result:\n                            # Add to lists\n                            polygons.append(poly)\n                            confidences.append(confidence)\n                            pixels.append(area)\n\n        # Create GeoDataFrame\n        if polygons:\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": polygons,\n                    \"confidence\": confidences,\n                    \"class\": [1] * len(polygons),\n                    \"pixels\": pixels,\n                },\n                crs=crs,\n            )\n\n            # Save to file if requested\n            if output_path:\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n                print(f\"Saved {len(gdf)} objects with confidence to {output_path}\")\n\n            end_time = time.time()\n            print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n            return gdf\n        else:\n            end_time = time.time()\n            print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n            print(\"No valid polygons found\")\n            return None\n</code></pre>"},{"location":"extract/#geoai.extract.ObjectDetector.visualize_results","title":"<code>visualize_results(raster_path, gdf=None, output_path=None, figsize=(12, 12))</code>","text":"<p>Visualize object detection results with proper coordinate transformation.</p> <p>This function displays objects on top of the raster image, ensuring proper alignment between the GeoDataFrame polygons and the image.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster</p> required <code>gdf</code> <code>Optional[GeoDataFrame]</code> <p>GeoDataFrame with object polygons (optional)</p> <code>None</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to save visualization (optional)</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size (width, height) in inches</p> <code>(12, 12)</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if visualization was successful</p> Source code in <code>geoai/extract.py</code> <pre><code>def visualize_results(\n    self,\n    raster_path: str,\n    gdf: Optional[gpd.GeoDataFrame] = None,\n    output_path: Optional[str] = None,\n    figsize: Tuple[int, int] = (12, 12),\n) -&gt; bool:\n    \"\"\"\n    Visualize object detection results with proper coordinate transformation.\n\n    This function displays objects on top of the raster image,\n    ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n    Args:\n        raster_path: Path to input raster\n        gdf: GeoDataFrame with object polygons (optional)\n        output_path: Path to save visualization (optional)\n        figsize: Figure size (width, height) in inches\n\n    Returns:\n        bool: True if visualization was successful\n    \"\"\"\n    # Check if raster file exists\n    if not os.path.exists(raster_path):\n        print(f\"Error: Raster file '{raster_path}' not found.\")\n        return False\n\n    # Process raster if GeoDataFrame not provided\n    if gdf is None:\n        gdf = self.process_raster(raster_path)\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No objects to visualize\")\n        return False\n\n    # Check if confidence column exists in the GeoDataFrame\n    has_confidence = False\n    if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n        # Try to access a confidence value to confirm it works\n        try:\n            if len(gdf) &gt; 0:\n                # Try getitem access\n                conf_val = gdf[\"confidence\"].iloc[0]\n                has_confidence = True\n                print(\n                    f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                )\n        except Exception as e:\n            print(f\"Confidence column exists but couldn't access values: {e}\")\n            has_confidence = False\n    else:\n        print(\"No confidence column found in GeoDataFrame\")\n        has_confidence = False\n\n    # Read raster for visualization\n    with rasterio.open(raster_path) as src:\n        # Read the entire image or a subset if it's very large\n        if src.height &gt; 2000 or src.width &gt; 2000:\n            # Calculate scale factor to reduce size\n            scale = min(2000 / src.height, 2000 / src.width)\n            out_shape = (\n                int(src.count),\n                int(src.height * scale),\n                int(src.width * scale),\n            )\n\n            # Read and resample\n            image = src.read(\n                out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n            )\n\n            # Create a scaled transform for the resampled image\n            # Calculate scaling factors\n            x_scale = src.width / out_shape[2]\n            y_scale = src.height / out_shape[1]\n\n            # Get the original transform\n            orig_transform = src.transform\n\n            # Create a scaled transform\n            scaled_transform = rasterio.transform.Affine(\n                orig_transform.a * x_scale,\n                orig_transform.b,\n                orig_transform.c,\n                orig_transform.d,\n                orig_transform.e * y_scale,\n                orig_transform.f,\n            )\n        else:\n            image = src.read()\n            scaled_transform = src.transform\n\n        # Convert to RGB for display\n        if image.shape[0] &gt; 3:\n            image = image[:3]\n        elif image.shape[0] == 1:\n            image = np.repeat(image, 3, axis=0)\n\n        # Normalize image for display\n        image = image.transpose(1, 2, 0)  # CHW to HWC\n        image = image.astype(np.float32)\n\n        if image.max() &gt; 10:  # Likely 0-255 range\n            image = image / 255.0\n\n        image = np.clip(image, 0, 1)\n\n        # Get image bounds\n        bounds = src.bounds\n        crs = src.crs\n\n    # Create figure with appropriate aspect ratio\n    aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n    plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n    ax = plt.gca()\n\n    # Display image\n    ax.imshow(image)\n\n    # Make sure the GeoDataFrame has the same CRS as the raster\n    if gdf.crs != crs:\n        print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Set up colors for confidence visualization\n    if has_confidence:\n        try:\n            import matplotlib.cm as cm\n            from matplotlib.colors import Normalize\n\n            # Get min/max confidence values\n            min_conf = gdf[\"confidence\"].min()\n            max_conf = gdf[\"confidence\"].max()\n\n            # Set up normalization and colormap\n            norm = Normalize(vmin=min_conf, vmax=max_conf)\n            cmap = cm.viridis\n\n            # Create scalar mappable for colorbar\n            sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n            sm.set_array([])\n\n            # Add colorbar\n            cbar = plt.colorbar(\n                sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n            )\n            cbar.set_label(\"Confidence Score\")\n        except Exception as e:\n            print(f\"Error setting up confidence visualization: {e}\")\n            has_confidence = False\n\n    # Function to convert coordinates\n    def geo_to_pixel(\n        geometry: Any, transform: Any\n    ) -&gt; Optional[Tuple[List[float], List[float]]]:\n        \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n        if geometry.is_empty:\n            return None\n\n        if geometry.geom_type == \"Polygon\":\n            # Get exterior coordinates\n            exterior_coords = list(geometry.exterior.coords)\n\n            # Convert to pixel coordinates\n            pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n            # Split into x and y lists\n            pixel_x = [coord[0] for coord in pixel_coords]\n            pixel_y = [coord[1] for coord in pixel_coords]\n\n            return pixel_x, pixel_y\n        else:\n            print(f\"Unsupported geometry type: {geometry.geom_type}\")\n            return None\n\n    # Plot each object\n    for idx, row in gdf.iterrows():\n        try:\n            # Convert polygon to pixel coordinates\n            coords = geo_to_pixel(row.geometry, scaled_transform)\n\n            if coords:\n                pixel_x, pixel_y = coords\n\n                if has_confidence:\n                    try:\n                        # Get confidence value using different methods\n                        # Method 1: Try direct attribute access\n                        confidence = None\n                        try:\n                            confidence = row.confidence\n                        except:\n                            pass\n\n                        # Method 2: Try dictionary-style access\n                        if confidence is None:\n                            try:\n                                confidence = row[\"confidence\"]\n                            except:\n                                pass\n\n                        # Method 3: Try accessing by index from the GeoDataFrame\n                        if confidence is None:\n                            try:\n                                confidence = gdf.iloc[idx][\"confidence\"]\n                            except:\n                                pass\n\n                        if confidence is not None:\n                            color = cmap(norm(confidence))\n                            # Fill polygon with semi-transparent color\n                            ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                            # Draw border\n                            ax.plot(\n                                pixel_x,\n                                pixel_y,\n                                color=color,\n                                linewidth=1,\n                                alpha=0.8,\n                            )\n                        else:\n                            # Fall back to red if confidence value couldn't be accessed\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    except Exception as e:\n                        print(\n                            f\"Error using confidence value for polygon {idx}: {e}\"\n                        )\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                else:\n                    # No confidence data, just plot outlines in red\n                    ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n        except Exception as e:\n            print(f\"Error plotting polygon {idx}: {e}\")\n\n    # Remove axes\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n    # Save if requested\n    if output_path:\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Visualization saved to {output_path}\")\n\n    plt.close()\n\n    # Create a simpler visualization focused just on a subset of objects\n    if len(gdf) &gt; 0:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n        # Choose a subset of the image to show\n        with rasterio.open(raster_path) as src:\n            # Get centroid of first object\n            sample_geom = gdf.iloc[0].geometry\n            centroid = sample_geom.centroid\n\n            # Convert to pixel coordinates\n            center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n            # Define a window around this object\n            window_size = 500  # pixels\n            window = rasterio.windows.Window(\n                max(0, int(center_x - window_size / 2)),\n                max(0, int(center_y - window_size / 2)),\n                min(window_size, src.width - int(center_x - window_size / 2)),\n                min(window_size, src.height - int(center_y - window_size / 2)),\n            )\n\n            # Read this window\n            sample_image = src.read(window=window)\n\n            # Convert to RGB for display\n            if sample_image.shape[0] &gt; 3:\n                sample_image = sample_image[:3]\n            elif sample_image.shape[0] == 1:\n                sample_image = np.repeat(sample_image, 3, axis=0)\n\n            # Normalize image for display\n            sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n            sample_image = sample_image.astype(np.float32)\n\n            if sample_image.max() &gt; 10:  # Likely 0-255 range\n                sample_image = sample_image / 255.0\n\n            sample_image = np.clip(sample_image, 0, 1)\n\n            # Display sample image\n            ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n            # Get the correct transform for this window\n            window_transform = src.window_transform(window)\n\n            # Calculate bounds of the window\n            window_bounds = rasterio.windows.bounds(window, src.transform)\n            window_box = box(*window_bounds)\n\n            # Filter objects that intersect with this window\n            visible_gdf = gdf[gdf.intersects(window_box)]\n\n            # Set up colors for sample view if confidence data exists\n            if has_confidence:\n                try:\n                    # Reuse the same normalization and colormap from main view\n                    sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                    sample_sm.set_array([])\n\n                    # Add colorbar to sample view\n                    sample_cbar = plt.colorbar(\n                        sample_sm,\n                        ax=ax,\n                        orientation=\"vertical\",\n                        shrink=0.7,\n                        pad=0.01,\n                    )\n                    sample_cbar.set_label(\"Confidence Score\")\n                except Exception as e:\n                    print(f\"Error setting up sample confidence visualization: {e}\")\n\n            # Plot objects in sample view\n            for idx, row in visible_gdf.iterrows():\n                try:\n                    # Get window-relative pixel coordinates\n                    geom = row.geometry\n\n                    # Skip empty geometries\n                    if geom.is_empty:\n                        continue\n\n                    # Get exterior coordinates\n                    exterior_coords = list(geom.exterior.coords)\n\n                    # Convert to pixel coordinates relative to window origin\n                    pixel_coords = []\n                    for x, y in exterior_coords:\n                        px, py = ~src.transform * (x, y)  # Convert to image pixels\n                        # Make coordinates relative to window\n                        px = px - window.col_off\n                        py = py - window.row_off\n                        pixel_coords.append((px, py))\n\n                    # Extract x and y coordinates\n                    pixel_x = [coord[0] for coord in pixel_coords]\n                    pixel_y = [coord[1] for coord in pixel_coords]\n\n                    # Use confidence colors if available\n                    if has_confidence:\n                        try:\n                            # Try different methods to access confidence\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1.5,\n                                    alpha=0.8,\n                                )\n                            else:\n                                ax.plot(\n                                    pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                )\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    else:\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                except Exception as e:\n                    print(f\"Error plotting polygon in sample view: {e}\")\n\n            # Set title\n            ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n            # Remove axes\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n            # Save if requested\n            if output_path:\n                sample_output = (\n                    os.path.splitext(output_path)[0]\n                    + \"_sample\"\n                    + os.path.splitext(output_path)[1]\n                )\n                plt.tight_layout()\n                plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                print(f\"Sample visualization saved to {sample_output}\")\n</code></pre>"},{"location":"extract/#geoai.extract.ParkingSplotDetector","title":"<code>ParkingSplotDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ParkingSplotDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the `ObjectDetector` class with additional methods for car detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"parking_spot_detection.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        num_classes: int = 3,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            num_classes: Number of classes for the model. Default: 3\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path,\n            repo_id=repo_id,\n            model=model,\n            num_classes=num_classes,\n            device=device,\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.ParkingSplotDetector.__init__","title":"<code>__init__(model_path='parking_spot_detection.pth', repo_id=None, model=None, num_classes=3, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'parking_spot_detection.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes for the model. Default: 3</p> <code>3</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"parking_spot_detection.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    num_classes: int = 3,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        num_classes: Number of classes for the model. Default: 3\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path,\n        repo_id=repo_id,\n        model=model,\n        num_classes=num_classes,\n        device=device,\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.ShipDetector","title":"<code>ShipDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Ship detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for ship detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class ShipDetector(ObjectDetector):\n    \"\"\"\n    Ship detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for ship detection.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"ship_detection.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.ShipDetector.__init__","title":"<code>__init__(model_path='ship_detection.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'ship_detection.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"ship_detection.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"extract/#geoai.extract.SolarPanelDetector","title":"<code>SolarPanelDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Solar panel detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for solar panel detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class SolarPanelDetector(ObjectDetector):\n    \"\"\"\n    Solar panel detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for solar panel detection.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"solar_panel_detection.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"extract/#geoai.extract.SolarPanelDetector.__init__","title":"<code>__init__(model_path='solar_panel_detection.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'solar_panel_detection.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"solar_panel_detection.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"geo_agents/","title":"geo_agents module","text":""},{"location":"geo_agents/#geoai.agents.geo_agents.CatalogAgent","title":"<code>CatalogAgent</code>","text":"<p>               Bases: <code>Agent</code></p> <p>AI agent for searching data catalogs with natural language queries.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>class CatalogAgent(Agent):\n    \"\"\"AI agent for searching data catalogs with natural language queries.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model: str = \"llama3.1\",\n        system_prompt: str = \"default\",\n        catalog_url: Optional[str] = None,\n        catalog_df: Optional[Any] = None,\n        model_args: dict = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the Catalog Agent.\n\n        Args:\n            model: Model identifier (default: \"llama3.1\").\n            system_prompt: System prompt for the agent (default: \"default\").\n            catalog_url: URL to a catalog file (TSV, CSV, or JSON). Use JSON format for spatial search support.\n                Example: \"https://raw.githubusercontent.com/opengeos/Earth-Engine-Catalog/refs/heads/master/gee_catalog.json\"\n            catalog_df: Pre-loaded catalog as a pandas DataFrame.\n            model_args: Additional keyword arguments for the model.\n            **kwargs: Additional keyword arguments for the Agent.\n        \"\"\"\n        self.catalog_tools: CatalogTools = CatalogTools(\n            catalog_url=catalog_url, catalog_df=catalog_df\n        )\n\n        if model_args is None:\n            model_args = {}\n\n        # --- save a model factory we can call each turn ---\n        if isinstance(model, str) and (\":\" in model or model.startswith(\"llama\")):\n            # treat ANY \"llama...\" model id as Ollama\n            self._model_factory = lambda m=model: create_ollama_model(\n                host=\"http://localhost:11434\", model_id=m, **model_args\n            )\n        elif isinstance(model, OllamaModel):\n            # Extract configuration from existing OllamaModel and create new instances\n            model_id = model.config[\"model_id\"]\n            host = model.host\n            client_args = model.client_args\n            self._model_factory: Callable[[], OllamaModel] = (\n                lambda: create_ollama_model(\n                    host=host, model_id=model_id, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, OpenAIModel):\n            # Extract configuration from existing OpenAIModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], OpenAIModel] = (\n                lambda mid=model_id, client_args=client_args: create_openai_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, AnthropicModel):\n            # Extract configuration from existing AnthropicModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], AnthropicModel] = (\n                lambda mid=model_id, client_args=client_args: create_anthropic_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, str):\n            # Only Bedrock IDs here, not LLaMA\n            self._model_factory = lambda m=model: create_bedrock_model(\n                model_id=m, **model_args\n            )\n        else:\n            raise ValueError(f\"Invalid model: {model}\")\n\n        # build initial model (first turn)\n        model = self._model_factory()\n\n        if system_prompt == \"default\":\n            system_prompt = \"\"\"You are a data catalog search agent. Your job is to help users find datasets from a data catalog.\n\nIMPORTANT: Follow these steps EXACTLY:\n\n1. Understand the user's query:\n   - What type of data are they looking for? (e.g., landcover, elevation, imagery)\n   - Are they searching for a specific geographic region? (e.g., California, San Francisco, bounding box)\n   - Are they filtering by time period? (e.g., \"from 2020\", \"between 2015-2020\", \"recent data\")\n   - Are they filtering by provider? (e.g., NASA, USGS)\n   - Are they filtering by dataset type? (e.g., image, image_collection, table)\n\n2. Use the appropriate tool:\n   - search_by_region: PREFERRED for spatial queries - search datasets covering a geographic region\n     * Use location parameter for place names (e.g., \"California\", \"San Francisco\")\n     * Use bbox parameter for coordinates [west, south, east, north]\n     * Can combine with keywords, dataset_type, provider, start_date, end_date filters\n   - search_datasets: For keyword-only searches without spatial filter\n     * Can filter by start_date and end_date for temporal queries\n   - geocode_location: Convert location names to coordinates (called automatically by search_by_region)\n   - get_dataset_info: Get details about a specific dataset by ID\n   - list_dataset_types: Show available dataset types\n   - list_providers: Show available data providers\n   - get_catalog_stats: Get overall catalog statistics\n\n3. Search strategy:\n   - SPATIAL QUERIES: If user mentions ANY location or region, IMMEDIATELY use search_by_region\n     * Pass location names directly to the location parameter - DO NOT ask user for bbox coordinates\n     * Examples of locations: California, San Francisco, New York, Paris, any city/state/country name\n     * search_by_region will automatically geocode location names - you don't need to call geocode_location separately\n   - TEMPORAL QUERIES: If user mentions ANY time period, ALWAYS add start_date/end_date parameters\n     * \"from 2022\" or \"since 2022\" or \"2022 onwards\" \u2192 start_date=\"2022-01-01\"\n     * \"until 2023\" or \"before 2023\" \u2192 end_date=\"2023-12-31\"\n     * \"between 2020 and 2023\" \u2192 start_date=\"2020-01-01\", end_date=\"2023-12-31\"\n     * \"recent\" or \"latest\" \u2192 start_date=\"2020-01-01\"\n     * Time indicators: from, since, after, before, until, between, onwards, recent, latest\n   - KEYWORD QUERIES: If no location mentioned, use search_datasets\n   - Extract key search terms from the user's query\n   - Use keywords parameter for the main search terms\n   - Use dataset_type parameter if user specifies type (image, table, etc.)\n   - Use provider parameter if user specifies provider (NASA, USGS, etc.)\n   - Default max_results is 10, but can be adjusted\n\nCRITICAL RULES:\n1. NEVER ask the user to provide bbox coordinates. If they mention a location name, pass it directly to search_by_region(location=\"name\")\n2. ALWAYS add start_date or end_date when user mentions ANY time period (from, since, onwards, recent, etc.)\n3. Convert years to YYYY-MM-DD format: 2022 \u2192 \"2022-01-01\"\n\n4. Examples:\n   - \"Find landcover datasets covering California\" \u2192 search_by_region(location=\"California\", keywords=\"landcover\")\n   - \"Show elevation data for San Francisco\" \u2192 search_by_region(location=\"San Francisco\", keywords=\"elevation\")\n   - \"Find datasets in bbox [-122, 37, -121, 38]\" \u2192 search_by_region(bbox=[-122, 37, -121, 38])\n   - \"Find landcover datasets from NASA\" \u2192 search_datasets(keywords=\"landcover\", provider=\"NASA\")\n   - \"Show me elevation data\" \u2192 search_datasets(keywords=\"elevation\")\n   - \"What types of datasets are available?\" \u2192 list_dataset_types()\n   - \"Find image collections about forests\" \u2192 search_datasets(keywords=\"forest\", dataset_type=\"image_collection\")\n   - \"Find landcover data from 2020 onwards\" \u2192 search_datasets(keywords=\"landcover\", start_date=\"2020-01-01\")\n   - \"Show California datasets between 2015 and 2020\" \u2192 search_by_region(location=\"California\", start_date=\"2015-01-01\", end_date=\"2020-12-31\")\n   - \"Find recent elevation data\" \u2192 search_datasets(keywords=\"elevation\", start_date=\"2020-01-01\")\n\n5. Return results clearly:\n   - Summarize the number of results found\n   - List the top results with their EXACT IDs and titles FROM THE TOOL RESPONSE\n   - Mention key information like provider, type, geographic coverage, and date range if available\n   - For spatial searches, mention the search region\n\nERROR HANDLING:\n- If no results found: Suggest trying different keywords, broader region, or removing filters\n- If location not found: Suggest alternative spellings or try a broader region\n- If tool error: Explain the error and suggest alternatives\n\nCRITICAL RULES - MUST FOLLOW:\n1. NEVER make up or hallucinate dataset IDs, titles, or any other information\n2. ONLY report datasets that appear in the actual tool response\n3. Copy dataset IDs and titles EXACTLY as they appear in the tool response\n4. If a field is null/None in the tool response, say \"N/A\" or omit it - DO NOT guess\n5. DO NOT use your training data knowledge about Earth Engine datasets\n6. DO NOT fill in missing information from your knowledge\n7. If unsure, say \"Information not available in results\"\n\nExample of CORRECT behavior:\nTool returns: {\"id\": \"AAFC/ACI\", \"title\": \"Canada AAFC Annual Crop Inventory\"}\nYour response: \"Found dataset: AAFC/ACI - Canada AAFC Annual Crop Inventory\"\n\nExample of INCORRECT behavior (DO NOT DO THIS):\nTool returns: {\"id\": \"AAFC/ACI\", \"title\": \"Canada AAFC Annual Crop Inventory\"}\nYour response: \"Found dataset: USGS/NED - USGS Elevation Data\"  \u2190 WRONG! This ID wasn't in the tool response!\"\"\"\n\n        super().__init__(\n            name=\"Catalog Search Agent\",\n            model=model,\n            tools=[\n                self.catalog_tools.search_datasets,\n                self.catalog_tools.search_by_region,\n                self.catalog_tools.get_dataset_info,\n                self.catalog_tools.geocode_location,\n                self.catalog_tools.list_dataset_types,\n                self.catalog_tools.list_providers,\n                self.catalog_tools.get_catalog_stats,\n            ],\n            system_prompt=system_prompt,\n            callback_handler=None,\n        )\n\n    def ask(self, prompt: str) -&gt; str:\n        \"\"\"Send a single-turn prompt to the agent.\n\n        Args:\n            prompt: The text prompt to send to the agent.\n\n        Returns:\n            The agent's response as a string.\n        \"\"\"\n        # Use strands' built-in __call__ method which now supports multiple calls\n        result = self(prompt)\n        return getattr(result, \"final_text\", str(result))\n\n    def search_datasets(\n        self,\n        keywords: Optional[str] = None,\n        dataset_type: Optional[str] = None,\n        provider: Optional[str] = None,\n        max_results: int = 10,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Search for datasets and return structured results.\n\n        This method directly uses the CatalogTools without LLM inference for faster searches.\n\n        Args:\n            keywords: Keywords to search for.\n            dataset_type: Filter by dataset type.\n            provider: Filter by provider.\n            max_results: Maximum number of results to return.\n\n        Returns:\n            List of dataset dictionaries.\n\n        Example:\n            &gt;&gt;&gt; agent = CatalogAgent(catalog_url=\"...\")\n            &gt;&gt;&gt; datasets = agent.search_datasets(keywords=\"landcover\", provider=\"NASA\")\n            &gt;&gt;&gt; for ds in datasets:\n            ...     print(ds['id'], ds['title'])\n        \"\"\"\n        result_json = self.catalog_tools.search_datasets(\n            keywords=keywords,\n            dataset_type=dataset_type,\n            provider=provider,\n            max_results=max_results,\n        )\n\n        result = json.loads(result_json)\n\n        if \"error\" in result:\n            print(f\"Search error: {result['error']}\")\n            return []\n\n        return result.get(\"datasets\", [])\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.CatalogAgent.__init__","title":"<code>__init__(*, model='llama3.1', system_prompt='default', catalog_url=None, catalog_df=None, model_args=None, **kwargs)</code>","text":"<p>Initialize the Catalog Agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (default: \"llama3.1\").</p> <code>'llama3.1'</code> <code>system_prompt</code> <code>str</code> <p>System prompt for the agent (default: \"default\").</p> <code>'default'</code> <code>catalog_url</code> <code>Optional[str]</code> <p>URL to a catalog file (TSV, CSV, or JSON). Use JSON format for spatial search support. Example: \"https://raw.githubusercontent.com/opengeos/Earth-Engine-Catalog/refs/heads/master/gee_catalog.json\"</p> <code>None</code> <code>catalog_df</code> <code>Optional[Any]</code> <p>Pre-loaded catalog as a pandas DataFrame.</p> <code>None</code> <code>model_args</code> <code>dict</code> <p>Additional keyword arguments for the model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the Agent.</p> <code>{}</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>    def __init__(\n        self,\n        *,\n        model: str = \"llama3.1\",\n        system_prompt: str = \"default\",\n        catalog_url: Optional[str] = None,\n        catalog_df: Optional[Any] = None,\n        model_args: dict = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the Catalog Agent.\n\n        Args:\n            model: Model identifier (default: \"llama3.1\").\n            system_prompt: System prompt for the agent (default: \"default\").\n            catalog_url: URL to a catalog file (TSV, CSV, or JSON). Use JSON format for spatial search support.\n                Example: \"https://raw.githubusercontent.com/opengeos/Earth-Engine-Catalog/refs/heads/master/gee_catalog.json\"\n            catalog_df: Pre-loaded catalog as a pandas DataFrame.\n            model_args: Additional keyword arguments for the model.\n            **kwargs: Additional keyword arguments for the Agent.\n        \"\"\"\n        self.catalog_tools: CatalogTools = CatalogTools(\n            catalog_url=catalog_url, catalog_df=catalog_df\n        )\n\n        if model_args is None:\n            model_args = {}\n\n        # --- save a model factory we can call each turn ---\n        if isinstance(model, str) and (\":\" in model or model.startswith(\"llama\")):\n            # treat ANY \"llama...\" model id as Ollama\n            self._model_factory = lambda m=model: create_ollama_model(\n                host=\"http://localhost:11434\", model_id=m, **model_args\n            )\n        elif isinstance(model, OllamaModel):\n            # Extract configuration from existing OllamaModel and create new instances\n            model_id = model.config[\"model_id\"]\n            host = model.host\n            client_args = model.client_args\n            self._model_factory: Callable[[], OllamaModel] = (\n                lambda: create_ollama_model(\n                    host=host, model_id=model_id, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, OpenAIModel):\n            # Extract configuration from existing OpenAIModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], OpenAIModel] = (\n                lambda mid=model_id, client_args=client_args: create_openai_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, AnthropicModel):\n            # Extract configuration from existing AnthropicModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], AnthropicModel] = (\n                lambda mid=model_id, client_args=client_args: create_anthropic_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, str):\n            # Only Bedrock IDs here, not LLaMA\n            self._model_factory = lambda m=model: create_bedrock_model(\n                model_id=m, **model_args\n            )\n        else:\n            raise ValueError(f\"Invalid model: {model}\")\n\n        # build initial model (first turn)\n        model = self._model_factory()\n\n        if system_prompt == \"default\":\n            system_prompt = \"\"\"You are a data catalog search agent. Your job is to help users find datasets from a data catalog.\n\nIMPORTANT: Follow these steps EXACTLY:\n\n1. Understand the user's query:\n   - What type of data are they looking for? (e.g., landcover, elevation, imagery)\n   - Are they searching for a specific geographic region? (e.g., California, San Francisco, bounding box)\n   - Are they filtering by time period? (e.g., \"from 2020\", \"between 2015-2020\", \"recent data\")\n   - Are they filtering by provider? (e.g., NASA, USGS)\n   - Are they filtering by dataset type? (e.g., image, image_collection, table)\n\n2. Use the appropriate tool:\n   - search_by_region: PREFERRED for spatial queries - search datasets covering a geographic region\n     * Use location parameter for place names (e.g., \"California\", \"San Francisco\")\n     * Use bbox parameter for coordinates [west, south, east, north]\n     * Can combine with keywords, dataset_type, provider, start_date, end_date filters\n   - search_datasets: For keyword-only searches without spatial filter\n     * Can filter by start_date and end_date for temporal queries\n   - geocode_location: Convert location names to coordinates (called automatically by search_by_region)\n   - get_dataset_info: Get details about a specific dataset by ID\n   - list_dataset_types: Show available dataset types\n   - list_providers: Show available data providers\n   - get_catalog_stats: Get overall catalog statistics\n\n3. Search strategy:\n   - SPATIAL QUERIES: If user mentions ANY location or region, IMMEDIATELY use search_by_region\n     * Pass location names directly to the location parameter - DO NOT ask user for bbox coordinates\n     * Examples of locations: California, San Francisco, New York, Paris, any city/state/country name\n     * search_by_region will automatically geocode location names - you don't need to call geocode_location separately\n   - TEMPORAL QUERIES: If user mentions ANY time period, ALWAYS add start_date/end_date parameters\n     * \"from 2022\" or \"since 2022\" or \"2022 onwards\" \u2192 start_date=\"2022-01-01\"\n     * \"until 2023\" or \"before 2023\" \u2192 end_date=\"2023-12-31\"\n     * \"between 2020 and 2023\" \u2192 start_date=\"2020-01-01\", end_date=\"2023-12-31\"\n     * \"recent\" or \"latest\" \u2192 start_date=\"2020-01-01\"\n     * Time indicators: from, since, after, before, until, between, onwards, recent, latest\n   - KEYWORD QUERIES: If no location mentioned, use search_datasets\n   - Extract key search terms from the user's query\n   - Use keywords parameter for the main search terms\n   - Use dataset_type parameter if user specifies type (image, table, etc.)\n   - Use provider parameter if user specifies provider (NASA, USGS, etc.)\n   - Default max_results is 10, but can be adjusted\n\nCRITICAL RULES:\n1. NEVER ask the user to provide bbox coordinates. If they mention a location name, pass it directly to search_by_region(location=\"name\")\n2. ALWAYS add start_date or end_date when user mentions ANY time period (from, since, onwards, recent, etc.)\n3. Convert years to YYYY-MM-DD format: 2022 \u2192 \"2022-01-01\"\n\n4. Examples:\n   - \"Find landcover datasets covering California\" \u2192 search_by_region(location=\"California\", keywords=\"landcover\")\n   - \"Show elevation data for San Francisco\" \u2192 search_by_region(location=\"San Francisco\", keywords=\"elevation\")\n   - \"Find datasets in bbox [-122, 37, -121, 38]\" \u2192 search_by_region(bbox=[-122, 37, -121, 38])\n   - \"Find landcover datasets from NASA\" \u2192 search_datasets(keywords=\"landcover\", provider=\"NASA\")\n   - \"Show me elevation data\" \u2192 search_datasets(keywords=\"elevation\")\n   - \"What types of datasets are available?\" \u2192 list_dataset_types()\n   - \"Find image collections about forests\" \u2192 search_datasets(keywords=\"forest\", dataset_type=\"image_collection\")\n   - \"Find landcover data from 2020 onwards\" \u2192 search_datasets(keywords=\"landcover\", start_date=\"2020-01-01\")\n   - \"Show California datasets between 2015 and 2020\" \u2192 search_by_region(location=\"California\", start_date=\"2015-01-01\", end_date=\"2020-12-31\")\n   - \"Find recent elevation data\" \u2192 search_datasets(keywords=\"elevation\", start_date=\"2020-01-01\")\n\n5. Return results clearly:\n   - Summarize the number of results found\n   - List the top results with their EXACT IDs and titles FROM THE TOOL RESPONSE\n   - Mention key information like provider, type, geographic coverage, and date range if available\n   - For spatial searches, mention the search region\n\nERROR HANDLING:\n- If no results found: Suggest trying different keywords, broader region, or removing filters\n- If location not found: Suggest alternative spellings or try a broader region\n- If tool error: Explain the error and suggest alternatives\n\nCRITICAL RULES - MUST FOLLOW:\n1. NEVER make up or hallucinate dataset IDs, titles, or any other information\n2. ONLY report datasets that appear in the actual tool response\n3. Copy dataset IDs and titles EXACTLY as they appear in the tool response\n4. If a field is null/None in the tool response, say \"N/A\" or omit it - DO NOT guess\n5. DO NOT use your training data knowledge about Earth Engine datasets\n6. DO NOT fill in missing information from your knowledge\n7. If unsure, say \"Information not available in results\"\n\nExample of CORRECT behavior:\nTool returns: {\"id\": \"AAFC/ACI\", \"title\": \"Canada AAFC Annual Crop Inventory\"}\nYour response: \"Found dataset: AAFC/ACI - Canada AAFC Annual Crop Inventory\"\n\nExample of INCORRECT behavior (DO NOT DO THIS):\nTool returns: {\"id\": \"AAFC/ACI\", \"title\": \"Canada AAFC Annual Crop Inventory\"}\nYour response: \"Found dataset: USGS/NED - USGS Elevation Data\"  \u2190 WRONG! This ID wasn't in the tool response!\"\"\"\n\n        super().__init__(\n            name=\"Catalog Search Agent\",\n            model=model,\n            tools=[\n                self.catalog_tools.search_datasets,\n                self.catalog_tools.search_by_region,\n                self.catalog_tools.get_dataset_info,\n                self.catalog_tools.geocode_location,\n                self.catalog_tools.list_dataset_types,\n                self.catalog_tools.list_providers,\n                self.catalog_tools.get_catalog_stats,\n            ],\n            system_prompt=system_prompt,\n            callback_handler=None,\n        )\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.CatalogAgent.ask","title":"<code>ask(prompt)</code>","text":"<p>Send a single-turn prompt to the agent.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to send to the agent.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The agent's response as a string.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def ask(self, prompt: str) -&gt; str:\n    \"\"\"Send a single-turn prompt to the agent.\n\n    Args:\n        prompt: The text prompt to send to the agent.\n\n    Returns:\n        The agent's response as a string.\n    \"\"\"\n    # Use strands' built-in __call__ method which now supports multiple calls\n    result = self(prompt)\n    return getattr(result, \"final_text\", str(result))\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.CatalogAgent.search_datasets","title":"<code>search_datasets(keywords=None, dataset_type=None, provider=None, max_results=10)</code>","text":"<p>Search for datasets and return structured results.</p> <p>This method directly uses the CatalogTools without LLM inference for faster searches.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>Optional[str]</code> <p>Keywords to search for.</p> <code>None</code> <code>dataset_type</code> <code>Optional[str]</code> <p>Filter by dataset type.</p> <code>None</code> <code>provider</code> <code>Optional[str]</code> <p>Filter by provider.</p> <code>None</code> <code>max_results</code> <code>int</code> <p>Maximum number of results to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of dataset dictionaries.</p> Example <p>agent = CatalogAgent(catalog_url=\"...\") datasets = agent.search_datasets(keywords=\"landcover\", provider=\"NASA\") for ds in datasets: ...     print(ds['id'], ds['title'])</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def search_datasets(\n    self,\n    keywords: Optional[str] = None,\n    dataset_type: Optional[str] = None,\n    provider: Optional[str] = None,\n    max_results: int = 10,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Search for datasets and return structured results.\n\n    This method directly uses the CatalogTools without LLM inference for faster searches.\n\n    Args:\n        keywords: Keywords to search for.\n        dataset_type: Filter by dataset type.\n        provider: Filter by provider.\n        max_results: Maximum number of results to return.\n\n    Returns:\n        List of dataset dictionaries.\n\n    Example:\n        &gt;&gt;&gt; agent = CatalogAgent(catalog_url=\"...\")\n        &gt;&gt;&gt; datasets = agent.search_datasets(keywords=\"landcover\", provider=\"NASA\")\n        &gt;&gt;&gt; for ds in datasets:\n        ...     print(ds['id'], ds['title'])\n    \"\"\"\n    result_json = self.catalog_tools.search_datasets(\n        keywords=keywords,\n        dataset_type=dataset_type,\n        provider=provider,\n        max_results=max_results,\n    )\n\n    result = json.loads(result_json)\n\n    if \"error\" in result:\n        print(f\"Search error: {result['error']}\")\n        return []\n\n    return result.get(\"datasets\", [])\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.GeoAgent","title":"<code>GeoAgent</code>","text":"<p>               Bases: <code>Agent</code></p> <p>Geospatial AI agent with interactive mapping capabilities.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>class GeoAgent(Agent):\n    \"\"\"Geospatial AI agent with interactive mapping capabilities.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model: str = \"llama3.1\",\n        map_instance: Optional[leafmap.Map] = None,\n        system_prompt: str = \"default\",\n        model_args: dict = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the GeoAgent.\n\n        Args:\n            model: Model identifier (default: \"llama3.1\").\n            map_instance: Optional existing map instance.\n            model_args: Additional keyword arguments for the model.\n            **kwargs: Additional keyword arguments for the model.\n        \"\"\"\n        self.session: MapSession = MapSession(map_instance)\n        self.tools: MapTools = MapTools(self.session)\n\n        if model_args is None:\n            model_args = {}\n\n        # --- save a model factory we can call each turn ---\n        if isinstance(model, str) and (\":\" in model or model.startswith(\"llama\")):\n            # treat ANY \"llama...\" model id as Ollama\n            self._model_factory = lambda m=model: create_ollama_model(\n                host=\"http://localhost:11434\", model_id=m, **model_args\n            )\n        elif isinstance(model, OllamaModel):\n            # Extract configuration from existing OllamaModel and create new instances\n            model_id = model.config[\"model_id\"]\n            host = model.host\n            client_args = model.client_args\n            self._model_factory: Callable[[], OllamaModel] = (\n                lambda: create_ollama_model(\n                    host=host, model_id=model_id, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, OpenAIModel):\n            # Extract configuration from existing OpenAIModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], OpenAIModel] = (\n                lambda mid=model_id, client_args=client_args: create_openai_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, AnthropicModel):\n            # Extract configuration from existing AnthropicModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], AnthropicModel] = (\n                lambda mid=model_id, client_args=client_args: create_anthropic_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, str):\n            # Only Bedrock IDs here, not LLaMA\n            self._model_factory = lambda m=model: create_bedrock_model(\n                model_id=m, **model_args\n            )\n        else:\n            raise ValueError(f\"Invalid model: {model}\")\n\n        # build initial model (first turn)\n        model = self._model_factory()\n\n        if system_prompt == \"default\":\n            system_prompt = \"\"\"\n            You are a map control agent. Call tools with MINIMAL parameters only.\n\n            CRITICAL: Treat all kwargs parameters as optional parameters.\n            CRITICAL: NEVER include optional parameters unless user explicitly asks for them.\n\n            TOOL CALL RULES:\n            - zoom_to(zoom=N) - ONLY zoom parameter, OMIT options completely\n            - add_cog_layer(url='X') - NEVER include bands, nodata, opacity, etc.\n            - fly_to(longitude=N, latitude=N) - NEVER include zoom parameter\n            - add_basemap(name='X') - NEVER include any other parameters\n            - add_marker(lng_lat=[lon,lat]) - NEVER include popup or options\n\n            - remove_layer(name='X') - call get_layer_names() to get the layer name closest to\n            the name of the layer you want to remove before calling this tool\n\n            - add_overture_3d_buildings(kwargs={}) - kwargs parameter required by tool validation\n            FORBIDDEN: Optional parameters, string representations like '{}' or '[1,2,3]'\n            REQUIRED: Minimal tool calls with only what's absolutely necessary\n            \"\"\"\n\n        super().__init__(\n            name=\"Leafmap Visualization Agent\",\n            model=model,\n            tools=[\n                # Core navigation tools\n                self.tools.fly_to,\n                self.tools.create_map,\n                self.tools.zoom_to,\n                self.tools.jump_to,\n                # Essential layer tools\n                self.tools.add_basemap,\n                self.tools.add_vector,\n                self.tools.add_raster,\n                self.tools.add_cog_layer,\n                self.tools.remove_layer,\n                self.tools.get_layer_names,\n                self.tools.set_terrain,\n                self.tools.remove_terrain,\n                self.tools.add_overture_3d_buildings,\n                self.tools.set_paint_property,\n                self.tools.set_layout_property,\n                self.tools.set_color,\n                self.tools.set_opacity,\n                self.tools.set_visibility,\n                # self.tools.save_map,\n                # Basic interaction tools\n                self.tools.add_marker,\n                self.tools.set_pitch,\n            ],\n            system_prompt=system_prompt,\n            callback_handler=None,\n        )\n\n    def ask(self, prompt: str) -&gt; str:\n        \"\"\"Send a single-turn prompt to the agent.\n\n        Args:\n            prompt: The text prompt to send to the agent.\n\n        Returns:\n            The agent's response as a string.\n        \"\"\"\n        # Use strands' built-in __call__ method which now supports multiple calls\n        result = self(prompt)\n        return getattr(result, \"final_text\", str(result))\n\n    def show_ui(self, *, height: int = 700) -&gt; None:\n        \"\"\"Display an interactive UI with map and chat interface.\n\n        Args:\n            height: Height of the UI in pixels (default: 700).\n        \"\"\"\n\n        m = self.tools.session.m\n        if not hasattr(m, \"container\") or m.container is None:\n            m.create_container()\n\n        map_panel = widgets.VBox(\n            [\n                widgets.HTML(\"&lt;h3 style='margin:0 0 8px 0'&gt;Map&lt;/h3&gt;\"),\n                m.floating_sidebar_widget,\n            ],\n            layout=widgets.Layout(\n                flex=\"2 1 0%\",\n                min_width=\"520px\",\n                border=\"1px solid #ddd\",\n                padding=\"8px\",\n                height=f\"{height}px\",\n                overflow=\"hidden\",\n            ),\n        )\n\n        # ----- chat widgets -----\n        session_id = str(uuid.uuid4())[:8]\n        title = widgets.HTML(\n            f\"&lt;h3 style='margin:0'&gt;Chatbot&lt;/h3&gt;\"\n            f\"&lt;p style='margin:4px 0 8px;color:#666'&gt;Session: {session_id}&lt;/p&gt;\"\n        )\n        log = widgets.HTML(\n            value=\"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\",\n            layout=widgets.Layout(\n                border=\"1px solid #ddd\",\n                padding=\"8px\",\n                height=\"520px\",\n                overflow_y=\"auto\",\n            ),\n        )\n        inp = widgets.Textarea(\n            placeholder=\"Ask to add/remove/list layers, set basemap, save the map, etc.\",\n            layout=widgets.Layout(width=\"99%\", height=\"90px\"),\n        )\n        btn_send = widgets.Button(\n            description=\"Send\",\n            button_style=\"primary\",\n            icon=\"paper-plane\",\n            layout=widgets.Layout(width=\"120px\"),\n        )\n        btn_stop = widgets.Button(\n            description=\"Stop\", icon=\"stop\", layout=widgets.Layout(width=\"120px\")\n        )\n        btn_clear = widgets.Button(\n            description=\"Clear\", icon=\"trash\", layout=widgets.Layout(width=\"120px\")\n        )\n        status = widgets.HTML(\n            \"&lt;link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'&gt;\"\n            \"&lt;span style='color:#666'&gt;Ready.&lt;/span&gt;\"\n        )\n\n        examples = widgets.Dropdown(\n            options=[\n                (\"\u2014 Examples \u2014\", \"\"),\n                (\"Fly to\", \"Fly to Chicago\"),\n                (\"Add basemap\", \"Add basemap OpenTopoMap\"),\n                (\n                    \"Add COG layer\",\n                    \"Add COG layer https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\",\n                ),\n                (\n                    \"Add GeoJSON\",\n                    \"Add GeoJSON layer: https://github.com/opengeos/datasets/releases/download/us/us_states.geojson\",\n                ),\n                (\"Remove layer\", \"Remove layer OpenTopoMap\"),\n                (\"Save map\", \"Save the map as demo.html and return the path\"),\n            ],\n            value=\"\",\n            layout=widgets.Layout(width=\"auto\"),\n        )\n\n        # --- state kept on self so it persists ---\n        self._ui = SimpleNamespace(\n            messages=[],\n            map_panel=map_panel,\n            title=title,\n            log=log,\n            inp=inp,\n            btn_send=btn_send,\n            btn_stop=btn_stop,\n            btn_clear=btn_clear,\n            status=status,\n            examples=examples,\n        )\n        self._pending = {\"fut\": None}\n\n        def _esc(s: str) -&gt; str:\n            \"\"\"Escape HTML characters in a string.\n\n            Args:\n                s: Input string to escape.\n\n            Returns:\n                HTML-escaped string.\n            \"\"\"\n            return (\n                s.replace(\"&amp;\", \"&amp;amp;\")\n                .replace(\"&lt;\", \"&amp;lt;\")\n                .replace(\"&gt;\", \"&amp;gt;\")\n                .replace(\"\\n\", \"&lt;br/&gt;\")\n            )\n\n        def _append(role: str, msg: str) -&gt; None:\n            \"\"\"Append a message to the chat log.\n\n            Args:\n                role: Role of the message sender (\"user\" or \"assistant\").\n                msg: Message content.\n            \"\"\"\n            self._ui.messages.append((role, msg))\n            parts = []\n            for r, mm in self._ui.messages:\n                if r == \"user\":\n                    parts.append(\n                        f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#eef;'&gt;&lt;b&gt;You&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                    )\n                else:\n                    parts.append(\n                        f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#f7f7f7;'&gt;&lt;b&gt;Agent&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                    )\n            self._ui.log.value = (\n                \"&lt;div style='height:100%; overflow-y:auto;'&gt;\"\n                + (\n                    \"\".join(parts)\n                    if parts\n                    else \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n                )\n                + \"&lt;/div&gt;\"\n            )\n\n        def _lock(lock: bool) -&gt; None:\n            \"\"\"Lock or unlock UI controls.\n\n            Args:\n                lock: True to lock controls, False to unlock.\n            \"\"\"\n            self._ui.btn_send.disabled = lock\n            self._ui.btn_stop.disabled = not lock\n            self._ui.btn_clear.disabled = lock\n            self._ui.inp.disabled = lock\n            self._ui.examples.disabled = lock\n\n        def _on_send(_: Any = None) -&gt; None:\n            \"\"\"Handle send button click or Enter key press.\"\"\"\n            text = self._ui.inp.value.strip()\n            if not text:\n                return\n            _append(\"user\", text)\n            _lock(True)\n            self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Running\u2026&lt;/span&gt;\"\n            try:\n                # Create a callback handler that updates the status widget\n                callback_handler = UICallbackHandler(status_widget=self._ui.status)\n\n                # Temporarily set callback_handler for this call\n                old_callback = self.callback_handler\n                self.callback_handler = callback_handler\n\n                out = self.ask(text)  # fresh Agent/model per call, with callback\n                _append(\"assistant\", out)\n                self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Done.&lt;/span&gt;\"\n            except Exception as e:\n                _append(\"assistant\", f\"[error] {type(e).__name__}: {e}\")\n                self._ui.status.value = (\n                    \"&lt;span style='color:#c00'&gt;Finished with an issue.&lt;/span&gt;\"\n                )\n            finally:\n                # Restore old callback handler\n                self.callback_handler = old_callback\n                self._ui.inp.value = \"\"\n                _lock(False)\n\n        def _on_stop(_: Any = None) -&gt; None:\n            \"\"\"Handle stop button click.\"\"\"\n            fut = self._pending.get(\"fut\")\n            if fut and not fut.done():\n                self._pending[\"fut\"] = None\n                self._ui.status.value = \"&lt;span style='color:#c00'&gt;Stop requested. If it finishes, result will be ignored.&lt;/span&gt;\"\n                _lock(False)\n\n        def _on_clear(_: Any = None) -&gt; None:\n            \"\"\"Handle clear button click.\"\"\"\n            self._ui.messages.clear()\n            self._ui.log.value = \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n            self._ui.status.value = \"&lt;span style='color:#666'&gt;Cleared.&lt;/span&gt;\"\n\n        def _on_example_change(change: dict[str, Any]) -&gt; None:\n            \"\"\"Handle example dropdown selection change.\n\n            Args:\n                change: Change event dictionary from the dropdown widget.\n            \"\"\"\n            if change[\"name\"] == \"value\" and change[\"new\"]:\n                self._ui.inp.value = change[\"new\"]\n                self._ui.examples.value = \"\"\n                self._ui.inp.send({\"method\": \"focus\"})\n\n        # keep handler refs\n        self._handlers = SimpleNamespace(\n            on_send=_on_send,\n            on_stop=_on_stop,\n            on_clear=_on_clear,\n            on_example_change=_on_example_change,\n        )\n\n        # wire events\n        self._ui.btn_send.on_click(self._handlers.on_send)\n        self._ui.btn_stop.on_click(self._handlers.on_stop)\n        self._ui.btn_clear.on_click(self._handlers.on_clear)\n        self._ui.examples.observe(self._handlers.on_example_change, names=\"value\")\n\n        # Ctrl+Enter on textarea (keyup only; do not block defaults)\n        self._keyev = Event(\n            source=self._ui.inp, watched_events=[\"keyup\"], prevent_default_action=False\n        )\n\n        def _on_key(e: dict[str, Any]) -&gt; None:\n            \"\"\"Handle keyboard events on the input textarea.\n\n            Args:\n                e: Keyboard event dictionary.\n            \"\"\"\n            if (\n                e.get(\"type\") == \"keyup\"\n                and e.get(\"key\") == \"Enter\"\n                and e.get(\"ctrlKey\")\n            ):\n                if self._ui.inp.value.endswith(\"\\n\"):\n                    self._ui.inp.value = self._ui.inp.value[:-1]\n                self._handlers.on_send()\n\n        # store callback too\n        self._on_key_cb: Callable[[dict[str, Any]], None] = _on_key\n        self._keyev.on_dom_event(self._on_key_cb)\n\n        buttons = widgets.HBox(\n            [\n                self._ui.btn_send,\n                self._ui.btn_stop,\n                self._ui.btn_clear,\n                widgets.Box(\n                    [self._ui.examples], layout=widgets.Layout(margin=\"0 0 0 auto\")\n                ),\n            ]\n        )\n        right = widgets.VBox(\n            [\n                self._ui.title if hasattr(self._ui, \"title\") else title,\n                self._ui.log,\n                self._ui.inp,\n                buttons,\n                self._ui.status,\n            ],\n            layout=widgets.Layout(flex=\"1 1 0%\", min_width=\"360px\"),\n        )\n        root = widgets.HBox(\n            [map_panel, right], layout=widgets.Layout(width=\"100%\", gap=\"8px\")\n        )\n        display(root)\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.GeoAgent.__init__","title":"<code>__init__(*, model='llama3.1', map_instance=None, system_prompt='default', model_args=None, **kwargs)</code>","text":"<p>Initialize the GeoAgent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (default: \"llama3.1\").</p> <code>'llama3.1'</code> <code>map_instance</code> <code>Optional[Map]</code> <p>Optional existing map instance.</p> <code>None</code> <code>model_args</code> <code>dict</code> <p>Additional keyword arguments for the model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the model.</p> <code>{}</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def __init__(\n    self,\n    *,\n    model: str = \"llama3.1\",\n    map_instance: Optional[leafmap.Map] = None,\n    system_prompt: str = \"default\",\n    model_args: dict = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the GeoAgent.\n\n    Args:\n        model: Model identifier (default: \"llama3.1\").\n        map_instance: Optional existing map instance.\n        model_args: Additional keyword arguments for the model.\n        **kwargs: Additional keyword arguments for the model.\n    \"\"\"\n    self.session: MapSession = MapSession(map_instance)\n    self.tools: MapTools = MapTools(self.session)\n\n    if model_args is None:\n        model_args = {}\n\n    # --- save a model factory we can call each turn ---\n    if isinstance(model, str) and (\":\" in model or model.startswith(\"llama\")):\n        # treat ANY \"llama...\" model id as Ollama\n        self._model_factory = lambda m=model: create_ollama_model(\n            host=\"http://localhost:11434\", model_id=m, **model_args\n        )\n    elif isinstance(model, OllamaModel):\n        # Extract configuration from existing OllamaModel and create new instances\n        model_id = model.config[\"model_id\"]\n        host = model.host\n        client_args = model.client_args\n        self._model_factory: Callable[[], OllamaModel] = (\n            lambda: create_ollama_model(\n                host=host, model_id=model_id, client_args=client_args, **model_args\n            )\n        )\n    elif isinstance(model, OpenAIModel):\n        # Extract configuration from existing OpenAIModel and create new instances\n        model_id = model.config[\"model_id\"]\n        client_args = model.client_args.copy()\n        self._model_factory: Callable[[], OpenAIModel] = (\n            lambda mid=model_id, client_args=client_args: create_openai_model(\n                model_id=mid, client_args=client_args, **model_args\n            )\n        )\n    elif isinstance(model, AnthropicModel):\n        # Extract configuration from existing AnthropicModel and create new instances\n        model_id = model.config[\"model_id\"]\n        client_args = model.client_args.copy()\n        self._model_factory: Callable[[], AnthropicModel] = (\n            lambda mid=model_id, client_args=client_args: create_anthropic_model(\n                model_id=mid, client_args=client_args, **model_args\n            )\n        )\n    elif isinstance(model, str):\n        # Only Bedrock IDs here, not LLaMA\n        self._model_factory = lambda m=model: create_bedrock_model(\n            model_id=m, **model_args\n        )\n    else:\n        raise ValueError(f\"Invalid model: {model}\")\n\n    # build initial model (first turn)\n    model = self._model_factory()\n\n    if system_prompt == \"default\":\n        system_prompt = \"\"\"\n        You are a map control agent. Call tools with MINIMAL parameters only.\n\n        CRITICAL: Treat all kwargs parameters as optional parameters.\n        CRITICAL: NEVER include optional parameters unless user explicitly asks for them.\n\n        TOOL CALL RULES:\n        - zoom_to(zoom=N) - ONLY zoom parameter, OMIT options completely\n        - add_cog_layer(url='X') - NEVER include bands, nodata, opacity, etc.\n        - fly_to(longitude=N, latitude=N) - NEVER include zoom parameter\n        - add_basemap(name='X') - NEVER include any other parameters\n        - add_marker(lng_lat=[lon,lat]) - NEVER include popup or options\n\n        - remove_layer(name='X') - call get_layer_names() to get the layer name closest to\n        the name of the layer you want to remove before calling this tool\n\n        - add_overture_3d_buildings(kwargs={}) - kwargs parameter required by tool validation\n        FORBIDDEN: Optional parameters, string representations like '{}' or '[1,2,3]'\n        REQUIRED: Minimal tool calls with only what's absolutely necessary\n        \"\"\"\n\n    super().__init__(\n        name=\"Leafmap Visualization Agent\",\n        model=model,\n        tools=[\n            # Core navigation tools\n            self.tools.fly_to,\n            self.tools.create_map,\n            self.tools.zoom_to,\n            self.tools.jump_to,\n            # Essential layer tools\n            self.tools.add_basemap,\n            self.tools.add_vector,\n            self.tools.add_raster,\n            self.tools.add_cog_layer,\n            self.tools.remove_layer,\n            self.tools.get_layer_names,\n            self.tools.set_terrain,\n            self.tools.remove_terrain,\n            self.tools.add_overture_3d_buildings,\n            self.tools.set_paint_property,\n            self.tools.set_layout_property,\n            self.tools.set_color,\n            self.tools.set_opacity,\n            self.tools.set_visibility,\n            # self.tools.save_map,\n            # Basic interaction tools\n            self.tools.add_marker,\n            self.tools.set_pitch,\n        ],\n        system_prompt=system_prompt,\n        callback_handler=None,\n    )\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.GeoAgent.ask","title":"<code>ask(prompt)</code>","text":"<p>Send a single-turn prompt to the agent.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to send to the agent.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The agent's response as a string.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def ask(self, prompt: str) -&gt; str:\n    \"\"\"Send a single-turn prompt to the agent.\n\n    Args:\n        prompt: The text prompt to send to the agent.\n\n    Returns:\n        The agent's response as a string.\n    \"\"\"\n    # Use strands' built-in __call__ method which now supports multiple calls\n    result = self(prompt)\n    return getattr(result, \"final_text\", str(result))\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.GeoAgent.show_ui","title":"<code>show_ui(*, height=700)</code>","text":"<p>Display an interactive UI with map and chat interface.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the UI in pixels (default: 700).</p> <code>700</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def show_ui(self, *, height: int = 700) -&gt; None:\n    \"\"\"Display an interactive UI with map and chat interface.\n\n    Args:\n        height: Height of the UI in pixels (default: 700).\n    \"\"\"\n\n    m = self.tools.session.m\n    if not hasattr(m, \"container\") or m.container is None:\n        m.create_container()\n\n    map_panel = widgets.VBox(\n        [\n            widgets.HTML(\"&lt;h3 style='margin:0 0 8px 0'&gt;Map&lt;/h3&gt;\"),\n            m.floating_sidebar_widget,\n        ],\n        layout=widgets.Layout(\n            flex=\"2 1 0%\",\n            min_width=\"520px\",\n            border=\"1px solid #ddd\",\n            padding=\"8px\",\n            height=f\"{height}px\",\n            overflow=\"hidden\",\n        ),\n    )\n\n    # ----- chat widgets -----\n    session_id = str(uuid.uuid4())[:8]\n    title = widgets.HTML(\n        f\"&lt;h3 style='margin:0'&gt;Chatbot&lt;/h3&gt;\"\n        f\"&lt;p style='margin:4px 0 8px;color:#666'&gt;Session: {session_id}&lt;/p&gt;\"\n    )\n    log = widgets.HTML(\n        value=\"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\",\n        layout=widgets.Layout(\n            border=\"1px solid #ddd\",\n            padding=\"8px\",\n            height=\"520px\",\n            overflow_y=\"auto\",\n        ),\n    )\n    inp = widgets.Textarea(\n        placeholder=\"Ask to add/remove/list layers, set basemap, save the map, etc.\",\n        layout=widgets.Layout(width=\"99%\", height=\"90px\"),\n    )\n    btn_send = widgets.Button(\n        description=\"Send\",\n        button_style=\"primary\",\n        icon=\"paper-plane\",\n        layout=widgets.Layout(width=\"120px\"),\n    )\n    btn_stop = widgets.Button(\n        description=\"Stop\", icon=\"stop\", layout=widgets.Layout(width=\"120px\")\n    )\n    btn_clear = widgets.Button(\n        description=\"Clear\", icon=\"trash\", layout=widgets.Layout(width=\"120px\")\n    )\n    status = widgets.HTML(\n        \"&lt;link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'&gt;\"\n        \"&lt;span style='color:#666'&gt;Ready.&lt;/span&gt;\"\n    )\n\n    examples = widgets.Dropdown(\n        options=[\n            (\"\u2014 Examples \u2014\", \"\"),\n            (\"Fly to\", \"Fly to Chicago\"),\n            (\"Add basemap\", \"Add basemap OpenTopoMap\"),\n            (\n                \"Add COG layer\",\n                \"Add COG layer https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\",\n            ),\n            (\n                \"Add GeoJSON\",\n                \"Add GeoJSON layer: https://github.com/opengeos/datasets/releases/download/us/us_states.geojson\",\n            ),\n            (\"Remove layer\", \"Remove layer OpenTopoMap\"),\n            (\"Save map\", \"Save the map as demo.html and return the path\"),\n        ],\n        value=\"\",\n        layout=widgets.Layout(width=\"auto\"),\n    )\n\n    # --- state kept on self so it persists ---\n    self._ui = SimpleNamespace(\n        messages=[],\n        map_panel=map_panel,\n        title=title,\n        log=log,\n        inp=inp,\n        btn_send=btn_send,\n        btn_stop=btn_stop,\n        btn_clear=btn_clear,\n        status=status,\n        examples=examples,\n    )\n    self._pending = {\"fut\": None}\n\n    def _esc(s: str) -&gt; str:\n        \"\"\"Escape HTML characters in a string.\n\n        Args:\n            s: Input string to escape.\n\n        Returns:\n            HTML-escaped string.\n        \"\"\"\n        return (\n            s.replace(\"&amp;\", \"&amp;amp;\")\n            .replace(\"&lt;\", \"&amp;lt;\")\n            .replace(\"&gt;\", \"&amp;gt;\")\n            .replace(\"\\n\", \"&lt;br/&gt;\")\n        )\n\n    def _append(role: str, msg: str) -&gt; None:\n        \"\"\"Append a message to the chat log.\n\n        Args:\n            role: Role of the message sender (\"user\" or \"assistant\").\n            msg: Message content.\n        \"\"\"\n        self._ui.messages.append((role, msg))\n        parts = []\n        for r, mm in self._ui.messages:\n            if r == \"user\":\n                parts.append(\n                    f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#eef;'&gt;&lt;b&gt;You&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                )\n            else:\n                parts.append(\n                    f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#f7f7f7;'&gt;&lt;b&gt;Agent&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                )\n        self._ui.log.value = (\n            \"&lt;div style='height:100%; overflow-y:auto;'&gt;\"\n            + (\n                \"\".join(parts)\n                if parts\n                else \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n            )\n            + \"&lt;/div&gt;\"\n        )\n\n    def _lock(lock: bool) -&gt; None:\n        \"\"\"Lock or unlock UI controls.\n\n        Args:\n            lock: True to lock controls, False to unlock.\n        \"\"\"\n        self._ui.btn_send.disabled = lock\n        self._ui.btn_stop.disabled = not lock\n        self._ui.btn_clear.disabled = lock\n        self._ui.inp.disabled = lock\n        self._ui.examples.disabled = lock\n\n    def _on_send(_: Any = None) -&gt; None:\n        \"\"\"Handle send button click or Enter key press.\"\"\"\n        text = self._ui.inp.value.strip()\n        if not text:\n            return\n        _append(\"user\", text)\n        _lock(True)\n        self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Running\u2026&lt;/span&gt;\"\n        try:\n            # Create a callback handler that updates the status widget\n            callback_handler = UICallbackHandler(status_widget=self._ui.status)\n\n            # Temporarily set callback_handler for this call\n            old_callback = self.callback_handler\n            self.callback_handler = callback_handler\n\n            out = self.ask(text)  # fresh Agent/model per call, with callback\n            _append(\"assistant\", out)\n            self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Done.&lt;/span&gt;\"\n        except Exception as e:\n            _append(\"assistant\", f\"[error] {type(e).__name__}: {e}\")\n            self._ui.status.value = (\n                \"&lt;span style='color:#c00'&gt;Finished with an issue.&lt;/span&gt;\"\n            )\n        finally:\n            # Restore old callback handler\n            self.callback_handler = old_callback\n            self._ui.inp.value = \"\"\n            _lock(False)\n\n    def _on_stop(_: Any = None) -&gt; None:\n        \"\"\"Handle stop button click.\"\"\"\n        fut = self._pending.get(\"fut\")\n        if fut and not fut.done():\n            self._pending[\"fut\"] = None\n            self._ui.status.value = \"&lt;span style='color:#c00'&gt;Stop requested. If it finishes, result will be ignored.&lt;/span&gt;\"\n            _lock(False)\n\n    def _on_clear(_: Any = None) -&gt; None:\n        \"\"\"Handle clear button click.\"\"\"\n        self._ui.messages.clear()\n        self._ui.log.value = \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n        self._ui.status.value = \"&lt;span style='color:#666'&gt;Cleared.&lt;/span&gt;\"\n\n    def _on_example_change(change: dict[str, Any]) -&gt; None:\n        \"\"\"Handle example dropdown selection change.\n\n        Args:\n            change: Change event dictionary from the dropdown widget.\n        \"\"\"\n        if change[\"name\"] == \"value\" and change[\"new\"]:\n            self._ui.inp.value = change[\"new\"]\n            self._ui.examples.value = \"\"\n            self._ui.inp.send({\"method\": \"focus\"})\n\n    # keep handler refs\n    self._handlers = SimpleNamespace(\n        on_send=_on_send,\n        on_stop=_on_stop,\n        on_clear=_on_clear,\n        on_example_change=_on_example_change,\n    )\n\n    # wire events\n    self._ui.btn_send.on_click(self._handlers.on_send)\n    self._ui.btn_stop.on_click(self._handlers.on_stop)\n    self._ui.btn_clear.on_click(self._handlers.on_clear)\n    self._ui.examples.observe(self._handlers.on_example_change, names=\"value\")\n\n    # Ctrl+Enter on textarea (keyup only; do not block defaults)\n    self._keyev = Event(\n        source=self._ui.inp, watched_events=[\"keyup\"], prevent_default_action=False\n    )\n\n    def _on_key(e: dict[str, Any]) -&gt; None:\n        \"\"\"Handle keyboard events on the input textarea.\n\n        Args:\n            e: Keyboard event dictionary.\n        \"\"\"\n        if (\n            e.get(\"type\") == \"keyup\"\n            and e.get(\"key\") == \"Enter\"\n            and e.get(\"ctrlKey\")\n        ):\n            if self._ui.inp.value.endswith(\"\\n\"):\n                self._ui.inp.value = self._ui.inp.value[:-1]\n            self._handlers.on_send()\n\n    # store callback too\n    self._on_key_cb: Callable[[dict[str, Any]], None] = _on_key\n    self._keyev.on_dom_event(self._on_key_cb)\n\n    buttons = widgets.HBox(\n        [\n            self._ui.btn_send,\n            self._ui.btn_stop,\n            self._ui.btn_clear,\n            widgets.Box(\n                [self._ui.examples], layout=widgets.Layout(margin=\"0 0 0 auto\")\n            ),\n        ]\n    )\n    right = widgets.VBox(\n        [\n            self._ui.title if hasattr(self._ui, \"title\") else title,\n            self._ui.log,\n            self._ui.inp,\n            buttons,\n            self._ui.status,\n        ],\n        layout=widgets.Layout(flex=\"1 1 0%\", min_width=\"360px\"),\n    )\n    root = widgets.HBox(\n        [map_panel, right], layout=widgets.Layout(width=\"100%\", gap=\"8px\")\n    )\n    display(root)\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>OllamaModel</code></p> <p>Fixed OllamaModel that ensures proper model_id handling.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>class OllamaModel(_OllamaModel):\n    \"\"\"Fixed OllamaModel that ensures proper model_id handling.\"\"\"\n\n    async def stream(self, *args, **kwargs):\n        \"\"\"Override stream to ensure model_id is passed as string.\"\"\"\n        # Patch the ollama client to handle model object correctly\n        import ollama\n\n        # Save original method if not already saved\n        if not hasattr(ollama.AsyncClient, \"_original_chat\"):\n            ollama.AsyncClient._original_chat = ollama.AsyncClient.chat\n\n            async def fixed_chat(self, **chat_kwargs):\n                # If model is an OllamaModel object, extract the model_id\n                if \"model\" in chat_kwargs and hasattr(chat_kwargs[\"model\"], \"config\"):\n                    chat_kwargs[\"model\"] = chat_kwargs[\"model\"].config[\"model_id\"]\n                return await ollama.AsyncClient._original_chat(self, **chat_kwargs)\n\n            ollama.AsyncClient.chat = fixed_chat\n\n        # Call the original stream method\n        async for chunk in super().stream(*args, **kwargs):\n            yield chunk\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.OllamaModel.stream","title":"<code>stream(*args, **kwargs)</code>  <code>async</code>","text":"<p>Override stream to ensure model_id is passed as string.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>async def stream(self, *args, **kwargs):\n    \"\"\"Override stream to ensure model_id is passed as string.\"\"\"\n    # Patch the ollama client to handle model object correctly\n    import ollama\n\n    # Save original method if not already saved\n    if not hasattr(ollama.AsyncClient, \"_original_chat\"):\n        ollama.AsyncClient._original_chat = ollama.AsyncClient.chat\n\n        async def fixed_chat(self, **chat_kwargs):\n            # If model is an OllamaModel object, extract the model_id\n            if \"model\" in chat_kwargs and hasattr(chat_kwargs[\"model\"], \"config\"):\n                chat_kwargs[\"model\"] = chat_kwargs[\"model\"].config[\"model_id\"]\n            return await ollama.AsyncClient._original_chat(self, **chat_kwargs)\n\n        ollama.AsyncClient.chat = fixed_chat\n\n    # Call the original stream method\n    async for chunk in super().stream(*args, **kwargs):\n        yield chunk\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.STACAgent","title":"<code>STACAgent</code>","text":"<p>               Bases: <code>Agent</code></p> <p>AI agent for searching and interacting with STAC catalogs.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>class STACAgent(Agent):\n    \"\"\"AI agent for searching and interacting with STAC catalogs.\"\"\"\n\n    def __init__(\n        self,\n        *,\n        model: str = \"llama3.1\",\n        system_prompt: str = \"default\",\n        endpoint: str = \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        model_args: dict = None,\n        map_instance: Optional[leafmap.Map] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the STAC Agent.\n\n        Args:\n            model: Model identifier (default: \"llama3.1\").\n            system_prompt: System prompt for the agent (default: \"default\").\n            endpoint: STAC API endpoint URL. Defaults to Microsoft Planetary Computer.\n            model_args: Additional keyword arguments for the model.\n            map_instance: Optional leafmap.Map instance for visualization. If None, creates a new one.\n            **kwargs: Additional keyword arguments for the Agent.\n        \"\"\"\n        self.stac_tools: STACTools = STACTools(endpoint=endpoint)\n        self.map_instance = map_instance if map_instance is not None else leafmap.Map()\n\n        if model_args is None:\n            model_args = {}\n\n        # --- save a model factory we can call each turn ---\n        if isinstance(model, str) and (\":\" in model or model.startswith(\"llama\")):\n            # treat ANY \"llama...\" model id as Ollama\n            self._model_factory = lambda m=model: create_ollama_model(\n                host=\"http://localhost:11434\", model_id=m, **model_args\n            )\n\n        elif isinstance(model, OllamaModel):\n            # Extract configuration from existing OllamaModel and create new instances\n            model_id = model.config[\"model_id\"]\n            host = model.host\n            client_args = model.client_args\n            self._model_factory: Callable[[], OllamaModel] = (\n                lambda: create_ollama_model(\n                    host=host, model_id=model_id, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, OpenAIModel):\n            # Extract configuration from existing OpenAIModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], OpenAIModel] = (\n                lambda mid=model_id, client_args=client_args: create_openai_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, AnthropicModel):\n            # Extract configuration from existing AnthropicModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], AnthropicModel] = (\n                lambda mid=model_id, client_args=client_args: create_anthropic_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, str):\n            # Only Bedrock IDs here, not LLaMA\n            self._model_factory = lambda m=model: create_bedrock_model(\n                model_id=m, **model_args\n            )\n        else:\n            raise ValueError(f\"Invalid model: {model}\")\n\n        # build initial model (first turn)\n        model = self._model_factory()\n\n        if system_prompt == \"default\":\n            system_prompt = \"\"\"You are a STAC search agent. Follow these steps EXACTLY:\n\n1. Determine collection ID based on data type:\n   - \"sentinel-2-l2a\" for Sentinel-2 or optical satellite imagery\n   - \"landsat-c2-l2\" for Landsat\n   - \"naip\" for NAIP or aerial imagery (USA only)\n   - \"sentinel-1-grd\" for Sentinel-1 or SAR/radar\n   - \"cop-dem-glo-30\" for DEM, elevation, or terrain data\n   - \"aster-l1t\" for ASTER\n   For other data (e.g., MODIS, land cover): call list_collections(filter_keyword=\"&lt;keyword&gt;\")\n\n2. If location mentioned:\n   - Call geocode_location(\"&lt;name&gt;\") FIRST\n   - WAIT for the response\n   - Extract the \"bbox\" array from the JSON response\n   - This bbox is [west, south, east, north] format\n\n3. Call search_items():\n   - collection: REQUIRED\n   - bbox: Use the EXACT bbox array from geocode_location (REQUIRED if location mentioned)\n   - time_range: \"YYYY-MM-DD/YYYY-MM-DD\" format if dates mentioned\n   - query: Use for cloud cover filtering (see examples)\n   - max_items: 1\n\nCloud cover filtering:\n   - \"&lt;10% cloud\": query={\"eo:cloud_cover\": {\"lt\": 10}}\n   - \"&lt;20% cloud\": query={\"eo:cloud_cover\": {\"lt\": 20}}\n   - \"&lt;5% cloud\": query={\"eo:cloud_cover\": {\"lt\": 5}}\n\nExamples:\n1. \"Find Landsat over Paris from June to July 2023\"\n   geocode_location(\"Paris\") \u2192 {\"bbox\": [2.224, 48.815, 2.469, 48.902], ...}\n   search_items(collection=\"landsat-c2-l2\", bbox=[2.224, 48.815, 2.469, 48.902], time_range=\"2023-06-01/2023-07-31\")\n\n2. \"Find Landsat with &lt;10% cloud cover over Paris\"\n   geocode_location(\"Paris\") \u2192 {\"bbox\": [2.224, 48.815, 2.469, 48.902], ...}\n   search_items(collection=\"landsat-c2-l2\", bbox=[2.224, 48.815, 2.469, 48.902], query={\"eo:cloud_cover\": {\"lt\": 10}})\n\n4. Return first item as JSON:\n{\"id\": \"...\", \"collection\": \"...\", \"datetime\": \"...\", \"bbox\": [...], \"assets\": [...]}\n\nERROR HANDLING:\n- If no items found: {\"error\": \"No items found\"}\n- If tool result too large: {\"error\": \"Result too large, try narrower search\"}\n- If tool error: {\"error\": \"Search failed: &lt;error message&gt;\"}\n\nCRITICAL: Return ONLY JSON. NO explanatory text, NO made-up data.\"\"\"\n\n        super().__init__(\n            name=\"STAC Search Agent\",\n            model=model,\n            tools=[\n                self.stac_tools.list_collections,\n                self.stac_tools.search_items,\n                self.stac_tools.get_item_info,\n                self.stac_tools.geocode_location,\n                self.stac_tools.get_common_collections,\n            ],\n            system_prompt=system_prompt,\n            callback_handler=None,\n        )\n\n    def _extract_search_items_payload(self, result: Any) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Return the parsed payload from the search_items tool, if available.\"\"\"\n        # Try to get tool_results from the result object\n        tool_results = getattr(result, \"tool_results\", None)\n        if tool_results:\n            for tool_result in tool_results:\n                if getattr(tool_result, \"tool_name\", \"\") != \"search_items\":\n                    continue\n\n                payload = getattr(tool_result, \"result\", None)\n                if payload is None:\n                    continue\n\n                if isinstance(payload, str):\n                    try:\n                        payload = json.loads(payload)\n                    except json.JSONDecodeError:\n                        continue\n\n                if isinstance(payload, dict):\n                    return payload\n\n        # Alternative: check messages for tool results\n        messages = getattr(self, \"messages\", [])\n        for msg in reversed(messages):  # Check recent messages first\n            # Handle dict-style messages\n            if isinstance(msg, dict):\n                role = msg.get(\"role\")\n                content = msg.get(\"content\", [])\n\n                # Look for tool results in user messages (strands pattern)\n                if role == \"user\" and isinstance(content, list):\n                    for item in content:\n                        if isinstance(item, dict) and \"toolResult\" in item:\n                            tool_result = item[\"toolResult\"]\n                            # Check if this is a search_items result\n                            # We need to look at the preceding assistant message to identify the tool\n                            if tool_result.get(\"status\") == \"success\":\n                                result_content = tool_result.get(\"content\", [])\n                                if isinstance(result_content, list) and result_content:\n                                    text_content = result_content[0].get(\"text\", \"\")\n                                    try:\n                                        payload = json.loads(text_content)\n                                        # Return ANY search_items payload, even if items is empty\n                                        # This is identified by having \"query\" and \"collection\" fields\n                                        if (\n                                            \"query\" in payload\n                                            and \"collection\" in payload\n                                            and \"items\" in payload\n                                        ):\n                                            return payload\n                                    except json.JSONDecodeError:\n                                        continue\n\n        return None\n\n    def ask(self, prompt: str) -&gt; str:\n        \"\"\"Send a single-turn prompt to the agent.\n\n        Args:\n            prompt: The text prompt to send to the agent.\n\n        Returns:\n            The agent's response as a string (JSON format for search queries).\n        \"\"\"\n        # Use strands' built-in __call__ method which now supports multiple calls\n        result = self(prompt)\n\n        search_payload = self._extract_search_items_payload(result)\n        if search_payload is not None:\n            if \"error\" in search_payload:\n                return json.dumps({\"error\": search_payload[\"error\"]}, indent=2)\n\n            items = search_payload.get(\"items\") or []\n            if items:\n                return json.dumps(items[0], indent=2)\n\n            return json.dumps({\"error\": \"No items found\"}, indent=2)\n\n        return getattr(result, \"final_text\", str(result))\n\n    def search_and_get_first_item(self, prompt: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Search for imagery and return the first item as a structured dict.\n\n        This method sends a search query to the agent, extracts the search results\n        directly from the tool calls, and returns the first item as a STACItemInfo-compatible\n        dictionary.\n\n        Note: This method uses LLM inference which adds ~5-10 seconds overhead.\n        For faster searches, use STACTools directly:\n            &gt;&gt;&gt; from geoai.agents import STACTools\n            &gt;&gt;&gt; tools = STACTools()\n            &gt;&gt;&gt; result = tools.search_items(\n            ...     collection=\"sentinel-2-l2a\",\n            ...     bbox=[-122.5, 37.7, -122.4, 37.8],\n            ...     time_range=\"2024-08-01/2024-08-31\"\n            ... )\n\n        Args:\n            prompt: Natural language search query (e.g., \"Find Sentinel-2 imagery\n                    over San Francisco in September 2024\").\n\n        Returns:\n            Dictionary containing STACItemInfo fields (id, collection, datetime,\n            bbox, assets, properties), or None if no results found.\n\n        Example:\n            &gt;&gt;&gt; agent = STACAgent()\n            &gt;&gt;&gt; item = agent.search_and_get_first_item(\n            ...     \"Find Sentinel-2 imagery over Paris in summer 2023\"\n            ... )\n            &gt;&gt;&gt; print(item['id'])\n            &gt;&gt;&gt; print(item['assets'][0]['key'])    # or 'title'\n        \"\"\"\n        # Use strands' built-in __call__ method which now supports multiple calls\n        result = self(prompt)\n\n        search_payload = self._extract_search_items_payload(result)\n        if search_payload is not None:\n            if \"error\" in search_payload:\n                print(f\"Search error: {search_payload['error']}\")\n                return None\n\n            items = search_payload.get(\"items\") or []\n            if items:\n                return items[0]\n\n            print(\"No items found in search results\")\n            return None\n\n        # Fallback: try to parse the final text response\n        response = getattr(result, \"final_text\", str(result))\n\n        try:\n            item_data = json.loads(response)\n\n            if \"error\" in item_data:\n                print(f\"Search error: {item_data['error']}\")\n                return None\n\n            if not all(k in item_data for k in [\"id\", \"collection\"]):\n                print(\"Response missing required fields (id, collection)\")\n                return None\n\n            return item_data\n\n        except json.JSONDecodeError:\n            print(\"Could not extract item data from agent response\")\n            return None\n\n    def _visualize_stac_item(self, item: Dict[str, Any]) -&gt; None:\n        \"\"\"Visualize a STAC item on the map.\n\n        Args:\n            item: STAC item dictionary with id, collection, assets, etc.\n        \"\"\"\n        if not item or \"id\" not in item or \"collection\" not in item:\n            return\n\n        # Get the collection and item ID\n        collection = item.get(\"collection\")\n        item_id = item.get(\"id\")\n\n        kwargs = {}\n\n        # Determine which assets to display based on collection\n        assets = None\n        if collection == \"sentinel-2-l2a\":\n            assets = [\"B04\", \"B03\", \"B02\"]  # True color RGB\n        elif collection == \"landsat-c2-l2\":\n            assets = [\"red\", \"green\", \"blue\"]  # Landsat RGB\n        elif collection == \"naip\":\n            assets = [\"image\"]  # NAIP 4-band imagery\n        elif \"sentinel-1\" in collection:\n            assets = [\"vv\"]  # Sentinel-1 VV polarization\n        elif collection == \"cop-dem-glo-30\":\n            assets = [\"data\"]\n            kwargs[\"colormap_name\"] = \"terrain\"\n        elif collection == \"aster-l1t\":\n            assets = [\"VNIR\"]  # ASTER L1T imagery\n        elif collection == \"3dep-lidar-hag\":\n            assets = [\"data\"]\n            kwargs[\"colormap_name\"] = \"terrain\"\n        else:\n            # Try to find common asset names\n            if \"assets\" in item:\n                asset_keys = [\n                    a.get(\"key\") for a in item[\"assets\"] if isinstance(a, dict)\n                ]\n                # Look for visual or RGB assets\n                for possible in [\"visual\", \"image\", \"data\"]:\n                    if possible in asset_keys:\n                        assets = [possible]\n                        break\n                # If still no assets, use first few assets\n                if not assets and asset_keys:\n                    assets = asset_keys[:1]\n\n        if not assets:\n            return None\n        try:\n            # Add the STAC layer to the map\n            layer_name = f\"{collection[:20]}_{item_id[:15]}\"\n            self.map_instance.add_stac_layer(\n                collection=collection,\n                item=item_id,\n                assets=assets,\n                name=layer_name,\n                before_id=self.map_instance.first_symbol_layer_id,\n                **kwargs,\n            )\n            return assets  # Return the assets that were visualized\n        except Exception as e:\n            print(f\"Could not visualize item on map: {e}\")\n            return None\n\n    def show_ui(self, *, height: int = 700) -&gt; None:\n        \"\"\"Display an interactive UI with map and chat interface for STAC searches.\n\n        Args:\n            height: Height of the UI in pixels (default: 700).\n        \"\"\"\n        m = self.map_instance\n        if not hasattr(m, \"container\") or m.container is None:\n            m.create_container()\n\n        map_panel = widgets.VBox(\n            [\n                widgets.HTML(\"&lt;h3 style='margin:0 0 8px 0'&gt;Map&lt;/h3&gt;\"),\n                m.floating_sidebar_widget,\n            ],\n            layout=widgets.Layout(\n                flex=\"2 1 0%\",\n                min_width=\"520px\",\n                border=\"1px solid #ddd\",\n                padding=\"8px\",\n                height=f\"{height}px\",\n                overflow=\"hidden\",\n            ),\n        )\n\n        # ----- chat widgets -----\n        session_id = str(uuid.uuid4())[:8]\n        title = widgets.HTML(\n            f\"&lt;h3 style='margin:0'&gt;STAC Search Agent&lt;/h3&gt;\"\n            f\"&lt;p style='margin:4px 0 8px;color:#666'&gt;Session: {session_id}&lt;/p&gt;\"\n        )\n        log = widgets.HTML(\n            value=\"&lt;div style='color:#777'&gt;No messages yet. Try searching for satellite imagery!&lt;/div&gt;\",\n            layout=widgets.Layout(\n                border=\"1px solid #ddd\",\n                padding=\"8px\",\n                height=\"520px\",\n                overflow_y=\"auto\",\n            ),\n        )\n        inp = widgets.Textarea(\n            placeholder=\"Search for satellite/aerial imagery (e.g., 'Find Sentinel-2 imagery over Paris in summer 2024')\",\n            layout=widgets.Layout(width=\"99%\", height=\"90px\"),\n        )\n        btn_send = widgets.Button(\n            description=\"Search\",\n            button_style=\"primary\",\n            icon=\"search\",\n            layout=widgets.Layout(width=\"120px\"),\n        )\n        btn_stop = widgets.Button(\n            description=\"Stop\", icon=\"stop\", layout=widgets.Layout(width=\"120px\")\n        )\n        btn_clear = widgets.Button(\n            description=\"Clear\", icon=\"trash\", layout=widgets.Layout(width=\"120px\")\n        )\n        status = widgets.HTML(\n            \"&lt;link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'&gt;\"\n            \"&lt;span style='color:#666'&gt;Ready to search.&lt;/span&gt;\"\n        )\n\n        examples = widgets.Dropdown(\n            options=[\n                (\"\u2014 Example Searches \u2014\", \"\"),\n                (\n                    \"Sentinel-2 over Las Vegas\",\n                    \"Find Sentinel-2 imagery over Las Vegas in August 2025\",\n                ),\n                (\n                    \"Landsat over Paris\",\n                    \"Find Landsat imagery over Paris from June to July 2025\",\n                ),\n                (\n                    \"Landsat with &lt;10% cloud cover\",\n                    \"Find Landsat imagery over Paris with &lt;10% cloud cover in June 2025\",\n                ),\n                (\"NAIP for NYC\", \"Show me NAIP aerial imagery for New York City\"),\n                (\"DEM for Seattle\", \"Show me DEM data for Seattle\"),\n                (\n                    \"3DEP Lidar HAG\",\n                    \"Show me data over Austin from 3dep-lidar-hag collection\",\n                ),\n                (\"ASTER for Tokyo\", \"Show me ASTER imagery for Tokyo\"),\n            ],\n            value=\"\",\n            layout=widgets.Layout(width=\"auto\"),\n        )\n\n        # --- state kept on self so it persists ---\n        self._ui = SimpleNamespace(\n            messages=[],\n            map_panel=map_panel,\n            title=title,\n            log=log,\n            inp=inp,\n            btn_send=btn_send,\n            btn_stop=btn_stop,\n            btn_clear=btn_clear,\n            status=status,\n            examples=examples,\n        )\n        self._pending = {\"fut\": None}\n\n        def _esc(s: str) -&gt; str:\n            \"\"\"Escape HTML characters in a string.\"\"\"\n            return (\n                s.replace(\"&amp;\", \"&amp;amp;\")\n                .replace(\"&lt;\", \"&amp;lt;\")\n                .replace(\"&gt;\", \"&amp;gt;\")\n                .replace(\"\\n\", \"&lt;br/&gt;\")\n            )\n\n        def _append(role: str, msg: str) -&gt; None:\n            \"\"\"Append a message to the chat log.\"\"\"\n            self._ui.messages.append((role, msg))\n            parts = []\n            for r, mm in self._ui.messages:\n                if r == \"user\":\n                    parts.append(\n                        f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#eef;'&gt;&lt;b&gt;You&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                    )\n                else:\n                    parts.append(\n                        f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#f7f7f7;'&gt;&lt;b&gt;Agent&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                    )\n            self._ui.log.value = (\n                \"&lt;div style='height:100%; overflow-y:auto;'&gt;\"\n                + (\n                    \"\".join(parts)\n                    if parts\n                    else \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n                )\n                + \"&lt;/div&gt;\"\n            )\n\n        def _lock(lock: bool) -&gt; None:\n            \"\"\"Lock or unlock UI controls.\"\"\"\n            self._ui.btn_send.disabled = lock\n            self._ui.btn_stop.disabled = not lock\n            self._ui.btn_clear.disabled = lock\n            self._ui.inp.disabled = lock\n            self._ui.examples.disabled = lock\n\n        def _on_send(_: Any = None) -&gt; None:\n            \"\"\"Handle send button click or Enter key press.\"\"\"\n            text = self._ui.inp.value.strip()\n            if not text:\n                return\n            _append(\"user\", text)\n            _lock(True)\n            self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Searching\u2026&lt;/span&gt;\"\n            try:\n                # Create a callback handler that updates the status widget\n                callback_handler = UICallbackHandler(status_widget=self._ui.status)\n\n                # Temporarily set callback_handler for this call\n                old_callback = self.callback_handler\n                self.callback_handler = callback_handler\n\n                # Get the structured search result directly (will show progress via callback)\n                item_data = self.search_and_get_first_item(text)\n\n                if item_data is not None:\n                    # Update status for visualization step\n                    self._ui.status.value = (\n                        \"&lt;span style='color:#0a7'&gt;Adding to map...&lt;/span&gt;\"\n                    )\n\n                    # Visualize on map\n                    visualized_assets = self._visualize_stac_item(item_data)\n\n                    # Format response for display\n                    formatted_response = (\n                        f\"Found item: {item_data['id']}\\n\"\n                        f\"Collection: {item_data['collection']}\\n\"\n                        f\"Date: {item_data.get('datetime', 'N/A')}\\n\"\n                    )\n\n                    if visualized_assets:\n                        assets_str = \", \".join(visualized_assets)\n                        formatted_response += f\"\u2713 Added to map (assets: {assets_str})\"\n                    else:\n                        formatted_response += \"\u2713 Added to map\"\n\n                    _append(\"assistant\", formatted_response)\n                else:\n                    _append(\n                        \"assistant\",\n                        \"No items found. Try adjusting your search query or date range.\",\n                    )\n\n                self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Done.&lt;/span&gt;\"\n            except Exception as e:\n                _append(\"assistant\", f\"[error] {type(e).__name__}: {e}\")\n                self._ui.status.value = (\n                    \"&lt;span style='color:#c00'&gt;Finished with an issue.&lt;/span&gt;\"\n                )\n            finally:\n                # Restore old callback handler\n                self.callback_handler = old_callback\n                self._ui.inp.value = \"\"\n                _lock(False)\n\n        def _on_stop(_: Any = None) -&gt; None:\n            \"\"\"Handle stop button click.\"\"\"\n            fut = self._pending.get(\"fut\")\n            if fut and not fut.done():\n                self._pending[\"fut\"] = None\n                self._ui.status.value = (\n                    \"&lt;span style='color:#c00'&gt;Stop requested.&lt;/span&gt;\"\n                )\n                _lock(False)\n\n        def _on_clear(_: Any = None) -&gt; None:\n            \"\"\"Handle clear button click.\"\"\"\n            self._ui.messages.clear()\n            self._ui.log.value = \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n            self._ui.status.value = \"&lt;span style='color:#666'&gt;Cleared.&lt;/span&gt;\"\n\n        def _on_example_change(change: dict[str, Any]) -&gt; None:\n            \"\"\"Handle example dropdown selection change.\"\"\"\n            if change[\"name\"] == \"value\" and change[\"new\"]:\n                self._ui.inp.value = change[\"new\"]\n                self._ui.examples.value = \"\"\n                self._ui.inp.send({\"method\": \"focus\"})\n\n        # keep handler refs\n        self._handlers = SimpleNamespace(\n            on_send=_on_send,\n            on_stop=_on_stop,\n            on_clear=_on_clear,\n            on_example_change=_on_example_change,\n        )\n\n        # wire events\n        self._ui.btn_send.on_click(self._handlers.on_send)\n        self._ui.btn_stop.on_click(self._handlers.on_stop)\n        self._ui.btn_clear.on_click(self._handlers.on_clear)\n        self._ui.examples.observe(self._handlers.on_example_change, names=\"value\")\n\n        # Ctrl+Enter on textarea\n        self._keyev = Event(\n            source=self._ui.inp, watched_events=[\"keyup\"], prevent_default_action=False\n        )\n\n        def _on_key(e: dict[str, Any]) -&gt; None:\n            \"\"\"Handle keyboard events on the input textarea.\"\"\"\n            if (\n                e.get(\"type\") == \"keyup\"\n                and e.get(\"key\") == \"Enter\"\n                and e.get(\"ctrlKey\")\n            ):\n                if self._ui.inp.value.endswith(\"\\n\"):\n                    self._ui.inp.value = self._ui.inp.value[:-1]\n                self._handlers.on_send()\n\n        # store callback too\n        self._on_key_cb: Callable[[dict[str, Any]], None] = _on_key\n        self._keyev.on_dom_event(self._on_key_cb)\n\n        buttons = widgets.HBox(\n            [\n                self._ui.btn_send,\n                self._ui.btn_stop,\n                self._ui.btn_clear,\n                widgets.Box(\n                    [self._ui.examples], layout=widgets.Layout(margin=\"0 0 0 auto\")\n                ),\n            ]\n        )\n        right = widgets.VBox(\n            [\n                self._ui.title,\n                self._ui.log,\n                self._ui.inp,\n                buttons,\n                self._ui.status,\n            ],\n            layout=widgets.Layout(flex=\"1 1 0%\", min_width=\"360px\"),\n        )\n        root = widgets.HBox(\n            [map_panel, right], layout=widgets.Layout(width=\"100%\", gap=\"8px\")\n        )\n        display(root)\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.STACAgent.__init__","title":"<code>__init__(*, model='llama3.1', system_prompt='default', endpoint='https://planetarycomputer.microsoft.com/api/stac/v1', model_args=None, map_instance=None, **kwargs)</code>","text":"<p>Initialize the STAC Agent.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier (default: \"llama3.1\").</p> <code>'llama3.1'</code> <code>system_prompt</code> <code>str</code> <p>System prompt for the agent (default: \"default\").</p> <code>'default'</code> <code>endpoint</code> <code>str</code> <p>STAC API endpoint URL. Defaults to Microsoft Planetary Computer.</p> <code>'https://planetarycomputer.microsoft.com/api/stac/v1'</code> <code>model_args</code> <code>dict</code> <p>Additional keyword arguments for the model.</p> <code>None</code> <code>map_instance</code> <code>Optional[Map]</code> <p>Optional leafmap.Map instance for visualization. If None, creates a new one.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the Agent.</p> <code>{}</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>    def __init__(\n        self,\n        *,\n        model: str = \"llama3.1\",\n        system_prompt: str = \"default\",\n        endpoint: str = \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n        model_args: dict = None,\n        map_instance: Optional[leafmap.Map] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the STAC Agent.\n\n        Args:\n            model: Model identifier (default: \"llama3.1\").\n            system_prompt: System prompt for the agent (default: \"default\").\n            endpoint: STAC API endpoint URL. Defaults to Microsoft Planetary Computer.\n            model_args: Additional keyword arguments for the model.\n            map_instance: Optional leafmap.Map instance for visualization. If None, creates a new one.\n            **kwargs: Additional keyword arguments for the Agent.\n        \"\"\"\n        self.stac_tools: STACTools = STACTools(endpoint=endpoint)\n        self.map_instance = map_instance if map_instance is not None else leafmap.Map()\n\n        if model_args is None:\n            model_args = {}\n\n        # --- save a model factory we can call each turn ---\n        if isinstance(model, str) and (\":\" in model or model.startswith(\"llama\")):\n            # treat ANY \"llama...\" model id as Ollama\n            self._model_factory = lambda m=model: create_ollama_model(\n                host=\"http://localhost:11434\", model_id=m, **model_args\n            )\n\n        elif isinstance(model, OllamaModel):\n            # Extract configuration from existing OllamaModel and create new instances\n            model_id = model.config[\"model_id\"]\n            host = model.host\n            client_args = model.client_args\n            self._model_factory: Callable[[], OllamaModel] = (\n                lambda: create_ollama_model(\n                    host=host, model_id=model_id, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, OpenAIModel):\n            # Extract configuration from existing OpenAIModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], OpenAIModel] = (\n                lambda mid=model_id, client_args=client_args: create_openai_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, AnthropicModel):\n            # Extract configuration from existing AnthropicModel and create new instances\n            model_id = model.config[\"model_id\"]\n            client_args = model.client_args.copy()\n            self._model_factory: Callable[[], AnthropicModel] = (\n                lambda mid=model_id, client_args=client_args: create_anthropic_model(\n                    model_id=mid, client_args=client_args, **model_args\n                )\n            )\n        elif isinstance(model, str):\n            # Only Bedrock IDs here, not LLaMA\n            self._model_factory = lambda m=model: create_bedrock_model(\n                model_id=m, **model_args\n            )\n        else:\n            raise ValueError(f\"Invalid model: {model}\")\n\n        # build initial model (first turn)\n        model = self._model_factory()\n\n        if system_prompt == \"default\":\n            system_prompt = \"\"\"You are a STAC search agent. Follow these steps EXACTLY:\n\n1. Determine collection ID based on data type:\n   - \"sentinel-2-l2a\" for Sentinel-2 or optical satellite imagery\n   - \"landsat-c2-l2\" for Landsat\n   - \"naip\" for NAIP or aerial imagery (USA only)\n   - \"sentinel-1-grd\" for Sentinel-1 or SAR/radar\n   - \"cop-dem-glo-30\" for DEM, elevation, or terrain data\n   - \"aster-l1t\" for ASTER\n   For other data (e.g., MODIS, land cover): call list_collections(filter_keyword=\"&lt;keyword&gt;\")\n\n2. If location mentioned:\n   - Call geocode_location(\"&lt;name&gt;\") FIRST\n   - WAIT for the response\n   - Extract the \"bbox\" array from the JSON response\n   - This bbox is [west, south, east, north] format\n\n3. Call search_items():\n   - collection: REQUIRED\n   - bbox: Use the EXACT bbox array from geocode_location (REQUIRED if location mentioned)\n   - time_range: \"YYYY-MM-DD/YYYY-MM-DD\" format if dates mentioned\n   - query: Use for cloud cover filtering (see examples)\n   - max_items: 1\n\nCloud cover filtering:\n   - \"&lt;10% cloud\": query={\"eo:cloud_cover\": {\"lt\": 10}}\n   - \"&lt;20% cloud\": query={\"eo:cloud_cover\": {\"lt\": 20}}\n   - \"&lt;5% cloud\": query={\"eo:cloud_cover\": {\"lt\": 5}}\n\nExamples:\n1. \"Find Landsat over Paris from June to July 2023\"\n   geocode_location(\"Paris\") \u2192 {\"bbox\": [2.224, 48.815, 2.469, 48.902], ...}\n   search_items(collection=\"landsat-c2-l2\", bbox=[2.224, 48.815, 2.469, 48.902], time_range=\"2023-06-01/2023-07-31\")\n\n2. \"Find Landsat with &lt;10% cloud cover over Paris\"\n   geocode_location(\"Paris\") \u2192 {\"bbox\": [2.224, 48.815, 2.469, 48.902], ...}\n   search_items(collection=\"landsat-c2-l2\", bbox=[2.224, 48.815, 2.469, 48.902], query={\"eo:cloud_cover\": {\"lt\": 10}})\n\n4. Return first item as JSON:\n{\"id\": \"...\", \"collection\": \"...\", \"datetime\": \"...\", \"bbox\": [...], \"assets\": [...]}\n\nERROR HANDLING:\n- If no items found: {\"error\": \"No items found\"}\n- If tool result too large: {\"error\": \"Result too large, try narrower search\"}\n- If tool error: {\"error\": \"Search failed: &lt;error message&gt;\"}\n\nCRITICAL: Return ONLY JSON. NO explanatory text, NO made-up data.\"\"\"\n\n        super().__init__(\n            name=\"STAC Search Agent\",\n            model=model,\n            tools=[\n                self.stac_tools.list_collections,\n                self.stac_tools.search_items,\n                self.stac_tools.get_item_info,\n                self.stac_tools.geocode_location,\n                self.stac_tools.get_common_collections,\n            ],\n            system_prompt=system_prompt,\n            callback_handler=None,\n        )\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.STACAgent.ask","title":"<code>ask(prompt)</code>","text":"<p>Send a single-turn prompt to the agent.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The text prompt to send to the agent.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The agent's response as a string (JSON format for search queries).</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def ask(self, prompt: str) -&gt; str:\n    \"\"\"Send a single-turn prompt to the agent.\n\n    Args:\n        prompt: The text prompt to send to the agent.\n\n    Returns:\n        The agent's response as a string (JSON format for search queries).\n    \"\"\"\n    # Use strands' built-in __call__ method which now supports multiple calls\n    result = self(prompt)\n\n    search_payload = self._extract_search_items_payload(result)\n    if search_payload is not None:\n        if \"error\" in search_payload:\n            return json.dumps({\"error\": search_payload[\"error\"]}, indent=2)\n\n        items = search_payload.get(\"items\") or []\n        if items:\n            return json.dumps(items[0], indent=2)\n\n        return json.dumps({\"error\": \"No items found\"}, indent=2)\n\n    return getattr(result, \"final_text\", str(result))\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.STACAgent.search_and_get_first_item","title":"<code>search_and_get_first_item(prompt)</code>","text":"<p>Search for imagery and return the first item as a structured dict.</p> <p>This method sends a search query to the agent, extracts the search results directly from the tool calls, and returns the first item as a STACItemInfo-compatible dictionary.</p> <p>Note: This method uses LLM inference which adds ~5-10 seconds overhead. For faster searches, use STACTools directly:     &gt;&gt;&gt; from geoai.agents import STACTools     &gt;&gt;&gt; tools = STACTools()     &gt;&gt;&gt; result = tools.search_items(     ...     collection=\"sentinel-2-l2a\",     ...     bbox=[-122.5, 37.7, -122.4, 37.8],     ...     time_range=\"2024-08-01/2024-08-31\"     ... )</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Natural language search query (e.g., \"Find Sentinel-2 imagery     over San Francisco in September 2024\").</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing STACItemInfo fields (id, collection, datetime,</p> <code>Optional[Dict[str, Any]]</code> <p>bbox, assets, properties), or None if no results found.</p> Example <p>agent = STACAgent() item = agent.search_and_get_first_item( ...     \"Find Sentinel-2 imagery over Paris in summer 2023\" ... ) print(item['id']) print(item['assets'][0]['key'])    # or 'title'</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def search_and_get_first_item(self, prompt: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Search for imagery and return the first item as a structured dict.\n\n    This method sends a search query to the agent, extracts the search results\n    directly from the tool calls, and returns the first item as a STACItemInfo-compatible\n    dictionary.\n\n    Note: This method uses LLM inference which adds ~5-10 seconds overhead.\n    For faster searches, use STACTools directly:\n        &gt;&gt;&gt; from geoai.agents import STACTools\n        &gt;&gt;&gt; tools = STACTools()\n        &gt;&gt;&gt; result = tools.search_items(\n        ...     collection=\"sentinel-2-l2a\",\n        ...     bbox=[-122.5, 37.7, -122.4, 37.8],\n        ...     time_range=\"2024-08-01/2024-08-31\"\n        ... )\n\n    Args:\n        prompt: Natural language search query (e.g., \"Find Sentinel-2 imagery\n                over San Francisco in September 2024\").\n\n    Returns:\n        Dictionary containing STACItemInfo fields (id, collection, datetime,\n        bbox, assets, properties), or None if no results found.\n\n    Example:\n        &gt;&gt;&gt; agent = STACAgent()\n        &gt;&gt;&gt; item = agent.search_and_get_first_item(\n        ...     \"Find Sentinel-2 imagery over Paris in summer 2023\"\n        ... )\n        &gt;&gt;&gt; print(item['id'])\n        &gt;&gt;&gt; print(item['assets'][0]['key'])    # or 'title'\n    \"\"\"\n    # Use strands' built-in __call__ method which now supports multiple calls\n    result = self(prompt)\n\n    search_payload = self._extract_search_items_payload(result)\n    if search_payload is not None:\n        if \"error\" in search_payload:\n            print(f\"Search error: {search_payload['error']}\")\n            return None\n\n        items = search_payload.get(\"items\") or []\n        if items:\n            return items[0]\n\n        print(\"No items found in search results\")\n        return None\n\n    # Fallback: try to parse the final text response\n    response = getattr(result, \"final_text\", str(result))\n\n    try:\n        item_data = json.loads(response)\n\n        if \"error\" in item_data:\n            print(f\"Search error: {item_data['error']}\")\n            return None\n\n        if not all(k in item_data for k in [\"id\", \"collection\"]):\n            print(\"Response missing required fields (id, collection)\")\n            return None\n\n        return item_data\n\n    except json.JSONDecodeError:\n        print(\"Could not extract item data from agent response\")\n        return None\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.STACAgent.show_ui","title":"<code>show_ui(*, height=700)</code>","text":"<p>Display an interactive UI with map and chat interface for STAC searches.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the UI in pixels (default: 700).</p> <code>700</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def show_ui(self, *, height: int = 700) -&gt; None:\n    \"\"\"Display an interactive UI with map and chat interface for STAC searches.\n\n    Args:\n        height: Height of the UI in pixels (default: 700).\n    \"\"\"\n    m = self.map_instance\n    if not hasattr(m, \"container\") or m.container is None:\n        m.create_container()\n\n    map_panel = widgets.VBox(\n        [\n            widgets.HTML(\"&lt;h3 style='margin:0 0 8px 0'&gt;Map&lt;/h3&gt;\"),\n            m.floating_sidebar_widget,\n        ],\n        layout=widgets.Layout(\n            flex=\"2 1 0%\",\n            min_width=\"520px\",\n            border=\"1px solid #ddd\",\n            padding=\"8px\",\n            height=f\"{height}px\",\n            overflow=\"hidden\",\n        ),\n    )\n\n    # ----- chat widgets -----\n    session_id = str(uuid.uuid4())[:8]\n    title = widgets.HTML(\n        f\"&lt;h3 style='margin:0'&gt;STAC Search Agent&lt;/h3&gt;\"\n        f\"&lt;p style='margin:4px 0 8px;color:#666'&gt;Session: {session_id}&lt;/p&gt;\"\n    )\n    log = widgets.HTML(\n        value=\"&lt;div style='color:#777'&gt;No messages yet. Try searching for satellite imagery!&lt;/div&gt;\",\n        layout=widgets.Layout(\n            border=\"1px solid #ddd\",\n            padding=\"8px\",\n            height=\"520px\",\n            overflow_y=\"auto\",\n        ),\n    )\n    inp = widgets.Textarea(\n        placeholder=\"Search for satellite/aerial imagery (e.g., 'Find Sentinel-2 imagery over Paris in summer 2024')\",\n        layout=widgets.Layout(width=\"99%\", height=\"90px\"),\n    )\n    btn_send = widgets.Button(\n        description=\"Search\",\n        button_style=\"primary\",\n        icon=\"search\",\n        layout=widgets.Layout(width=\"120px\"),\n    )\n    btn_stop = widgets.Button(\n        description=\"Stop\", icon=\"stop\", layout=widgets.Layout(width=\"120px\")\n    )\n    btn_clear = widgets.Button(\n        description=\"Clear\", icon=\"trash\", layout=widgets.Layout(width=\"120px\")\n    )\n    status = widgets.HTML(\n        \"&lt;link rel='stylesheet' href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css'&gt;\"\n        \"&lt;span style='color:#666'&gt;Ready to search.&lt;/span&gt;\"\n    )\n\n    examples = widgets.Dropdown(\n        options=[\n            (\"\u2014 Example Searches \u2014\", \"\"),\n            (\n                \"Sentinel-2 over Las Vegas\",\n                \"Find Sentinel-2 imagery over Las Vegas in August 2025\",\n            ),\n            (\n                \"Landsat over Paris\",\n                \"Find Landsat imagery over Paris from June to July 2025\",\n            ),\n            (\n                \"Landsat with &lt;10% cloud cover\",\n                \"Find Landsat imagery over Paris with &lt;10% cloud cover in June 2025\",\n            ),\n            (\"NAIP for NYC\", \"Show me NAIP aerial imagery for New York City\"),\n            (\"DEM for Seattle\", \"Show me DEM data for Seattle\"),\n            (\n                \"3DEP Lidar HAG\",\n                \"Show me data over Austin from 3dep-lidar-hag collection\",\n            ),\n            (\"ASTER for Tokyo\", \"Show me ASTER imagery for Tokyo\"),\n        ],\n        value=\"\",\n        layout=widgets.Layout(width=\"auto\"),\n    )\n\n    # --- state kept on self so it persists ---\n    self._ui = SimpleNamespace(\n        messages=[],\n        map_panel=map_panel,\n        title=title,\n        log=log,\n        inp=inp,\n        btn_send=btn_send,\n        btn_stop=btn_stop,\n        btn_clear=btn_clear,\n        status=status,\n        examples=examples,\n    )\n    self._pending = {\"fut\": None}\n\n    def _esc(s: str) -&gt; str:\n        \"\"\"Escape HTML characters in a string.\"\"\"\n        return (\n            s.replace(\"&amp;\", \"&amp;amp;\")\n            .replace(\"&lt;\", \"&amp;lt;\")\n            .replace(\"&gt;\", \"&amp;gt;\")\n            .replace(\"\\n\", \"&lt;br/&gt;\")\n        )\n\n    def _append(role: str, msg: str) -&gt; None:\n        \"\"\"Append a message to the chat log.\"\"\"\n        self._ui.messages.append((role, msg))\n        parts = []\n        for r, mm in self._ui.messages:\n            if r == \"user\":\n                parts.append(\n                    f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#eef;'&gt;&lt;b&gt;You&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                )\n            else:\n                parts.append(\n                    f\"&lt;div style='margin:6px 0;padding:6px 8px;border-radius:8px;background:#f7f7f7;'&gt;&lt;b&gt;Agent&lt;/b&gt;: {_esc(mm)}&lt;/div&gt;\"\n                )\n        self._ui.log.value = (\n            \"&lt;div style='height:100%; overflow-y:auto;'&gt;\"\n            + (\n                \"\".join(parts)\n                if parts\n                else \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n            )\n            + \"&lt;/div&gt;\"\n        )\n\n    def _lock(lock: bool) -&gt; None:\n        \"\"\"Lock or unlock UI controls.\"\"\"\n        self._ui.btn_send.disabled = lock\n        self._ui.btn_stop.disabled = not lock\n        self._ui.btn_clear.disabled = lock\n        self._ui.inp.disabled = lock\n        self._ui.examples.disabled = lock\n\n    def _on_send(_: Any = None) -&gt; None:\n        \"\"\"Handle send button click or Enter key press.\"\"\"\n        text = self._ui.inp.value.strip()\n        if not text:\n            return\n        _append(\"user\", text)\n        _lock(True)\n        self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Searching\u2026&lt;/span&gt;\"\n        try:\n            # Create a callback handler that updates the status widget\n            callback_handler = UICallbackHandler(status_widget=self._ui.status)\n\n            # Temporarily set callback_handler for this call\n            old_callback = self.callback_handler\n            self.callback_handler = callback_handler\n\n            # Get the structured search result directly (will show progress via callback)\n            item_data = self.search_and_get_first_item(text)\n\n            if item_data is not None:\n                # Update status for visualization step\n                self._ui.status.value = (\n                    \"&lt;span style='color:#0a7'&gt;Adding to map...&lt;/span&gt;\"\n                )\n\n                # Visualize on map\n                visualized_assets = self._visualize_stac_item(item_data)\n\n                # Format response for display\n                formatted_response = (\n                    f\"Found item: {item_data['id']}\\n\"\n                    f\"Collection: {item_data['collection']}\\n\"\n                    f\"Date: {item_data.get('datetime', 'N/A')}\\n\"\n                )\n\n                if visualized_assets:\n                    assets_str = \", \".join(visualized_assets)\n                    formatted_response += f\"\u2713 Added to map (assets: {assets_str})\"\n                else:\n                    formatted_response += \"\u2713 Added to map\"\n\n                _append(\"assistant\", formatted_response)\n            else:\n                _append(\n                    \"assistant\",\n                    \"No items found. Try adjusting your search query or date range.\",\n                )\n\n            self._ui.status.value = \"&lt;span style='color:#0a7'&gt;Done.&lt;/span&gt;\"\n        except Exception as e:\n            _append(\"assistant\", f\"[error] {type(e).__name__}: {e}\")\n            self._ui.status.value = (\n                \"&lt;span style='color:#c00'&gt;Finished with an issue.&lt;/span&gt;\"\n            )\n        finally:\n            # Restore old callback handler\n            self.callback_handler = old_callback\n            self._ui.inp.value = \"\"\n            _lock(False)\n\n    def _on_stop(_: Any = None) -&gt; None:\n        \"\"\"Handle stop button click.\"\"\"\n        fut = self._pending.get(\"fut\")\n        if fut and not fut.done():\n            self._pending[\"fut\"] = None\n            self._ui.status.value = (\n                \"&lt;span style='color:#c00'&gt;Stop requested.&lt;/span&gt;\"\n            )\n            _lock(False)\n\n    def _on_clear(_: Any = None) -&gt; None:\n        \"\"\"Handle clear button click.\"\"\"\n        self._ui.messages.clear()\n        self._ui.log.value = \"&lt;div style='color:#777'&gt;No messages yet.&lt;/div&gt;\"\n        self._ui.status.value = \"&lt;span style='color:#666'&gt;Cleared.&lt;/span&gt;\"\n\n    def _on_example_change(change: dict[str, Any]) -&gt; None:\n        \"\"\"Handle example dropdown selection change.\"\"\"\n        if change[\"name\"] == \"value\" and change[\"new\"]:\n            self._ui.inp.value = change[\"new\"]\n            self._ui.examples.value = \"\"\n            self._ui.inp.send({\"method\": \"focus\"})\n\n    # keep handler refs\n    self._handlers = SimpleNamespace(\n        on_send=_on_send,\n        on_stop=_on_stop,\n        on_clear=_on_clear,\n        on_example_change=_on_example_change,\n    )\n\n    # wire events\n    self._ui.btn_send.on_click(self._handlers.on_send)\n    self._ui.btn_stop.on_click(self._handlers.on_stop)\n    self._ui.btn_clear.on_click(self._handlers.on_clear)\n    self._ui.examples.observe(self._handlers.on_example_change, names=\"value\")\n\n    # Ctrl+Enter on textarea\n    self._keyev = Event(\n        source=self._ui.inp, watched_events=[\"keyup\"], prevent_default_action=False\n    )\n\n    def _on_key(e: dict[str, Any]) -&gt; None:\n        \"\"\"Handle keyboard events on the input textarea.\"\"\"\n        if (\n            e.get(\"type\") == \"keyup\"\n            and e.get(\"key\") == \"Enter\"\n            and e.get(\"ctrlKey\")\n        ):\n            if self._ui.inp.value.endswith(\"\\n\"):\n                self._ui.inp.value = self._ui.inp.value[:-1]\n            self._handlers.on_send()\n\n    # store callback too\n    self._on_key_cb: Callable[[dict[str, Any]], None] = _on_key\n    self._keyev.on_dom_event(self._on_key_cb)\n\n    buttons = widgets.HBox(\n        [\n            self._ui.btn_send,\n            self._ui.btn_stop,\n            self._ui.btn_clear,\n            widgets.Box(\n                [self._ui.examples], layout=widgets.Layout(margin=\"0 0 0 auto\")\n            ),\n        ]\n    )\n    right = widgets.VBox(\n        [\n            self._ui.title,\n            self._ui.log,\n            self._ui.inp,\n            buttons,\n            self._ui.status,\n        ],\n        layout=widgets.Layout(flex=\"1 1 0%\", min_width=\"360px\"),\n    )\n    root = widgets.HBox(\n        [map_panel, right], layout=widgets.Layout(width=\"100%\", gap=\"8px\")\n    )\n    display(root)\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.UICallbackHandler","title":"<code>UICallbackHandler</code>","text":"<p>Callback handler that updates UI status widget with agent progress.</p> <p>This handler intercepts tool calls and progress events to provide real-time feedback without overwhelming the user.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>class UICallbackHandler:\n    \"\"\"Callback handler that updates UI status widget with agent progress.\n\n    This handler intercepts tool calls and progress events to provide\n    real-time feedback without overwhelming the user.\n    \"\"\"\n\n    def __init__(self, status_widget=None):\n        \"\"\"Initialize the callback handler.\n\n        Args:\n            status_widget: Optional ipywidgets.HTML widget to update with status.\n        \"\"\"\n        self.status_widget = status_widget\n        self.current_tool = None\n\n    def __call__(self, **kwargs):\n        \"\"\"Handle callback events from the agent.\n\n        Args:\n            **kwargs: Event data from the agent execution.\n        \"\"\"\n        # Track when tools are being called\n        if \"current_tool_use\" in kwargs and kwargs[\"current_tool_use\"].get(\"name\"):\n            tool_name = kwargs[\"current_tool_use\"][\"name\"]\n            self.current_tool = tool_name\n\n            # Update status widget if available\n            if self.status_widget is not None:\n                # Make tool names more user-friendly\n                friendly_name = tool_name.replace(\"_\", \" \").title()\n                self.status_widget.value = (\n                    f\"&lt;span style='color:#0a7'&gt;\"\n                    f\"&lt;i class='fas fa-spinner fa-spin' style='font-size:1.2em'&gt;&lt;/i&gt; \"\n                    f\"{friendly_name}...&lt;/span&gt;\"\n                )\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.UICallbackHandler.__call__","title":"<code>__call__(**kwargs)</code>","text":"<p>Handle callback events from the agent.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Event data from the agent execution.</p> <code>{}</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def __call__(self, **kwargs):\n    \"\"\"Handle callback events from the agent.\n\n    Args:\n        **kwargs: Event data from the agent execution.\n    \"\"\"\n    # Track when tools are being called\n    if \"current_tool_use\" in kwargs and kwargs[\"current_tool_use\"].get(\"name\"):\n        tool_name = kwargs[\"current_tool_use\"][\"name\"]\n        self.current_tool = tool_name\n\n        # Update status widget if available\n        if self.status_widget is not None:\n            # Make tool names more user-friendly\n            friendly_name = tool_name.replace(\"_\", \" \").title()\n            self.status_widget.value = (\n                f\"&lt;span style='color:#0a7'&gt;\"\n                f\"&lt;i class='fas fa-spinner fa-spin' style='font-size:1.2em'&gt;&lt;/i&gt; \"\n                f\"{friendly_name}...&lt;/span&gt;\"\n            )\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.UICallbackHandler.__init__","title":"<code>__init__(status_widget=None)</code>","text":"<p>Initialize the callback handler.</p> <p>Parameters:</p> Name Type Description Default <code>status_widget</code> <p>Optional ipywidgets.HTML widget to update with status.</p> <code>None</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def __init__(self, status_widget=None):\n    \"\"\"Initialize the callback handler.\n\n    Args:\n        status_widget: Optional ipywidgets.HTML widget to update with status.\n    \"\"\"\n    self.status_widget = status_widget\n    self.current_tool = None\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.create_anthropic_model","title":"<code>create_anthropic_model(model_id='claude-sonnet-4-20250514', api_key=None, client_args=None, **kwargs)</code>","text":"<p>Create an Anthropic model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Anthropic model ID. Defaults to \"claude-sonnet-4-20250514\". For a complete list of supported models, see https://docs.claude.com/en/docs/about-claude/models/overview.</p> <code>'claude-sonnet-4-20250514'</code> <code>api_key</code> <code>str</code> <p>Anthropic API key.</p> <code>None</code> <code>client_args</code> <code>dict</code> <p>Client arguments for the Anthropic model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the Anthropic model.</p> <code>{}</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def create_anthropic_model(\n    model_id: str = \"claude-sonnet-4-20250514\",\n    api_key: str = None,\n    client_args: dict = None,\n    **kwargs: Any,\n) -&gt; AnthropicModel:\n    \"\"\"Create an Anthropic model.\n\n    Args:\n        model_id: Anthropic model ID. Defaults to \"claude-sonnet-4-20250514\".\n            For a complete list of supported models,\n            see https://docs.claude.com/en/docs/about-claude/models/overview.\n        api_key: Anthropic API key.\n        client_args: Client arguments for the Anthropic model.\n        **kwargs: Additional keyword arguments for the Anthropic model.\n    \"\"\"\n\n    if api_key is None:\n        try:\n            api_key = os.getenv(\"ANTHROPIC_API_KEY\", None)\n            if api_key is None:\n                raise ValueError(\"ANTHROPIC_API_KEY is not set\")\n        except Exception:\n            raise ValueError(\"ANTHROPIC_API_KEY is not set\")\n\n    if client_args is None:\n        client_args = kwargs.get(\"client_args\", {})\n    if \"api_key\" not in client_args and api_key is not None:\n        client_args[\"api_key\"] = api_key\n\n    return AnthropicModel(client_args=client_args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.create_bedrock_model","title":"<code>create_bedrock_model(model_id='anthropic.claude-sonnet-4-20250514-v1:0', region_name=None, boto_session=None, boto_client_config=None, **kwargs)</code>","text":"<p>Create a Bedrock model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>Bedrock model ID. Run the following command to get the model ID: aws bedrock list-foundation-models | jq -r '.modelSummaries[].modelId'</p> <code>'anthropic.claude-sonnet-4-20250514-v1:0'</code> <code>region_name</code> <code>str</code> <p>Bedrock region name.</p> <code>None</code> <code>boto_session</code> <code>Optional[Session]</code> <p>Bedrock boto session.</p> <code>None</code> <code>boto_client_config</code> <code>Optional[Config]</code> <p>Bedrock boto client config.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the Bedrock model.</p> <code>{}</code> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def create_bedrock_model(\n    model_id: str = \"anthropic.claude-sonnet-4-20250514-v1:0\",\n    region_name: str = None,\n    boto_session: Optional[boto3.Session] = None,\n    boto_client_config: Optional[BotocoreConfig] = None,\n    **kwargs: Any,\n) -&gt; BedrockModel:\n    \"\"\"Create a Bedrock model.\n\n    Args:\n        model_id: Bedrock model ID. Run the following command to get the model ID:\n            aws bedrock list-foundation-models | jq -r '.modelSummaries[].modelId'\n        region_name: Bedrock region name.\n        boto_session: Bedrock boto session.\n        boto_client_config: Bedrock boto client config.\n        **kwargs: Additional keyword arguments for the Bedrock model.\n    \"\"\"\n\n    return BedrockModel(\n        model_id=model_id,\n        region_name=region_name,\n        boto_session=boto_session,\n        boto_client_config=boto_client_config,\n        **kwargs,\n    )\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.create_ollama_model","title":"<code>create_ollama_model(host='http://localhost:11434', model_id='llama3.1', client_args=None, **kwargs)</code>","text":"<p>Create an Ollama model.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>Ollama host URL.</p> <code>'http://localhost:11434'</code> <code>model_id</code> <code>str</code> <p>Ollama model ID.</p> <code>'llama3.1'</code> <code>client_args</code> <code>dict</code> <p>Client arguments for the Ollama model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the Ollama model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OllamaModel</code> <code>OllamaModel</code> <p>An Ollama model.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def create_ollama_model(\n    host: str = \"http://localhost:11434\",\n    model_id: str = \"llama3.1\",\n    client_args: dict = None,\n    **kwargs: Any,\n) -&gt; OllamaModel:\n    \"\"\"Create an Ollama model.\n\n    Args:\n        host: Ollama host URL.\n        model_id: Ollama model ID.\n        client_args: Client arguments for the Ollama model.\n        **kwargs: Additional keyword arguments for the Ollama model.\n\n    Returns:\n        OllamaModel: An Ollama model.\n    \"\"\"\n    if client_args is None:\n        client_args = {}\n    return OllamaModel(host=host, model_id=model_id, client_args=client_args, **kwargs)\n</code></pre>"},{"location":"geo_agents/#geoai.agents.geo_agents.create_openai_model","title":"<code>create_openai_model(model_id='gpt-4o-mini', api_key=None, client_args=None, **kwargs)</code>","text":"<p>Create an OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>OpenAI model ID.</p> <code>'gpt-4o-mini'</code> <code>api_key</code> <code>str</code> <p>OpenAI API key.</p> <code>None</code> <code>client_args</code> <code>dict</code> <p>Client arguments for the OpenAI model.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the OpenAI model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>OpenAIModel</code> <code>OpenAIModel</code> <p>An OpenAI model.</p> Source code in <code>geoai/agents/geo_agents.py</code> <pre><code>def create_openai_model(\n    model_id: str = \"gpt-4o-mini\",\n    api_key: str = None,\n    client_args: dict = None,\n    **kwargs: Any,\n) -&gt; OpenAIModel:\n    \"\"\"Create an OpenAI model.\n\n    Args:\n        model_id: OpenAI model ID.\n        api_key: OpenAI API key.\n        client_args: Client arguments for the OpenAI model.\n        **kwargs: Additional keyword arguments for the OpenAI model.\n\n    Returns:\n        OpenAIModel: An OpenAI model.\n    \"\"\"\n\n    if api_key is None:\n        try:\n            api_key = os.getenv(\"OPENAI_API_KEY\", None)\n            if api_key is None:\n                raise ValueError(\"OPENAI_API_KEY is not set\")\n        except Exception:\n            raise ValueError(\"OPENAI_API_KEY is not set\")\n\n    if client_args is None:\n        client_args = kwargs.get(\"client_args\", {})\n    if \"api_key\" not in client_args and api_key is not None:\n        client_args[\"api_key\"] = api_key\n\n    return OpenAIModel(client_args=client_args, model_id=model_id, **kwargs)\n</code></pre>"},{"location":"geoai/","title":"geoai module","text":"<p>Main module.</p>"},{"location":"geoai/#geoai.geoai.AgricultureFieldDelineator","title":"<code>AgricultureFieldDelineator</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Agricultural field boundary delineation using a pre-trained Mask R-CNN model.</p> <p>This class extends the ObjectDetector class to specifically handle Sentinel-2 imagery with 12 spectral bands for agricultural field boundary detection.</p> <p>Attributes:</p> Name Type Description <code>band_selection</code> <p>List of band indices to use for prediction (default: RGB)</p> <code>sentinel_band_stats</code> <p>Per-band statistics for Sentinel-2 data</p> <code>use_ndvi</code> <p>Whether to calculate and include NDVI as an additional channel</p> Source code in <code>geoai/extract.py</code> <pre><code>class AgricultureFieldDelineator(ObjectDetector):\n    \"\"\"\n    Agricultural field boundary delineation using a pre-trained Mask R-CNN model.\n\n    This class extends the ObjectDetector class to specifically handle Sentinel-2\n    imagery with 12 spectral bands for agricultural field boundary detection.\n\n    Attributes:\n        band_selection: List of band indices to use for prediction (default: RGB)\n        sentinel_band_stats: Per-band statistics for Sentinel-2 data\n        use_ndvi: Whether to calculate and include NDVI as an additional channel\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"field_boundary_detector.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n        band_selection: Optional[List[int]] = None,\n        use_ndvi: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the field boundary delineator.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n            band_selection: List of Sentinel-2 band indices to use (None = adapt based on model)\n            use_ndvi: Whether to calculate and include NDVI as an additional channel\n        \"\"\"\n        # Save parameters before calling parent constructor\n        self.custom_band_selection = band_selection\n        self.use_ndvi = use_ndvi\n\n        # Set device (copied from parent init to ensure it's set before initialize_model)\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Initialize model differently for multi-spectral input\n        model = self.initialize_sentinel2_model(model)\n\n        # Call parent but with our custom model\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n        # Default Sentinel-2 band statistics (can be overridden with actual stats)\n        # Band order: [B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12]\n        self.sentinel_band_stats = {\n            \"means\": [\n                0.0975,\n                0.0476,\n                0.0598,\n                0.0697,\n                0.1077,\n                0.1859,\n                0.2378,\n                0.2061,\n                0.2598,\n                0.4120,\n                0.1956,\n                0.1410,\n            ],\n            \"stds\": [\n                0.0551,\n                0.0290,\n                0.0298,\n                0.0479,\n                0.0506,\n                0.0505,\n                0.0747,\n                0.0642,\n                0.0782,\n                0.1187,\n                0.0651,\n                0.0679,\n            ],\n        }\n\n        # Set default band selection (RGB - typically B4, B3, B2 for Sentinel-2)\n        self.band_selection = (\n            self.custom_band_selection\n            if self.custom_band_selection is not None\n            else [3, 2, 1]\n        )  # R, G, B bands\n\n        # Customize parameters for field delineation\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.overlap = 0.5  # Higher overlap for field boundary detection\n        self.min_object_area = 1000  # Minimum area in pixels for field detection\n        self.simplify_tolerance = 2.0  # Higher tolerance for field boundaries\n\n    def initialize_sentinel2_model(self, model: Optional[Any] = None) -&gt; Any:\n        \"\"\"\n        Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.\n\n        Args:\n            model: Pre-initialized model (optional)\n\n        Returns:\n            Modified model with appropriate input channels\n        \"\"\"\n        import torchvision\n        from torchvision.models.detection import maskrcnn_resnet50_fpn\n        from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n        if model is not None:\n            return model\n\n        # Determine number of input channels based on band selection and NDVI\n        num_input_channels = (\n            len(self.custom_band_selection)\n            if self.custom_band_selection is not None\n            else 3\n        )\n        if self.use_ndvi:\n            num_input_channels += 1\n\n        print(f\"Initializing Mask R-CNN model with {num_input_channels} input channels\")\n\n        # Create a ResNet50 backbone with modified input channels\n        backbone = resnet_fpn_backbone(\"resnet50\", weights=None)\n\n        # Replace the first conv layer to accept multi-spectral input\n        original_conv = backbone.body.conv1\n        backbone.body.conv1 = torch.nn.Conv2d(\n            num_input_channels,\n            original_conv.out_channels,\n            kernel_size=original_conv.kernel_size,\n            stride=original_conv.stride,\n            padding=original_conv.padding,\n            bias=original_conv.bias is not None,\n        )\n\n        # Create Mask R-CNN with our modified backbone\n        model = maskrcnn_resnet50_fpn(\n            backbone=backbone,\n            num_classes=2,  # Background + field\n            image_mean=[0.485] * num_input_channels,  # Extend mean to all channels\n            image_std=[0.229] * num_input_channels,  # Extend std to all channels\n        )\n\n        model.to(self.device)\n        return model\n\n    def preprocess_sentinel_bands(\n        self,\n        image_data: np.ndarray,\n        band_selection: Optional[List[int]] = None,\n        use_ndvi: Optional[bool] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Preprocess Sentinel-2 band data for model input.\n\n        Args:\n            image_data: Raw Sentinel-2 image data as numpy array [bands, height, width]\n            band_selection: List of band indices to use (overrides instance default if provided)\n            use_ndvi: Whether to include NDVI (overrides instance default if provided)\n\n        Returns:\n            Processed tensor ready for model input\n        \"\"\"\n        # Use instance defaults if not specified\n        band_selection = (\n            band_selection if band_selection is not None else self.band_selection\n        )\n        use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n        # Select bands\n        selected_bands = image_data[band_selection]\n\n        # Calculate NDVI if requested (using B8 and B4 which are indices 7 and 3)\n        if (\n            use_ndvi\n            and 7 in range(image_data.shape[0])\n            and 3 in range(image_data.shape[0])\n        ):\n            nir = image_data[7].astype(np.float32)  # B8 (NIR)\n            red = image_data[3].astype(np.float32)  # B4 (Red)\n\n            # Avoid division by zero\n            denominator = nir + red\n            ndvi = np.zeros_like(nir)\n            valid_mask = denominator &gt; 0\n            ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / denominator[\n                valid_mask\n            ]\n\n            # Rescale NDVI from [-1, 1] to [0, 1]\n            ndvi = (ndvi + 1) / 2\n\n            # Add NDVI as an additional channel\n            selected_bands = np.vstack([selected_bands, ndvi[np.newaxis, :, :]])\n\n        # Convert to tensor\n        image_tensor = torch.from_numpy(selected_bands).float()\n\n        # Normalize using band statistics\n        for i, band_idx in enumerate(band_selection):\n            # Make sure band_idx is within range of our statistics\n            if band_idx &lt; len(self.sentinel_band_stats[\"means\"]):\n                mean = self.sentinel_band_stats[\"means\"][band_idx]\n                std = self.sentinel_band_stats[\"stds\"][band_idx]\n                image_tensor[i] = (image_tensor[i] - mean) / std\n\n        # If NDVI was added, normalize it too (last channel)\n        if use_ndvi:\n            # NDVI is already roughly in [0,1] range, just standardize it slightly\n            image_tensor[-1] = (image_tensor[-1] - 0.5) / 0.5\n\n        return image_tensor\n\n    def update_band_stats(\n        self,\n        raster_path: str,\n        band_selection: Optional[List[int]] = None,\n        sample_size: int = 1000,\n    ) -&gt; Dict[str, List[float]]:\n        \"\"\"\n        Update band statistics from the input Sentinel-2 raster.\n\n        Args:\n            raster_path: Path to the Sentinel-2 raster file\n            band_selection: Specific bands to update (None = update all available)\n            sample_size: Number of random pixels to sample for statistics calculation\n\n        Returns:\n            Updated band statistics dictionary\n        \"\"\"\n        with rasterio.open(raster_path) as src:\n            # Check if this is likely a Sentinel-2 product\n            band_count = src.count\n            if band_count &lt; 3:\n                print(\n                    f\"Warning: Raster has only {band_count} bands, may not be Sentinel-2 data\"\n                )\n\n            # Get dimensions\n            height, width = src.height, src.width\n\n            # Determine which bands to analyze\n            if band_selection is None:\n                band_selection = list(range(1, band_count + 1))  # 1-indexed\n\n            # Initialize arrays for band statistics\n            means = []\n            stds = []\n\n            # Sample random pixels\n            np.random.seed(42)  # For reproducibility\n            sample_rows = np.random.randint(0, height, sample_size)\n            sample_cols = np.random.randint(0, width, sample_size)\n\n            # Calculate statistics for each band\n            for band in band_selection:\n                # Read band data\n                band_data = src.read(band)\n\n                # Sample values\n                sample_values = band_data[sample_rows, sample_cols]\n\n                # Remove invalid values (e.g., nodata)\n                valid_samples = sample_values[np.isfinite(sample_values)]\n\n                # Calculate statistics\n                mean = float(np.mean(valid_samples))\n                std = float(np.std(valid_samples))\n\n                # Store results\n                means.append(mean)\n                stds.append(std)\n\n                print(f\"Band {band}: mean={mean:.4f}, std={std:.4f}\")\n\n            # Update instance variables\n            self.sentinel_band_stats = {\"means\": means, \"stds\": stds}\n\n            return self.sentinel_band_stats\n\n    def process_sentinel_raster(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        batch_size: int = 4,\n        band_selection: Optional[List[int]] = None,\n        use_ndvi: Optional[bool] = None,\n        filter_edges: bool = True,\n        edge_buffer: int = 20,\n        **kwargs: Any,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Process a Sentinel-2 raster to extract field boundaries.\n\n        Args:\n            raster_path: Path to Sentinel-2 raster file\n            output_path: Path to output GeoJSON or Parquet file (optional)\n            batch_size: Batch size for processing\n            band_selection: List of bands to use (None = use instance default)\n            use_ndvi: Whether to include NDVI (None = use instance default)\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects\n            **kwargs: Additional parameters for processing\n\n        Returns:\n            GeoDataFrame with field boundaries\n        \"\"\"\n        # Use instance defaults if not specified\n        band_selection = (\n            band_selection if band_selection is not None else self.band_selection\n        )\n        use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Update band statistics if not already done\n        if kwargs.get(\"update_stats\", True):\n            self.update_band_stats(raster_path, band_selection)\n\n        print(f\"Processing with parameters:\")\n        print(f\"- Using bands: {band_selection}\")\n        print(f\"- Include NDVI: {use_ndvi}\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n\n        # Create a custom Sentinel-2 dataset class\n        class Sentinel2Dataset(torch.utils.data.Dataset):\n            def __init__(\n                self,\n                raster_path: str,\n                chip_size: Tuple[int, int],\n                stride_x: int,\n                stride_y: int,\n                band_selection: List[int],\n                use_ndvi: bool,\n                field_delineator: Any,\n            ) -&gt; None:\n                self.raster_path = raster_path\n                self.chip_size = chip_size\n                self.stride_x = stride_x\n                self.stride_y = stride_y\n                self.band_selection = band_selection\n                self.use_ndvi = use_ndvi\n                self.field_delineator = field_delineator\n\n                with rasterio.open(self.raster_path) as src:\n                    self.height = src.height\n                    self.width = src.width\n                    self.count = src.count\n                    self.crs = src.crs\n                    self.transform = src.transform\n\n                    # Calculate row_starts and col_starts\n                    self.row_starts = []\n                    self.col_starts = []\n\n                    # Normal row starts using stride\n                    for r in range((self.height - 1) // self.stride_y):\n                        self.row_starts.append(r * self.stride_y)\n\n                    # Add a special last row that ensures we reach the bottom edge\n                    if self.height &gt; self.chip_size[0]:\n                        self.row_starts.append(max(0, self.height - self.chip_size[0]))\n                    else:\n                        # If the image is smaller than chip size, just start at 0\n                        if not self.row_starts:\n                            self.row_starts.append(0)\n\n                    # Normal column starts using stride\n                    for c in range((self.width - 1) // self.stride_x):\n                        self.col_starts.append(c * self.stride_x)\n\n                    # Add a special last column that ensures we reach the right edge\n                    if self.width &gt; self.chip_size[1]:\n                        self.col_starts.append(max(0, self.width - self.chip_size[1]))\n                    else:\n                        # If the image is smaller than chip size, just start at 0\n                        if not self.col_starts:\n                            self.col_starts.append(0)\n\n                # Calculate number of tiles\n                self.rows = len(self.row_starts)\n                self.cols = len(self.col_starts)\n\n                print(\n                    f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n                )\n                print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n                print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n\n            def __len__(self) -&gt; int:\n                return self.rows * self.cols\n\n            def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n                # Convert flat index to grid position\n                row = idx // self.cols\n                col = idx % self.cols\n\n                # Get pre-calculated starting positions\n                j = self.row_starts[row]\n                i = self.col_starts[col]\n\n                # Read window from raster\n                with rasterio.open(self.raster_path) as src:\n                    # Make sure we don't read outside the image\n                    width = min(self.chip_size[1], self.width - i)\n                    height = min(self.chip_size[0], self.height - j)\n\n                    window = Window(i, j, width, height)\n\n                    # Read all bands\n                    image = src.read(window=window)\n\n                    # Handle partial windows at edges by padding\n                    if (\n                        image.shape[1] != self.chip_size[0]\n                        or image.shape[2] != self.chip_size[1]\n                    ):\n                        temp = np.zeros(\n                            (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                            dtype=image.dtype,\n                        )\n                        temp[:, : image.shape[1], : image.shape[2]] = image\n                        image = temp\n\n                # Preprocess bands for the model\n                image_tensor = self.field_delineator.preprocess_sentinel_bands(\n                    image, self.band_selection, self.use_ndvi\n                )\n\n                # Get geographic bounds for the window\n                with rasterio.open(self.raster_path) as src:\n                    window_transform = src.window_transform(window)\n                    minx, miny = window_transform * (0, height)\n                    maxx, maxy = window_transform * (width, 0)\n                    bbox = [minx, miny, maxx, maxy]\n\n                return {\n                    \"image\": image_tensor,\n                    \"bbox\": bbox,\n                    \"coords\": torch.tensor([i, j], dtype=torch.long),\n                    \"window_size\": torch.tensor([width, height], dtype=torch.long),\n                }\n\n        # Calculate stride based on overlap\n        stride_x = int(chip_size[1] * (1 - overlap))\n        stride_y = int(chip_size[0] * (1 - overlap))\n\n        # Create dataset\n        dataset = Sentinel2Dataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            stride_x=stride_x,\n            stride_y=stride_y,\n            band_selection=band_selection,\n            use_ndvi=use_ndvi,\n            field_delineator=self,\n        )\n\n        # Define custom collate function\n        def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate bbox objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches (call the parent class's process_raster method)\n        # We'll adapt the process_raster method to work with our Sentinel2Dataset\n        results = super().process_raster(\n            raster_path=raster_path,\n            output_path=output_path,\n            batch_size=batch_size,\n            filter_edges=filter_edges,\n            edge_buffer=edge_buffer,\n            confidence_threshold=confidence_threshold,\n            overlap=overlap,\n            chip_size=chip_size,\n            nms_iou_threshold=nms_iou_threshold,\n            mask_threshold=mask_threshold,\n            min_object_area=min_object_area,\n            simplify_tolerance=simplify_tolerance,\n        )\n\n        return results\n</code></pre>"},{"location":"geoai/#geoai.geoai.AgricultureFieldDelineator.__init__","title":"<code>__init__(model_path='field_boundary_detector.pth', repo_id=None, model=None, device=None, band_selection=None, use_ndvi=False)</code>","text":"<p>Initialize the field boundary delineator.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'field_boundary_detector.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> <code>band_selection</code> <code>Optional[List[int]]</code> <p>List of Sentinel-2 band indices to use (None = adapt based on model)</p> <code>None</code> <code>use_ndvi</code> <code>bool</code> <p>Whether to calculate and include NDVI as an additional channel</p> <code>False</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"field_boundary_detector.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n    band_selection: Optional[List[int]] = None,\n    use_ndvi: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the field boundary delineator.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        band_selection: List of Sentinel-2 band indices to use (None = adapt based on model)\n        use_ndvi: Whether to calculate and include NDVI as an additional channel\n    \"\"\"\n    # Save parameters before calling parent constructor\n    self.custom_band_selection = band_selection\n    self.use_ndvi = use_ndvi\n\n    # Set device (copied from parent init to ensure it's set before initialize_model)\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Initialize model differently for multi-spectral input\n    model = self.initialize_sentinel2_model(model)\n\n    # Call parent but with our custom model\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n\n    # Default Sentinel-2 band statistics (can be overridden with actual stats)\n    # Band order: [B1, B2, B3, B4, B5, B6, B7, B8, B8A, B9, B10, B11, B12]\n    self.sentinel_band_stats = {\n        \"means\": [\n            0.0975,\n            0.0476,\n            0.0598,\n            0.0697,\n            0.1077,\n            0.1859,\n            0.2378,\n            0.2061,\n            0.2598,\n            0.4120,\n            0.1956,\n            0.1410,\n        ],\n        \"stds\": [\n            0.0551,\n            0.0290,\n            0.0298,\n            0.0479,\n            0.0506,\n            0.0505,\n            0.0747,\n            0.0642,\n            0.0782,\n            0.1187,\n            0.0651,\n            0.0679,\n        ],\n    }\n\n    # Set default band selection (RGB - typically B4, B3, B2 for Sentinel-2)\n    self.band_selection = (\n        self.custom_band_selection\n        if self.custom_band_selection is not None\n        else [3, 2, 1]\n    )  # R, G, B bands\n\n    # Customize parameters for field delineation\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.overlap = 0.5  # Higher overlap for field boundary detection\n    self.min_object_area = 1000  # Minimum area in pixels for field detection\n    self.simplify_tolerance = 2.0  # Higher tolerance for field boundaries\n</code></pre>"},{"location":"geoai/#geoai.geoai.AgricultureFieldDelineator.initialize_sentinel2_model","title":"<code>initialize_sentinel2_model(model=None)</code>","text":"<p>Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[Any]</code> <p>Pre-initialized model (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Modified model with appropriate input channels</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_sentinel2_model(self, model: Optional[Any] = None) -&gt; Any:\n    \"\"\"\n    Initialize a Mask R-CNN model with a modified first layer to accept Sentinel-2 data.\n\n    Args:\n        model: Pre-initialized model (optional)\n\n    Returns:\n        Modified model with appropriate input channels\n    \"\"\"\n    import torchvision\n    from torchvision.models.detection import maskrcnn_resnet50_fpn\n    from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n    if model is not None:\n        return model\n\n    # Determine number of input channels based on band selection and NDVI\n    num_input_channels = (\n        len(self.custom_band_selection)\n        if self.custom_band_selection is not None\n        else 3\n    )\n    if self.use_ndvi:\n        num_input_channels += 1\n\n    print(f\"Initializing Mask R-CNN model with {num_input_channels} input channels\")\n\n    # Create a ResNet50 backbone with modified input channels\n    backbone = resnet_fpn_backbone(\"resnet50\", weights=None)\n\n    # Replace the first conv layer to accept multi-spectral input\n    original_conv = backbone.body.conv1\n    backbone.body.conv1 = torch.nn.Conv2d(\n        num_input_channels,\n        original_conv.out_channels,\n        kernel_size=original_conv.kernel_size,\n        stride=original_conv.stride,\n        padding=original_conv.padding,\n        bias=original_conv.bias is not None,\n    )\n\n    # Create Mask R-CNN with our modified backbone\n    model = maskrcnn_resnet50_fpn(\n        backbone=backbone,\n        num_classes=2,  # Background + field\n        image_mean=[0.485] * num_input_channels,  # Extend mean to all channels\n        image_std=[0.229] * num_input_channels,  # Extend std to all channels\n    )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"geoai/#geoai.geoai.AgricultureFieldDelineator.preprocess_sentinel_bands","title":"<code>preprocess_sentinel_bands(image_data, band_selection=None, use_ndvi=None)</code>","text":"<p>Preprocess Sentinel-2 band data for model input.</p> <p>Parameters:</p> Name Type Description Default <code>image_data</code> <code>ndarray</code> <p>Raw Sentinel-2 image data as numpy array [bands, height, width]</p> required <code>band_selection</code> <code>Optional[List[int]]</code> <p>List of band indices to use (overrides instance default if provided)</p> <code>None</code> <code>use_ndvi</code> <code>Optional[bool]</code> <p>Whether to include NDVI (overrides instance default if provided)</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Processed tensor ready for model input</p> Source code in <code>geoai/extract.py</code> <pre><code>def preprocess_sentinel_bands(\n    self,\n    image_data: np.ndarray,\n    band_selection: Optional[List[int]] = None,\n    use_ndvi: Optional[bool] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocess Sentinel-2 band data for model input.\n\n    Args:\n        image_data: Raw Sentinel-2 image data as numpy array [bands, height, width]\n        band_selection: List of band indices to use (overrides instance default if provided)\n        use_ndvi: Whether to include NDVI (overrides instance default if provided)\n\n    Returns:\n        Processed tensor ready for model input\n    \"\"\"\n    # Use instance defaults if not specified\n    band_selection = (\n        band_selection if band_selection is not None else self.band_selection\n    )\n    use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n    # Select bands\n    selected_bands = image_data[band_selection]\n\n    # Calculate NDVI if requested (using B8 and B4 which are indices 7 and 3)\n    if (\n        use_ndvi\n        and 7 in range(image_data.shape[0])\n        and 3 in range(image_data.shape[0])\n    ):\n        nir = image_data[7].astype(np.float32)  # B8 (NIR)\n        red = image_data[3].astype(np.float32)  # B4 (Red)\n\n        # Avoid division by zero\n        denominator = nir + red\n        ndvi = np.zeros_like(nir)\n        valid_mask = denominator &gt; 0\n        ndvi[valid_mask] = (nir[valid_mask] - red[valid_mask]) / denominator[\n            valid_mask\n        ]\n\n        # Rescale NDVI from [-1, 1] to [0, 1]\n        ndvi = (ndvi + 1) / 2\n\n        # Add NDVI as an additional channel\n        selected_bands = np.vstack([selected_bands, ndvi[np.newaxis, :, :]])\n\n    # Convert to tensor\n    image_tensor = torch.from_numpy(selected_bands).float()\n\n    # Normalize using band statistics\n    for i, band_idx in enumerate(band_selection):\n        # Make sure band_idx is within range of our statistics\n        if band_idx &lt; len(self.sentinel_band_stats[\"means\"]):\n            mean = self.sentinel_band_stats[\"means\"][band_idx]\n            std = self.sentinel_band_stats[\"stds\"][band_idx]\n            image_tensor[i] = (image_tensor[i] - mean) / std\n\n    # If NDVI was added, normalize it too (last channel)\n    if use_ndvi:\n        # NDVI is already roughly in [0,1] range, just standardize it slightly\n        image_tensor[-1] = (image_tensor[-1] - 0.5) / 0.5\n\n    return image_tensor\n</code></pre>"},{"location":"geoai/#geoai.geoai.AgricultureFieldDelineator.process_sentinel_raster","title":"<code>process_sentinel_raster(raster_path, output_path=None, batch_size=4, band_selection=None, use_ndvi=None, filter_edges=True, edge_buffer=20, **kwargs)</code>","text":"<p>Process a Sentinel-2 raster to extract field boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to Sentinel-2 raster file</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to output GeoJSON or Parquet file (optional)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>band_selection</code> <code>Optional[List[int]]</code> <p>List of bands to use (None = use instance default)</p> <code>None</code> <code>use_ndvi</code> <code>Optional[bool]</code> <p>Whether to include NDVI (None = use instance default)</p> <code>None</code> <code>filter_edges</code> <code>bool</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <code>int</code> <p>Size of edge buffer in pixels to filter out objects</p> <code>20</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for processing</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with field boundaries</p> Source code in <code>geoai/extract.py</code> <pre><code>def process_sentinel_raster(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    batch_size: int = 4,\n    band_selection: Optional[List[int]] = None,\n    use_ndvi: Optional[bool] = None,\n    filter_edges: bool = True,\n    edge_buffer: int = 20,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Process a Sentinel-2 raster to extract field boundaries.\n\n    Args:\n        raster_path: Path to Sentinel-2 raster file\n        output_path: Path to output GeoJSON or Parquet file (optional)\n        batch_size: Batch size for processing\n        band_selection: List of bands to use (None = use instance default)\n        use_ndvi: Whether to include NDVI (None = use instance default)\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects\n        **kwargs: Additional parameters for processing\n\n    Returns:\n        GeoDataFrame with field boundaries\n    \"\"\"\n    # Use instance defaults if not specified\n    band_selection = (\n        band_selection if band_selection is not None else self.band_selection\n    )\n    use_ndvi = use_ndvi if use_ndvi is not None else self.use_ndvi\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Update band statistics if not already done\n    if kwargs.get(\"update_stats\", True):\n        self.update_band_stats(raster_path, band_selection)\n\n    print(f\"Processing with parameters:\")\n    print(f\"- Using bands: {band_selection}\")\n    print(f\"- Include NDVI: {use_ndvi}\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n\n    # Create a custom Sentinel-2 dataset class\n    class Sentinel2Dataset(torch.utils.data.Dataset):\n        def __init__(\n            self,\n            raster_path: str,\n            chip_size: Tuple[int, int],\n            stride_x: int,\n            stride_y: int,\n            band_selection: List[int],\n            use_ndvi: bool,\n            field_delineator: Any,\n        ) -&gt; None:\n            self.raster_path = raster_path\n            self.chip_size = chip_size\n            self.stride_x = stride_x\n            self.stride_y = stride_y\n            self.band_selection = band_selection\n            self.use_ndvi = use_ndvi\n            self.field_delineator = field_delineator\n\n            with rasterio.open(self.raster_path) as src:\n                self.height = src.height\n                self.width = src.width\n                self.count = src.count\n                self.crs = src.crs\n                self.transform = src.transform\n\n                # Calculate row_starts and col_starts\n                self.row_starts = []\n                self.col_starts = []\n\n                # Normal row starts using stride\n                for r in range((self.height - 1) // self.stride_y):\n                    self.row_starts.append(r * self.stride_y)\n\n                # Add a special last row that ensures we reach the bottom edge\n                if self.height &gt; self.chip_size[0]:\n                    self.row_starts.append(max(0, self.height - self.chip_size[0]))\n                else:\n                    # If the image is smaller than chip size, just start at 0\n                    if not self.row_starts:\n                        self.row_starts.append(0)\n\n                # Normal column starts using stride\n                for c in range((self.width - 1) // self.stride_x):\n                    self.col_starts.append(c * self.stride_x)\n\n                # Add a special last column that ensures we reach the right edge\n                if self.width &gt; self.chip_size[1]:\n                    self.col_starts.append(max(0, self.width - self.chip_size[1]))\n                else:\n                    # If the image is smaller than chip size, just start at 0\n                    if not self.col_starts:\n                        self.col_starts.append(0)\n\n            # Calculate number of tiles\n            self.rows = len(self.row_starts)\n            self.cols = len(self.col_starts)\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n\n        def __len__(self) -&gt; int:\n            return self.rows * self.cols\n\n        def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n            # Convert flat index to grid position\n            row = idx // self.cols\n            col = idx % self.cols\n\n            # Get pre-calculated starting positions\n            j = self.row_starts[row]\n            i = self.col_starts[col]\n\n            # Read window from raster\n            with rasterio.open(self.raster_path) as src:\n                # Make sure we don't read outside the image\n                width = min(self.chip_size[1], self.width - i)\n                height = min(self.chip_size[0], self.height - j)\n\n                window = Window(i, j, width, height)\n\n                # Read all bands\n                image = src.read(window=window)\n\n                # Handle partial windows at edges by padding\n                if (\n                    image.shape[1] != self.chip_size[0]\n                    or image.shape[2] != self.chip_size[1]\n                ):\n                    temp = np.zeros(\n                        (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                        dtype=image.dtype,\n                    )\n                    temp[:, : image.shape[1], : image.shape[2]] = image\n                    image = temp\n\n            # Preprocess bands for the model\n            image_tensor = self.field_delineator.preprocess_sentinel_bands(\n                image, self.band_selection, self.use_ndvi\n            )\n\n            # Get geographic bounds for the window\n            with rasterio.open(self.raster_path) as src:\n                window_transform = src.window_transform(window)\n                minx, miny = window_transform * (0, height)\n                maxx, maxy = window_transform * (width, 0)\n                bbox = [minx, miny, maxx, maxy]\n\n            return {\n                \"image\": image_tensor,\n                \"bbox\": bbox,\n                \"coords\": torch.tensor([i, j], dtype=torch.long),\n                \"window_size\": torch.tensor([width, height], dtype=torch.long),\n            }\n\n    # Calculate stride based on overlap\n    stride_x = int(chip_size[1] * (1 - overlap))\n    stride_y = int(chip_size[0] * (1 - overlap))\n\n    # Create dataset\n    dataset = Sentinel2Dataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        stride_x=stride_x,\n        stride_y=stride_y,\n        band_selection=band_selection,\n        use_ndvi=use_ndvi,\n        field_delineator=self,\n    )\n\n    # Define custom collate function\n    def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate bbox objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches (call the parent class's process_raster method)\n    # We'll adapt the process_raster method to work with our Sentinel2Dataset\n    results = super().process_raster(\n        raster_path=raster_path,\n        output_path=output_path,\n        batch_size=batch_size,\n        filter_edges=filter_edges,\n        edge_buffer=edge_buffer,\n        confidence_threshold=confidence_threshold,\n        overlap=overlap,\n        chip_size=chip_size,\n        nms_iou_threshold=nms_iou_threshold,\n        mask_threshold=mask_threshold,\n        min_object_area=min_object_area,\n        simplify_tolerance=simplify_tolerance,\n    )\n\n    return results\n</code></pre>"},{"location":"geoai/#geoai.geoai.AgricultureFieldDelineator.update_band_stats","title":"<code>update_band_stats(raster_path, band_selection=None, sample_size=1000)</code>","text":"<p>Update band statistics from the input Sentinel-2 raster.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the Sentinel-2 raster file</p> required <code>band_selection</code> <code>Optional[List[int]]</code> <p>Specific bands to update (None = update all available)</p> <code>None</code> <code>sample_size</code> <code>int</code> <p>Number of random pixels to sample for statistics calculation</p> <code>1000</code> <p>Returns:</p> Type Description <code>Dict[str, List[float]]</code> <p>Updated band statistics dictionary</p> Source code in <code>geoai/extract.py</code> <pre><code>def update_band_stats(\n    self,\n    raster_path: str,\n    band_selection: Optional[List[int]] = None,\n    sample_size: int = 1000,\n) -&gt; Dict[str, List[float]]:\n    \"\"\"\n    Update band statistics from the input Sentinel-2 raster.\n\n    Args:\n        raster_path: Path to the Sentinel-2 raster file\n        band_selection: Specific bands to update (None = update all available)\n        sample_size: Number of random pixels to sample for statistics calculation\n\n    Returns:\n        Updated band statistics dictionary\n    \"\"\"\n    with rasterio.open(raster_path) as src:\n        # Check if this is likely a Sentinel-2 product\n        band_count = src.count\n        if band_count &lt; 3:\n            print(\n                f\"Warning: Raster has only {band_count} bands, may not be Sentinel-2 data\"\n            )\n\n        # Get dimensions\n        height, width = src.height, src.width\n\n        # Determine which bands to analyze\n        if band_selection is None:\n            band_selection = list(range(1, band_count + 1))  # 1-indexed\n\n        # Initialize arrays for band statistics\n        means = []\n        stds = []\n\n        # Sample random pixels\n        np.random.seed(42)  # For reproducibility\n        sample_rows = np.random.randint(0, height, sample_size)\n        sample_cols = np.random.randint(0, width, sample_size)\n\n        # Calculate statistics for each band\n        for band in band_selection:\n            # Read band data\n            band_data = src.read(band)\n\n            # Sample values\n            sample_values = band_data[sample_rows, sample_cols]\n\n            # Remove invalid values (e.g., nodata)\n            valid_samples = sample_values[np.isfinite(sample_values)]\n\n            # Calculate statistics\n            mean = float(np.mean(valid_samples))\n            std = float(np.std(valid_samples))\n\n            # Store results\n            means.append(mean)\n            stds.append(std)\n\n            print(f\"Band {band}: mean={mean:.4f}, std={std:.4f}\")\n\n        # Update instance variables\n        self.sentinel_band_stats = {\"means\": means, \"stds\": stds}\n\n        return self.sentinel_band_stats\n</code></pre>"},{"location":"geoai/#geoai.geoai.BoundingBox","title":"<code>BoundingBox</code>  <code>dataclass</code>","text":"<p>Represents a bounding box with coordinates.</p> Source code in <code>geoai/segment.py</code> <pre><code>@dataclass\nclass BoundingBox:\n    \"\"\"Represents a bounding box with coordinates.\"\"\"\n\n    xmin: int\n    ymin: int\n    xmax: int\n    ymax: int\n\n    @property\n    def xyxy(self) -&gt; List[float]:\n        return [self.xmin, self.ymin, self.xmax, self.ymax]\n</code></pre>"},{"location":"geoai/#geoai.geoai.BuildingFootprintExtractor","title":"<code>BuildingFootprintExtractor</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Building footprint extraction using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for building footprint extraction.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class BuildingFootprintExtractor(ObjectDetector):\n    \"\"\"\n    Building footprint extraction using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for building footprint extraction.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"building_footprints_usa.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n\n    def regularize_buildings(\n        self,\n        gdf: gpd.GeoDataFrame,\n        min_area: int = 10,\n        angle_threshold: int = 15,\n        orthogonality_threshold: float = 0.3,\n        rectangularity_threshold: float = 0.7,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Regularize building footprints to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with building footprints\n            min_area: Minimum area in square units to keep a building\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n            rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized building footprints\n        \"\"\"\n        return self.regularize_objects(\n            gdf,\n            min_area=min_area,\n            angle_threshold=angle_threshold,\n            orthogonality_threshold=orthogonality_threshold,\n            rectangularity_threshold=rectangularity_threshold,\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.BuildingFootprintExtractor.__init__","title":"<code>__init__(model_path='building_footprints_usa.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'building_footprints_usa.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"building_footprints_usa.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.BuildingFootprintExtractor.regularize_buildings","title":"<code>regularize_buildings(gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize building footprints to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with building footprints</p> required <code>min_area</code> <code>int</code> <p>Minimum area in square units to keep a building</p> <code>10</code> <code>angle_threshold</code> <code>int</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <code>float</code> <p>Percentage of angles that must be orthogonal for a building to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <code>float</code> <p>Minimum area ratio to building's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with regularized building footprints</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_buildings(\n    self,\n    gdf: gpd.GeoDataFrame,\n    min_area: int = 10,\n    angle_threshold: int = 15,\n    orthogonality_threshold: float = 0.3,\n    rectangularity_threshold: float = 0.7,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Regularize building footprints to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with building footprints\n        min_area: Minimum area in square units to keep a building\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for a building to be regularized\n        rectangularity_threshold: Minimum area ratio to building's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized building footprints\n    \"\"\"\n    return self.regularize_objects(\n        gdf,\n        min_area=min_area,\n        angle_threshold=angle_threshold,\n        orthogonality_threshold=orthogonality_threshold,\n        rectangularity_threshold=rectangularity_threshold,\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.CLIPSegmentation","title":"<code>CLIPSegmentation</code>","text":"<p>A class for segmenting high-resolution satellite imagery using text prompts with CLIP-based models.</p> <p>This segmenter utilizes the CLIP-Seg model to perform semantic segmentation based on text prompts. It can process large GeoTIFF files by tiling them and handles proper georeferencing in the output.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".</p> <code>'CIDAS/clipseg-rd64-refined'</code> <code>device</code> <code>str</code> <p>Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 352.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 16.</p> <code>32</code> <p>Attributes:</p> Name Type Description <code>processor</code> <code>CLIPSegProcessor</code> <p>The processor for the CLIP-Seg model.</p> <code>model</code> <code>CLIPSegForImageSegmentation</code> <p>The CLIP-Seg model for segmentation.</p> <code>device</code> <code>str</code> <p>The device being used ('cuda' or 'cpu').</p> <code>tile_size</code> <code>int</code> <p>Size of tiles for processing.</p> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> Source code in <code>geoai/segment.py</code> <pre><code>class CLIPSegmentation:\n    \"\"\"\n    A class for segmenting high-resolution satellite imagery using text prompts with CLIP-based models.\n\n    This segmenter utilizes the CLIP-Seg model to perform semantic segmentation based on text prompts.\n    It can process large GeoTIFF files by tiling them and handles proper georeferencing in the output.\n\n    Args:\n        model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n        device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 352.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 16.\n\n    Attributes:\n        processor (CLIPSegProcessor): The processor for the CLIP-Seg model.\n        model (CLIPSegForImageSegmentation): The CLIP-Seg model for segmentation.\n        device (str): The device being used ('cuda' or 'cpu').\n        tile_size (int): Size of tiles for processing.\n        overlap (int): Overlap between tiles.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"CIDAS/clipseg-rd64-refined\",\n        device: Optional[str] = None,\n        tile_size: int = 512,\n        overlap: int = 32,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ImageSegmenter with the specified model and settings.\n\n        Args:\n            model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n            device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n            tile_size (int): Size of tiles to process the image in chunks. Defaults to 512.\n            overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 32.\n        \"\"\"\n        self.tile_size = tile_size\n        self.overlap = overlap\n\n        # Set device\n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n\n        # Load model and processor\n        self.processor = CLIPSegProcessor.from_pretrained(model_name)\n        self.model = CLIPSegForImageSegmentation.from_pretrained(model_name).to(\n            self.device\n        )\n\n        print(f\"Model loaded on {self.device}\")\n\n    def segment_image(\n        self,\n        input_path: str,\n        output_path: str,\n        text_prompt: str,\n        threshold: float = 0.5,\n        smoothing_sigma: float = 1.0,\n    ) -&gt; str:\n        \"\"\"\n        Segment a GeoTIFF image using the provided text prompt.\n\n        The function processes the image in tiles and saves the result as a GeoTIFF with two bands:\n        - Band 1: Binary segmentation mask (0 or 1)\n        - Band 2: Probability scores (0.0 to 1.0)\n\n        Args:\n            input_path (str): Path to the input GeoTIFF file.\n            output_path (str): Path where the output GeoTIFF will be saved.\n            text_prompt (str): Text description of what to segment (e.g., \"water\", \"buildings\").\n            threshold (float): Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n\n        Returns:\n            str: Path to the saved output file.\n        \"\"\"\n        # Open the input GeoTIFF\n        with rasterio.open(input_path) as src:\n            # Get metadata\n            meta = src.meta\n            height = src.height\n            width = src.width\n\n            # Create output metadata\n            out_meta = meta.copy()\n            out_meta.update({\"count\": 2, \"dtype\": \"float32\", \"nodata\": None})\n\n            # Create arrays for results\n            segmentation = np.zeros((height, width), dtype=np.float32)\n            probabilities = np.zeros((height, width), dtype=np.float32)\n\n            # Calculate effective tile size (accounting for overlap)\n            effective_tile_size = self.tile_size - 2 * self.overlap\n\n            # Calculate number of tiles\n            n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n            n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n            total_tiles = n_tiles_x * n_tiles_y\n\n            # Process tiles with tqdm progress bar\n            with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n                # Iterate through tiles\n                for y in range(n_tiles_y):\n                    for x in range(n_tiles_x):\n                        # Calculate tile coordinates with overlap\n                        x_start = max(0, x * effective_tile_size - self.overlap)\n                        y_start = max(0, y * effective_tile_size - self.overlap)\n                        x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                        y_end = min(\n                            height, (y + 1) * effective_tile_size + self.overlap\n                        )\n\n                        tile_width = x_end - x_start\n                        tile_height = y_end - y_start\n\n                        # Read the tile\n                        window = Window(x_start, y_start, tile_width, tile_height)\n                        tile_data = src.read(window=window)\n\n                        # Process the tile\n                        try:\n                            # Convert to RGB if necessary (handling different satellite bands)\n                            if tile_data.shape[0] &gt; 3:\n                                # Use first three bands for RGB representation\n                                rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                                # Normalize data to 0-255 range if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n                            elif tile_data.shape[0] == 1:\n                                # Create RGB from grayscale\n                                rgb_tile = np.repeat(\n                                    tile_data[0][:, :, np.newaxis], 3, axis=2\n                                )\n                                # Normalize if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n                            else:\n                                # Already 3-channel, assume RGB\n                                rgb_tile = tile_data.transpose(1, 2, 0)\n                                # Normalize if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n\n                            # Convert to PIL Image\n                            pil_image = Image.fromarray(rgb_tile)\n\n                            # Resize if needed to match model's requirements\n                            if (\n                                pil_image.width &gt; self.tile_size\n                                or pil_image.height &gt; self.tile_size\n                            ):\n                                # Keep aspect ratio - use LANCZOS resampling instead of deprecated constant\n                                pil_image.thumbnail(\n                                    (self.tile_size, self.tile_size),\n                                    Image.Resampling.LANCZOS,\n                                )\n\n                            # Process with CLIP-Seg\n                            inputs = self.processor(\n                                text=text_prompt, images=pil_image, return_tensors=\"pt\"\n                            ).to(self.device)\n\n                            # Forward pass\n                            with torch.no_grad():\n                                outputs = self.model(**inputs)\n\n                            # Get logits and resize to original tile size\n                            logits = outputs.logits[0]\n\n                            # Convert logits to probabilities with sigmoid\n                            probs = torch.sigmoid(logits).cpu().numpy()\n\n                            # Resize back to original tile size if needed\n                            if probs.shape != (tile_height, tile_width):\n                                # Use bicubic interpolation for smoother results\n                                probs_resized = np.array(\n                                    Image.fromarray(probs).resize(\n                                        (tile_width, tile_height),\n                                        Image.Resampling.BICUBIC,\n                                    )\n                                )\n                            else:\n                                probs_resized = probs\n\n                            # Apply gaussian blur to reduce blockiness\n                            try:\n                                from scipy.ndimage import gaussian_filter\n\n                                probs_resized = gaussian_filter(\n                                    probs_resized, sigma=smoothing_sigma\n                                )\n                            except ImportError:\n                                pass  # Continue without smoothing if scipy is not available\n\n                            # Store results in the full arrays\n                            # Only store the non-overlapping part (except at edges)\n                            valid_x_start = self.overlap if x &gt; 0 else 0\n                            valid_y_start = self.overlap if y &gt; 0 else 0\n                            valid_x_end = (\n                                tile_width - self.overlap\n                                if x &lt; n_tiles_x - 1\n                                else tile_width\n                            )\n                            valid_y_end = (\n                                tile_height - self.overlap\n                                if y &lt; n_tiles_y - 1\n                                else tile_height\n                            )\n\n                            dest_x_start = x_start + valid_x_start\n                            dest_y_start = y_start + valid_y_start\n                            dest_x_end = x_start + valid_x_end\n                            dest_y_end = y_start + valid_y_end\n\n                            # Store probabilities\n                            probabilities[\n                                dest_y_start:dest_y_end, dest_x_start:dest_x_end\n                            ] = probs_resized[\n                                valid_y_start:valid_y_end, valid_x_start:valid_x_end\n                            ]\n\n                        except Exception as e:\n                            print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                            # Continue with next tile\n\n                        # Update progress bar\n                        pbar.update(1)\n\n            # Create binary segmentation from probabilities\n            segmentation = (probabilities &gt;= threshold).astype(np.float32)\n\n            # Write the output GeoTIFF\n            with rasterio.open(output_path, \"w\", **out_meta) as dst:\n                dst.write(segmentation, 1)\n                dst.write(probabilities, 2)\n\n                # Add descriptions to bands\n                dst.set_band_description(1, \"Binary Segmentation\")\n                dst.set_band_description(2, \"Probability Scores\")\n\n            print(f\"Segmentation saved to {output_path}\")\n            return output_path\n\n    def segment_image_batch(\n        self,\n        input_paths: List[str],\n        output_dir: str,\n        text_prompt: str,\n        threshold: float = 0.5,\n        smoothing_sigma: float = 1.0,\n        suffix: str = \"_segmented\",\n    ) -&gt; List[str]:\n        \"\"\"\n        Segment multiple GeoTIFF images using the provided text prompt.\n\n        Args:\n            input_paths (list): List of paths to input GeoTIFF files.\n            output_dir (str): Directory where output GeoTIFFs will be saved.\n            text_prompt (str): Text description of what to segment.\n            threshold (float): Threshold for binary segmentation. Defaults to 0.5.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n            suffix (str): Suffix to add to output filenames. Defaults to \"_segmented\".\n\n        Returns:\n            list: Paths to all saved output files.\n        \"\"\"\n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        output_paths = []\n\n        # Process each input file\n        for input_path in tqdm(input_paths, desc=\"Processing files\"):\n            # Generate output path\n            filename = os.path.basename(input_path)\n            base_name, ext = os.path.splitext(filename)\n            output_path = os.path.join(output_dir, f\"{base_name}{suffix}{ext}\")\n\n            # Segment the image\n            result_path = self.segment_image(\n                input_path, output_path, text_prompt, threshold, smoothing_sigma\n            )\n            output_paths.append(result_path)\n\n        return output_paths\n</code></pre>"},{"location":"geoai/#geoai.geoai.CLIPSegmentation.__init__","title":"<code>__init__(model_name='CIDAS/clipseg-rd64-refined', device=None, tile_size=512, overlap=32)</code>","text":"<p>Initialize the ImageSegmenter with the specified model and settings.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".</p> <code>'CIDAS/clipseg-rd64-refined'</code> <code>device</code> <code>str</code> <p>Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 512.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 32.</p> <code>32</code> Source code in <code>geoai/segment.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"CIDAS/clipseg-rd64-refined\",\n    device: Optional[str] = None,\n    tile_size: int = 512,\n    overlap: int = 32,\n) -&gt; None:\n    \"\"\"\n    Initialize the ImageSegmenter with the specified model and settings.\n\n    Args:\n        model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n        device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 512.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 32.\n    \"\"\"\n    self.tile_size = tile_size\n    self.overlap = overlap\n\n    # Set device\n    if device is None:\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    else:\n        self.device = device\n\n    # Load model and processor\n    self.processor = CLIPSegProcessor.from_pretrained(model_name)\n    self.model = CLIPSegForImageSegmentation.from_pretrained(model_name).to(\n        self.device\n    )\n\n    print(f\"Model loaded on {self.device}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.CLIPSegmentation.segment_image","title":"<code>segment_image(input_path, output_path, text_prompt, threshold=0.5, smoothing_sigma=1.0)</code>","text":"<p>Segment a GeoTIFF image using the provided text prompt.</p> <p>The function processes the image in tiles and saves the result as a GeoTIFF with two bands: - Band 1: Binary segmentation mask (0 or 1) - Band 2: Probability scores (0.0 to 1.0)</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output GeoTIFF will be saved.</p> required <code>text_prompt</code> <code>str</code> <p>Text description of what to segment (e.g., \"water\", \"buildings\").</p> required <code>threshold</code> <code>float</code> <p>Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.</p> <code>0.5</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved output file.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image(\n    self,\n    input_path: str,\n    output_path: str,\n    text_prompt: str,\n    threshold: float = 0.5,\n    smoothing_sigma: float = 1.0,\n) -&gt; str:\n    \"\"\"\n    Segment a GeoTIFF image using the provided text prompt.\n\n    The function processes the image in tiles and saves the result as a GeoTIFF with two bands:\n    - Band 1: Binary segmentation mask (0 or 1)\n    - Band 2: Probability scores (0.0 to 1.0)\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str): Path where the output GeoTIFF will be saved.\n        text_prompt (str): Text description of what to segment (e.g., \"water\", \"buildings\").\n        threshold (float): Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n\n    Returns:\n        str: Path to the saved output file.\n    \"\"\"\n    # Open the input GeoTIFF\n    with rasterio.open(input_path) as src:\n        # Get metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Create output metadata\n        out_meta = meta.copy()\n        out_meta.update({\"count\": 2, \"dtype\": \"float32\", \"nodata\": None})\n\n        # Create arrays for results\n        segmentation = np.zeros((height, width), dtype=np.float32)\n        probabilities = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate effective tile size (accounting for overlap)\n        effective_tile_size = self.tile_size - 2 * self.overlap\n\n        # Calculate number of tiles\n        n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n        n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n        total_tiles = n_tiles_x * n_tiles_y\n\n        # Process tiles with tqdm progress bar\n        with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n            # Iterate through tiles\n            for y in range(n_tiles_y):\n                for x in range(n_tiles_x):\n                    # Calculate tile coordinates with overlap\n                    x_start = max(0, x * effective_tile_size - self.overlap)\n                    y_start = max(0, y * effective_tile_size - self.overlap)\n                    x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                    y_end = min(\n                        height, (y + 1) * effective_tile_size + self.overlap\n                    )\n\n                    tile_width = x_end - x_start\n                    tile_height = y_end - y_start\n\n                    # Read the tile\n                    window = Window(x_start, y_start, tile_width, tile_height)\n                    tile_data = src.read(window=window)\n\n                    # Process the tile\n                    try:\n                        # Convert to RGB if necessary (handling different satellite bands)\n                        if tile_data.shape[0] &gt; 3:\n                            # Use first three bands for RGB representation\n                            rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                            # Normalize data to 0-255 range if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n                        elif tile_data.shape[0] == 1:\n                            # Create RGB from grayscale\n                            rgb_tile = np.repeat(\n                                tile_data[0][:, :, np.newaxis], 3, axis=2\n                            )\n                            # Normalize if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n                        else:\n                            # Already 3-channel, assume RGB\n                            rgb_tile = tile_data.transpose(1, 2, 0)\n                            # Normalize if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n\n                        # Convert to PIL Image\n                        pil_image = Image.fromarray(rgb_tile)\n\n                        # Resize if needed to match model's requirements\n                        if (\n                            pil_image.width &gt; self.tile_size\n                            or pil_image.height &gt; self.tile_size\n                        ):\n                            # Keep aspect ratio - use LANCZOS resampling instead of deprecated constant\n                            pil_image.thumbnail(\n                                (self.tile_size, self.tile_size),\n                                Image.Resampling.LANCZOS,\n                            )\n\n                        # Process with CLIP-Seg\n                        inputs = self.processor(\n                            text=text_prompt, images=pil_image, return_tensors=\"pt\"\n                        ).to(self.device)\n\n                        # Forward pass\n                        with torch.no_grad():\n                            outputs = self.model(**inputs)\n\n                        # Get logits and resize to original tile size\n                        logits = outputs.logits[0]\n\n                        # Convert logits to probabilities with sigmoid\n                        probs = torch.sigmoid(logits).cpu().numpy()\n\n                        # Resize back to original tile size if needed\n                        if probs.shape != (tile_height, tile_width):\n                            # Use bicubic interpolation for smoother results\n                            probs_resized = np.array(\n                                Image.fromarray(probs).resize(\n                                    (tile_width, tile_height),\n                                    Image.Resampling.BICUBIC,\n                                )\n                            )\n                        else:\n                            probs_resized = probs\n\n                        # Apply gaussian blur to reduce blockiness\n                        try:\n                            from scipy.ndimage import gaussian_filter\n\n                            probs_resized = gaussian_filter(\n                                probs_resized, sigma=smoothing_sigma\n                            )\n                        except ImportError:\n                            pass  # Continue without smoothing if scipy is not available\n\n                        # Store results in the full arrays\n                        # Only store the non-overlapping part (except at edges)\n                        valid_x_start = self.overlap if x &gt; 0 else 0\n                        valid_y_start = self.overlap if y &gt; 0 else 0\n                        valid_x_end = (\n                            tile_width - self.overlap\n                            if x &lt; n_tiles_x - 1\n                            else tile_width\n                        )\n                        valid_y_end = (\n                            tile_height - self.overlap\n                            if y &lt; n_tiles_y - 1\n                            else tile_height\n                        )\n\n                        dest_x_start = x_start + valid_x_start\n                        dest_y_start = y_start + valid_y_start\n                        dest_x_end = x_start + valid_x_end\n                        dest_y_end = y_start + valid_y_end\n\n                        # Store probabilities\n                        probabilities[\n                            dest_y_start:dest_y_end, dest_x_start:dest_x_end\n                        ] = probs_resized[\n                            valid_y_start:valid_y_end, valid_x_start:valid_x_end\n                        ]\n\n                    except Exception as e:\n                        print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                        # Continue with next tile\n\n                    # Update progress bar\n                    pbar.update(1)\n\n        # Create binary segmentation from probabilities\n        segmentation = (probabilities &gt;= threshold).astype(np.float32)\n\n        # Write the output GeoTIFF\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(segmentation, 1)\n            dst.write(probabilities, 2)\n\n            # Add descriptions to bands\n            dst.set_band_description(1, \"Binary Segmentation\")\n            dst.set_band_description(2, \"Probability Scores\")\n\n        print(f\"Segmentation saved to {output_path}\")\n        return output_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.CLIPSegmentation.segment_image_batch","title":"<code>segment_image_batch(input_paths, output_dir, text_prompt, threshold=0.5, smoothing_sigma=1.0, suffix='_segmented')</code>","text":"<p>Segment multiple GeoTIFF images using the provided text prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>list</code> <p>List of paths to input GeoTIFF files.</p> required <code>output_dir</code> <code>str</code> <p>Directory where output GeoTIFFs will be saved.</p> required <code>text_prompt</code> <code>str</code> <p>Text description of what to segment.</p> required <code>threshold</code> <code>float</code> <p>Threshold for binary segmentation. Defaults to 0.5.</p> <code>0.5</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.</p> <code>1.0</code> <code>suffix</code> <code>str</code> <p>Suffix to add to output filenames. Defaults to \"_segmented\".</p> <code>'_segmented'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>Paths to all saved output files.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image_batch(\n    self,\n    input_paths: List[str],\n    output_dir: str,\n    text_prompt: str,\n    threshold: float = 0.5,\n    smoothing_sigma: float = 1.0,\n    suffix: str = \"_segmented\",\n) -&gt; List[str]:\n    \"\"\"\n    Segment multiple GeoTIFF images using the provided text prompt.\n\n    Args:\n        input_paths (list): List of paths to input GeoTIFF files.\n        output_dir (str): Directory where output GeoTIFFs will be saved.\n        text_prompt (str): Text description of what to segment.\n        threshold (float): Threshold for binary segmentation. Defaults to 0.5.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n        suffix (str): Suffix to add to output filenames. Defaults to \"_segmented\".\n\n    Returns:\n        list: Paths to all saved output files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    output_paths = []\n\n    # Process each input file\n    for input_path in tqdm(input_paths, desc=\"Processing files\"):\n        # Generate output path\n        filename = os.path.basename(input_path)\n        base_name, ext = os.path.splitext(filename)\n        output_path = os.path.join(output_dir, f\"{base_name}{suffix}{ext}\")\n\n        # Segment the image\n        result_path = self.segment_image(\n            input_path, output_path, text_prompt, threshold, smoothing_sigma\n        )\n        output_paths.append(result_path)\n\n    return output_paths\n</code></pre>"},{"location":"geoai/#geoai.geoai.CarDetector","title":"<code>CarDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CarDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the `ObjectDetector` class with additional methods for car detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"car_detection_usa.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.CarDetector.__init__","title":"<code>__init__(model_path='car_detection_usa.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'car_detection_usa.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"car_detection_usa.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>               Bases: <code>NonGeoDataset</code></p> <p>A TorchGeo dataset for object extraction with overlapping tiles support.</p> <p>This dataset class creates overlapping image tiles for object detection, ensuring complete coverage of the input raster including right and bottom edges. It inherits from NonGeoDataset to avoid spatial indexing issues.</p> <p>Attributes:</p> Name Type Description <code>raster_path</code> <p>Path to the input raster file.</p> <code>chip_size</code> <p>Size of image chips to extract (height, width).</p> <code>overlap</code> <p>Amount of overlap between adjacent tiles (0.0-1.0).</p> <code>transforms</code> <p>Transforms to apply to the image.</p> <code>verbose</code> <p>Whether to print detailed processing information.</p> <code>stride_x</code> <p>Horizontal stride between tiles based on overlap.</p> <code>stride_y</code> <p>Vertical stride between tiles based on overlap.</p> <code>row_starts</code> <p>Starting Y positions for each row of tiles.</p> <code>col_starts</code> <p>Starting X positions for each column of tiles.</p> <code>crs</code> <p>Coordinate reference system of the raster.</p> <code>transform</code> <p>Affine transform of the raster.</p> <code>height</code> <p>Height of the raster in pixels.</p> <code>width</code> <p>Width of the raster in pixels.</p> <code>count</code> <p>Number of bands in the raster.</p> <code>bounds</code> <p>Geographic bounds of the raster (west, south, east, north).</p> <code>roi</code> <p>Shapely box representing the region of interest.</p> <code>rows</code> <p>Number of rows of tiles.</p> <code>cols</code> <p>Number of columns of tiles.</p> <code>raster_stats</code> <p>Statistics of the raster.</p> Source code in <code>geoai/extract.py</code> <pre><code>class CustomDataset(NonGeoDataset):\n    \"\"\"\n    A TorchGeo dataset for object extraction with overlapping tiles support.\n\n    This dataset class creates overlapping image tiles for object detection,\n    ensuring complete coverage of the input raster including right and bottom edges.\n    It inherits from NonGeoDataset to avoid spatial indexing issues.\n\n    Attributes:\n        raster_path: Path to the input raster file.\n        chip_size: Size of image chips to extract (height, width).\n        overlap: Amount of overlap between adjacent tiles (0.0-1.0).\n        transforms: Transforms to apply to the image.\n        verbose: Whether to print detailed processing information.\n        stride_x: Horizontal stride between tiles based on overlap.\n        stride_y: Vertical stride between tiles based on overlap.\n        row_starts: Starting Y positions for each row of tiles.\n        col_starts: Starting X positions for each column of tiles.\n        crs: Coordinate reference system of the raster.\n        transform: Affine transform of the raster.\n        height: Height of the raster in pixels.\n        width: Width of the raster in pixels.\n        count: Number of bands in the raster.\n        bounds: Geographic bounds of the raster (west, south, east, north).\n        roi: Shapely box representing the region of interest.\n        rows: Number of rows of tiles.\n        cols: Number of columns of tiles.\n        raster_stats: Statistics of the raster.\n    \"\"\"\n\n    def __init__(\n        self,\n        raster_path: str,\n        chip_size: Tuple[int, int] = (512, 512),\n        overlap: float = 0.5,\n        transforms: Optional[Any] = None,\n        band_indexes: Optional[List[int]] = None,\n        verbose: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the dataset with overlapping tiles.\n\n        Args:\n            raster_path: Path to the input raster file.\n            chip_size: Size of image chips to extract (height, width). Default is (512, 512).\n            overlap: Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).\n            transforms: Transforms to apply to the image. Default is None.\n            band_indexes: List of band indexes to use. Default is None (use all bands).\n            verbose: Whether to print detailed processing information. Default is False.\n\n        Raises:\n            ValueError: If overlap is too high resulting in non-positive stride.\n        \"\"\"\n        super().__init__()\n\n        # Initialize parameters\n        self.raster_path = raster_path\n        self.chip_size = chip_size\n        self.overlap = overlap\n        self.transforms = transforms\n        self.band_indexes = band_indexes\n        self.verbose = verbose\n        self.warned_about_bands = False\n\n        # Calculate stride based on overlap\n        self.stride_x = int(chip_size[1] * (1 - overlap))\n        self.stride_y = int(chip_size[0] * (1 - overlap))\n\n        if self.stride_x &lt;= 0 or self.stride_y &lt;= 0:\n            raise ValueError(\n                f\"Overlap {overlap} is too high, resulting in non-positive stride\"\n            )\n\n        with rasterio.open(self.raster_path) as src:\n            self.crs = src.crs\n            self.transform = src.transform\n            self.height = src.height\n            self.width = src.width\n            self.count = src.count\n\n            # Define the bounds of the dataset\n            west, south, east, north = src.bounds\n            self.bounds = (west, south, east, north)\n            self.roi = box(*self.bounds)\n\n            # Calculate starting positions for each tile\n            self.row_starts = []\n            self.col_starts = []\n\n            # Normal row starts using stride\n            for r in range((self.height - 1) // self.stride_y):\n                self.row_starts.append(r * self.stride_y)\n\n            # Add a special last row that ensures we reach the bottom edge\n            if self.height &gt; self.chip_size[0]:\n                self.row_starts.append(max(0, self.height - self.chip_size[0]))\n            else:\n                # If the image is smaller than chip size, just start at 0\n                if not self.row_starts:\n                    self.row_starts.append(0)\n\n            # Normal column starts using stride\n            for c in range((self.width - 1) // self.stride_x):\n                self.col_starts.append(c * self.stride_x)\n\n            # Add a special last column that ensures we reach the right edge\n            if self.width &gt; self.chip_size[1]:\n                self.col_starts.append(max(0, self.width - self.chip_size[1]))\n            else:\n                # If the image is smaller than chip size, just start at 0\n                if not self.col_starts:\n                    self.col_starts.append(0)\n\n            # Update rows and cols based on actual starting positions\n            self.rows = len(self.row_starts)\n            self.cols = len(self.col_starts)\n\n            print(\n                f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n            )\n            print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n            print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n            print(\n                f\"Overlap: {overlap*100}% (stride_x={self.stride_x}, stride_y={self.stride_y})\"\n            )\n            if src.crs:\n                print(f\"CRS: {src.crs}\")\n\n        # Get raster stats\n        self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get an image chip from the dataset by index.\n\n        Retrieves an image tile with the specified overlap pattern, ensuring\n        proper coverage of the entire raster including edges.\n\n        Args:\n            idx: Index of the chip to retrieve.\n\n        Returns:\n            dict: Dictionary containing:\n                - image: Image tensor.\n                - bbox: Geographic bounding box for the window.\n                - coords: Pixel coordinates as tensor [i, j].\n                - window_size: Window size as tensor [width, height].\n        \"\"\"\n        # Convert flat index to grid position\n        row = idx // self.cols\n        col = idx % self.cols\n\n        # Get pre-calculated starting positions\n        j = self.row_starts[row]\n        i = self.col_starts[col]\n\n        # Read window from raster\n        with rasterio.open(self.raster_path) as src:\n            # Make sure we don't read outside the image\n            width = min(self.chip_size[1], self.width - i)\n            height = min(self.chip_size[0], self.height - j)\n\n            window = Window(i, j, width, height)\n            image = src.read(window=window)\n\n            # Handle RGBA or multispectral images - keep only first 3 bands\n            if image.shape[0] &gt; 3:\n                if not self.warned_about_bands and self.verbose:\n                    print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                    self.warned_about_bands = True\n                if self.band_indexes is not None:\n                    image = image[self.band_indexes]\n                else:\n                    image = image[:3]\n            elif image.shape[0] &lt; 3:\n                # If image has fewer than 3 bands, duplicate the last band to make 3\n                if not self.warned_about_bands and self.verbose:\n                    print(\n                        f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                    )\n                    self.warned_about_bands = True\n                temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n                for c in range(3):\n                    temp[c] = image[min(c, image.shape[0] - 1)]\n                image = temp\n\n            # Handle partial windows at edges by padding\n            if (\n                image.shape[1] != self.chip_size[0]\n                or image.shape[2] != self.chip_size[1]\n            ):\n                temp = np.zeros(\n                    (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                    dtype=image.dtype,\n                )\n                temp[:, : image.shape[1], : image.shape[2]] = image\n                image = temp\n\n        # Convert to format expected by model (C,H,W)\n        image = torch.from_numpy(image).float()\n\n        # Normalize to [0, 1]\n        if image.max() &gt; 1:\n            image = image / 255.0\n\n        # Apply transforms if any\n        if self.transforms is not None:\n            image = self.transforms(image)\n\n        # Create geographic bounding box for the window\n        minx, miny = self.transform * (i, j + height)\n        maxx, maxy = self.transform * (i + width, j)\n        bbox = box(minx, miny, maxx, maxy)\n\n        return {\n            \"image\": image,\n            \"bbox\": bbox,\n            \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n            \"window_size\": torch.tensor(\n                [width, height], dtype=torch.long\n            ),  # Consistent format\n        }\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Return the number of samples in the dataset.\n\n        Returns:\n            int: Total number of tiles in the dataset.\n        \"\"\"\n        return self.rows * self.cols\n</code></pre>"},{"location":"geoai/#geoai.geoai.CustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get an image chip from the dataset by index.</p> <p>Retrieves an image tile with the specified overlap pattern, ensuring proper coverage of the entire raster including edges.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the chip to retrieve.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing: - image: Image tensor. - bbox: Geographic bounding box for the window. - coords: Pixel coordinates as tensor [i, j]. - window_size: Window size as tensor [width, height].</p> Source code in <code>geoai/extract.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get an image chip from the dataset by index.\n\n    Retrieves an image tile with the specified overlap pattern, ensuring\n    proper coverage of the entire raster including edges.\n\n    Args:\n        idx: Index of the chip to retrieve.\n\n    Returns:\n        dict: Dictionary containing:\n            - image: Image tensor.\n            - bbox: Geographic bounding box for the window.\n            - coords: Pixel coordinates as tensor [i, j].\n            - window_size: Window size as tensor [width, height].\n    \"\"\"\n    # Convert flat index to grid position\n    row = idx // self.cols\n    col = idx % self.cols\n\n    # Get pre-calculated starting positions\n    j = self.row_starts[row]\n    i = self.col_starts[col]\n\n    # Read window from raster\n    with rasterio.open(self.raster_path) as src:\n        # Make sure we don't read outside the image\n        width = min(self.chip_size[1], self.width - i)\n        height = min(self.chip_size[0], self.height - j)\n\n        window = Window(i, j, width, height)\n        image = src.read(window=window)\n\n        # Handle RGBA or multispectral images - keep only first 3 bands\n        if image.shape[0] &gt; 3:\n            if not self.warned_about_bands and self.verbose:\n                print(f\"Image has {image.shape[0]} bands, using first 3 bands only\")\n                self.warned_about_bands = True\n            if self.band_indexes is not None:\n                image = image[self.band_indexes]\n            else:\n                image = image[:3]\n        elif image.shape[0] &lt; 3:\n            # If image has fewer than 3 bands, duplicate the last band to make 3\n            if not self.warned_about_bands and self.verbose:\n                print(\n                    f\"Image has {image.shape[0]} bands, duplicating bands to make 3\"\n                )\n                self.warned_about_bands = True\n            temp = np.zeros((3, image.shape[1], image.shape[2]), dtype=image.dtype)\n            for c in range(3):\n                temp[c] = image[min(c, image.shape[0] - 1)]\n            image = temp\n\n        # Handle partial windows at edges by padding\n        if (\n            image.shape[1] != self.chip_size[0]\n            or image.shape[2] != self.chip_size[1]\n        ):\n            temp = np.zeros(\n                (image.shape[0], self.chip_size[0], self.chip_size[1]),\n                dtype=image.dtype,\n            )\n            temp[:, : image.shape[1], : image.shape[2]] = image\n            image = temp\n\n    # Convert to format expected by model (C,H,W)\n    image = torch.from_numpy(image).float()\n\n    # Normalize to [0, 1]\n    if image.max() &gt; 1:\n        image = image / 255.0\n\n    # Apply transforms if any\n    if self.transforms is not None:\n        image = self.transforms(image)\n\n    # Create geographic bounding box for the window\n    minx, miny = self.transform * (i, j + height)\n    maxx, maxy = self.transform * (i + width, j)\n    bbox = box(minx, miny, maxx, maxy)\n\n    return {\n        \"image\": image,\n        \"bbox\": bbox,\n        \"coords\": torch.tensor([i, j], dtype=torch.long),  # Consistent format\n        \"window_size\": torch.tensor(\n            [width, height], dtype=torch.long\n        ),  # Consistent format\n    }\n</code></pre>"},{"location":"geoai/#geoai.geoai.CustomDataset.__init__","title":"<code>__init__(raster_path, chip_size=(512, 512), overlap=0.5, transforms=None, band_indexes=None, verbose=False)</code>","text":"<p>Initialize the dataset with overlapping tiles.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file.</p> required <code>chip_size</code> <code>Tuple[int, int]</code> <p>Size of image chips to extract (height, width). Default is (512, 512).</p> <code>(512, 512)</code> <code>overlap</code> <code>float</code> <p>Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).</p> <code>0.5</code> <code>transforms</code> <code>Optional[Any]</code> <p>Transforms to apply to the image. Default is None.</p> <code>None</code> <code>band_indexes</code> <code>Optional[List[int]]</code> <p>List of band indexes to use. Default is None (use all bands).</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed processing information. Default is False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If overlap is too high resulting in non-positive stride.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    raster_path: str,\n    chip_size: Tuple[int, int] = (512, 512),\n    overlap: float = 0.5,\n    transforms: Optional[Any] = None,\n    band_indexes: Optional[List[int]] = None,\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the dataset with overlapping tiles.\n\n    Args:\n        raster_path: Path to the input raster file.\n        chip_size: Size of image chips to extract (height, width). Default is (512, 512).\n        overlap: Amount of overlap between adjacent tiles (0.0-1.0). Default is 0.5 (50%).\n        transforms: Transforms to apply to the image. Default is None.\n        band_indexes: List of band indexes to use. Default is None (use all bands).\n        verbose: Whether to print detailed processing information. Default is False.\n\n    Raises:\n        ValueError: If overlap is too high resulting in non-positive stride.\n    \"\"\"\n    super().__init__()\n\n    # Initialize parameters\n    self.raster_path = raster_path\n    self.chip_size = chip_size\n    self.overlap = overlap\n    self.transforms = transforms\n    self.band_indexes = band_indexes\n    self.verbose = verbose\n    self.warned_about_bands = False\n\n    # Calculate stride based on overlap\n    self.stride_x = int(chip_size[1] * (1 - overlap))\n    self.stride_y = int(chip_size[0] * (1 - overlap))\n\n    if self.stride_x &lt;= 0 or self.stride_y &lt;= 0:\n        raise ValueError(\n            f\"Overlap {overlap} is too high, resulting in non-positive stride\"\n        )\n\n    with rasterio.open(self.raster_path) as src:\n        self.crs = src.crs\n        self.transform = src.transform\n        self.height = src.height\n        self.width = src.width\n        self.count = src.count\n\n        # Define the bounds of the dataset\n        west, south, east, north = src.bounds\n        self.bounds = (west, south, east, north)\n        self.roi = box(*self.bounds)\n\n        # Calculate starting positions for each tile\n        self.row_starts = []\n        self.col_starts = []\n\n        # Normal row starts using stride\n        for r in range((self.height - 1) // self.stride_y):\n            self.row_starts.append(r * self.stride_y)\n\n        # Add a special last row that ensures we reach the bottom edge\n        if self.height &gt; self.chip_size[0]:\n            self.row_starts.append(max(0, self.height - self.chip_size[0]))\n        else:\n            # If the image is smaller than chip size, just start at 0\n            if not self.row_starts:\n                self.row_starts.append(0)\n\n        # Normal column starts using stride\n        for c in range((self.width - 1) // self.stride_x):\n            self.col_starts.append(c * self.stride_x)\n\n        # Add a special last column that ensures we reach the right edge\n        if self.width &gt; self.chip_size[1]:\n            self.col_starts.append(max(0, self.width - self.chip_size[1]))\n        else:\n            # If the image is smaller than chip size, just start at 0\n            if not self.col_starts:\n                self.col_starts.append(0)\n\n        # Update rows and cols based on actual starting positions\n        self.rows = len(self.row_starts)\n        self.cols = len(self.col_starts)\n\n        print(\n            f\"Dataset initialized with {self.rows} rows and {self.cols} columns of chips\"\n        )\n        print(f\"Image dimensions: {self.width} x {self.height} pixels\")\n        print(f\"Chip size: {self.chip_size[1]} x {self.chip_size[0]} pixels\")\n        print(\n            f\"Overlap: {overlap*100}% (stride_x={self.stride_x}, stride_y={self.stride_y})\"\n        )\n        if src.crs:\n            print(f\"CRS: {src.crs}\")\n\n    # Get raster stats\n    self.raster_stats = get_raster_stats(raster_path, divide_by=255)\n</code></pre>"},{"location":"geoai/#geoai.geoai.CustomDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of tiles in the dataset.</p> Source code in <code>geoai/extract.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Return the number of samples in the dataset.\n\n    Returns:\n        int: Total number of tiles in the dataset.\n    \"\"\"\n    return self.rows * self.cols\n</code></pre>"},{"location":"geoai/#geoai.geoai.DetectionResult","title":"<code>DetectionResult</code>  <code>dataclass</code>","text":"<p>Represents a detection result with score, label, bounding box, and optional mask.</p> Source code in <code>geoai/segment.py</code> <pre><code>@dataclass\nclass DetectionResult:\n    \"\"\"Represents a detection result with score, label, bounding box, and optional mask.\"\"\"\n\n    score: float\n    label: str\n    box: BoundingBox\n    mask: Optional[np.array] = None\n\n    @classmethod\n    def from_dict(cls, detection_dict: Dict) -&gt; \"DetectionResult\":\n        return cls(\n            score=detection_dict[\"score\"],\n            label=detection_dict[\"label\"],\n            box=BoundingBox(\n                xmin=detection_dict[\"box\"][\"xmin\"],\n                ymin=detection_dict[\"box\"][\"ymin\"],\n                xmax=detection_dict[\"box\"][\"xmax\"],\n                ymax=detection_dict[\"box\"][\"ymax\"],\n            ),\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.FocalLoss","title":"<code>FocalLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Focal Loss for addressing class imbalance in segmentation.</p> <p>Reference: Lin, T. Y., Goyal, P., Girshick, R., He, K., &amp; Doll\u00e1r, P. (2017). Focal loss for dense object detection. ICCV.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <p>Weighting factor in range (0,1) to balance positive/negative examples</p> <code>1.0</code> <code>gamma</code> <p>Exponent of the modulating factor (1 - p_t)^gamma</p> <code>2.0</code> <code>ignore_index</code> <p>Specifies a target value that is ignored</p> <code>-100</code> <code>reduction</code> <p>Specifies the reduction to apply to the output</p> <code>'mean'</code> <code>weight</code> <p>Manual rescaling weight given to each class</p> <code>None</code> Source code in <code>geoai/landcover_train.py</code> <pre><code>class FocalLoss(nn.Module):\n    \"\"\"\n    Focal Loss for addressing class imbalance in segmentation.\n\n    Reference: Lin, T. Y., Goyal, P., Girshick, R., He, K., &amp; Doll\u00e1r, P. (2017).\n    Focal loss for dense object detection. ICCV.\n\n    Args:\n        alpha: Weighting factor in range (0,1) to balance positive/negative examples\n        gamma: Exponent of the modulating factor (1 - p_t)^gamma\n        ignore_index: Specifies a target value that is ignored\n        reduction: Specifies the reduction to apply to the output\n        weight: Manual rescaling weight given to each class\n    \"\"\"\n\n    def __init__(\n        self, alpha=1.0, gamma=2.0, ignore_index=-100, reduction=\"mean\", weight=None\n    ):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.weight = weight\n\n    def forward(self, inputs, targets):\n        \"\"\"\n        Forward pass of focal loss.\n\n        Args:\n            inputs: Predictions (N, C, H, W) where C = number of classes\n            targets: Ground truth (N, H, W) with class indices\n\n        Returns:\n            Loss value\n        \"\"\"\n        # Get class probabilities\n        ce_loss = F.cross_entropy(\n            inputs,\n            targets,\n            weight=self.weight,\n            ignore_index=self.ignore_index,\n            reduction=\"none\",\n        )\n\n        # Get probability of true class\n        p_t = torch.exp(-ce_loss)\n\n        # Calculate focal loss\n        focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss\n\n        # Apply reduction\n        if self.reduction == \"mean\":\n            return focal_loss.mean()\n        elif self.reduction == \"sum\":\n            return focal_loss.sum()\n        else:\n            return focal_loss\n</code></pre>"},{"location":"geoai/#geoai.geoai.FocalLoss.forward","title":"<code>forward(inputs, targets)</code>","text":"<p>Forward pass of focal loss.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <p>Predictions (N, C, H, W) where C = number of classes</p> required <code>targets</code> <p>Ground truth (N, H, W) with class indices</p> required <p>Returns:</p> Type Description <p>Loss value</p> Source code in <code>geoai/landcover_train.py</code> <pre><code>def forward(self, inputs, targets):\n    \"\"\"\n    Forward pass of focal loss.\n\n    Args:\n        inputs: Predictions (N, C, H, W) where C = number of classes\n        targets: Ground truth (N, H, W) with class indices\n\n    Returns:\n        Loss value\n    \"\"\"\n    # Get class probabilities\n    ce_loss = F.cross_entropy(\n        inputs,\n        targets,\n        weight=self.weight,\n        ignore_index=self.ignore_index,\n        reduction=\"none\",\n    )\n\n    # Get probability of true class\n    p_t = torch.exp(-ce_loss)\n\n    # Calculate focal loss\n    focal_loss = self.alpha * (1 - p_t) ** self.gamma * ce_loss\n\n    # Apply reduction\n    if self.reduction == \"mean\":\n        return focal_loss.mean()\n    elif self.reduction == \"sum\":\n        return focal_loss.sum()\n    else:\n        return focal_loss\n</code></pre>"},{"location":"geoai/#geoai.geoai.GroundedSAM","title":"<code>GroundedSAM</code>","text":"<p>A class for segmenting remote sensing imagery using text prompts with Grounding DINO + SAM.</p> <p>This class combines Grounding DINO for object detection and Segment Anything Model (SAM) for precise segmentation based on text prompts. It can process large GeoTIFF files by tiling them and handles proper georeferencing in the outputs.</p> <p>Parameters:</p> Name Type Description Default <code>detector_id</code> <code>str</code> <p>Hugging Face model ID for Grounding DINO. Defaults to \"IDEA-Research/grounding-dino-tiny\".</p> <code>'IDEA-Research/grounding-dino-tiny'</code> <code>segmenter_id</code> <code>str</code> <p>Hugging Face model ID for SAM. Defaults to \"facebook/sam-vit-base\".</p> <code>'facebook/sam-vit-base'</code> <code>device</code> <code>str</code> <p>Device to run the models on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 128.</p> <code>128</code> <code>threshold</code> <code>float</code> <p>Detection threshold for Grounding DINO. Defaults to 0.3.</p> <code>0.3</code> <p>Attributes:</p> Name Type Description <code>detector_id</code> <code>str</code> <p>The Grounding DINO model ID.</p> <code>segmenter_id</code> <code>str</code> <p>The SAM model ID.</p> <code>device</code> <code>str</code> <p>The device being used ('cuda' or 'cpu').</p> <code>tile_size</code> <code>int</code> <p>Size of tiles for processing.</p> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> <code>threshold</code> <code>float</code> <p>Detection threshold.</p> <code>object_detector</code> <code>float</code> <p>The Grounding DINO pipeline.</p> <code>segmentator</code> <code>float</code> <p>The SAM model.</p> <code>processor</code> <code>float</code> <p>The SAM processor.</p> Source code in <code>geoai/segment.py</code> <pre><code>class GroundedSAM:\n    \"\"\"\n    A class for segmenting remote sensing imagery using text prompts with Grounding DINO + SAM.\n\n    This class combines Grounding DINO for object detection and Segment Anything Model (SAM) for\n    precise segmentation based on text prompts. It can process large GeoTIFF files by tiling them\n    and handles proper georeferencing in the outputs.\n\n    Args:\n        detector_id (str): Hugging Face model ID for Grounding DINO. Defaults to \"IDEA-Research/grounding-dino-tiny\".\n        segmenter_id (str): Hugging Face model ID for SAM. Defaults to \"facebook/sam-vit-base\".\n        device (str): Device to run the models on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 1024.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 128.\n        threshold (float): Detection threshold for Grounding DINO. Defaults to 0.3.\n\n    Attributes:\n        detector_id (str): The Grounding DINO model ID.\n        segmenter_id (str): The SAM model ID.\n        device (str): The device being used ('cuda' or 'cpu').\n        tile_size (int): Size of tiles for processing.\n        overlap (int): Overlap between tiles.\n        threshold (float): Detection threshold.\n        object_detector: The Grounding DINO pipeline.\n        segmentator: The SAM model.\n        processor: The SAM processor.\n    \"\"\"\n\n    def __init__(\n        self,\n        detector_id: str = \"IDEA-Research/grounding-dino-tiny\",\n        segmenter_id: str = \"facebook/sam-vit-base\",\n        device: Optional[str] = None,\n        tile_size: int = 1024,\n        overlap: int = 128,\n        threshold: float = 0.3,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GroundedSAM with the specified models and settings.\n\n        Args:\n            detector_id (str): Hugging Face model ID for Grounding DINO.\n            segmenter_id (str): Hugging Face model ID for SAM.\n            device (str): Device to run the models on ('cuda', 'cpu').\n            tile_size (int): Size of tiles to process the image in chunks.\n            overlap (int): Overlap between tiles to avoid edge artifacts.\n            threshold (float): Detection threshold for Grounding DINO.\n        \"\"\"\n        self.detector_id = detector_id\n        self.segmenter_id = segmenter_id\n        self.tile_size = tile_size\n        self.overlap = overlap\n        self.threshold = threshold\n\n        # Set device\n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n\n        # Load models\n        self._load_models()\n\n        print(f\"GroundedSAM initialized on {self.device}\")\n\n    def _load_models(self) -&gt; None:\n        \"\"\"Load the Grounding DINO and SAM models.\"\"\"\n        # Load Grounding DINO\n        self.object_detector = pipeline(\n            model=self.detector_id,\n            task=\"zero-shot-object-detection\",\n            device=self.device,\n        )\n\n        # Load SAM\n        self.segmentator = AutoModelForMaskGeneration.from_pretrained(\n            self.segmenter_id\n        ).to(self.device)\n        self.processor = AutoProcessor.from_pretrained(self.segmenter_id)\n\n    def _detect(self, image: Image.Image, labels: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"\n        Use Grounding DINO to detect objects in an image.\n\n        Args:\n            image (Image.Image): PIL image to detect objects in.\n            labels (List[str]): List of text labels to detect.\n\n        Returns:\n            List[DetectionResult]: List of detection results.\n        \"\"\"\n        # Ensure labels end with periods\n        labels = [label if label.endswith(\".\") else label + \".\" for label in labels]\n\n        results = self.object_detector(\n            image, candidate_labels=labels, threshold=self.threshold\n        )\n        results = [DetectionResult.from_dict(result) for result in results]\n\n        return results\n\n    def _apply_nms(\n        self, detections: List[DetectionResult], iou_threshold: float = 0.5\n    ) -&gt; List[DetectionResult]:\n        \"\"\"\n        Apply Non-Maximum Suppression to remove overlapping detections.\n\n        Args:\n            detections (List[DetectionResult]): List of detection results.\n            iou_threshold (float): IoU threshold for NMS.\n\n        Returns:\n            List[DetectionResult]: Filtered detection results.\n        \"\"\"\n        if not detections:\n            return detections\n\n        # Convert to format for NMS\n        boxes = []\n        scores = []\n\n        for detection in detections:\n            boxes.append(\n                [\n                    detection.box.xmin,\n                    detection.box.ymin,\n                    detection.box.xmax,\n                    detection.box.ymax,\n                ]\n            )\n            scores.append(detection.score)\n\n        boxes = np.array(boxes, dtype=np.float32)\n        scores = np.array(scores, dtype=np.float32)\n\n        # Apply NMS using OpenCV\n        indices = cv2.dnn.NMSBoxes(boxes, scores, self.threshold, iou_threshold)\n\n        if len(indices) &gt; 0:\n            indices = indices.flatten()\n            return [detections[i] for i in indices]\n        else:\n            return []\n\n    def _get_boxes(self, results: List[DetectionResult]) -&gt; List[List[List[float]]]:\n        \"\"\"Extract bounding boxes from detection results.\"\"\"\n        boxes = []\n        for result in results:\n            xyxy = result.box.xyxy\n            boxes.append(xyxy)\n        return [boxes]\n\n    def _refine_masks(\n        self, masks: torch.BoolTensor, polygon_refinement: bool = False\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Refine masks from SAM output.\"\"\"\n        masks = masks.cpu().float()\n        masks = masks.permute(0, 2, 3, 1)\n        masks = masks.mean(axis=-1)\n        masks = (masks &gt; 0).int()\n        masks = masks.numpy().astype(np.uint8)\n        masks = list(masks)\n\n        if polygon_refinement:\n            for idx, mask in enumerate(masks):\n                shape = mask.shape\n                polygon = self._mask_to_polygon(mask)\n                if polygon:\n                    mask = self._polygon_to_mask(polygon, shape)\n                    masks[idx] = mask\n\n        return masks\n\n    def _mask_to_polygon(self, mask: np.ndarray) -&gt; List[List[int]]:\n        \"\"\"Convert mask to polygon coordinates.\"\"\"\n        # Find contours in the binary mask\n        contours, _ = cv2.findContours(\n            mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        if not contours:\n            return []\n\n        # Find the contour with the largest area\n        largest_contour = max(contours, key=cv2.contourArea)\n\n        # Extract the vertices of the contour\n        polygon = largest_contour.reshape(-1, 2).tolist()\n\n        return polygon\n\n    def _polygon_to_mask(\n        self, polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]\n    ) -&gt; np.ndarray:\n        \"\"\"Convert polygon to mask.\"\"\"\n        # Create an empty mask\n        mask = np.zeros(image_shape, dtype=np.uint8)\n\n        # Convert polygon to an array of points\n        pts = np.array(polygon, dtype=np.int32)\n\n        # Fill the polygon with white color (255)\n        cv2.fillPoly(mask, [pts], color=(255,))\n\n        return mask\n\n    def _separate_instances(\n        self, mask: np.ndarray, min_area: int = 50\n    ) -&gt; List[np.ndarray]:\n        \"\"\"\n        Separate individual instances from a combined mask using connected components.\n\n        Args:\n            mask (np.ndarray): Combined binary mask.\n            min_area (int): Minimum area threshold for valid instances.\n\n        Returns:\n            List[np.ndarray]: List of individual instance masks.\n        \"\"\"\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            mask.astype(np.uint8), connectivity=8\n        )\n\n        instances = []\n        for i in range(1, num_labels):  # Skip background (label 0)\n            # Get area of the component\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Filter by minimum area\n            if area &gt;= min_area:\n                # Create mask for this instance\n                instance_mask = (labels == i).astype(np.uint8) * 255\n                instances.append(instance_mask)\n\n        return instances\n\n    def _mask_to_polygons(\n        self,\n        mask: np.ndarray,\n        transform,\n        x_offset: int = 0,\n        y_offset: int = 0,\n        min_area: int = 50,\n        simplify_tolerance: float = 1.0,\n    ) -&gt; List[Dict]:\n        \"\"\"\n        Convert mask to individual polygons with geospatial coordinates.\n\n        Args:\n            mask (np.ndarray): Binary mask.\n            transform: Rasterio transform object.\n            x_offset (int): X offset for tile position.\n            y_offset (int): Y offset for tile position.\n            min_area (int): Minimum area threshold for valid polygons.\n            simplify_tolerance (float): Tolerance for polygon simplification.\n\n        Returns:\n            List[Dict]: List of polygon dictionaries with geometry and properties.\n        \"\"\"\n        polygons = []\n\n        # Get individual instances\n        instances = self._separate_instances(mask, min_area)\n\n        for instance_mask in instances:\n            # Find contours\n            contours, _ = cv2.findContours(\n                instance_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            for contour in contours:\n                # Filter by minimum area\n                area = cv2.contourArea(contour)\n                if area &lt; min_area:\n                    continue\n\n                # Simplify contour\n                epsilon = simplify_tolerance\n                simplified_contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to pixel coordinates (add offsets)\n                pixel_coords = simplified_contour.reshape(-1, 2)\n                pixel_coords = pixel_coords + [x_offset, y_offset]\n\n                # Convert to geographic coordinates\n                geo_coords = []\n                for x, y in pixel_coords:\n                    geo_x, geo_y = transform * (x, y)\n                    geo_coords.append([geo_x, geo_y])\n\n                # Close the polygon if needed\n                if len(geo_coords) &gt; 2:\n                    if geo_coords[0] != geo_coords[-1]:\n                        geo_coords.append(geo_coords[0])\n\n                    # Create Shapely polygon\n                    try:\n                        polygon = Polygon(geo_coords)\n                        if polygon.is_valid and polygon.area &gt; 0:\n                            polygons.append({\"geometry\": polygon, \"area_pixels\": area})\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n                        continue\n\n        return polygons\n\n    def _segment(\n        self,\n        image: Image.Image,\n        detection_results: List[DetectionResult],\n        polygon_refinement: bool = False,\n    ) -&gt; List[DetectionResult]:\n        \"\"\"\n        Use SAM to generate masks for detected objects.\n\n        Args:\n            image (Image.Image): PIL image.\n            detection_results (List[DetectionResult]): Detection results from Grounding DINO.\n            polygon_refinement (bool): Whether to refine masks using polygon fitting.\n\n        Returns:\n            List[DetectionResult]: Detection results with masks.\n        \"\"\"\n        if not detection_results:\n            return detection_results\n\n        boxes = self._get_boxes(detection_results)\n        inputs = self.processor(\n            images=image, input_boxes=boxes, return_tensors=\"pt\"\n        ).to(self.device)\n\n        outputs = self.segmentator(**inputs)\n        masks = self.processor.post_process_masks(\n            masks=outputs.pred_masks,\n            original_sizes=inputs.original_sizes,\n            reshaped_input_sizes=inputs.reshaped_input_sizes,\n        )[0]\n\n        masks = self._refine_masks(masks, polygon_refinement)\n\n        for detection_result, mask in zip(detection_results, masks):\n            detection_result.mask = mask\n\n        return detection_results\n\n    def segment_image(\n        self,\n        input_path: str,\n        output_path: str,\n        text_prompts: Union[str, List[str]],\n        polygon_refinement: bool = False,\n        export_boxes: bool = False,\n        export_polygons: bool = True,\n        smoothing_sigma: float = 1.0,\n        nms_threshold: float = 0.5,\n        min_polygon_area: int = 50,\n        simplify_tolerance: float = 2.0,\n    ) -&gt; str:\n        \"\"\"\n        Segment a GeoTIFF image using text prompts with improved instance segmentation.\n\n        Args:\n            input_path (str): Path to the input GeoTIFF file.\n            output_path (str): Path where the output GeoTIFF will be saved.\n            text_prompts (Union[str, List[str]]): Text prompt(s) describing what to segment.\n            polygon_refinement (bool): Whether to refine masks using polygon fitting.\n            export_boxes (bool): Whether to export bounding boxes as a separate vector file.\n            export_polygons (bool): Whether to export segmentation polygons as vector file.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness.\n            nms_threshold (float): Non-maximum suppression threshold for removing overlapping detections.\n            min_polygon_area (int): Minimum area in pixels for valid polygons.\n            simplify_tolerance (float): Tolerance for polygon simplification.\n\n        Returns:\n            Dict: Dictionary containing paths to output files.\n        \"\"\"\n        if isinstance(text_prompts, str):\n            text_prompts = [text_prompts]\n\n        # Open the input GeoTIFF\n        with rasterio.open(input_path) as src:\n            # Get metadata\n            meta = src.meta\n            height = src.height\n            width = src.width\n            transform = src.transform\n            crs = src.crs\n\n            # Create output metadata for segmentation masks\n            out_meta = meta.copy()\n            out_meta.update(\n                {\"count\": len(text_prompts) + 1, \"dtype\": \"uint8\", \"nodata\": 0}\n            )\n\n            # Create arrays for results\n            all_masks = np.zeros((len(text_prompts), height, width), dtype=np.uint8)\n            all_boxes = []\n            all_polygons = []\n\n            # Calculate effective tile size (accounting for overlap)\n            effective_tile_size = self.tile_size - 2 * self.overlap\n\n            # Calculate number of tiles\n            n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n            n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n            total_tiles = n_tiles_x * n_tiles_y\n\n            print(f\"Processing {total_tiles} tiles ({n_tiles_x}x{n_tiles_y})\")\n\n            # Process tiles with tqdm progress bar\n            with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n                # Iterate through tiles\n                for y in range(n_tiles_y):\n                    for x in range(n_tiles_x):\n                        # Calculate tile coordinates with overlap\n                        x_start = max(0, x * effective_tile_size - self.overlap)\n                        y_start = max(0, y * effective_tile_size - self.overlap)\n                        x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                        y_end = min(\n                            height, (y + 1) * effective_tile_size + self.overlap\n                        )\n\n                        tile_width = x_end - x_start\n                        tile_height = y_end - y_start\n\n                        # Read the tile\n                        window = Window(x_start, y_start, tile_width, tile_height)\n                        tile_data = src.read(window=window)\n\n                        # Process the tile\n                        try:\n                            # Convert to RGB format for processing\n                            if tile_data.shape[0] &gt;= 3:\n                                # Use first three bands for RGB representation\n                                rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                            elif tile_data.shape[0] == 1:\n                                # Create RGB from grayscale\n                                rgb_tile = np.repeat(\n                                    tile_data[0][:, :, np.newaxis], 3, axis=2\n                                )\n                            else:\n                                print(\n                                    f\"Unsupported number of bands: {tile_data.shape[0]}\"\n                                )\n                                continue\n\n                            # Normalize to 0-255 range if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n\n                            # Convert to PIL Image\n                            pil_image = Image.fromarray(rgb_tile)\n\n                            # Detect objects\n                            detections = self._detect(pil_image, text_prompts)\n\n                            if detections:\n                                # Apply Non-Maximum Suppression to reduce overlapping detections\n                                detections = self._apply_nms(detections, nms_threshold)\n\n                                if detections:\n                                    # Segment objects\n                                    detections = self._segment(\n                                        pil_image, detections, polygon_refinement\n                                    )\n\n                                    # Process results\n                                    for i, prompt in enumerate(text_prompts):\n                                        prompt_polygons = []\n                                        prompt_mask = np.zeros(\n                                            (tile_height, tile_width), dtype=np.uint8\n                                        )\n\n                                        for detection in detections:\n                                            if (\n                                                detection.label.replace(\".\", \"\")\n                                                .strip()\n                                                .lower()\n                                                == prompt.lower()\n                                            ):\n                                                if detection.mask is not None:\n                                                    # Apply gaussian blur to reduce blockiness\n                                                    try:\n                                                        from scipy.ndimage import (\n                                                            gaussian_filter,\n                                                        )\n\n                                                        smoothed_mask = gaussian_filter(\n                                                            detection.mask.astype(\n                                                                float\n                                                            ),\n                                                            sigma=smoothing_sigma,\n                                                        )\n                                                        detection.mask = (\n                                                            smoothed_mask &gt; 0.5\n                                                        ).astype(np.uint8)\n                                                    except ImportError:\n                                                        pass\n\n                                                    # Add to combined mask for this prompt\n                                                    prompt_mask = np.maximum(\n                                                        prompt_mask, detection.mask\n                                                    )\n\n                                                # Store bounding box with geospatial coordinates\n                                                if export_boxes:\n                                                    bbox = detection.box\n                                                    x_geo_min, y_geo_min = transform * (\n                                                        x_start + bbox.xmin,\n                                                        y_start + bbox.ymin,\n                                                    )\n                                                    x_geo_max, y_geo_max = transform * (\n                                                        x_start + bbox.xmax,\n                                                        y_start + bbox.ymax,\n                                                    )\n\n                                                    geo_box = {\n                                                        \"label\": detection.label,\n                                                        \"score\": detection.score,\n                                                        \"prompt\": prompt,\n                                                        \"geometry\": box(\n                                                            x_geo_min,\n                                                            y_geo_max,\n                                                            x_geo_max,\n                                                            y_geo_min,\n                                                        ),\n                                                    }\n                                                    all_boxes.append(geo_box)\n\n                                        # Convert masks to individual polygons\n                                        if export_polygons and np.any(prompt_mask):\n                                            tile_polygons = self._mask_to_polygons(\n                                                prompt_mask,\n                                                transform,\n                                                x_start,\n                                                y_start,\n                                                min_polygon_area,\n                                                simplify_tolerance,\n                                            )\n\n                                            # Add metadata to polygons\n                                            for poly_data in tile_polygons:\n                                                poly_data.update(\n                                                    {\n                                                        \"label\": prompt,\n                                                        \"score\": max(\n                                                            [\n                                                                d.score\n                                                                for d in detections\n                                                                if d.label.replace(\n                                                                    \".\", \"\"\n                                                                )\n                                                                .strip()\n                                                                .lower()\n                                                                == prompt.lower()\n                                                            ],\n                                                            default=0.0,\n                                                        ),\n                                                        \"tile_x\": x,\n                                                        \"tile_y\": y,\n                                                    }\n                                                )\n                                                all_polygons.append(poly_data)\n\n                                        # Store mask in the global array\n                                        valid_x_start = self.overlap if x &gt; 0 else 0\n                                        valid_y_start = self.overlap if y &gt; 0 else 0\n                                        valid_x_end = (\n                                            tile_width - self.overlap\n                                            if x &lt; n_tiles_x - 1\n                                            else tile_width\n                                        )\n                                        valid_y_end = (\n                                            tile_height - self.overlap\n                                            if y &lt; n_tiles_y - 1\n                                            else tile_height\n                                        )\n\n                                        dest_x_start = x_start + valid_x_start\n                                        dest_y_start = y_start + valid_y_start\n                                        dest_x_end = x_start + valid_x_end\n                                        dest_y_end = y_start + valid_y_end\n\n                                        mask_slice = prompt_mask[\n                                            valid_y_start:valid_y_end,\n                                            valid_x_start:valid_x_end,\n                                        ]\n                                        all_masks[\n                                            i,\n                                            dest_y_start:dest_y_end,\n                                            dest_x_start:dest_x_end,\n                                        ] = np.maximum(\n                                            all_masks[\n                                                i,\n                                                dest_y_start:dest_y_end,\n                                                dest_x_start:dest_x_end,\n                                            ],\n                                            mask_slice,\n                                        )\n\n                        except Exception as e:\n                            print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                            continue\n\n                        # Update progress bar\n                        pbar.update(1)\n\n            # Create combined mask (union of all individual masks)\n            combined_mask = np.any(all_masks, axis=0).astype(np.uint8)\n\n            # Write the output GeoTIFF\n            with rasterio.open(output_path, \"w\", **out_meta) as dst:\n                # Write combined mask as first band\n                dst.write(combined_mask, 1)\n\n                # Write individual masks for each prompt\n                for i, mask in enumerate(all_masks):\n                    dst.write(mask, i + 2)\n\n                # Add descriptions to bands\n                dst.set_band_description(1, \"Combined Segmentation\")\n                for i, prompt in enumerate(text_prompts):\n                    dst.set_band_description(i + 2, f\"Segmentation: {prompt}\")\n\n            result_files = {\"segmentation\": output_path}\n\n            # Export bounding boxes if requested\n            if export_boxes and all_boxes:\n                boxes_path = output_path.replace(\".tif\", \"_boxes.geojson\")\n                gdf = gpd.GeoDataFrame(all_boxes, crs=crs)\n                gdf.to_file(boxes_path, driver=\"GeoJSON\")\n                result_files[\"boxes\"] = boxes_path\n                print(f\"Exported {len(all_boxes)} bounding boxes to {boxes_path}\")\n\n            # Export instance polygons if requested\n            if export_polygons and all_polygons:\n                polygons_path = output_path.replace(\".tif\", \"_polygons.geojson\")\n                gdf = gpd.GeoDataFrame(all_polygons, crs=crs)\n                gdf.to_file(polygons_path, driver=\"GeoJSON\")\n                result_files[\"polygons\"] = polygons_path\n                print(\n                    f\"Exported {len(all_polygons)} instance polygons to {polygons_path}\"\n                )\n\n            print(f\"Segmentation saved to {output_path}\")\n            print(\n                f\"Found {len(all_polygons)} individual building instances\"\n                if export_polygons\n                else \"\"\n            )\n\n            return result_files\n</code></pre>"},{"location":"geoai/#geoai.geoai.GroundedSAM.__init__","title":"<code>__init__(detector_id='IDEA-Research/grounding-dino-tiny', segmenter_id='facebook/sam-vit-base', device=None, tile_size=1024, overlap=128, threshold=0.3)</code>","text":"<p>Initialize the GroundedSAM with the specified models and settings.</p> <p>Parameters:</p> Name Type Description Default <code>detector_id</code> <code>str</code> <p>Hugging Face model ID for Grounding DINO.</p> <code>'IDEA-Research/grounding-dino-tiny'</code> <code>segmenter_id</code> <code>str</code> <p>Hugging Face model ID for SAM.</p> <code>'facebook/sam-vit-base'</code> <code>device</code> <code>str</code> <p>Device to run the models on ('cuda', 'cpu').</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts.</p> <code>128</code> <code>threshold</code> <code>float</code> <p>Detection threshold for Grounding DINO.</p> <code>0.3</code> Source code in <code>geoai/segment.py</code> <pre><code>def __init__(\n    self,\n    detector_id: str = \"IDEA-Research/grounding-dino-tiny\",\n    segmenter_id: str = \"facebook/sam-vit-base\",\n    device: Optional[str] = None,\n    tile_size: int = 1024,\n    overlap: int = 128,\n    threshold: float = 0.3,\n) -&gt; None:\n    \"\"\"\n    Initialize the GroundedSAM with the specified models and settings.\n\n    Args:\n        detector_id (str): Hugging Face model ID for Grounding DINO.\n        segmenter_id (str): Hugging Face model ID for SAM.\n        device (str): Device to run the models on ('cuda', 'cpu').\n        tile_size (int): Size of tiles to process the image in chunks.\n        overlap (int): Overlap between tiles to avoid edge artifacts.\n        threshold (float): Detection threshold for Grounding DINO.\n    \"\"\"\n    self.detector_id = detector_id\n    self.segmenter_id = segmenter_id\n    self.tile_size = tile_size\n    self.overlap = overlap\n    self.threshold = threshold\n\n    # Set device\n    if device is None:\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    else:\n        self.device = device\n\n    # Load models\n    self._load_models()\n\n    print(f\"GroundedSAM initialized on {self.device}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.GroundedSAM.segment_image","title":"<code>segment_image(input_path, output_path, text_prompts, polygon_refinement=False, export_boxes=False, export_polygons=True, smoothing_sigma=1.0, nms_threshold=0.5, min_polygon_area=50, simplify_tolerance=2.0)</code>","text":"<p>Segment a GeoTIFF image using text prompts with improved instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output GeoTIFF will be saved.</p> required <code>text_prompts</code> <code>Union[str, List[str]]</code> <p>Text prompt(s) describing what to segment.</p> required <code>polygon_refinement</code> <code>bool</code> <p>Whether to refine masks using polygon fitting.</p> <code>False</code> <code>export_boxes</code> <code>bool</code> <p>Whether to export bounding boxes as a separate vector file.</p> <code>False</code> <code>export_polygons</code> <code>bool</code> <p>Whether to export segmentation polygons as vector file.</p> <code>True</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness.</p> <code>1.0</code> <code>nms_threshold</code> <code>float</code> <p>Non-maximum suppression threshold for removing overlapping detections.</p> <code>0.5</code> <code>min_polygon_area</code> <code>int</code> <p>Minimum area in pixels for valid polygons.</p> <code>50</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for polygon simplification.</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>str</code> <p>Dictionary containing paths to output files.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image(\n    self,\n    input_path: str,\n    output_path: str,\n    text_prompts: Union[str, List[str]],\n    polygon_refinement: bool = False,\n    export_boxes: bool = False,\n    export_polygons: bool = True,\n    smoothing_sigma: float = 1.0,\n    nms_threshold: float = 0.5,\n    min_polygon_area: int = 50,\n    simplify_tolerance: float = 2.0,\n) -&gt; str:\n    \"\"\"\n    Segment a GeoTIFF image using text prompts with improved instance segmentation.\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str): Path where the output GeoTIFF will be saved.\n        text_prompts (Union[str, List[str]]): Text prompt(s) describing what to segment.\n        polygon_refinement (bool): Whether to refine masks using polygon fitting.\n        export_boxes (bool): Whether to export bounding boxes as a separate vector file.\n        export_polygons (bool): Whether to export segmentation polygons as vector file.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness.\n        nms_threshold (float): Non-maximum suppression threshold for removing overlapping detections.\n        min_polygon_area (int): Minimum area in pixels for valid polygons.\n        simplify_tolerance (float): Tolerance for polygon simplification.\n\n    Returns:\n        Dict: Dictionary containing paths to output files.\n    \"\"\"\n    if isinstance(text_prompts, str):\n        text_prompts = [text_prompts]\n\n    # Open the input GeoTIFF\n    with rasterio.open(input_path) as src:\n        # Get metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n        transform = src.transform\n        crs = src.crs\n\n        # Create output metadata for segmentation masks\n        out_meta = meta.copy()\n        out_meta.update(\n            {\"count\": len(text_prompts) + 1, \"dtype\": \"uint8\", \"nodata\": 0}\n        )\n\n        # Create arrays for results\n        all_masks = np.zeros((len(text_prompts), height, width), dtype=np.uint8)\n        all_boxes = []\n        all_polygons = []\n\n        # Calculate effective tile size (accounting for overlap)\n        effective_tile_size = self.tile_size - 2 * self.overlap\n\n        # Calculate number of tiles\n        n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n        n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n        total_tiles = n_tiles_x * n_tiles_y\n\n        print(f\"Processing {total_tiles} tiles ({n_tiles_x}x{n_tiles_y})\")\n\n        # Process tiles with tqdm progress bar\n        with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n            # Iterate through tiles\n            for y in range(n_tiles_y):\n                for x in range(n_tiles_x):\n                    # Calculate tile coordinates with overlap\n                    x_start = max(0, x * effective_tile_size - self.overlap)\n                    y_start = max(0, y * effective_tile_size - self.overlap)\n                    x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                    y_end = min(\n                        height, (y + 1) * effective_tile_size + self.overlap\n                    )\n\n                    tile_width = x_end - x_start\n                    tile_height = y_end - y_start\n\n                    # Read the tile\n                    window = Window(x_start, y_start, tile_width, tile_height)\n                    tile_data = src.read(window=window)\n\n                    # Process the tile\n                    try:\n                        # Convert to RGB format for processing\n                        if tile_data.shape[0] &gt;= 3:\n                            # Use first three bands for RGB representation\n                            rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                        elif tile_data.shape[0] == 1:\n                            # Create RGB from grayscale\n                            rgb_tile = np.repeat(\n                                tile_data[0][:, :, np.newaxis], 3, axis=2\n                            )\n                        else:\n                            print(\n                                f\"Unsupported number of bands: {tile_data.shape[0]}\"\n                            )\n                            continue\n\n                        # Normalize to 0-255 range if needed\n                        if rgb_tile.max() &gt; 0:\n                            rgb_tile = (\n                                (rgb_tile - rgb_tile.min())\n                                / (rgb_tile.max() - rgb_tile.min())\n                                * 255\n                            ).astype(np.uint8)\n\n                        # Convert to PIL Image\n                        pil_image = Image.fromarray(rgb_tile)\n\n                        # Detect objects\n                        detections = self._detect(pil_image, text_prompts)\n\n                        if detections:\n                            # Apply Non-Maximum Suppression to reduce overlapping detections\n                            detections = self._apply_nms(detections, nms_threshold)\n\n                            if detections:\n                                # Segment objects\n                                detections = self._segment(\n                                    pil_image, detections, polygon_refinement\n                                )\n\n                                # Process results\n                                for i, prompt in enumerate(text_prompts):\n                                    prompt_polygons = []\n                                    prompt_mask = np.zeros(\n                                        (tile_height, tile_width), dtype=np.uint8\n                                    )\n\n                                    for detection in detections:\n                                        if (\n                                            detection.label.replace(\".\", \"\")\n                                            .strip()\n                                            .lower()\n                                            == prompt.lower()\n                                        ):\n                                            if detection.mask is not None:\n                                                # Apply gaussian blur to reduce blockiness\n                                                try:\n                                                    from scipy.ndimage import (\n                                                        gaussian_filter,\n                                                    )\n\n                                                    smoothed_mask = gaussian_filter(\n                                                        detection.mask.astype(\n                                                            float\n                                                        ),\n                                                        sigma=smoothing_sigma,\n                                                    )\n                                                    detection.mask = (\n                                                        smoothed_mask &gt; 0.5\n                                                    ).astype(np.uint8)\n                                                except ImportError:\n                                                    pass\n\n                                                # Add to combined mask for this prompt\n                                                prompt_mask = np.maximum(\n                                                    prompt_mask, detection.mask\n                                                )\n\n                                            # Store bounding box with geospatial coordinates\n                                            if export_boxes:\n                                                bbox = detection.box\n                                                x_geo_min, y_geo_min = transform * (\n                                                    x_start + bbox.xmin,\n                                                    y_start + bbox.ymin,\n                                                )\n                                                x_geo_max, y_geo_max = transform * (\n                                                    x_start + bbox.xmax,\n                                                    y_start + bbox.ymax,\n                                                )\n\n                                                geo_box = {\n                                                    \"label\": detection.label,\n                                                    \"score\": detection.score,\n                                                    \"prompt\": prompt,\n                                                    \"geometry\": box(\n                                                        x_geo_min,\n                                                        y_geo_max,\n                                                        x_geo_max,\n                                                        y_geo_min,\n                                                    ),\n                                                }\n                                                all_boxes.append(geo_box)\n\n                                    # Convert masks to individual polygons\n                                    if export_polygons and np.any(prompt_mask):\n                                        tile_polygons = self._mask_to_polygons(\n                                            prompt_mask,\n                                            transform,\n                                            x_start,\n                                            y_start,\n                                            min_polygon_area,\n                                            simplify_tolerance,\n                                        )\n\n                                        # Add metadata to polygons\n                                        for poly_data in tile_polygons:\n                                            poly_data.update(\n                                                {\n                                                    \"label\": prompt,\n                                                    \"score\": max(\n                                                        [\n                                                            d.score\n                                                            for d in detections\n                                                            if d.label.replace(\n                                                                \".\", \"\"\n                                                            )\n                                                            .strip()\n                                                            .lower()\n                                                            == prompt.lower()\n                                                        ],\n                                                        default=0.0,\n                                                    ),\n                                                    \"tile_x\": x,\n                                                    \"tile_y\": y,\n                                                }\n                                            )\n                                            all_polygons.append(poly_data)\n\n                                    # Store mask in the global array\n                                    valid_x_start = self.overlap if x &gt; 0 else 0\n                                    valid_y_start = self.overlap if y &gt; 0 else 0\n                                    valid_x_end = (\n                                        tile_width - self.overlap\n                                        if x &lt; n_tiles_x - 1\n                                        else tile_width\n                                    )\n                                    valid_y_end = (\n                                        tile_height - self.overlap\n                                        if y &lt; n_tiles_y - 1\n                                        else tile_height\n                                    )\n\n                                    dest_x_start = x_start + valid_x_start\n                                    dest_y_start = y_start + valid_y_start\n                                    dest_x_end = x_start + valid_x_end\n                                    dest_y_end = y_start + valid_y_end\n\n                                    mask_slice = prompt_mask[\n                                        valid_y_start:valid_y_end,\n                                        valid_x_start:valid_x_end,\n                                    ]\n                                    all_masks[\n                                        i,\n                                        dest_y_start:dest_y_end,\n                                        dest_x_start:dest_x_end,\n                                    ] = np.maximum(\n                                        all_masks[\n                                            i,\n                                            dest_y_start:dest_y_end,\n                                            dest_x_start:dest_x_end,\n                                        ],\n                                        mask_slice,\n                                    )\n\n                    except Exception as e:\n                        print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                        continue\n\n                    # Update progress bar\n                    pbar.update(1)\n\n        # Create combined mask (union of all individual masks)\n        combined_mask = np.any(all_masks, axis=0).astype(np.uint8)\n\n        # Write the output GeoTIFF\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            # Write combined mask as first band\n            dst.write(combined_mask, 1)\n\n            # Write individual masks for each prompt\n            for i, mask in enumerate(all_masks):\n                dst.write(mask, i + 2)\n\n            # Add descriptions to bands\n            dst.set_band_description(1, \"Combined Segmentation\")\n            for i, prompt in enumerate(text_prompts):\n                dst.set_band_description(i + 2, f\"Segmentation: {prompt}\")\n\n        result_files = {\"segmentation\": output_path}\n\n        # Export bounding boxes if requested\n        if export_boxes and all_boxes:\n            boxes_path = output_path.replace(\".tif\", \"_boxes.geojson\")\n            gdf = gpd.GeoDataFrame(all_boxes, crs=crs)\n            gdf.to_file(boxes_path, driver=\"GeoJSON\")\n            result_files[\"boxes\"] = boxes_path\n            print(f\"Exported {len(all_boxes)} bounding boxes to {boxes_path}\")\n\n        # Export instance polygons if requested\n        if export_polygons and all_polygons:\n            polygons_path = output_path.replace(\".tif\", \"_polygons.geojson\")\n            gdf = gpd.GeoDataFrame(all_polygons, crs=crs)\n            gdf.to_file(polygons_path, driver=\"GeoJSON\")\n            result_files[\"polygons\"] = polygons_path\n            print(\n                f\"Exported {len(all_polygons)} instance polygons to {polygons_path}\"\n            )\n\n        print(f\"Segmentation saved to {output_path}\")\n        print(\n            f\"Found {len(all_polygons)} individual building instances\"\n            if export_polygons\n            else \"\"\n        )\n\n        return result_files\n</code></pre>"},{"location":"geoai/#geoai.geoai.LandcoverCrossEntropyLoss","title":"<code>LandcoverCrossEntropyLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Enhanced CrossEntropyLoss with optional ignore_index and class weights.</p> <p>This extends the standard CrossEntropyLoss with more flexible ignore_index handling, specifically designed for landcover classification tasks.</p> <p>Parameters:</p> Name Type Description Default <code>weight</code> <code>Optional[Tensor]</code> <p>Manual rescaling weight given to each class</p> <code>None</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Specifies a target value that is ignored (default: None) - None: No values ignored (standard behavior) - int: Specific class index to ignore (e.g., 0 for background)</p> <code>None</code> <code>reduction</code> <code>str</code> <p>Specifies the reduction to apply ('mean', 'sum', 'none')</p> <code>'mean'</code> Source code in <code>geoai/landcover_train.py</code> <pre><code>class LandcoverCrossEntropyLoss(nn.Module):\n    \"\"\"\n    Enhanced CrossEntropyLoss with optional ignore_index and class weights.\n\n    This extends the standard CrossEntropyLoss with more flexible ignore_index\n    handling, specifically designed for landcover classification tasks.\n\n    Args:\n        weight: Manual rescaling weight given to each class\n        ignore_index: Specifies a target value that is ignored (default: None)\n            - None: No values ignored (standard behavior)\n            - int: Specific class index to ignore (e.g., 0 for background)\n        reduction: Specifies the reduction to apply ('mean', 'sum', 'none')\n    \"\"\"\n\n    def __init__(\n        self,\n        weight: Optional[torch.Tensor] = None,\n        ignore_index: Optional[int] = None,\n        reduction: str = \"mean\",\n    ):\n        super().__init__()\n        self.weight = weight\n        self.ignore_index = ignore_index if ignore_index is not None else -100\n        self.reduction = reduction\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Compute cross entropy loss.\n\n        Args:\n            input: Predictions (N, C, H, W) where C = number of classes\n            target: Ground truth (N, H, W) with class indices\n\n        Returns:\n            Loss value\n        \"\"\"\n        return F.cross_entropy(\n            input,\n            target,\n            weight=self.weight,\n            ignore_index=self.ignore_index,\n            reduction=self.reduction,\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.LandcoverCrossEntropyLoss.forward","title":"<code>forward(input, target)</code>","text":"<p>Compute cross entropy loss.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>Predictions (N, C, H, W) where C = number of classes</p> required <code>target</code> <code>Tensor</code> <p>Ground truth (N, H, W) with class indices</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Loss value</p> Source code in <code>geoai/landcover_train.py</code> <pre><code>def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Compute cross entropy loss.\n\n    Args:\n        input: Predictions (N, C, H, W) where C = number of classes\n        target: Ground truth (N, H, W) with class indices\n\n    Returns:\n        Loss value\n    \"\"\"\n    return F.cross_entropy(\n        input,\n        target,\n        weight=self.weight,\n        ignore_index=self.ignore_index,\n        reduction=self.reduction,\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.LeafMap","title":"<code>LeafMap</code>","text":"<p>               Bases: <code>Map</code></p> <p>A subclass of leafmap.Map for GeoAI applications.</p> Source code in <code>geoai/geoai.py</code> <pre><code>class LeafMap(leafmap.Map):\n    \"\"\"A subclass of leafmap.Map for GeoAI applications.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the Map class.\"\"\"\n        super().__init__(*args, **kwargs)\n\n    def add_dinov3_gui(\n        self,\n        raster: str,\n        processor: \"DINOv3GeoProcessor\",\n        features: torch.Tensor,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Add a DINOv3 GUI to the map.\"\"\"\n        return DINOv3GUI(raster, processor, features, host_map=self, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.LeafMap.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the Map class.</p> Source code in <code>geoai/geoai.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the Map class.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.LeafMap.add_dinov3_gui","title":"<code>add_dinov3_gui(raster, processor, features, **kwargs)</code>","text":"<p>Add a DINOv3 GUI to the map.</p> Source code in <code>geoai/geoai.py</code> <pre><code>def add_dinov3_gui(\n    self,\n    raster: str,\n    processor: \"DINOv3GeoProcessor\",\n    features: torch.Tensor,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Add a DINOv3 GUI to the map.\"\"\"\n    return DINOv3GUI(raster, processor, features, host_map=self, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.Map","title":"<code>Map</code>","text":"<p>               Bases: <code>Map</code></p> <p>A subclass of maplibregl.Map for GeoAI applications.</p> Source code in <code>geoai/geoai.py</code> <pre><code>class Map(maplibregl.Map):\n    \"\"\"A subclass of maplibregl.Map for GeoAI applications.\"\"\"\n\n    def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n        \"\"\"Initialize the Map class.\"\"\"\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.Map.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialize the Map class.</p> Source code in <code>geoai/geoai.py</code> <pre><code>def __init__(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Initialize the Map class.\"\"\"\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector","title":"<code>ObjectDetector</code>","text":"<p>Object extraction using Mask R-CNN with TorchGeo.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ObjectDetector:\n    \"\"\"\n    Object extraction using Mask R-CNN with TorchGeo.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: Optional[str] = None,\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        num_classes: int = 2,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Hugging Face repository ID for model download.\n            model: Pre-initialized model object (optional).\n            num_classes: Number of classes for detection (default: 2).\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        # Set device\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = torch.device(device)\n\n        # Default parameters for object detection - these can be overridden in process_raster\n        self.chip_size = (512, 512)  # Size of image chips for processing\n        self.overlap = 0.25  # Default overlap between tiles\n        self.confidence_threshold = 0.5  # Default confidence threshold\n        self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n        self.min_object_area = 100  # Minimum area in pixels to keep an object\n        self.max_object_area = None  # Maximum area in pixels to keep an object\n        self.mask_threshold = 0.5  # Threshold for mask binarization\n        self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n        # Initialize model\n        self.model = self.initialize_model(model, num_classes=num_classes)\n\n        # Download model if needed\n        if model_path is None or (not os.path.exists(model_path)):\n            model_path = self.download_model_from_hf(model_path, repo_id)\n\n        # Load model weights\n        self.load_weights(model_path)\n\n        # Set model to evaluation mode\n        self.model.eval()\n\n    def download_model_from_hf(\n        self, model_path: Optional[str] = None, repo_id: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"\n        Download the object detection model from Hugging Face.\n\n        Args:\n            model_path: Path to the model file.\n            repo_id: Hugging Face repository ID.\n\n        Returns:\n            Path to the downloaded model file\n        \"\"\"\n        try:\n\n            print(\"Model path not specified, downloading from Hugging Face...\")\n\n            # Define the repository ID and model filename\n            if repo_id is None:\n                repo_id = \"giswqs/geoai\"\n\n            if model_path is None:\n                model_path = \"building_footprints_usa.pth\"\n\n            # Download the model\n            model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n            print(f\"Model downloaded to: {model_path}\")\n\n            return model_path\n\n        except Exception as e:\n            print(f\"Error downloading model from Hugging Face: {e}\")\n            print(\"Please specify a local model path or ensure internet connectivity.\")\n            raise\n\n    def initialize_model(self, model: Optional[Any], num_classes: int = 2) -&gt; Any:\n        \"\"\"Initialize a deep learning model for object detection.\n\n        Args:\n            model (torch.nn.Module): A pre-initialized model object.\n            num_classes (int): Number of classes for detection.\n\n        Returns:\n            torch.nn.Module: A deep learning model for object detection.\n        \"\"\"\n\n        if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n            # Standard image mean and std for pre-trained models\n            image_mean = [0.485, 0.456, 0.406]\n            image_std = [0.229, 0.224, 0.225]\n\n            # Create model with explicit normalization parameters\n            model = maskrcnn_resnet50_fpn(\n                weights=None,\n                progress=False,\n                num_classes=num_classes,  # Background + object\n                weights_backbone=None,\n                # These parameters ensure consistent normalization\n                image_mean=image_mean,\n                image_std=image_std,\n            )\n\n        model.to(self.device)\n        return model\n\n    def load_weights(self, model_path: str) -&gt; None:\n        \"\"\"\n        Load weights from file with error handling for different formats.\n\n        Args:\n            model_path: Path to model weights\n        \"\"\"\n        if not os.path.exists(model_path):\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n        try:\n            state_dict = torch.load(model_path, map_location=self.device)\n\n            # Handle different state dict formats\n            if isinstance(state_dict, dict):\n                if \"model\" in state_dict:\n                    state_dict = state_dict[\"model\"]\n                elif \"state_dict\" in state_dict:\n                    state_dict = state_dict[\"state_dict\"]\n\n            # Try to load state dict\n            try:\n                self.model.load_state_dict(state_dict)\n                print(\"Model loaded successfully\")\n            except Exception as e:\n                print(f\"Error loading model: {e}\")\n                print(\"Attempting to fix state_dict keys...\")\n\n                # Try to fix state_dict keys (remove module prefix if needed)\n                new_state_dict = {}\n                for k, v in state_dict.items():\n                    if k.startswith(\"module.\"):\n                        new_state_dict[k[7:]] = v\n                    else:\n                        new_state_dict[k] = v\n\n                self.model.load_state_dict(new_state_dict)\n                print(\"Model loaded successfully after key fixing\")\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model: {e}\")\n\n    def mask_to_polygons(self, mask: np.ndarray, **kwargs: Any) -&gt; List[Polygon]:\n        \"\"\"\n        Convert binary mask to polygon contours using OpenCV.\n\n        Args:\n            mask: Binary mask as numpy array\n            **kwargs: Optional parameters:\n                simplify_tolerance: Tolerance for polygon simplification\n                mask_threshold: Threshold for mask binarization\n                min_object_area: Minimum area in pixels to keep an object\n                max_object_area: Maximum area in pixels to keep an object\n\n        Returns:\n            List of polygons as lists of (x, y) coordinates\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n        # Ensure binary mask\n        mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n        # Optional: apply morphological operations to improve mask quality\n        kernel = np.ones((3, 3), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Convert to list of [x, y] coordinates\n        polygons = []\n        for contour in contours:\n            # Filter out too small contours\n            if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n                continue\n\n            # Filter out too large contours\n            if (\n                max_object_area is not None\n                and cv2.contourArea(contour) &gt; max_object_area\n            ):\n                continue\n\n            # Simplify contour if it has many points\n            if contour.shape[0] &gt; 50:\n                epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                contour = cv2.approxPolyDP(contour, epsilon, True)\n\n            # Convert to list of [x, y] coordinates\n            polygon = contour.reshape(-1, 2).tolist()\n            polygons.append(polygon)\n\n        return polygons\n\n    def filter_overlapping_polygons(\n        self, gdf: gpd.GeoDataFrame, **kwargs: Any\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Filter overlapping polygons using non-maximum suppression.\n\n        Args:\n            gdf: GeoDataFrame with polygons\n            **kwargs: Optional parameters:\n                nms_iou_threshold: IoU threshold for filtering\n\n        Returns:\n            Filtered GeoDataFrame\n        \"\"\"\n        if len(gdf) &lt;= 1:\n            return gdf\n\n        # Get parameters from kwargs or use instance defaults\n        iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n        # Sort by confidence\n        gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n        # Fix any invalid geometries\n        gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n            lambda geom: geom.buffer(0) if not geom.is_valid else geom\n        )\n\n        keep_indices = []\n        polygons = gdf.geometry.values\n\n        for i in range(len(polygons)):\n            if i in keep_indices:\n                continue\n\n            keep = True\n            for j in keep_indices:\n                # Skip invalid geometries\n                if not polygons[i].is_valid or not polygons[j].is_valid:\n                    continue\n\n                # Calculate IoU\n                try:\n                    intersection = polygons[i].intersection(polygons[j]).area\n                    union = polygons[i].area + polygons[j].area - intersection\n                    iou = intersection / union if union &gt; 0 else 0\n\n                    if iou &gt; iou_threshold:\n                        keep = False\n                        break\n                except Exception:\n                    # Skip on topology exceptions\n                    continue\n\n            if keep:\n                keep_indices.append(i)\n\n        return gdf.iloc[keep_indices]\n\n    def filter_edge_objects(\n        self, gdf: gpd.GeoDataFrame, raster_path: str, edge_buffer: int = 10\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Filter out object detections that fall in padding/edge areas of the image.\n\n        Args:\n            gdf: GeoDataFrame with object detections\n            raster_path: Path to the original raster file\n            edge_buffer: Buffer in pixels to consider as edge region\n\n        Returns:\n            GeoDataFrame with filtered objects\n        \"\"\"\n        import rasterio\n        from shapely.geometry import box\n\n        # If no objects detected, return empty GeoDataFrame\n        if gdf is None or len(gdf) == 0:\n            return gdf\n\n        print(f\"Objects before filtering: {len(gdf)}\")\n\n        with rasterio.open(raster_path) as src:\n            # Get raster bounds\n            raster_bounds = src.bounds\n            raster_width = src.width\n            raster_height = src.height\n\n            # Convert edge buffer from pixels to geographic units\n            # We need the smallest dimension of a pixel in geographic units\n            pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n            pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n            buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n            # Create a slightly smaller bounding box to exclude edge regions\n            inner_bounds = (\n                raster_bounds[0] + buffer_size,  # min x (west)\n                raster_bounds[1] + buffer_size,  # min y (south)\n                raster_bounds[2] - buffer_size,  # max x (east)\n                raster_bounds[3] - buffer_size,  # max y (north)\n            )\n\n            # Check that inner bounds are valid\n            if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n                print(\"Warning: Edge buffer too large, using original bounds\")\n                inner_box = box(*raster_bounds)\n            else:\n                inner_box = box(*inner_bounds)\n\n            # Filter out objects that intersect with the edge of the image\n            filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n            # Additional check for objects that have &gt;50% of their area outside the valid region\n            valid_objects = []\n            for idx, row in filtered_gdf.iterrows():\n                if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                    valid_objects.append(idx)\n\n            filtered_gdf = filtered_gdf.loc[valid_objects]\n\n            print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n            return filtered_gdf\n\n    def masks_to_vector(\n        self,\n        mask_path: str,\n        output_path: Optional[str] = None,\n        simplify_tolerance: Optional[float] = None,\n        mask_threshold: Optional[float] = None,\n        min_object_area: Optional[int] = None,\n        max_object_area: Optional[int] = None,\n        nms_iou_threshold: Optional[float] = None,\n        regularize: bool = True,\n        angle_threshold: int = 15,\n        rectangularity_threshold: float = 0.7,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n        Args:\n            mask_path: Path to the object masks GeoTIFF\n            output_path: Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)\n            simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n            mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n            min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n            max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n            regularize: Whether to regularize objects to right angles (default: True)\n            angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n            rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Use class defaults if parameters not provided\n        simplify_tolerance = (\n            simplify_tolerance\n            if simplify_tolerance is not None\n            else self.simplify_tolerance\n        )\n        mask_threshold = (\n            mask_threshold if mask_threshold is not None else self.mask_threshold\n        )\n        min_object_area = (\n            min_object_area if min_object_area is not None else self.min_object_area\n        )\n        max_object_area = (\n            max_object_area if max_object_area is not None else self.max_object_area\n        )\n        nms_iou_threshold = (\n            nms_iou_threshold\n            if nms_iou_threshold is not None\n            else self.nms_iou_threshold\n        )\n\n        # Set default output path if not provided\n        # if output_path is None:\n        #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n        print(f\"Converting mask to GeoJSON with parameters:\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Regularize objects: {regularize}\")\n        if regularize:\n            print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n            print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n        # Open the mask raster\n        with rasterio.open(mask_path) as src:\n            # Read the mask data\n            mask_data = src.read(1)\n            transform = src.transform\n            crs = src.crs\n\n            # Print mask statistics\n            print(f\"Mask dimensions: {mask_data.shape}\")\n            print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n            # Prepare for connected component analysis\n            # Binarize the mask based on threshold\n            binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n            # Apply morphological operations for better results (optional)\n            kernel = np.ones((3, 3), np.uint8)\n            binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n            # Find connected components\n            num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask, connectivity=8\n            )\n\n            print(\n                f\"Found {num_labels-1} potential objects\"\n            )  # Subtract 1 for background\n\n            # Create list to store polygons and confidence values\n            all_polygons = []\n            all_confidences = []\n\n            # Process each component (skip the first one which is background)\n            for i in tqdm(range(1, num_labels)):\n                # Extract this object\n                area = stats[i, cv2.CC_STAT_AREA]\n\n                # Skip if too small\n                if area &lt; min_object_area:\n                    continue\n\n                # Skip if too large\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Create a mask for this object\n                object_mask = (labels == i).astype(np.uint8)\n\n                # Find contours\n                contours, _ = cv2.findContours(\n                    object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n                )\n\n                # Process each contour\n                for contour in contours:\n                    # Skip if too few points\n                    if contour.shape[0] &lt; 3:\n                        continue\n\n                    # Simplify contour if it has many points\n                    if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                        epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                        contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                    # Convert to list of (x, y) coordinates\n                    polygon_points = contour.reshape(-1, 2)\n\n                    # Convert pixel coordinates to geographic coordinates\n                    geo_points = []\n                    for x, y in polygon_points:\n                        gx, gy = transform * (x, y)\n                        geo_points.append((gx, gy))\n\n                    # Create Shapely polygon\n                    if len(geo_points) &gt;= 3:\n                        try:\n                            shapely_poly = Polygon(geo_points)\n                            if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                all_polygons.append(shapely_poly)\n\n                                # Calculate \"confidence\" as normalized size\n                                # This is a proxy since we don't have model confidence scores\n                                normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                                all_confidences.append(normalized_size)\n                        except Exception as e:\n                            print(f\"Error creating polygon: {e}\")\n\n            print(f\"Created {len(all_polygons)} valid polygons\")\n\n            # Create GeoDataFrame\n            if not all_polygons:\n                print(\"No valid polygons found\")\n                return None\n\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": all_polygons,\n                    \"confidence\": all_confidences,\n                    \"class\": 1,  # Object class\n                },\n                crs=crs,\n            )\n\n            # Apply non-maximum suppression to remove overlapping polygons\n            gdf = self.filter_overlapping_polygons(\n                gdf, nms_iou_threshold=nms_iou_threshold\n            )\n\n            print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n            # Apply regularization if requested\n            if regularize and len(gdf) &gt; 0:\n                # Convert pixel area to geographic units for min_area parameter\n                # Estimate pixel size in geographic units\n                with rasterio.open(mask_path) as src:\n                    pixel_size_x = src.transform[\n                        0\n                    ]  # width of a pixel in geographic units\n                    pixel_size_y = abs(\n                        src.transform[4]\n                    )  # height of a pixel in geographic units\n                    avg_pixel_area = pixel_size_x * pixel_size_y\n\n                # Use 10 pixels as minimum area in geographic units\n                min_geo_area = 10 * avg_pixel_area\n\n                # Regularize objects\n                gdf = self.regularize_objects(\n                    gdf,\n                    min_area=min_geo_area,\n                    angle_threshold=angle_threshold,\n                    rectangularity_threshold=rectangularity_threshold,\n                )\n\n            # Save to file\n            if output_path:\n                if output_path.endswith(\".parquet\"):\n                    gdf.to_parquet(output_path)\n                else:\n                    gdf.to_file(output_path)\n                print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n            return gdf\n\n    @torch.no_grad()\n    def process_raster(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        batch_size: int = 4,\n        filter_edges: bool = True,\n        edge_buffer: int = 20,\n        band_indexes: Optional[List[int]] = None,\n        **kwargs: Any,\n    ) -&gt; \"gpd.GeoDataFrame\":\n        \"\"\"\n        Process a raster file to extract objects with customizable parameters.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoJSON or Parquet file (optional)\n            batch_size: Batch size for processing\n            filter_edges: Whether to filter out objects at the edges of the image\n            edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n            band_indexes: List of band indexes to use (if None, use all bands)\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                overlap: Overlap between adjacent tiles (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n                min_object_area: Minimum area in pixels to keep an object\n                simplify_tolerance: Tolerance for polygon simplification\n\n        Returns:\n            GeoDataFrame with objects\n        \"\"\"\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        overlap = kwargs.get(\"overlap\", self.overlap)\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n        max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n        simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n        # Print parameters being used\n        print(f\"Processing with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Tile overlap: {overlap}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n        print(f\"- Min object area: {min_object_area}\")\n        print(f\"- Max object area: {max_object_area}\")\n        print(f\"- Simplify tolerance: {simplify_tolerance}\")\n        print(f\"- Filter edge objects: {filter_edges}\")\n        if filter_edges:\n            print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            band_indexes=band_indexes,\n        )\n        self.raster_stats = dataset.raster_stats\n\n        # Custom collate function to handle Shapely objects\n        def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with simple indexing and custom collate\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        all_polygons = []\n        all_scores = []\n\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n            bboxes = batch[\n                \"bbox\"\n            ]  # Geographic bounding boxes - now a list, not a tensor\n\n            # Run inference\n            predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n                labels = prediction[\"labels\"].cpu().numpy()\n\n                # Skip if no predictions\n                if len(scores) == 0:\n                    continue\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n                labels = labels[valid_indices]\n\n                # Skip if no valid predictions\n                if len(scores) == 0:\n                    continue\n\n                # Get window coordinates\n                # The coords might be in different formats depending on batch handling\n                if isinstance(coords, list):\n                    # If coords is a list of tuples\n                    coord_item = coords[idx]\n                    if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                        i, j = coord_item\n                    elif isinstance(coord_item, torch.Tensor):\n                        i, j = coord_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords format: {type(coord_item)}\")\n                        continue\n                elif isinstance(coords, torch.Tensor):\n                    # If coords is a tensor of shape [batch_size, 2]\n                    i, j = coords[idx].cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords type: {type(coords)}\")\n                    continue\n\n                # Get window size\n                if isinstance(batch[\"window_size\"], list):\n                    window_item = batch[\"window_size\"][idx]\n                    if isinstance(window_item, tuple) and len(window_item) == 2:\n                        window_width, window_height = window_item\n                    elif isinstance(window_item, torch.Tensor):\n                        window_width, window_height = window_item.cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected window_size format: {type(window_item)}\")\n                        continue\n                elif isinstance(batch[\"window_size\"], torch.Tensor):\n                    window_width, window_height = (\n                        batch[\"window_size\"][idx].cpu().numpy().tolist()\n                    )\n                else:\n                    print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                    continue\n\n                # Process masks to polygons\n                for mask_idx, mask in enumerate(masks):\n                    # Get binary mask\n                    binary_mask = mask[0]  # Get binary mask\n\n                    # Convert mask to polygon with custom parameters\n                    contours = self.mask_to_polygons(\n                        binary_mask,\n                        simplify_tolerance=simplify_tolerance,\n                        mask_threshold=mask_threshold,\n                        min_object_area=min_object_area,\n                        max_object_area=max_object_area,\n                    )\n\n                    # Skip if no valid polygons\n                    if not contours:\n                        continue\n\n                    # Transform polygons to geographic coordinates\n                    with rasterio.open(raster_path) as src:\n                        transform = src.transform\n\n                        for contour in contours:\n                            # Convert polygon to global coordinates\n                            global_polygon = []\n                            for x, y in contour:\n                                # Adjust coordinates based on window position\n                                gx, gy = transform * (i + x, j + y)\n                                global_polygon.append((gx, gy))\n\n                            # Create Shapely polygon\n                            if len(global_polygon) &gt;= 3:\n                                try:\n                                    shapely_poly = Polygon(global_polygon)\n                                    if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                        all_polygons.append(shapely_poly)\n                                        all_scores.append(float(scores[mask_idx]))\n                                except Exception as e:\n                                    print(f\"Error creating polygon: {e}\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_scores,\n                \"class\": 1,  # Object class\n            },\n            crs=dataset.crs,\n        )\n\n        # Remove overlapping polygons with custom threshold\n        gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        # Filter edge objects if requested\n        if filter_edges:\n            gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n        # Save to file if requested\n        if output_path:\n            if output_path.endswith(\".parquet\"):\n                gdf.to_parquet(output_path)\n            else:\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n\n    def save_masks_as_geotiff(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        batch_size: int = 4,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Process a raster file to extract object masks and save as GeoTIFF.\n\n        Args:\n            raster_path: Path to input raster file\n            output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n            batch_size: Batch size for processing\n            verbose: Whether to print detailed processing information\n            **kwargs: Additional parameters:\n                confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n                chip_size: Size of image chips for processing (height, width)\n                mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n        Returns:\n            Path to the saved GeoTIFF file\n        \"\"\"\n\n        # Get parameters from kwargs or use instance defaults\n        confidence_threshold = kwargs.get(\n            \"confidence_threshold\", self.confidence_threshold\n        )\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n        mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n        overlap = kwargs.get(\"overlap\", self.overlap)\n\n        # Set default output path if not provided\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n        # Print parameters being used\n        print(f\"Processing masks with parameters:\")\n        print(f\"- Confidence threshold: {confidence_threshold}\")\n        print(f\"- Chip size: {chip_size}\")\n        print(f\"- Mask threshold: {mask_threshold}\")\n\n        # Create dataset\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            verbose=verbose,\n        )\n\n        # Store a flag to avoid repetitive messages\n        self.raster_stats = dataset.raster_stats\n        seen_warnings = {\n            \"bands\": False,\n            \"resize\": {},  # Dictionary to track resize warnings by shape\n        }\n\n        # Open original raster to get metadata\n        with rasterio.open(raster_path) as src:\n            # Create output binary mask raster with same dimensions as input\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=1,  # Single band for object mask\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Create output mask raster\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                # Initialize mask with zeros\n                mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n                # Custom collate function to handle Shapely objects\n                def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n                    \"\"\"Custom collate function for DataLoader\"\"\"\n                    elem = batch[0]\n                    if isinstance(elem, dict):\n                        result = {}\n                        for key in elem:\n                            if key == \"bbox\":\n                                # Don't collate shapely objects, keep as list\n                                result[key] = [d[key] for d in batch]\n                            else:\n                                # For tensors and other collatable types\n                                try:\n                                    result[key] = (\n                                        torch.utils.data._utils.collate.default_collate(\n                                            [d[key] for d in batch]\n                                        )\n                                    )\n                                except TypeError:\n                                    # Fall back to list for non-collatable types\n                                    result[key] = [d[key] for d in batch]\n                        return result\n                    else:\n                        # Default collate for non-dict types\n                        return torch.utils.data._utils.collate.default_collate(batch)\n\n                # Create dataloader\n                dataloader = torch.utils.data.DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=False,\n                    num_workers=0,\n                    collate_fn=custom_collate,\n                )\n\n                # Process batches\n                print(f\"Processing raster with {len(dataloader)} batches\")\n                for batch in tqdm(dataloader):\n                    # Move images to device\n                    images = batch[\"image\"].to(self.device)\n                    coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                    # Run inference\n                    with torch.no_grad():\n                        predictions = self.model(images)\n\n                    # Process predictions\n                    for idx, prediction in enumerate(predictions):\n                        masks = prediction[\"masks\"].cpu().numpy()\n                        scores = prediction[\"scores\"].cpu().numpy()\n\n                        # Skip if no predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Filter by confidence threshold\n                        valid_indices = scores &gt;= confidence_threshold\n                        masks = masks[valid_indices]\n                        scores = scores[valid_indices]\n\n                        # Skip if no valid predictions\n                        if len(scores) == 0:\n                            continue\n\n                        # Get window coordinates\n                        if isinstance(coords, list):\n                            coord_item = coords[idx]\n                            if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                                i, j = coord_item\n                            elif isinstance(coord_item, torch.Tensor):\n                                i, j = coord_item.cpu().numpy().tolist()\n                            else:\n                                print(f\"Unexpected coords format: {type(coord_item)}\")\n                                continue\n                        elif isinstance(coords, torch.Tensor):\n                            i, j = coords[idx].cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords type: {type(coords)}\")\n                            continue\n\n                        # Get window size\n                        if isinstance(batch[\"window_size\"], list):\n                            window_item = batch[\"window_size\"][idx]\n                            if isinstance(window_item, tuple) and len(window_item) == 2:\n                                window_width, window_height = window_item\n                            elif isinstance(window_item, torch.Tensor):\n                                window_width, window_height = (\n                                    window_item.cpu().numpy().tolist()\n                                )\n                            else:\n                                print(\n                                    f\"Unexpected window_size format: {type(window_item)}\"\n                                )\n                                continue\n                        elif isinstance(batch[\"window_size\"], torch.Tensor):\n                            window_width, window_height = (\n                                batch[\"window_size\"][idx].cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                            )\n                            continue\n\n                        # Combine all masks for this window\n                        combined_mask = np.zeros(\n                            (window_height, window_width), dtype=np.uint8\n                        )\n\n                        for mask in masks:\n                            # Get the binary mask\n                            binary_mask = (mask[0] &gt; mask_threshold).astype(\n                                np.uint8\n                            ) * 255\n\n                            # Handle size mismatch - resize binary_mask if needed\n                            mask_h, mask_w = binary_mask.shape\n                            if mask_h != window_height or mask_w != window_width:\n                                resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                                if resize_key not in seen_warnings[\"resize\"]:\n                                    if verbose:\n                                        print(\n                                            f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                        )\n                                    else:\n                                        if not seen_warnings[\n                                            \"resize\"\n                                        ]:  # If this is the first resize warning\n                                            print(\n                                                f\"Resizing masks at image edges (set verbose=True for details)\"\n                                            )\n                                    seen_warnings[\"resize\"][resize_key] = True\n\n                                # Crop or pad the binary mask to match window size\n                                resized_mask = np.zeros(\n                                    (window_height, window_width), dtype=np.uint8\n                                )\n                                copy_h = min(mask_h, window_height)\n                                copy_w = min(mask_w, window_width)\n                                resized_mask[:copy_h, :copy_w] = binary_mask[\n                                    :copy_h, :copy_w\n                                ]\n                                binary_mask = resized_mask\n\n                            # Update combined mask (taking maximum where masks overlap)\n                            combined_mask = np.maximum(combined_mask, binary_mask)\n\n                        # Write combined mask to output array\n                        # Handle edge cases where window might be smaller than chip size\n                        h, w = combined_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                combined_mask[:valid_h, :valid_w],\n                            )\n\n                # Write the final mask to the output file\n                dst.write(mask_array, 1)\n\n        print(f\"Object masks saved to {output_path}\")\n        return output_path\n\n    def regularize_objects(\n        self,\n        gdf: gpd.GeoDataFrame,\n        min_area: int = 10,\n        angle_threshold: int = 15,\n        orthogonality_threshold: float = 0.3,\n        rectangularity_threshold: float = 0.7,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Regularize objects to enforce right angles and rectangular shapes.\n\n        Args:\n            gdf: GeoDataFrame with objects\n            min_area: Minimum area in square units to keep an object\n            angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n            orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n            rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n        Returns:\n            GeoDataFrame with regularized objects\n        \"\"\"\n        import math\n\n        import cv2\n        import geopandas as gpd\n        import numpy as np\n        from shapely.affinity import rotate, translate\n        from shapely.geometry import MultiPolygon, Polygon, box\n        from tqdm import tqdm\n\n        def get_angle(\n            p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]\n        ) -&gt; float:\n            \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n            a = np.array(p1)\n            b = np.array(p2)\n            c = np.array(p3)\n\n            ba = a - b\n            bc = c - b\n\n            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n            # Handle numerical errors that could push cosine outside [-1, 1]\n            cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n            angle = np.degrees(np.arccos(cosine_angle))\n\n            return angle\n\n        def is_orthogonal(angle: float, threshold: int = angle_threshold) -&gt; bool:\n            \"\"\"Check if angle is close to 90 degrees\"\"\"\n            return abs(angle - 90) &lt;= threshold\n\n        def calculate_dominant_direction(polygon: Polygon) -&gt; float:\n            \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n            # Extract coordinates\n            coords = np.array(polygon.exterior.coords)\n\n            # Mean center the coordinates\n            mean = np.mean(coords, axis=0)\n            centered_coords = coords - mean\n\n            # Calculate covariance matrix and its eigenvalues/eigenvectors\n            cov_matrix = np.cov(centered_coords.T)\n            eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n            # Get the index of the largest eigenvalue\n            largest_idx = np.argmax(eigenvalues)\n\n            # Get the corresponding eigenvector (principal axis)\n            principal_axis = eigenvectors[:, largest_idx]\n\n            # Calculate the angle in degrees\n            angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n            angle_deg = np.degrees(angle_rad)\n\n            # Normalize to range 0-180\n            if angle_deg &lt; 0:\n                angle_deg += 180\n\n            return angle_deg\n\n        def create_oriented_envelope(polygon: Polygon, angle_deg: float) -&gt; Polygon:\n            \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n            # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n            coords = np.array(polygon.exterior.coords)[:-1].astype(\n                np.float32\n            )  # Skip the last point (same as first)\n\n            # Use OpenCV's minAreaRect\n            rect = cv2.minAreaRect(coords)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to shapely polygon\n            oriented_box = Polygon(box_points)\n\n            return oriented_box\n\n        def get_rectangularity(polygon: Polygon, oriented_box: Polygon) -&gt; float:\n            \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n            if oriented_box.area == 0:\n                return 0\n            return polygon.area / oriented_box.area\n\n        def check_orthogonality(polygon: Polygon) -&gt; float:\n            \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n            coords = list(polygon.exterior.coords)\n            if len(coords) &lt;= 4:  # Triangle or point\n                return 0\n\n            # Remove last point (same as first)\n            coords = coords[:-1]\n\n            orthogonal_count = 0\n            total_angles = len(coords)\n\n            for i in range(total_angles):\n                p1 = coords[i]\n                p2 = coords[(i + 1) % total_angles]\n                p3 = coords[(i + 2) % total_angles]\n\n                angle = get_angle(p1, p2, p3)\n                if is_orthogonal(angle):\n                    orthogonal_count += 1\n\n            return orthogonal_count / total_angles\n\n        def simplify_to_rectangle(polygon: Polygon) -&gt; Polygon:\n            \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n            # Get dominant direction\n            angle = calculate_dominant_direction(polygon)\n\n            # Create oriented envelope\n            rect = create_oriented_envelope(polygon, angle)\n\n            return rect\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No Objects to regularize\")\n            return gdf\n\n        print(f\"Regularizing {len(gdf)} objects...\")\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n        print(\n            f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n        )\n\n        # Create a copy to avoid modifying the original\n        result_gdf = gdf.copy()\n\n        # Track statistics\n        total_objects = len(gdf)\n        regularized_count = 0\n        rectangularized_count = 0\n\n        # Process each Object\n        for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n            geom = row.geometry\n\n            # Skip invalid or empty geometries\n            if geom is None or geom.is_empty:\n                continue\n\n            # Handle MultiPolygons by processing the largest part\n            if isinstance(geom, MultiPolygon):\n                areas = [p.area for p in geom.geoms]\n                if not areas:\n                    continue\n                geom = list(geom.geoms)[np.argmax(areas)]\n\n            # Filter out tiny Objects\n            if geom.area &lt; min_area:\n                continue\n\n            # Check orthogonality\n            orthogonality = check_orthogonality(geom)\n\n            # Create oriented envelope\n            oriented_box = create_oriented_envelope(\n                geom, calculate_dominant_direction(geom)\n            )\n\n            # Check rectangularity\n            rectangularity = get_rectangularity(geom, oriented_box)\n\n            # Decide how to regularize\n            if rectangularity &gt;= rectangularity_threshold:\n                # Object is already quite rectangular, simplify to a rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n                rectangularized_count += 1\n            elif orthogonality &gt;= orthogonality_threshold:\n                # Object has many orthogonal angles but isn't rectangular\n                # Could implement more sophisticated regularization here\n                # For now, we'll still use the oriented rectangle\n                result_gdf.at[idx, \"geometry\"] = oriented_box\n                result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n                regularized_count += 1\n            else:\n                # Object doesn't have clear orthogonal structure\n                # Keep original but flag as unmodified\n                result_gdf.at[idx, \"regularized\"] = \"original\"\n\n        # Report statistics\n        print(f\"Regularization completed:\")\n        print(f\"- Total objects: {total_objects}\")\n        print(\n            f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n        )\n        print(\n            f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n        )\n\n        return result_gdf\n\n    def visualize_results(\n        self,\n        raster_path: str,\n        gdf: Optional[gpd.GeoDataFrame] = None,\n        output_path: Optional[str] = None,\n        figsize: Tuple[int, int] = (12, 12),\n    ) -&gt; bool:\n        \"\"\"\n        Visualize object detection results with proper coordinate transformation.\n\n        This function displays objects on top of the raster image,\n        ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n        Args:\n            raster_path: Path to input raster\n            gdf: GeoDataFrame with object polygons (optional)\n            output_path: Path to save visualization (optional)\n            figsize: Figure size (width, height) in inches\n\n        Returns:\n            bool: True if visualization was successful\n        \"\"\"\n        # Check if raster file exists\n        if not os.path.exists(raster_path):\n            print(f\"Error: Raster file '{raster_path}' not found.\")\n            return False\n\n        # Process raster if GeoDataFrame not provided\n        if gdf is None:\n            gdf = self.process_raster(raster_path)\n\n        if gdf is None or len(gdf) == 0:\n            print(\"No objects to visualize\")\n            return False\n\n        # Check if confidence column exists in the GeoDataFrame\n        has_confidence = False\n        if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n            # Try to access a confidence value to confirm it works\n            try:\n                if len(gdf) &gt; 0:\n                    # Try getitem access\n                    conf_val = gdf[\"confidence\"].iloc[0]\n                    has_confidence = True\n                    print(\n                        f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                    )\n            except Exception as e:\n                print(f\"Confidence column exists but couldn't access values: {e}\")\n                has_confidence = False\n        else:\n            print(\"No confidence column found in GeoDataFrame\")\n            has_confidence = False\n\n        # Read raster for visualization\n        with rasterio.open(raster_path) as src:\n            # Read the entire image or a subset if it's very large\n            if src.height &gt; 2000 or src.width &gt; 2000:\n                # Calculate scale factor to reduce size\n                scale = min(2000 / src.height, 2000 / src.width)\n                out_shape = (\n                    int(src.count),\n                    int(src.height * scale),\n                    int(src.width * scale),\n                )\n\n                # Read and resample\n                image = src.read(\n                    out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n                )\n\n                # Create a scaled transform for the resampled image\n                # Calculate scaling factors\n                x_scale = src.width / out_shape[2]\n                y_scale = src.height / out_shape[1]\n\n                # Get the original transform\n                orig_transform = src.transform\n\n                # Create a scaled transform\n                scaled_transform = rasterio.transform.Affine(\n                    orig_transform.a * x_scale,\n                    orig_transform.b,\n                    orig_transform.c,\n                    orig_transform.d,\n                    orig_transform.e * y_scale,\n                    orig_transform.f,\n                )\n            else:\n                image = src.read()\n                scaled_transform = src.transform\n\n            # Convert to RGB for display\n            if image.shape[0] &gt; 3:\n                image = image[:3]\n            elif image.shape[0] == 1:\n                image = np.repeat(image, 3, axis=0)\n\n            # Normalize image for display\n            image = image.transpose(1, 2, 0)  # CHW to HWC\n            image = image.astype(np.float32)\n\n            if image.max() &gt; 10:  # Likely 0-255 range\n                image = image / 255.0\n\n            image = np.clip(image, 0, 1)\n\n            # Get image bounds\n            bounds = src.bounds\n            crs = src.crs\n\n        # Create figure with appropriate aspect ratio\n        aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n        plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n        ax = plt.gca()\n\n        # Display image\n        ax.imshow(image)\n\n        # Make sure the GeoDataFrame has the same CRS as the raster\n        if gdf.crs != crs:\n            print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n            gdf = gdf.to_crs(crs)\n\n        # Set up colors for confidence visualization\n        if has_confidence:\n            try:\n                import matplotlib.cm as cm\n                from matplotlib.colors import Normalize\n\n                # Get min/max confidence values\n                min_conf = gdf[\"confidence\"].min()\n                max_conf = gdf[\"confidence\"].max()\n\n                # Set up normalization and colormap\n                norm = Normalize(vmin=min_conf, vmax=max_conf)\n                cmap = cm.viridis\n\n                # Create scalar mappable for colorbar\n                sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                sm.set_array([])\n\n                # Add colorbar\n                cbar = plt.colorbar(\n                    sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n                )\n                cbar.set_label(\"Confidence Score\")\n            except Exception as e:\n                print(f\"Error setting up confidence visualization: {e}\")\n                has_confidence = False\n\n        # Function to convert coordinates\n        def geo_to_pixel(\n            geometry: Any, transform: Any\n        ) -&gt; Optional[Tuple[List[float], List[float]]]:\n            \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n            if geometry.is_empty:\n                return None\n\n            if geometry.geom_type == \"Polygon\":\n                # Get exterior coordinates\n                exterior_coords = list(geometry.exterior.coords)\n\n                # Convert to pixel coordinates\n                pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n                # Split into x and y lists\n                pixel_x = [coord[0] for coord in pixel_coords]\n                pixel_y = [coord[1] for coord in pixel_coords]\n\n                return pixel_x, pixel_y\n            else:\n                print(f\"Unsupported geometry type: {geometry.geom_type}\")\n                return None\n\n        # Plot each object\n        for idx, row in gdf.iterrows():\n            try:\n                # Convert polygon to pixel coordinates\n                coords = geo_to_pixel(row.geometry, scaled_transform)\n\n                if coords:\n                    pixel_x, pixel_y = coords\n\n                    if has_confidence:\n                        try:\n                            # Get confidence value using different methods\n                            # Method 1: Try direct attribute access\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            # Method 2: Try dictionary-style access\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            # Method 3: Try accessing by index from the GeoDataFrame\n                            if confidence is None:\n                                try:\n                                    confidence = gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1,\n                                    alpha=0.8,\n                                )\n                            else:\n                                # Fall back to red if confidence value couldn't be accessed\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence value for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    else:\n                        # No confidence data, just plot outlines in red\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n            except Exception as e:\n                print(f\"Error plotting polygon {idx}: {e}\")\n\n        # Remove axes\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n        # Save if requested\n        if output_path:\n            plt.tight_layout()\n            plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n            print(f\"Visualization saved to {output_path}\")\n\n        plt.close()\n\n        # Create a simpler visualization focused just on a subset of objects\n        if len(gdf) &gt; 0:\n            plt.figure(figsize=figsize)\n            ax = plt.gca()\n\n            # Choose a subset of the image to show\n            with rasterio.open(raster_path) as src:\n                # Get centroid of first object\n                sample_geom = gdf.iloc[0].geometry\n                centroid = sample_geom.centroid\n\n                # Convert to pixel coordinates\n                center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n                # Define a window around this object\n                window_size = 500  # pixels\n                window = rasterio.windows.Window(\n                    max(0, int(center_x - window_size / 2)),\n                    max(0, int(center_y - window_size / 2)),\n                    min(window_size, src.width - int(center_x - window_size / 2)),\n                    min(window_size, src.height - int(center_y - window_size / 2)),\n                )\n\n                # Read this window\n                sample_image = src.read(window=window)\n\n                # Convert to RGB for display\n                if sample_image.shape[0] &gt; 3:\n                    sample_image = sample_image[:3]\n                elif sample_image.shape[0] == 1:\n                    sample_image = np.repeat(sample_image, 3, axis=0)\n\n                # Normalize image for display\n                sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n                sample_image = sample_image.astype(np.float32)\n\n                if sample_image.max() &gt; 10:  # Likely 0-255 range\n                    sample_image = sample_image / 255.0\n\n                sample_image = np.clip(sample_image, 0, 1)\n\n                # Display sample image\n                ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n                # Get the correct transform for this window\n                window_transform = src.window_transform(window)\n\n                # Calculate bounds of the window\n                window_bounds = rasterio.windows.bounds(window, src.transform)\n                window_box = box(*window_bounds)\n\n                # Filter objects that intersect with this window\n                visible_gdf = gdf[gdf.intersects(window_box)]\n\n                # Set up colors for sample view if confidence data exists\n                if has_confidence:\n                    try:\n                        # Reuse the same normalization and colormap from main view\n                        sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                        sample_sm.set_array([])\n\n                        # Add colorbar to sample view\n                        sample_cbar = plt.colorbar(\n                            sample_sm,\n                            ax=ax,\n                            orientation=\"vertical\",\n                            shrink=0.7,\n                            pad=0.01,\n                        )\n                        sample_cbar.set_label(\"Confidence Score\")\n                    except Exception as e:\n                        print(f\"Error setting up sample confidence visualization: {e}\")\n\n                # Plot objects in sample view\n                for idx, row in visible_gdf.iterrows():\n                    try:\n                        # Get window-relative pixel coordinates\n                        geom = row.geometry\n\n                        # Skip empty geometries\n                        if geom.is_empty:\n                            continue\n\n                        # Get exterior coordinates\n                        exterior_coords = list(geom.exterior.coords)\n\n                        # Convert to pixel coordinates relative to window origin\n                        pixel_coords = []\n                        for x, y in exterior_coords:\n                            px, py = ~src.transform * (x, y)  # Convert to image pixels\n                            # Make coordinates relative to window\n                            px = px - window.col_off\n                            py = py - window.row_off\n                            pixel_coords.append((px, py))\n\n                        # Extract x and y coordinates\n                        pixel_x = [coord[0] for coord in pixel_coords]\n                        pixel_y = [coord[1] for coord in pixel_coords]\n\n                        # Use confidence colors if available\n                        if has_confidence:\n                            try:\n                                # Try different methods to access confidence\n                                confidence = None\n                                try:\n                                    confidence = row.confidence\n                                except:\n                                    pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = row[\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is None:\n                                    try:\n                                        confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                    except:\n                                        pass\n\n                                if confidence is not None:\n                                    color = cmap(norm(confidence))\n                                    # Fill polygon with semi-transparent color\n                                    ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                    # Draw border\n                                    ax.plot(\n                                        pixel_x,\n                                        pixel_y,\n                                        color=color,\n                                        linewidth=1.5,\n                                        alpha=0.8,\n                                    )\n                                else:\n                                    ax.plot(\n                                        pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                    )\n                            except Exception as e:\n                                print(\n                                    f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                                )\n                                ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                        else:\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    except Exception as e:\n                        print(f\"Error plotting polygon in sample view: {e}\")\n\n                # Set title\n                ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n                # Remove axes\n                ax.set_xticks([])\n                ax.set_yticks([])\n\n                # Save if requested\n                if output_path:\n                    sample_output = (\n                        os.path.splitext(output_path)[0]\n                        + \"_sample\"\n                        + os.path.splitext(output_path)[1]\n                    )\n                    plt.tight_layout()\n                    plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                    print(f\"Sample visualization saved to {sample_output}\")\n\n    def generate_masks(\n        self,\n        raster_path: str,\n        output_path: Optional[str] = None,\n        confidence_threshold: Optional[float] = None,\n        mask_threshold: Optional[float] = None,\n        min_object_area: int = 10,\n        max_object_area: float = float(\"inf\"),\n        overlap: float = 0.25,\n        batch_size: int = 4,\n        band_indexes: Optional[List[int]] = None,\n        verbose: bool = False,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Save masks with confidence values as a multi-band GeoTIFF.\n\n        Objects with area smaller than min_object_area or larger than max_object_area\n        will be filtered out.\n\n        Args:\n            raster_path: Path to input raster\n            output_path: Path for output GeoTIFF\n            confidence_threshold: Minimum confidence score (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area (in pixels) for an object to be included\n            max_object_area: Maximum area (in pixels) for an object to be included\n            overlap: Overlap between tiles (0.0-1.0)\n            batch_size: Batch size for processing\n            band_indexes: List of band indexes to use (default: all bands)\n            verbose: Whether to print detailed processing information\n\n        Returns:\n            Path to the saved GeoTIFF\n        \"\"\"\n        # Use provided thresholds or fall back to instance defaults\n        if confidence_threshold is None:\n            confidence_threshold = self.confidence_threshold\n        if mask_threshold is None:\n            mask_threshold = self.mask_threshold\n\n        chip_size = kwargs.get(\"chip_size\", self.chip_size)\n\n        # Default output path\n        if output_path is None:\n            output_path = os.path.splitext(raster_path)[0] + \"_masks_conf.tif\"\n\n        # Process the raster to get individual masks with confidence\n        with rasterio.open(raster_path) as src:\n            # Create dataset with the specified overlap\n            dataset = CustomDataset(\n                raster_path=raster_path,\n                chip_size=chip_size,\n                overlap=overlap,\n                band_indexes=band_indexes,\n                verbose=verbose,\n            )\n\n            # Create output profile\n            output_profile = src.profile.copy()\n            output_profile.update(\n                dtype=rasterio.uint8,\n                count=2,  # Two bands: mask and confidence\n                compress=\"lzw\",\n                nodata=0,\n            )\n\n            # Initialize mask and confidence arrays\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n            conf_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Define custom collate function to handle Shapely objects\n            def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n                \"\"\"\n                Custom collate function that handles Shapely geometries\n                by keeping them as Python objects rather than trying to collate them.\n                \"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader with custom collate function\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # Tensor of shape [batch_size, 2]\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(masks) == 0:\n                        continue\n\n                    # Get window coordinates\n                    i, j = coords[idx].cpu().numpy()\n\n                    # Process each mask\n                    for mask_idx, mask in enumerate(masks):\n                        # Convert to binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(np.uint8) * 255\n\n                        # Check object area - calculate number of pixels in the mask\n                        object_area = np.sum(binary_mask &gt; 0)\n\n                        # Skip objects that don't meet area criteria\n                        if (\n                            object_area &lt; min_object_area\n                            or object_area &gt; max_object_area\n                        ):\n                            if verbose:\n                                print(\n                                    f\"Filtering out object with area {object_area} pixels\"\n                                )\n                            continue\n\n                        conf_value = int(scores[mask_idx] * 255)  # Scale to 0-255\n\n                        # Update the mask and confidence arrays\n                        h, w = binary_mask.shape\n                        valid_h = min(h, src.height - j)\n                        valid_w = min(w, src.width - i)\n\n                        if valid_h &gt; 0 and valid_w &gt; 0:\n                            # Use maximum for overlapping regions in the mask\n                            mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                                mask_array[j : j + valid_h, i : i + valid_w],\n                                binary_mask[:valid_h, :valid_w],\n                            )\n\n                            # For confidence, only update where mask is positive\n                            # and confidence is higher than existing\n                            mask_region = binary_mask[:valid_h, :valid_w] &gt; 0\n                            if np.any(mask_region):\n                                # Only update where mask is positive and new confidence is higher\n                                current_conf = conf_array[\n                                    j : j + valid_h, i : i + valid_w\n                                ]\n\n                                # Where to update confidence (mask positive &amp; higher confidence)\n                                update_mask = np.logical_and(\n                                    mask_region,\n                                    np.logical_or(\n                                        current_conf == 0, current_conf &lt; conf_value\n                                    ),\n                                )\n\n                                if np.any(update_mask):\n                                    conf_array[j : j + valid_h, i : i + valid_w][\n                                        update_mask\n                                    ] = conf_value\n\n            # Write to GeoTIFF\n            with rasterio.open(output_path, \"w\", **output_profile) as dst:\n                dst.write(mask_array, 1)\n                dst.write(conf_array, 2)\n\n            print(f\"Masks with confidence values saved to {output_path}\")\n            return output_path\n\n    def vectorize_masks(\n        self,\n        masks_path: str,\n        output_path: Optional[str] = None,\n        confidence_threshold: float = 0.5,\n        min_object_area: int = 100,\n        max_object_area: Optional[int] = None,\n        n_workers: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"\n        Convert masks with confidence to vector polygons.\n\n        Args:\n            masks_path: Path to masks GeoTIFF with confidence band.\n            output_path: Path for output GeoJSON.\n            confidence_threshold: Minimum confidence score (0.0-1.0). Default: 0.5\n            min_object_area: Minimum area in pixels to keep an object. Default: 100\n            max_object_area: Maximum area in pixels to keep an object. Default: None\n            n_workers: int, default=None\n                The number of worker threads to use.\n                \"None\" means single-threaded processing.\n                \"-1\"   means using all available CPU processors.\n                Positive integer means using that specific number of threads.\n            **kwargs: Additional parameters\n\n        Returns:\n            GeoDataFrame with car detections and confidence values\n        \"\"\"\n\n        def _process_single_component(\n            component_mask: np.ndarray,\n            conf_data: np.ndarray,\n            transform: Any,\n            confidence_threshold: float,\n            min_object_area: int,\n            max_object_area: Optional[int],\n        ) -&gt; Optional[Dict[str, Any]]:\n            # Get confidence value\n            conf_region = conf_data[component_mask &gt; 0]\n            if len(conf_region) &gt; 0:\n                confidence = np.mean(conf_region) / 255.0\n            else:\n                confidence = 0.0\n\n            # Skip if confidence is below threshold\n            if confidence &lt; confidence_threshold:\n                return None\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            results = []\n\n            for contour in contours:\n                # Filter by size\n                area = cv2.contourArea(contour)\n                if area &lt; min_object_area:\n                    continue\n\n                if max_object_area is not None and area &gt; max_object_area:\n                    continue\n\n                # Get minimum area rectangle\n                rect = cv2.minAreaRect(contour)\n                box_points = cv2.boxPoints(rect)\n\n                # Convert to geographic coordinates\n                geo_points = []\n                for x, y in box_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create polygon\n                poly = Polygon(geo_points)\n                results.append((poly, confidence, area))\n\n            return results\n\n        import concurrent.futures\n        from functools import partial\n\n        def process_component(\n            args: Tuple[int, np.ndarray, np.ndarray, Any, float, int, Optional[int]],\n        ) -&gt; Optional[Dict[str, Any]]:\n            \"\"\"\n            Helper function to process a single component\n            \"\"\"\n            (\n                label,\n                labeled_mask,\n                conf_data,\n                transform,\n                confidence_threshold,\n                min_object_area,\n                max_object_area,\n            ) = args\n\n            # Create mask for this component\n            component_mask = (labeled_mask == label).astype(np.uint8)\n\n            return _process_single_component(\n                component_mask,\n                conf_data,\n                transform,\n                confidence_threshold,\n                min_object_area,\n                max_object_area,\n            )\n\n        start_time = time.time()\n        print(f\"Processing masks from: {masks_path}\")\n\n        if n_workers == -1:\n            n_workers = os.cpu_count()\n\n        with rasterio.open(masks_path) as src:\n            # Read mask and confidence bands\n            mask_data = src.read(1)\n            conf_data = src.read(2)\n            transform = src.transform\n            crs = src.crs\n\n            # Convert to binary mask\n            binary_mask = mask_data &gt; 0\n\n            # Find connected components\n            labeled_mask, num_features = ndimage.label(binary_mask)\n            print(f\"Found {num_features} connected components\")\n\n            # Process each component\n            polygons = []\n            confidences = []\n            pixels = []\n\n            if n_workers is None or n_workers == 1:\n                print(\n                    \"Using single-threaded processing, you can speed up processing by setting n_workers &gt; 1\"\n                )\n                # Add progress bar\n                for label in tqdm(\n                    range(1, num_features + 1), desc=\"Processing components\"\n                ):\n                    # Create mask for this component\n                    component_mask = (labeled_mask == label).astype(np.uint8)\n\n                    result = _process_single_component(\n                        component_mask,\n                        conf_data,\n                        transform,\n                        confidence_threshold,\n                        min_object_area,\n                        max_object_area,\n                    )\n\n                    if result:\n                        for poly, confidence, area in result:\n                            # Add to lists\n                            polygons.append(poly)\n                            confidences.append(confidence)\n                            pixels.append(area)\n\n            else:\n                # Process components in parallel\n                print(f\"Using {n_workers} workers for parallel processing\")\n\n                process_args = [\n                    (\n                        label,\n                        labeled_mask,\n                        conf_data,\n                        transform,\n                        confidence_threshold,\n                        min_object_area,\n                        max_object_area,\n                    )\n                    for label in range(1, num_features + 1)\n                ]\n\n                with concurrent.futures.ThreadPoolExecutor(\n                    max_workers=n_workers\n                ) as executor:\n                    results = list(\n                        tqdm(\n                            executor.map(process_component, process_args),\n                            total=num_features,\n                            desc=\"Processing components\",\n                        )\n                    )\n\n                    for result in results:\n                        if result:\n                            for poly, confidence, area in result:\n                                # Add to lists\n                                polygons.append(poly)\n                                confidences.append(confidence)\n                                pixels.append(area)\n\n            # Create GeoDataFrame\n            if polygons:\n                gdf = gpd.GeoDataFrame(\n                    {\n                        \"geometry\": polygons,\n                        \"confidence\": confidences,\n                        \"class\": [1] * len(polygons),\n                        \"pixels\": pixels,\n                    },\n                    crs=crs,\n                )\n\n                # Save to file if requested\n                if output_path:\n                    gdf.to_file(output_path, driver=\"GeoJSON\")\n                    print(f\"Saved {len(gdf)} objects with confidence to {output_path}\")\n\n                end_time = time.time()\n                print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n                return gdf\n            else:\n                end_time = time.time()\n                print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n                print(\"No valid polygons found\")\n                return None\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.__init__","title":"<code>__init__(model_path=None, repo_id=None, model=None, num_classes=2, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Optional[str]</code> <p>Path to the .pth model file.</p> <code>None</code> <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID for model download.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Pre-initialized model object (optional).</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes for detection (default: 2).</p> <code>2</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: Optional[str] = None,\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    num_classes: int = 2,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Hugging Face repository ID for model download.\n        model: Pre-initialized model object (optional).\n        num_classes: Number of classes for detection (default: 2).\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    # Set device\n    if device is None:\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    else:\n        self.device = torch.device(device)\n\n    # Default parameters for object detection - these can be overridden in process_raster\n    self.chip_size = (512, 512)  # Size of image chips for processing\n    self.overlap = 0.25  # Default overlap between tiles\n    self.confidence_threshold = 0.5  # Default confidence threshold\n    self.nms_iou_threshold = 0.5  # IoU threshold for non-maximum suppression\n    self.min_object_area = 100  # Minimum area in pixels to keep an object\n    self.max_object_area = None  # Maximum area in pixels to keep an object\n    self.mask_threshold = 0.5  # Threshold for mask binarization\n    self.simplify_tolerance = 1.0  # Tolerance for polygon simplification\n\n    # Initialize model\n    self.model = self.initialize_model(model, num_classes=num_classes)\n\n    # Download model if needed\n    if model_path is None or (not os.path.exists(model_path)):\n        model_path = self.download_model_from_hf(model_path, repo_id)\n\n    # Load model weights\n    self.load_weights(model_path)\n\n    # Set model to evaluation mode\n    self.model.eval()\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.download_model_from_hf","title":"<code>download_model_from_hf(model_path=None, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Optional[str]</code> <p>Path to the model file.</p> <code>None</code> <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded model file</p> Source code in <code>geoai/extract.py</code> <pre><code>def download_model_from_hf(\n    self, model_path: Optional[str] = None, repo_id: Optional[str] = None\n) -&gt; str:\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    try:\n\n        print(\"Model path not specified, downloading from Hugging Face...\")\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            repo_id = \"giswqs/geoai\"\n\n        if model_path is None:\n            model_path = \"building_footprints_usa.pth\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.filter_edge_objects","title":"<code>filter_edge_objects(gdf, raster_path, edge_buffer=10)</code>","text":"<p>Filter out object detections that fall in padding/edge areas of the image.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with object detections</p> required <code>raster_path</code> <code>str</code> <p>Path to the original raster file</p> required <code>edge_buffer</code> <code>int</code> <p>Buffer in pixels to consider as edge region</p> <code>10</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with filtered objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_edge_objects(\n    self, gdf: gpd.GeoDataFrame, raster_path: str, edge_buffer: int = 10\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Filter out object detections that fall in padding/edge areas of the image.\n\n    Args:\n        gdf: GeoDataFrame with object detections\n        raster_path: Path to the original raster file\n        edge_buffer: Buffer in pixels to consider as edge region\n\n    Returns:\n        GeoDataFrame with filtered objects\n    \"\"\"\n    import rasterio\n    from shapely.geometry import box\n\n    # If no objects detected, return empty GeoDataFrame\n    if gdf is None or len(gdf) == 0:\n        return gdf\n\n    print(f\"Objects before filtering: {len(gdf)}\")\n\n    with rasterio.open(raster_path) as src:\n        # Get raster bounds\n        raster_bounds = src.bounds\n        raster_width = src.width\n        raster_height = src.height\n\n        # Convert edge buffer from pixels to geographic units\n        # We need the smallest dimension of a pixel in geographic units\n        pixel_width = (raster_bounds[2] - raster_bounds[0]) / raster_width\n        pixel_height = (raster_bounds[3] - raster_bounds[1]) / raster_height\n        buffer_size = min(pixel_width, pixel_height) * edge_buffer\n\n        # Create a slightly smaller bounding box to exclude edge regions\n        inner_bounds = (\n            raster_bounds[0] + buffer_size,  # min x (west)\n            raster_bounds[1] + buffer_size,  # min y (south)\n            raster_bounds[2] - buffer_size,  # max x (east)\n            raster_bounds[3] - buffer_size,  # max y (north)\n        )\n\n        # Check that inner bounds are valid\n        if inner_bounds[0] &gt;= inner_bounds[2] or inner_bounds[1] &gt;= inner_bounds[3]:\n            print(\"Warning: Edge buffer too large, using original bounds\")\n            inner_box = box(*raster_bounds)\n        else:\n            inner_box = box(*inner_bounds)\n\n        # Filter out objects that intersect with the edge of the image\n        filtered_gdf = gdf[gdf.intersects(inner_box)]\n\n        # Additional check for objects that have &gt;50% of their area outside the valid region\n        valid_objects = []\n        for idx, row in filtered_gdf.iterrows():\n            if row.geometry.intersection(inner_box).area &gt;= 0.5 * row.geometry.area:\n                valid_objects.append(idx)\n\n        filtered_gdf = filtered_gdf.loc[valid_objects]\n\n        print(f\"Objects after filtering: {len(filtered_gdf)}\")\n\n        return filtered_gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.filter_overlapping_polygons","title":"<code>filter_overlapping_polygons(gdf, **kwargs)</code>","text":"<p>Filter overlapping polygons using non-maximum suppression.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with polygons</p> required <code>**kwargs</code> <code>Any</code> <p>Optional parameters: nms_iou_threshold: IoU threshold for filtering</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>Filtered GeoDataFrame</p> Source code in <code>geoai/extract.py</code> <pre><code>def filter_overlapping_polygons(\n    self, gdf: gpd.GeoDataFrame, **kwargs: Any\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Filter overlapping polygons using non-maximum suppression.\n\n    Args:\n        gdf: GeoDataFrame with polygons\n        **kwargs: Optional parameters:\n            nms_iou_threshold: IoU threshold for filtering\n\n    Returns:\n        Filtered GeoDataFrame\n    \"\"\"\n    if len(gdf) &lt;= 1:\n        return gdf\n\n    # Get parameters from kwargs or use instance defaults\n    iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n\n    # Sort by confidence\n    gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n    # Fix any invalid geometries\n    gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n        lambda geom: geom.buffer(0) if not geom.is_valid else geom\n    )\n\n    keep_indices = []\n    polygons = gdf.geometry.values\n\n    for i in range(len(polygons)):\n        if i in keep_indices:\n            continue\n\n        keep = True\n        for j in keep_indices:\n            # Skip invalid geometries\n            if not polygons[i].is_valid or not polygons[j].is_valid:\n                continue\n\n            # Calculate IoU\n            try:\n                intersection = polygons[i].intersection(polygons[j]).area\n                union = polygons[i].area + polygons[j].area - intersection\n                iou = intersection / union if union &gt; 0 else 0\n\n                if iou &gt; iou_threshold:\n                    keep = False\n                    break\n            except Exception:\n                # Skip on topology exceptions\n                continue\n\n        if keep:\n            keep_indices.append(i)\n\n    return gdf.iloc[keep_indices]\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.generate_masks","title":"<code>generate_masks(raster_path, output_path=None, confidence_threshold=None, mask_threshold=None, min_object_area=10, max_object_area=float('inf'), overlap=0.25, batch_size=4, band_indexes=None, verbose=False, **kwargs)</code>","text":"<p>Save masks with confidence values as a multi-band GeoTIFF.</p> <p>Objects with area smaller than min_object_area or larger than max_object_area will be filtered out.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path for output GeoTIFF</p> <code>None</code> <code>confidence_threshold</code> <code>Optional[float]</code> <p>Minimum confidence score (0.0-1.0)</p> <code>None</code> <code>mask_threshold</code> <code>Optional[float]</code> <p>Threshold for mask binarization (0.0-1.0)</p> <code>None</code> <code>min_object_area</code> <code>int</code> <p>Minimum area (in pixels) for an object to be included</p> <code>10</code> <code>max_object_area</code> <code>float</code> <p>Maximum area (in pixels) for an object to be included</p> <code>float('inf')</code> <code>overlap</code> <code>float</code> <p>Overlap between tiles (0.0-1.0)</p> <code>0.25</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>band_indexes</code> <code>Optional[List[int]]</code> <p>List of band indexes to use (default: all bands)</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed processing information</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved GeoTIFF</p> Source code in <code>geoai/extract.py</code> <pre><code>def generate_masks(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    confidence_threshold: Optional[float] = None,\n    mask_threshold: Optional[float] = None,\n    min_object_area: int = 10,\n    max_object_area: float = float(\"inf\"),\n    overlap: float = 0.25,\n    batch_size: int = 4,\n    band_indexes: Optional[List[int]] = None,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Save masks with confidence values as a multi-band GeoTIFF.\n\n    Objects with area smaller than min_object_area or larger than max_object_area\n    will be filtered out.\n\n    Args:\n        raster_path: Path to input raster\n        output_path: Path for output GeoTIFF\n        confidence_threshold: Minimum confidence score (0.0-1.0)\n        mask_threshold: Threshold for mask binarization (0.0-1.0)\n        min_object_area: Minimum area (in pixels) for an object to be included\n        max_object_area: Maximum area (in pixels) for an object to be included\n        overlap: Overlap between tiles (0.0-1.0)\n        batch_size: Batch size for processing\n        band_indexes: List of band indexes to use (default: all bands)\n        verbose: Whether to print detailed processing information\n\n    Returns:\n        Path to the saved GeoTIFF\n    \"\"\"\n    # Use provided thresholds or fall back to instance defaults\n    if confidence_threshold is None:\n        confidence_threshold = self.confidence_threshold\n    if mask_threshold is None:\n        mask_threshold = self.mask_threshold\n\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n\n    # Default output path\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks_conf.tif\"\n\n    # Process the raster to get individual masks with confidence\n    with rasterio.open(raster_path) as src:\n        # Create dataset with the specified overlap\n        dataset = CustomDataset(\n            raster_path=raster_path,\n            chip_size=chip_size,\n            overlap=overlap,\n            band_indexes=band_indexes,\n            verbose=verbose,\n        )\n\n        # Create output profile\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=2,  # Two bands: mask and confidence\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Initialize mask and confidence arrays\n        mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n        conf_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n        # Define custom collate function to handle Shapely objects\n        def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n            \"\"\"\n            Custom collate function that handles Shapely geometries\n            by keeping them as Python objects rather than trying to collate them.\n            \"\"\"\n            elem = batch[0]\n            if isinstance(elem, dict):\n                result = {}\n                for key in elem:\n                    if key == \"bbox\":\n                        # Don't collate shapely objects, keep as list\n                        result[key] = [d[key] for d in batch]\n                    else:\n                        # For tensors and other collatable types\n                        try:\n                            result[key] = (\n                                torch.utils.data._utils.collate.default_collate(\n                                    [d[key] for d in batch]\n                                )\n                            )\n                        except TypeError:\n                            # Fall back to list for non-collatable types\n                            result[key] = [d[key] for d in batch]\n                return result\n            else:\n                # Default collate for non-dict types\n                return torch.utils.data._utils.collate.default_collate(batch)\n\n        # Create dataloader with custom collate function\n        dataloader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=0,\n            collate_fn=custom_collate,\n        )\n\n        # Process batches\n        print(f\"Processing raster with {len(dataloader)} batches\")\n        for batch in tqdm(dataloader):\n            # Move images to device\n            images = batch[\"image\"].to(self.device)\n            coords = batch[\"coords\"]  # Tensor of shape [batch_size, 2]\n\n            # Run inference\n            with torch.no_grad():\n                predictions = self.model(images)\n\n            # Process predictions\n            for idx, prediction in enumerate(predictions):\n                masks = prediction[\"masks\"].cpu().numpy()\n                scores = prediction[\"scores\"].cpu().numpy()\n\n                # Filter by confidence threshold\n                valid_indices = scores &gt;= confidence_threshold\n                masks = masks[valid_indices]\n                scores = scores[valid_indices]\n\n                # Skip if no valid predictions\n                if len(masks) == 0:\n                    continue\n\n                # Get window coordinates\n                i, j = coords[idx].cpu().numpy()\n\n                # Process each mask\n                for mask_idx, mask in enumerate(masks):\n                    # Convert to binary mask\n                    binary_mask = (mask[0] &gt; mask_threshold).astype(np.uint8) * 255\n\n                    # Check object area - calculate number of pixels in the mask\n                    object_area = np.sum(binary_mask &gt; 0)\n\n                    # Skip objects that don't meet area criteria\n                    if (\n                        object_area &lt; min_object_area\n                        or object_area &gt; max_object_area\n                    ):\n                        if verbose:\n                            print(\n                                f\"Filtering out object with area {object_area} pixels\"\n                            )\n                        continue\n\n                    conf_value = int(scores[mask_idx] * 255)  # Scale to 0-255\n\n                    # Update the mask and confidence arrays\n                    h, w = binary_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        # Use maximum for overlapping regions in the mask\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            binary_mask[:valid_h, :valid_w],\n                        )\n\n                        # For confidence, only update where mask is positive\n                        # and confidence is higher than existing\n                        mask_region = binary_mask[:valid_h, :valid_w] &gt; 0\n                        if np.any(mask_region):\n                            # Only update where mask is positive and new confidence is higher\n                            current_conf = conf_array[\n                                j : j + valid_h, i : i + valid_w\n                            ]\n\n                            # Where to update confidence (mask positive &amp; higher confidence)\n                            update_mask = np.logical_and(\n                                mask_region,\n                                np.logical_or(\n                                    current_conf == 0, current_conf &lt; conf_value\n                                ),\n                            )\n\n                            if np.any(update_mask):\n                                conf_array[j : j + valid_h, i : i + valid_w][\n                                    update_mask\n                                ] = conf_value\n\n        # Write to GeoTIFF\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            dst.write(mask_array, 1)\n            dst.write(conf_array, 2)\n\n        print(f\"Masks with confidence values saved to {output_path}\")\n        return output_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.initialize_model","title":"<code>initialize_model(model, num_classes=2)</code>","text":"<p>Initialize a deep learning model for object detection.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>A pre-initialized model object.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for detection.</p> <code>2</code> <p>Returns:</p> Type Description <code>Any</code> <p>torch.nn.Module: A deep learning model for object detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>def initialize_model(self, model: Optional[Any], num_classes: int = 2) -&gt; Any:\n    \"\"\"Initialize a deep learning model for object detection.\n\n    Args:\n        model (torch.nn.Module): A pre-initialized model object.\n        num_classes (int): Number of classes for detection.\n\n    Returns:\n        torch.nn.Module: A deep learning model for object detection.\n    \"\"\"\n\n    if model is None:  # Initialize Mask R-CNN model with ResNet50 backbone.\n        # Standard image mean and std for pre-trained models\n        image_mean = [0.485, 0.456, 0.406]\n        image_std = [0.229, 0.224, 0.225]\n\n        # Create model with explicit normalization parameters\n        model = maskrcnn_resnet50_fpn(\n            weights=None,\n            progress=False,\n            num_classes=num_classes,  # Background + object\n            weights_backbone=None,\n            # These parameters ensure consistent normalization\n            image_mean=image_mean,\n            image_std=image_std,\n        )\n\n    model.to(self.device)\n    return model\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.load_weights","title":"<code>load_weights(model_path)</code>","text":"<p>Load weights from file with error handling for different formats.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to model weights</p> required Source code in <code>geoai/extract.py</code> <pre><code>def load_weights(self, model_path: str) -&gt; None:\n    \"\"\"\n    Load weights from file with error handling for different formats.\n\n    Args:\n        model_path: Path to model weights\n    \"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    try:\n        state_dict = torch.load(model_path, map_location=self.device)\n\n        # Handle different state dict formats\n        if isinstance(state_dict, dict):\n            if \"model\" in state_dict:\n                state_dict = state_dict[\"model\"]\n            elif \"state_dict\" in state_dict:\n                state_dict = state_dict[\"state_dict\"]\n\n        # Try to load state dict\n        try:\n            self.model.load_state_dict(state_dict)\n            print(\"Model loaded successfully\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Attempting to fix state_dict keys...\")\n\n            # Try to fix state_dict keys (remove module prefix if needed)\n            new_state_dict = {}\n            for k, v in state_dict.items():\n                if k.startswith(\"module.\"):\n                    new_state_dict[k[7:]] = v\n                else:\n                    new_state_dict[k] = v\n\n            self.model.load_state_dict(new_state_dict)\n            print(\"Model loaded successfully after key fixing\")\n\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load model: {e}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.mask_to_polygons","title":"<code>mask_to_polygons(mask, **kwargs)</code>","text":"<p>Convert binary mask to polygon contours using OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>Binary mask as numpy array</p> required <code>**kwargs</code> <code>Any</code> <p>Optional parameters: simplify_tolerance: Tolerance for polygon simplification mask_threshold: Threshold for mask binarization min_object_area: Minimum area in pixels to keep an object max_object_area: Maximum area in pixels to keep an object</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Polygon]</code> <p>List of polygons as lists of (x, y) coordinates</p> Source code in <code>geoai/extract.py</code> <pre><code>def mask_to_polygons(self, mask: np.ndarray, **kwargs: Any) -&gt; List[Polygon]:\n    \"\"\"\n    Convert binary mask to polygon contours using OpenCV.\n\n    Args:\n        mask: Binary mask as numpy array\n        **kwargs: Optional parameters:\n            simplify_tolerance: Tolerance for polygon simplification\n            mask_threshold: Threshold for mask binarization\n            min_object_area: Minimum area in pixels to keep an object\n            max_object_area: Maximum area in pixels to keep an object\n\n    Returns:\n        List of polygons as lists of (x, y) coordinates\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n\n    # Ensure binary mask\n    mask = (mask &gt; mask_threshold).astype(np.uint8)\n\n    # Optional: apply morphological operations to improve mask quality\n    kernel = np.ones((3, 3), np.uint8)\n    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n    # Find contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Convert to list of [x, y] coordinates\n    polygons = []\n    for contour in contours:\n        # Filter out too small contours\n        if contour.shape[0] &lt; 3 or cv2.contourArea(contour) &lt; min_object_area:\n            continue\n\n        # Filter out too large contours\n        if (\n            max_object_area is not None\n            and cv2.contourArea(contour) &gt; max_object_area\n        ):\n            continue\n\n        # Simplify contour if it has many points\n        if contour.shape[0] &gt; 50:\n            epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n            contour = cv2.approxPolyDP(contour, epsilon, True)\n\n        # Convert to list of [x, y] coordinates\n        polygon = contour.reshape(-1, 2).tolist()\n        polygons.append(polygon)\n\n    return polygons\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=None, mask_threshold=None, min_object_area=None, max_object_area=None, nms_iou_threshold=None, regularize=True, angle_threshold=15, rectangularity_threshold=0.7)</code>","text":"<p>Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <code>str</code> <p>Path to the object masks GeoTIFF</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <code>Optional[float]</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>None</code> <code>mask_threshold</code> <code>Optional[float]</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>None</code> <code>min_object_area</code> <code>Optional[int]</code> <p>Minimum area in pixels to keep an object (default: self.min_object_area)</p> <code>None</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Minimum area in pixels to keep an object (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <code>Optional[float]</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>None</code> <code>regularize</code> <code>bool</code> <p>Whether to regularize objects to right angles (default: True)</p> <code>True</code> <code>angle_threshold</code> <code>int</code> <p>Maximum deviation from 90 degrees for regularization (default: 15)</p> <code>15</code> <code>rectangularity_threshold</code> <code>float</code> <p>Threshold for rectangle simplification (default: 0.7)</p> <code>0.7</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def masks_to_vector(\n    self,\n    mask_path: str,\n    output_path: Optional[str] = None,\n    simplify_tolerance: Optional[float] = None,\n    mask_threshold: Optional[float] = None,\n    min_object_area: Optional[int] = None,\n    max_object_area: Optional[int] = None,\n    nms_iou_threshold: Optional[float] = None,\n    regularize: bool = True,\n    angle_threshold: int = 15,\n    rectangularity_threshold: float = 0.7,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert an object mask GeoTIFF to vector polygons and save as GeoJSON.\n\n    Args:\n        mask_path: Path to the object masks GeoTIFF\n        output_path: Path to save the output GeoJSON or Parquet file (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep an object (default: self.min_object_area)\n        max_object_area: Minimum area in pixels to keep an object (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n        regularize: Whether to regularize objects to right angles (default: True)\n        angle_threshold: Maximum deviation from 90 degrees for regularization (default: 15)\n        rectangularity_threshold: Threshold for rectangle simplification (default: 0.7)\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Use class defaults if parameters not provided\n    simplify_tolerance = (\n        simplify_tolerance\n        if simplify_tolerance is not None\n        else self.simplify_tolerance\n    )\n    mask_threshold = (\n        mask_threshold if mask_threshold is not None else self.mask_threshold\n    )\n    min_object_area = (\n        min_object_area if min_object_area is not None else self.min_object_area\n    )\n    max_object_area = (\n        max_object_area if max_object_area is not None else self.max_object_area\n    )\n    nms_iou_threshold = (\n        nms_iou_threshold\n        if nms_iou_threshold is not None\n        else self.nms_iou_threshold\n    )\n\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Regularize objects: {regularize}\")\n    if regularize:\n        print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n        print(f\"- Rectangularity threshold: {rectangularity_threshold*100}%\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(\n            f\"Found {num_labels-1} potential objects\"\n        )  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this object\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this object\n            object_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                object_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Object class\n            },\n            crs=crs,\n        )\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = self.filter_overlapping_polygons(\n            gdf, nms_iou_threshold=nms_iou_threshold\n        )\n\n        print(f\"Object count after NMS filtering: {len(gdf)}\")\n\n        # Apply regularization if requested\n        if regularize and len(gdf) &gt; 0:\n            # Convert pixel area to geographic units for min_area parameter\n            # Estimate pixel size in geographic units\n            with rasterio.open(mask_path) as src:\n                pixel_size_x = src.transform[\n                    0\n                ]  # width of a pixel in geographic units\n                pixel_size_y = abs(\n                    src.transform[4]\n                )  # height of a pixel in geographic units\n                avg_pixel_area = pixel_size_x * pixel_size_y\n\n            # Use 10 pixels as minimum area in geographic units\n            min_geo_area = 10 * avg_pixel_area\n\n            # Regularize objects\n            gdf = self.regularize_objects(\n                gdf,\n                min_area=min_geo_area,\n                angle_threshold=angle_threshold,\n                rectangularity_threshold=rectangularity_threshold,\n            )\n\n        # Save to file\n        if output_path:\n            if output_path.endswith(\".parquet\"):\n                gdf.to_parquet(output_path)\n            else:\n                gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.process_raster","title":"<code>process_raster(raster_path, output_path=None, batch_size=4, filter_edges=True, edge_buffer=20, band_indexes=None, **kwargs)</code>","text":"<p>Process a raster file to extract objects with customizable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster file</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to output GeoJSON or Parquet file (optional)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>filter_edges</code> <code>bool</code> <p>Whether to filter out objects at the edges of the image</p> <code>True</code> <code>edge_buffer</code> <code>int</code> <p>Size of edge buffer in pixels to filter out objects (if filter_edges=True)</p> <code>20</code> <code>band_indexes</code> <code>Optional[List[int]]</code> <p>List of band indexes to use (if None, use all bands)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) overlap: Overlap between adjacent tiles (0.0-1.0) chip_size: Size of image chips for processing (height, width) nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0) mask_threshold: Threshold for mask binarization (0.0-1.0) min_object_area: Minimum area in pixels to keep an object simplify_tolerance: Tolerance for polygon simplification</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with objects</p> Source code in <code>geoai/extract.py</code> <pre><code>@torch.no_grad()\ndef process_raster(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    batch_size: int = 4,\n    filter_edges: bool = True,\n    edge_buffer: int = 20,\n    band_indexes: Optional[List[int]] = None,\n    **kwargs: Any,\n) -&gt; \"gpd.GeoDataFrame\":\n    \"\"\"\n    Process a raster file to extract objects with customizable parameters.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoJSON or Parquet file (optional)\n        batch_size: Batch size for processing\n        filter_edges: Whether to filter out objects at the edges of the image\n        edge_buffer: Size of edge buffer in pixels to filter out objects (if filter_edges=True)\n        band_indexes: List of band indexes to use (if None, use all bands)\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            overlap: Overlap between adjacent tiles (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            nms_iou_threshold: IoU threshold for non-maximum suppression (0.0-1.0)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n            min_object_area: Minimum area in pixels to keep an object\n            simplify_tolerance: Tolerance for polygon simplification\n\n    Returns:\n        GeoDataFrame with objects\n    \"\"\"\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    overlap = kwargs.get(\"overlap\", self.overlap)\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    nms_iou_threshold = kwargs.get(\"nms_iou_threshold\", self.nms_iou_threshold)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    min_object_area = kwargs.get(\"min_object_area\", self.min_object_area)\n    max_object_area = kwargs.get(\"max_object_area\", self.max_object_area)\n    simplify_tolerance = kwargs.get(\"simplify_tolerance\", self.simplify_tolerance)\n\n    # Print parameters being used\n    print(f\"Processing with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Tile overlap: {overlap}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min object area: {min_object_area}\")\n    print(f\"- Max object area: {max_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- Filter edge objects: {filter_edges}\")\n    if filter_edges:\n        print(f\"- Edge buffer size: {edge_buffer} pixels\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        overlap=overlap,\n        band_indexes=band_indexes,\n    )\n    self.raster_stats = dataset.raster_stats\n\n    # Custom collate function to handle Shapely objects\n    def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Custom collate function that handles Shapely geometries\n        by keeping them as Python objects rather than trying to collate them.\n        \"\"\"\n        elem = batch[0]\n        if isinstance(elem, dict):\n            result = {}\n            for key in elem:\n                if key == \"bbox\":\n                    # Don't collate shapely objects, keep as list\n                    result[key] = [d[key] for d in batch]\n                else:\n                    # For tensors and other collatable types\n                    try:\n                        result[key] = (\n                            torch.utils.data._utils.collate.default_collate(\n                                [d[key] for d in batch]\n                            )\n                        )\n                    except TypeError:\n                        # Fall back to list for non-collatable types\n                        result[key] = [d[key] for d in batch]\n            return result\n        else:\n            # Default collate for non-dict types\n            return torch.utils.data._utils.collate.default_collate(batch)\n\n    # Create dataloader with simple indexing and custom collate\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=0,\n        collate_fn=custom_collate,\n    )\n\n    # Process batches\n    all_polygons = []\n    all_scores = []\n\n    print(f\"Processing raster with {len(dataloader)} batches\")\n    for batch in tqdm(dataloader):\n        # Move images to device\n        images = batch[\"image\"].to(self.device)\n        coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n        bboxes = batch[\n            \"bbox\"\n        ]  # Geographic bounding boxes - now a list, not a tensor\n\n        # Run inference\n        predictions = self.model(images)\n\n        # Process predictions\n        for idx, prediction in enumerate(predictions):\n            masks = prediction[\"masks\"].cpu().numpy()\n            scores = prediction[\"scores\"].cpu().numpy()\n            labels = prediction[\"labels\"].cpu().numpy()\n\n            # Skip if no predictions\n            if len(scores) == 0:\n                continue\n\n            # Filter by confidence threshold\n            valid_indices = scores &gt;= confidence_threshold\n            masks = masks[valid_indices]\n            scores = scores[valid_indices]\n            labels = labels[valid_indices]\n\n            # Skip if no valid predictions\n            if len(scores) == 0:\n                continue\n\n            # Get window coordinates\n            # The coords might be in different formats depending on batch handling\n            if isinstance(coords, list):\n                # If coords is a list of tuples\n                coord_item = coords[idx]\n                if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                    i, j = coord_item\n                elif isinstance(coord_item, torch.Tensor):\n                    i, j = coord_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected coords format: {type(coord_item)}\")\n                    continue\n            elif isinstance(coords, torch.Tensor):\n                # If coords is a tensor of shape [batch_size, 2]\n                i, j = coords[idx].cpu().numpy().tolist()\n            else:\n                print(f\"Unexpected coords type: {type(coords)}\")\n                continue\n\n            # Get window size\n            if isinstance(batch[\"window_size\"], list):\n                window_item = batch[\"window_size\"][idx]\n                if isinstance(window_item, tuple) and len(window_item) == 2:\n                    window_width, window_height = window_item\n                elif isinstance(window_item, torch.Tensor):\n                    window_width, window_height = window_item.cpu().numpy().tolist()\n                else:\n                    print(f\"Unexpected window_size format: {type(window_item)}\")\n                    continue\n            elif isinstance(batch[\"window_size\"], torch.Tensor):\n                window_width, window_height = (\n                    batch[\"window_size\"][idx].cpu().numpy().tolist()\n                )\n            else:\n                print(f\"Unexpected window_size type: {type(batch['window_size'])}\")\n                continue\n\n            # Process masks to polygons\n            for mask_idx, mask in enumerate(masks):\n                # Get binary mask\n                binary_mask = mask[0]  # Get binary mask\n\n                # Convert mask to polygon with custom parameters\n                contours = self.mask_to_polygons(\n                    binary_mask,\n                    simplify_tolerance=simplify_tolerance,\n                    mask_threshold=mask_threshold,\n                    min_object_area=min_object_area,\n                    max_object_area=max_object_area,\n                )\n\n                # Skip if no valid polygons\n                if not contours:\n                    continue\n\n                # Transform polygons to geographic coordinates\n                with rasterio.open(raster_path) as src:\n                    transform = src.transform\n\n                    for contour in contours:\n                        # Convert polygon to global coordinates\n                        global_polygon = []\n                        for x, y in contour:\n                            # Adjust coordinates based on window position\n                            gx, gy = transform * (i + x, j + y)\n                            global_polygon.append((gx, gy))\n\n                        # Create Shapely polygon\n                        if len(global_polygon) &gt;= 3:\n                            try:\n                                shapely_poly = Polygon(global_polygon)\n                                if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                                    all_polygons.append(shapely_poly)\n                                    all_scores.append(float(scores[mask_idx]))\n                            except Exception as e:\n                                print(f\"Error creating polygon: {e}\")\n\n    # Create GeoDataFrame\n    if not all_polygons:\n        print(\"No valid polygons found\")\n        return None\n\n    gdf = gpd.GeoDataFrame(\n        {\n            \"geometry\": all_polygons,\n            \"confidence\": all_scores,\n            \"class\": 1,  # Object class\n        },\n        crs=dataset.crs,\n    )\n\n    # Remove overlapping polygons with custom threshold\n    gdf = self.filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n    # Filter edge objects if requested\n    if filter_edges:\n        gdf = self.filter_edge_objects(gdf, raster_path, edge_buffer=edge_buffer)\n\n    # Save to file if requested\n    if output_path:\n        if output_path.endswith(\".parquet\"):\n            gdf.to_parquet(output_path)\n        else:\n            gdf.to_file(output_path, driver=\"GeoJSON\")\n        print(f\"Saved {len(gdf)} objects to {output_path}\")\n\n    return gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.regularize_objects","title":"<code>regularize_objects(gdf, min_area=10, angle_threshold=15, orthogonality_threshold=0.3, rectangularity_threshold=0.7)</code>","text":"<p>Regularize objects to enforce right angles and rectangular shapes.</p> <p>Parameters:</p> Name Type Description Default <code>gdf</code> <code>GeoDataFrame</code> <p>GeoDataFrame with objects</p> required <code>min_area</code> <code>int</code> <p>Minimum area in square units to keep an object</p> <code>10</code> <code>angle_threshold</code> <code>int</code> <p>Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)</p> <code>15</code> <code>orthogonality_threshold</code> <code>float</code> <p>Percentage of angles that must be orthogonal for an object to be regularized</p> <code>0.3</code> <code>rectangularity_threshold</code> <code>float</code> <p>Minimum area ratio to Object's oriented bounding box for rectangular simplification</p> <code>0.7</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with regularized objects</p> Source code in <code>geoai/extract.py</code> <pre><code>def regularize_objects(\n    self,\n    gdf: gpd.GeoDataFrame,\n    min_area: int = 10,\n    angle_threshold: int = 15,\n    orthogonality_threshold: float = 0.3,\n    rectangularity_threshold: float = 0.7,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Regularize objects to enforce right angles and rectangular shapes.\n\n    Args:\n        gdf: GeoDataFrame with objects\n        min_area: Minimum area in square units to keep an object\n        angle_threshold: Maximum deviation from 90 degrees to consider an angle as orthogonal (degrees)\n        orthogonality_threshold: Percentage of angles that must be orthogonal for an object to be regularized\n        rectangularity_threshold: Minimum area ratio to Object's oriented bounding box for rectangular simplification\n\n    Returns:\n        GeoDataFrame with regularized objects\n    \"\"\"\n    import math\n\n    import cv2\n    import geopandas as gpd\n    import numpy as np\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import MultiPolygon, Polygon, box\n    from tqdm import tqdm\n\n    def get_angle(\n        p1: Tuple[float, float], p2: Tuple[float, float], p3: Tuple[float, float]\n    ) -&gt; float:\n        \"\"\"Calculate angle between three points in degrees (0-180)\"\"\"\n        a = np.array(p1)\n        b = np.array(p2)\n        c = np.array(p3)\n\n        ba = a - b\n        bc = c - b\n\n        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n        # Handle numerical errors that could push cosine outside [-1, 1]\n        cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n        angle = np.degrees(np.arccos(cosine_angle))\n\n        return angle\n\n    def is_orthogonal(angle: float, threshold: int = angle_threshold) -&gt; bool:\n        \"\"\"Check if angle is close to 90 degrees\"\"\"\n        return abs(angle - 90) &lt;= threshold\n\n    def calculate_dominant_direction(polygon: Polygon) -&gt; float:\n        \"\"\"Find the dominant direction of a polygon using PCA\"\"\"\n        # Extract coordinates\n        coords = np.array(polygon.exterior.coords)\n\n        # Mean center the coordinates\n        mean = np.mean(coords, axis=0)\n        centered_coords = coords - mean\n\n        # Calculate covariance matrix and its eigenvalues/eigenvectors\n        cov_matrix = np.cov(centered_coords.T)\n        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n\n        # Get the index of the largest eigenvalue\n        largest_idx = np.argmax(eigenvalues)\n\n        # Get the corresponding eigenvector (principal axis)\n        principal_axis = eigenvectors[:, largest_idx]\n\n        # Calculate the angle in degrees\n        angle_rad = np.arctan2(principal_axis[1], principal_axis[0])\n        angle_deg = np.degrees(angle_rad)\n\n        # Normalize to range 0-180\n        if angle_deg &lt; 0:\n            angle_deg += 180\n\n        return angle_deg\n\n    def create_oriented_envelope(polygon: Polygon, angle_deg: float) -&gt; Polygon:\n        \"\"\"Create an oriented minimum area rectangle for the polygon\"\"\"\n        # Create a rotated rectangle using OpenCV method (more robust than Shapely methods)\n        coords = np.array(polygon.exterior.coords)[:-1].astype(\n            np.float32\n        )  # Skip the last point (same as first)\n\n        # Use OpenCV's minAreaRect\n        rect = cv2.minAreaRect(coords)\n        box_points = cv2.boxPoints(rect)\n\n        # Convert to shapely polygon\n        oriented_box = Polygon(box_points)\n\n        return oriented_box\n\n    def get_rectangularity(polygon: Polygon, oriented_box: Polygon) -&gt; float:\n        \"\"\"Calculate the rectangularity (area ratio to its oriented bounding box)\"\"\"\n        if oriented_box.area == 0:\n            return 0\n        return polygon.area / oriented_box.area\n\n    def check_orthogonality(polygon: Polygon) -&gt; float:\n        \"\"\"Check what percentage of angles in the polygon are orthogonal\"\"\"\n        coords = list(polygon.exterior.coords)\n        if len(coords) &lt;= 4:  # Triangle or point\n            return 0\n\n        # Remove last point (same as first)\n        coords = coords[:-1]\n\n        orthogonal_count = 0\n        total_angles = len(coords)\n\n        for i in range(total_angles):\n            p1 = coords[i]\n            p2 = coords[(i + 1) % total_angles]\n            p3 = coords[(i + 2) % total_angles]\n\n            angle = get_angle(p1, p2, p3)\n            if is_orthogonal(angle):\n                orthogonal_count += 1\n\n        return orthogonal_count / total_angles\n\n    def simplify_to_rectangle(polygon: Polygon) -&gt; Polygon:\n        \"\"\"Simplify a polygon to a rectangle using its oriented bounding box\"\"\"\n        # Get dominant direction\n        angle = calculate_dominant_direction(polygon)\n\n        # Create oriented envelope\n        rect = create_oriented_envelope(polygon, angle)\n\n        return rect\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No Objects to regularize\")\n        return gdf\n\n    print(f\"Regularizing {len(gdf)} objects...\")\n    print(f\"- Angle threshold: {angle_threshold}\u00b0 from 90\u00b0\")\n    print(f\"- Min orthogonality: {orthogonality_threshold*100}% of angles\")\n    print(\n        f\"- Min rectangularity: {rectangularity_threshold*100}% of bounding box area\"\n    )\n\n    # Create a copy to avoid modifying the original\n    result_gdf = gdf.copy()\n\n    # Track statistics\n    total_objects = len(gdf)\n    regularized_count = 0\n    rectangularized_count = 0\n\n    # Process each Object\n    for idx, row in tqdm(gdf.iterrows(), total=len(gdf)):\n        geom = row.geometry\n\n        # Skip invalid or empty geometries\n        if geom is None or geom.is_empty:\n            continue\n\n        # Handle MultiPolygons by processing the largest part\n        if isinstance(geom, MultiPolygon):\n            areas = [p.area for p in geom.geoms]\n            if not areas:\n                continue\n            geom = list(geom.geoms)[np.argmax(areas)]\n\n        # Filter out tiny Objects\n        if geom.area &lt; min_area:\n            continue\n\n        # Check orthogonality\n        orthogonality = check_orthogonality(geom)\n\n        # Create oriented envelope\n        oriented_box = create_oriented_envelope(\n            geom, calculate_dominant_direction(geom)\n        )\n\n        # Check rectangularity\n        rectangularity = get_rectangularity(geom, oriented_box)\n\n        # Decide how to regularize\n        if rectangularity &gt;= rectangularity_threshold:\n            # Object is already quite rectangular, simplify to a rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"rectangle\"\n            rectangularized_count += 1\n        elif orthogonality &gt;= orthogonality_threshold:\n            # Object has many orthogonal angles but isn't rectangular\n            # Could implement more sophisticated regularization here\n            # For now, we'll still use the oriented rectangle\n            result_gdf.at[idx, \"geometry\"] = oriented_box\n            result_gdf.at[idx, \"regularized\"] = \"orthogonal\"\n            regularized_count += 1\n        else:\n            # Object doesn't have clear orthogonal structure\n            # Keep original but flag as unmodified\n            result_gdf.at[idx, \"regularized\"] = \"original\"\n\n    # Report statistics\n    print(f\"Regularization completed:\")\n    print(f\"- Total objects: {total_objects}\")\n    print(\n        f\"- Rectangular objects: {rectangularized_count} ({rectangularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Other regularized objects: {regularized_count} ({regularized_count/total_objects*100:.1f}%)\"\n    )\n    print(\n        f\"- Unmodified objects: {total_objects-rectangularized_count-regularized_count} ({(total_objects-rectangularized_count-regularized_count)/total_objects*100:.1f}%)\"\n    )\n\n    return result_gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.save_masks_as_geotiff","title":"<code>save_masks_as_geotiff(raster_path, output_path=None, batch_size=4, verbose=False, **kwargs)</code>","text":"<p>Process a raster file to extract object masks and save as GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster file</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to output GeoTIFF file (optional, default: input_masks.tif)</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for processing</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed processing information</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters: confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0) chip_size: Size of image chips for processing (height, width) mask_threshold: Threshold for mask binarization (0.0-1.0)</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the saved GeoTIFF file</p> Source code in <code>geoai/extract.py</code> <pre><code>def save_masks_as_geotiff(\n    self,\n    raster_path: str,\n    output_path: Optional[str] = None,\n    batch_size: int = 4,\n    verbose: bool = False,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Process a raster file to extract object masks and save as GeoTIFF.\n\n    Args:\n        raster_path: Path to input raster file\n        output_path: Path to output GeoTIFF file (optional, default: input_masks.tif)\n        batch_size: Batch size for processing\n        verbose: Whether to print detailed processing information\n        **kwargs: Additional parameters:\n            confidence_threshold: Minimum confidence score to keep a detection (0.0-1.0)\n            chip_size: Size of image chips for processing (height, width)\n            mask_threshold: Threshold for mask binarization (0.0-1.0)\n\n    Returns:\n        Path to the saved GeoTIFF file\n    \"\"\"\n\n    # Get parameters from kwargs or use instance defaults\n    confidence_threshold = kwargs.get(\n        \"confidence_threshold\", self.confidence_threshold\n    )\n    chip_size = kwargs.get(\"chip_size\", self.chip_size)\n    mask_threshold = kwargs.get(\"mask_threshold\", self.mask_threshold)\n    overlap = kwargs.get(\"overlap\", self.overlap)\n\n    # Set default output path if not provided\n    if output_path is None:\n        output_path = os.path.splitext(raster_path)[0] + \"_masks.tif\"\n\n    # Print parameters being used\n    print(f\"Processing masks with parameters:\")\n    print(f\"- Confidence threshold: {confidence_threshold}\")\n    print(f\"- Chip size: {chip_size}\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n\n    # Create dataset\n    dataset = CustomDataset(\n        raster_path=raster_path,\n        chip_size=chip_size,\n        overlap=overlap,\n        verbose=verbose,\n    )\n\n    # Store a flag to avoid repetitive messages\n    self.raster_stats = dataset.raster_stats\n    seen_warnings = {\n        \"bands\": False,\n        \"resize\": {},  # Dictionary to track resize warnings by shape\n    }\n\n    # Open original raster to get metadata\n    with rasterio.open(raster_path) as src:\n        # Create output binary mask raster with same dimensions as input\n        output_profile = src.profile.copy()\n        output_profile.update(\n            dtype=rasterio.uint8,\n            count=1,  # Single band for object mask\n            compress=\"lzw\",\n            nodata=0,\n        )\n\n        # Create output mask raster\n        with rasterio.open(output_path, \"w\", **output_profile) as dst:\n            # Initialize mask with zeros\n            mask_array = np.zeros((src.height, src.width), dtype=np.uint8)\n\n            # Custom collate function to handle Shapely objects\n            def custom_collate(batch: List[Dict[str, Any]]) -&gt; Dict[str, Any]:\n                \"\"\"Custom collate function for DataLoader\"\"\"\n                elem = batch[0]\n                if isinstance(elem, dict):\n                    result = {}\n                    for key in elem:\n                        if key == \"bbox\":\n                            # Don't collate shapely objects, keep as list\n                            result[key] = [d[key] for d in batch]\n                        else:\n                            # For tensors and other collatable types\n                            try:\n                                result[key] = (\n                                    torch.utils.data._utils.collate.default_collate(\n                                        [d[key] for d in batch]\n                                    )\n                                )\n                            except TypeError:\n                                # Fall back to list for non-collatable types\n                                result[key] = [d[key] for d in batch]\n                    return result\n                else:\n                    # Default collate for non-dict types\n                    return torch.utils.data._utils.collate.default_collate(batch)\n\n            # Create dataloader\n            dataloader = torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=0,\n                collate_fn=custom_collate,\n            )\n\n            # Process batches\n            print(f\"Processing raster with {len(dataloader)} batches\")\n            for batch in tqdm(dataloader):\n                # Move images to device\n                images = batch[\"image\"].to(self.device)\n                coords = batch[\"coords\"]  # (i, j) coordinates in pixels\n\n                # Run inference\n                with torch.no_grad():\n                    predictions = self.model(images)\n\n                # Process predictions\n                for idx, prediction in enumerate(predictions):\n                    masks = prediction[\"masks\"].cpu().numpy()\n                    scores = prediction[\"scores\"].cpu().numpy()\n\n                    # Skip if no predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Filter by confidence threshold\n                    valid_indices = scores &gt;= confidence_threshold\n                    masks = masks[valid_indices]\n                    scores = scores[valid_indices]\n\n                    # Skip if no valid predictions\n                    if len(scores) == 0:\n                        continue\n\n                    # Get window coordinates\n                    if isinstance(coords, list):\n                        coord_item = coords[idx]\n                        if isinstance(coord_item, tuple) and len(coord_item) == 2:\n                            i, j = coord_item\n                        elif isinstance(coord_item, torch.Tensor):\n                            i, j = coord_item.cpu().numpy().tolist()\n                        else:\n                            print(f\"Unexpected coords format: {type(coord_item)}\")\n                            continue\n                    elif isinstance(coords, torch.Tensor):\n                        i, j = coords[idx].cpu().numpy().tolist()\n                    else:\n                        print(f\"Unexpected coords type: {type(coords)}\")\n                        continue\n\n                    # Get window size\n                    if isinstance(batch[\"window_size\"], list):\n                        window_item = batch[\"window_size\"][idx]\n                        if isinstance(window_item, tuple) and len(window_item) == 2:\n                            window_width, window_height = window_item\n                        elif isinstance(window_item, torch.Tensor):\n                            window_width, window_height = (\n                                window_item.cpu().numpy().tolist()\n                            )\n                        else:\n                            print(\n                                f\"Unexpected window_size format: {type(window_item)}\"\n                            )\n                            continue\n                    elif isinstance(batch[\"window_size\"], torch.Tensor):\n                        window_width, window_height = (\n                            batch[\"window_size\"][idx].cpu().numpy().tolist()\n                        )\n                    else:\n                        print(\n                            f\"Unexpected window_size type: {type(batch['window_size'])}\"\n                        )\n                        continue\n\n                    # Combine all masks for this window\n                    combined_mask = np.zeros(\n                        (window_height, window_width), dtype=np.uint8\n                    )\n\n                    for mask in masks:\n                        # Get the binary mask\n                        binary_mask = (mask[0] &gt; mask_threshold).astype(\n                            np.uint8\n                        ) * 255\n\n                        # Handle size mismatch - resize binary_mask if needed\n                        mask_h, mask_w = binary_mask.shape\n                        if mask_h != window_height or mask_w != window_width:\n                            resize_key = f\"{(mask_h, mask_w)}-&gt;{(window_height, window_width)}\"\n                            if resize_key not in seen_warnings[\"resize\"]:\n                                if verbose:\n                                    print(\n                                        f\"Resizing mask from {binary_mask.shape} to {(window_height, window_width)}\"\n                                    )\n                                else:\n                                    if not seen_warnings[\n                                        \"resize\"\n                                    ]:  # If this is the first resize warning\n                                        print(\n                                            f\"Resizing masks at image edges (set verbose=True for details)\"\n                                        )\n                                seen_warnings[\"resize\"][resize_key] = True\n\n                            # Crop or pad the binary mask to match window size\n                            resized_mask = np.zeros(\n                                (window_height, window_width), dtype=np.uint8\n                            )\n                            copy_h = min(mask_h, window_height)\n                            copy_w = min(mask_w, window_width)\n                            resized_mask[:copy_h, :copy_w] = binary_mask[\n                                :copy_h, :copy_w\n                            ]\n                            binary_mask = resized_mask\n\n                        # Update combined mask (taking maximum where masks overlap)\n                        combined_mask = np.maximum(combined_mask, binary_mask)\n\n                    # Write combined mask to output array\n                    # Handle edge cases where window might be smaller than chip size\n                    h, w = combined_mask.shape\n                    valid_h = min(h, src.height - j)\n                    valid_w = min(w, src.width - i)\n\n                    if valid_h &gt; 0 and valid_w &gt; 0:\n                        mask_array[j : j + valid_h, i : i + valid_w] = np.maximum(\n                            mask_array[j : j + valid_h, i : i + valid_w],\n                            combined_mask[:valid_h, :valid_w],\n                        )\n\n            # Write the final mask to the output file\n            dst.write(mask_array, 1)\n\n    print(f\"Object masks saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.vectorize_masks","title":"<code>vectorize_masks(masks_path, output_path=None, confidence_threshold=0.5, min_object_area=100, max_object_area=None, n_workers=None, **kwargs)</code>","text":"<p>Convert masks with confidence to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>masks_path</code> <code>str</code> <p>Path to masks GeoTIFF with confidence band.</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path for output GeoJSON.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence score (0.0-1.0). Default: 0.5</p> <code>0.5</code> <code>min_object_area</code> <code>int</code> <p>Minimum area in pixels to keep an object. Default: 100</p> <code>100</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Maximum area in pixels to keep an object. Default: None</p> <code>None</code> <code>n_workers</code> <code>Optional[int]</code> <p>int, default=None The number of worker threads to use. \"None\" means single-threaded processing. \"-1\"   means using all available CPU processors. Positive integer means using that specific number of threads.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>GeoDataFrame with car detections and confidence values</p> Source code in <code>geoai/extract.py</code> <pre><code>def vectorize_masks(\n    self,\n    masks_path: str,\n    output_path: Optional[str] = None,\n    confidence_threshold: float = 0.5,\n    min_object_area: int = 100,\n    max_object_area: Optional[int] = None,\n    n_workers: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert masks with confidence to vector polygons.\n\n    Args:\n        masks_path: Path to masks GeoTIFF with confidence band.\n        output_path: Path for output GeoJSON.\n        confidence_threshold: Minimum confidence score (0.0-1.0). Default: 0.5\n        min_object_area: Minimum area in pixels to keep an object. Default: 100\n        max_object_area: Maximum area in pixels to keep an object. Default: None\n        n_workers: int, default=None\n            The number of worker threads to use.\n            \"None\" means single-threaded processing.\n            \"-1\"   means using all available CPU processors.\n            Positive integer means using that specific number of threads.\n        **kwargs: Additional parameters\n\n    Returns:\n        GeoDataFrame with car detections and confidence values\n    \"\"\"\n\n    def _process_single_component(\n        component_mask: np.ndarray,\n        conf_data: np.ndarray,\n        transform: Any,\n        confidence_threshold: float,\n        min_object_area: int,\n        max_object_area: Optional[int],\n    ) -&gt; Optional[Dict[str, Any]]:\n        # Get confidence value\n        conf_region = conf_data[component_mask &gt; 0]\n        if len(conf_region) &gt; 0:\n            confidence = np.mean(conf_region) / 255.0\n        else:\n            confidence = 0.0\n\n        # Skip if confidence is below threshold\n        if confidence &lt; confidence_threshold:\n            return None\n\n        # Find contours\n        contours, _ = cv2.findContours(\n            component_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        results = []\n\n        for contour in contours:\n            # Filter by size\n            area = cv2.contourArea(contour)\n            if area &lt; min_object_area:\n                continue\n\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Get minimum area rectangle\n            rect = cv2.minAreaRect(contour)\n            box_points = cv2.boxPoints(rect)\n\n            # Convert to geographic coordinates\n            geo_points = []\n            for x, y in box_points:\n                gx, gy = transform * (x, y)\n                geo_points.append((gx, gy))\n\n            # Create polygon\n            poly = Polygon(geo_points)\n            results.append((poly, confidence, area))\n\n        return results\n\n    import concurrent.futures\n    from functools import partial\n\n    def process_component(\n        args: Tuple[int, np.ndarray, np.ndarray, Any, float, int, Optional[int]],\n    ) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Helper function to process a single component\n        \"\"\"\n        (\n            label,\n            labeled_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        ) = args\n\n        # Create mask for this component\n        component_mask = (labeled_mask == label).astype(np.uint8)\n\n        return _process_single_component(\n            component_mask,\n            conf_data,\n            transform,\n            confidence_threshold,\n            min_object_area,\n            max_object_area,\n        )\n\n    start_time = time.time()\n    print(f\"Processing masks from: {masks_path}\")\n\n    if n_workers == -1:\n        n_workers = os.cpu_count()\n\n    with rasterio.open(masks_path) as src:\n        # Read mask and confidence bands\n        mask_data = src.read(1)\n        conf_data = src.read(2)\n        transform = src.transform\n        crs = src.crs\n\n        # Convert to binary mask\n        binary_mask = mask_data &gt; 0\n\n        # Find connected components\n        labeled_mask, num_features = ndimage.label(binary_mask)\n        print(f\"Found {num_features} connected components\")\n\n        # Process each component\n        polygons = []\n        confidences = []\n        pixels = []\n\n        if n_workers is None or n_workers == 1:\n            print(\n                \"Using single-threaded processing, you can speed up processing by setting n_workers &gt; 1\"\n            )\n            # Add progress bar\n            for label in tqdm(\n                range(1, num_features + 1), desc=\"Processing components\"\n            ):\n                # Create mask for this component\n                component_mask = (labeled_mask == label).astype(np.uint8)\n\n                result = _process_single_component(\n                    component_mask,\n                    conf_data,\n                    transform,\n                    confidence_threshold,\n                    min_object_area,\n                    max_object_area,\n                )\n\n                if result:\n                    for poly, confidence, area in result:\n                        # Add to lists\n                        polygons.append(poly)\n                        confidences.append(confidence)\n                        pixels.append(area)\n\n        else:\n            # Process components in parallel\n            print(f\"Using {n_workers} workers for parallel processing\")\n\n            process_args = [\n                (\n                    label,\n                    labeled_mask,\n                    conf_data,\n                    transform,\n                    confidence_threshold,\n                    min_object_area,\n                    max_object_area,\n                )\n                for label in range(1, num_features + 1)\n            ]\n\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=n_workers\n            ) as executor:\n                results = list(\n                    tqdm(\n                        executor.map(process_component, process_args),\n                        total=num_features,\n                        desc=\"Processing components\",\n                    )\n                )\n\n                for result in results:\n                    if result:\n                        for poly, confidence, area in result:\n                            # Add to lists\n                            polygons.append(poly)\n                            confidences.append(confidence)\n                            pixels.append(area)\n\n        # Create GeoDataFrame\n        if polygons:\n            gdf = gpd.GeoDataFrame(\n                {\n                    \"geometry\": polygons,\n                    \"confidence\": confidences,\n                    \"class\": [1] * len(polygons),\n                    \"pixels\": pixels,\n                },\n                crs=crs,\n            )\n\n            # Save to file if requested\n            if output_path:\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n                print(f\"Saved {len(gdf)} objects with confidence to {output_path}\")\n\n            end_time = time.time()\n            print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n            return gdf\n        else:\n            end_time = time.time()\n            print(f\"Total processing time: {end_time - start_time:.2f} seconds\")\n            print(\"No valid polygons found\")\n            return None\n</code></pre>"},{"location":"geoai/#geoai.geoai.ObjectDetector.visualize_results","title":"<code>visualize_results(raster_path, gdf=None, output_path=None, figsize=(12, 12))</code>","text":"<p>Visualize object detection results with proper coordinate transformation.</p> <p>This function displays objects on top of the raster image, ensuring proper alignment between the GeoDataFrame polygons and the image.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to input raster</p> required <code>gdf</code> <code>Optional[GeoDataFrame]</code> <p>GeoDataFrame with object polygons (optional)</p> <code>None</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to save visualization (optional)</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Figure size (width, height) in inches</p> <code>(12, 12)</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if visualization was successful</p> Source code in <code>geoai/extract.py</code> <pre><code>def visualize_results(\n    self,\n    raster_path: str,\n    gdf: Optional[gpd.GeoDataFrame] = None,\n    output_path: Optional[str] = None,\n    figsize: Tuple[int, int] = (12, 12),\n) -&gt; bool:\n    \"\"\"\n    Visualize object detection results with proper coordinate transformation.\n\n    This function displays objects on top of the raster image,\n    ensuring proper alignment between the GeoDataFrame polygons and the image.\n\n    Args:\n        raster_path: Path to input raster\n        gdf: GeoDataFrame with object polygons (optional)\n        output_path: Path to save visualization (optional)\n        figsize: Figure size (width, height) in inches\n\n    Returns:\n        bool: True if visualization was successful\n    \"\"\"\n    # Check if raster file exists\n    if not os.path.exists(raster_path):\n        print(f\"Error: Raster file '{raster_path}' not found.\")\n        return False\n\n    # Process raster if GeoDataFrame not provided\n    if gdf is None:\n        gdf = self.process_raster(raster_path)\n\n    if gdf is None or len(gdf) == 0:\n        print(\"No objects to visualize\")\n        return False\n\n    # Check if confidence column exists in the GeoDataFrame\n    has_confidence = False\n    if hasattr(gdf, \"columns\") and \"confidence\" in gdf.columns:\n        # Try to access a confidence value to confirm it works\n        try:\n            if len(gdf) &gt; 0:\n                # Try getitem access\n                conf_val = gdf[\"confidence\"].iloc[0]\n                has_confidence = True\n                print(\n                    f\"Using confidence values (range: {gdf['confidence'].min():.2f} - {gdf['confidence'].max():.2f})\"\n                )\n        except Exception as e:\n            print(f\"Confidence column exists but couldn't access values: {e}\")\n            has_confidence = False\n    else:\n        print(\"No confidence column found in GeoDataFrame\")\n        has_confidence = False\n\n    # Read raster for visualization\n    with rasterio.open(raster_path) as src:\n        # Read the entire image or a subset if it's very large\n        if src.height &gt; 2000 or src.width &gt; 2000:\n            # Calculate scale factor to reduce size\n            scale = min(2000 / src.height, 2000 / src.width)\n            out_shape = (\n                int(src.count),\n                int(src.height * scale),\n                int(src.width * scale),\n            )\n\n            # Read and resample\n            image = src.read(\n                out_shape=out_shape, resampling=rasterio.enums.Resampling.bilinear\n            )\n\n            # Create a scaled transform for the resampled image\n            # Calculate scaling factors\n            x_scale = src.width / out_shape[2]\n            y_scale = src.height / out_shape[1]\n\n            # Get the original transform\n            orig_transform = src.transform\n\n            # Create a scaled transform\n            scaled_transform = rasterio.transform.Affine(\n                orig_transform.a * x_scale,\n                orig_transform.b,\n                orig_transform.c,\n                orig_transform.d,\n                orig_transform.e * y_scale,\n                orig_transform.f,\n            )\n        else:\n            image = src.read()\n            scaled_transform = src.transform\n\n        # Convert to RGB for display\n        if image.shape[0] &gt; 3:\n            image = image[:3]\n        elif image.shape[0] == 1:\n            image = np.repeat(image, 3, axis=0)\n\n        # Normalize image for display\n        image = image.transpose(1, 2, 0)  # CHW to HWC\n        image = image.astype(np.float32)\n\n        if image.max() &gt; 10:  # Likely 0-255 range\n            image = image / 255.0\n\n        image = np.clip(image, 0, 1)\n\n        # Get image bounds\n        bounds = src.bounds\n        crs = src.crs\n\n    # Create figure with appropriate aspect ratio\n    aspect_ratio = image.shape[1] / image.shape[0]  # width / height\n    plt.figure(figsize=(figsize[0], figsize[0] / aspect_ratio))\n    ax = plt.gca()\n\n    # Display image\n    ax.imshow(image)\n\n    # Make sure the GeoDataFrame has the same CRS as the raster\n    if gdf.crs != crs:\n        print(f\"Reprojecting GeoDataFrame from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Set up colors for confidence visualization\n    if has_confidence:\n        try:\n            import matplotlib.cm as cm\n            from matplotlib.colors import Normalize\n\n            # Get min/max confidence values\n            min_conf = gdf[\"confidence\"].min()\n            max_conf = gdf[\"confidence\"].max()\n\n            # Set up normalization and colormap\n            norm = Normalize(vmin=min_conf, vmax=max_conf)\n            cmap = cm.viridis\n\n            # Create scalar mappable for colorbar\n            sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n            sm.set_array([])\n\n            # Add colorbar\n            cbar = plt.colorbar(\n                sm, ax=ax, orientation=\"vertical\", shrink=0.7, pad=0.01\n            )\n            cbar.set_label(\"Confidence Score\")\n        except Exception as e:\n            print(f\"Error setting up confidence visualization: {e}\")\n            has_confidence = False\n\n    # Function to convert coordinates\n    def geo_to_pixel(\n        geometry: Any, transform: Any\n    ) -&gt; Optional[Tuple[List[float], List[float]]]:\n        \"\"\"Convert geometry to pixel coordinates using the provided transform.\"\"\"\n        if geometry.is_empty:\n            return None\n\n        if geometry.geom_type == \"Polygon\":\n            # Get exterior coordinates\n            exterior_coords = list(geometry.exterior.coords)\n\n            # Convert to pixel coordinates\n            pixel_coords = [~transform * (x, y) for x, y in exterior_coords]\n\n            # Split into x and y lists\n            pixel_x = [coord[0] for coord in pixel_coords]\n            pixel_y = [coord[1] for coord in pixel_coords]\n\n            return pixel_x, pixel_y\n        else:\n            print(f\"Unsupported geometry type: {geometry.geom_type}\")\n            return None\n\n    # Plot each object\n    for idx, row in gdf.iterrows():\n        try:\n            # Convert polygon to pixel coordinates\n            coords = geo_to_pixel(row.geometry, scaled_transform)\n\n            if coords:\n                pixel_x, pixel_y = coords\n\n                if has_confidence:\n                    try:\n                        # Get confidence value using different methods\n                        # Method 1: Try direct attribute access\n                        confidence = None\n                        try:\n                            confidence = row.confidence\n                        except:\n                            pass\n\n                        # Method 2: Try dictionary-style access\n                        if confidence is None:\n                            try:\n                                confidence = row[\"confidence\"]\n                            except:\n                                pass\n\n                        # Method 3: Try accessing by index from the GeoDataFrame\n                        if confidence is None:\n                            try:\n                                confidence = gdf.iloc[idx][\"confidence\"]\n                            except:\n                                pass\n\n                        if confidence is not None:\n                            color = cmap(norm(confidence))\n                            # Fill polygon with semi-transparent color\n                            ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                            # Draw border\n                            ax.plot(\n                                pixel_x,\n                                pixel_y,\n                                color=color,\n                                linewidth=1,\n                                alpha=0.8,\n                            )\n                        else:\n                            # Fall back to red if confidence value couldn't be accessed\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                    except Exception as e:\n                        print(\n                            f\"Error using confidence value for polygon {idx}: {e}\"\n                        )\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n                else:\n                    # No confidence data, just plot outlines in red\n                    ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1)\n        except Exception as e:\n            print(f\"Error plotting polygon {idx}: {e}\")\n\n    # Remove axes\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f\"objects (Found: {len(gdf)})\")\n\n    # Save if requested\n    if output_path:\n        plt.tight_layout()\n        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Visualization saved to {output_path}\")\n\n    plt.close()\n\n    # Create a simpler visualization focused just on a subset of objects\n    if len(gdf) &gt; 0:\n        plt.figure(figsize=figsize)\n        ax = plt.gca()\n\n        # Choose a subset of the image to show\n        with rasterio.open(raster_path) as src:\n            # Get centroid of first object\n            sample_geom = gdf.iloc[0].geometry\n            centroid = sample_geom.centroid\n\n            # Convert to pixel coordinates\n            center_x, center_y = ~src.transform * (centroid.x, centroid.y)\n\n            # Define a window around this object\n            window_size = 500  # pixels\n            window = rasterio.windows.Window(\n                max(0, int(center_x - window_size / 2)),\n                max(0, int(center_y - window_size / 2)),\n                min(window_size, src.width - int(center_x - window_size / 2)),\n                min(window_size, src.height - int(center_y - window_size / 2)),\n            )\n\n            # Read this window\n            sample_image = src.read(window=window)\n\n            # Convert to RGB for display\n            if sample_image.shape[0] &gt; 3:\n                sample_image = sample_image[:3]\n            elif sample_image.shape[0] == 1:\n                sample_image = np.repeat(sample_image, 3, axis=0)\n\n            # Normalize image for display\n            sample_image = sample_image.transpose(1, 2, 0)  # CHW to HWC\n            sample_image = sample_image.astype(np.float32)\n\n            if sample_image.max() &gt; 10:  # Likely 0-255 range\n                sample_image = sample_image / 255.0\n\n            sample_image = np.clip(sample_image, 0, 1)\n\n            # Display sample image\n            ax.imshow(sample_image, extent=[0, window.width, window.height, 0])\n\n            # Get the correct transform for this window\n            window_transform = src.window_transform(window)\n\n            # Calculate bounds of the window\n            window_bounds = rasterio.windows.bounds(window, src.transform)\n            window_box = box(*window_bounds)\n\n            # Filter objects that intersect with this window\n            visible_gdf = gdf[gdf.intersects(window_box)]\n\n            # Set up colors for sample view if confidence data exists\n            if has_confidence:\n                try:\n                    # Reuse the same normalization and colormap from main view\n                    sample_sm = cm.ScalarMappable(cmap=cmap, norm=norm)\n                    sample_sm.set_array([])\n\n                    # Add colorbar to sample view\n                    sample_cbar = plt.colorbar(\n                        sample_sm,\n                        ax=ax,\n                        orientation=\"vertical\",\n                        shrink=0.7,\n                        pad=0.01,\n                    )\n                    sample_cbar.set_label(\"Confidence Score\")\n                except Exception as e:\n                    print(f\"Error setting up sample confidence visualization: {e}\")\n\n            # Plot objects in sample view\n            for idx, row in visible_gdf.iterrows():\n                try:\n                    # Get window-relative pixel coordinates\n                    geom = row.geometry\n\n                    # Skip empty geometries\n                    if geom.is_empty:\n                        continue\n\n                    # Get exterior coordinates\n                    exterior_coords = list(geom.exterior.coords)\n\n                    # Convert to pixel coordinates relative to window origin\n                    pixel_coords = []\n                    for x, y in exterior_coords:\n                        px, py = ~src.transform * (x, y)  # Convert to image pixels\n                        # Make coordinates relative to window\n                        px = px - window.col_off\n                        py = py - window.row_off\n                        pixel_coords.append((px, py))\n\n                    # Extract x and y coordinates\n                    pixel_x = [coord[0] for coord in pixel_coords]\n                    pixel_y = [coord[1] for coord in pixel_coords]\n\n                    # Use confidence colors if available\n                    if has_confidence:\n                        try:\n                            # Try different methods to access confidence\n                            confidence = None\n                            try:\n                                confidence = row.confidence\n                            except:\n                                pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = row[\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is None:\n                                try:\n                                    confidence = visible_gdf.iloc[idx][\"confidence\"]\n                                except:\n                                    pass\n\n                            if confidence is not None:\n                                color = cmap(norm(confidence))\n                                # Fill polygon with semi-transparent color\n                                ax.fill(pixel_x, pixel_y, color=color, alpha=0.5)\n                                # Draw border\n                                ax.plot(\n                                    pixel_x,\n                                    pixel_y,\n                                    color=color,\n                                    linewidth=1.5,\n                                    alpha=0.8,\n                                )\n                            else:\n                                ax.plot(\n                                    pixel_x, pixel_y, color=\"red\", linewidth=1.5\n                                )\n                        except Exception as e:\n                            print(\n                                f\"Error using confidence in sample view for polygon {idx}: {e}\"\n                            )\n                            ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                    else:\n                        ax.plot(pixel_x, pixel_y, color=\"red\", linewidth=1.5)\n                except Exception as e:\n                    print(f\"Error plotting polygon in sample view: {e}\")\n\n            # Set title\n            ax.set_title(f\"Sample Area - objects (Showing: {len(visible_gdf)})\")\n\n            # Remove axes\n            ax.set_xticks([])\n            ax.set_yticks([])\n\n            # Save if requested\n            if output_path:\n                sample_output = (\n                    os.path.splitext(output_path)[0]\n                    + \"_sample\"\n                    + os.path.splitext(output_path)[1]\n                )\n                plt.tight_layout()\n                plt.savefig(sample_output, dpi=300, bbox_inches=\"tight\")\n                print(f\"Sample visualization saved to {sample_output}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.ParkingSplotDetector","title":"<code>ParkingSplotDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Car detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for car detection.</p> Source code in <code>geoai/extract.py</code> <pre><code>class ParkingSplotDetector(ObjectDetector):\n    \"\"\"\n    Car detection using a pre-trained Mask R-CNN model.\n\n    This class extends the `ObjectDetector` class with additional methods for car detection.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"parking_spot_detection.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        num_classes: int = 3,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            num_classes: Number of classes for the model. Default: 3\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path,\n            repo_id=repo_id,\n            model=model,\n            num_classes=num_classes,\n            device=device,\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.ParkingSplotDetector.__init__","title":"<code>__init__(model_path='parking_spot_detection.pth', repo_id=None, model=None, num_classes=3, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'parking_spot_detection.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>num_classes</code> <code>int</code> <p>Number of classes for the model. Default: 3</p> <code>3</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"parking_spot_detection.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    num_classes: int = 3,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        num_classes: Number of classes for the model. Default: 3\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path,\n        repo_id=repo_id,\n        model=model,\n        num_classes=num_classes,\n        device=device,\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.ShipDetector","title":"<code>ShipDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Ship detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for ship detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class ShipDetector(ObjectDetector):\n    \"\"\"\n    Ship detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for ship detection.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"ship_detection.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.ShipDetector.__init__","title":"<code>__init__(model_path='ship_detection.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'ship_detection.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"ship_detection.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.SolarPanelDetector","title":"<code>SolarPanelDetector</code>","text":"<p>               Bases: <code>ObjectDetector</code></p> <p>Solar panel detection using a pre-trained Mask R-CNN model.</p> <p>This class extends the <code>ObjectDetector</code> class with additional methods for solar panel detection.\"</p> Source code in <code>geoai/extract.py</code> <pre><code>class SolarPanelDetector(ObjectDetector):\n    \"\"\"\n    Solar panel detection using a pre-trained Mask R-CNN model.\n\n    This class extends the\n    `ObjectDetector` class with additional methods for solar panel detection.\"\n    \"\"\"\n\n    def __init__(\n        self,\n        model_path: str = \"solar_panel_detection.pth\",\n        repo_id: Optional[str] = None,\n        model: Optional[Any] = None,\n        device: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the object extractor.\n\n        Args:\n            model_path: Path to the .pth model file.\n            repo_id: Repo ID for loading models from the Hub.\n            model: Custom model to use for inference.\n            device: Device to use for inference ('cuda:0', 'cpu', etc.).\n        \"\"\"\n        super().__init__(\n            model_path=model_path, repo_id=repo_id, model=model, device=device\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.SolarPanelDetector.__init__","title":"<code>__init__(model_path='solar_panel_detection.pth', repo_id=None, model=None, device=None)</code>","text":"<p>Initialize the object extractor.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the .pth model file.</p> <code>'solar_panel_detection.pth'</code> <code>repo_id</code> <code>Optional[str]</code> <p>Repo ID for loading models from the Hub.</p> <code>None</code> <code>model</code> <code>Optional[Any]</code> <p>Custom model to use for inference.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for inference ('cuda:0', 'cpu', etc.).</p> <code>None</code> Source code in <code>geoai/extract.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"solar_panel_detection.pth\",\n    repo_id: Optional[str] = None,\n    model: Optional[Any] = None,\n    device: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the object extractor.\n\n    Args:\n        model_path: Path to the .pth model file.\n        repo_id: Repo ID for loading models from the Hub.\n        model: Custom model to use for inference.\n        device: Device to use for inference ('cuda:0', 'cpu', etc.).\n    \"\"\"\n    super().__init__(\n        model_path=model_path, repo_id=repo_id, model=model, device=device\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.adaptive_regularization","title":"<code>adaptive_regularization(building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True)</code>","text":"<p>Adaptively regularizes building footprints based on their characteristics.</p> <p>This approach determines the best regularization method for each building.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons</p> required <code>simplify_tolerance</code> <code>float</code> <p>Distance tolerance for simplification</p> <code>0.5</code> <code>area_threshold</code> <code>float</code> <p>Minimum acceptable area ratio</p> <code>0.9</code> <code>preserve_shape</code> <code>bool</code> <p>Whether to preserve overall shape for complex buildings</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def adaptive_regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n    simplify_tolerance: float = 0.5,\n    area_threshold: float = 0.9,\n    preserve_shape: bool = True,\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    Adaptively regularizes building footprints based on their characteristics.\n\n    This approach determines the best regularization method for each building.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons\n        simplify_tolerance: Distance tolerance for simplification\n        area_threshold: Minimum acceptable area ratio\n        preserve_shape: Whether to preserve overall shape for complex buildings\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Analyze the overall dataset to set appropriate parameters\n    if is_gdf := isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # Skip invalid geometries\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Measure building complexity\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Determine if the building has a clear principal direction\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Normalize angles to 0-180 range and get histogram\n        norm_angles = angles % 180\n        hist, bins = np.histogram(\n            norm_angles, bins=18, range=(0, 180), weights=segment_lengths\n        )\n\n        # Calculate direction clarity (ratio of longest direction to total)\n        direction_clarity = np.max(hist) / np.sum(hist) if np.sum(hist) &gt; 0 else 0\n\n        # Choose regularization method based on building characteristics\n        if complexity &lt; 1.2 and direction_clarity &gt; 0.5:\n            # Simple building with clear direction: use rotated rectangle\n            bin_max = np.argmax(hist)\n            bin_centers = (bins[:-1] + bins[1:]) / 2\n            dominant_angle = bin_centers[bin_max]\n\n            # Rotate to align with coordinate system\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create bounding box in rotated space\n            bounds = rotated.bounds\n            rect = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            # Rotate back\n            result = rotate(rect, dominant_angle, origin=\"centroid\")\n\n            # Quality check\n            if (\n                result.area / building.area &lt; area_threshold\n                or result.area / building.area &gt; (1.0 / area_threshold)\n            ):\n                # Too much area change, use simplified original\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n\n        else:\n            # Complex building or no clear direction: preserve shape\n            if preserve_shape:\n                # Simplify with topology preservation\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n            else:\n                # Fall back to convex hull for very complex shapes\n                result = building.convex_hull\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"geoai/#geoai.geoai.add_geometric_properties","title":"<code>add_geometric_properties(data, properties=None, area_unit='m2', length_unit='m')</code>","text":"<p>Calculates geometric properties and adds them to the GeoDataFrame.</p> <p>This function calculates various geometric properties of features in a GeoDataFrame and adds them as new columns without modifying existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing vector features.</p> required <code>properties</code> <code>Optional[List[str]]</code> <p>List of geometric properties to calculate. Options include: 'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds', 'convex_hull_area', 'orientation', 'complexity', 'area_bbox', 'area_convex', 'area_filled', 'major_length', 'minor_length', 'eccentricity', 'diameter_areagth', 'extent', 'solidity', 'elongation'. Defaults to ['area', 'length'] if None.</p> <code>None</code> <code>area_unit</code> <code>str</code> <p>String specifying the unit for area calculation ('m2', 'km2', 'ha'). Defaults to 'm2'.</p> <code>'m2'</code> <code>length_unit</code> <code>str</code> <p>String specifying the unit for length calculation ('m', 'km'). Defaults to 'm'.</p> <code>'m'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added</p> <code>GeoDataFrame</code> <p>geometric property columns.</p> Source code in <code>geoai/utils.py</code> <pre><code>def add_geometric_properties(\n    data: gpd.GeoDataFrame,\n    properties: Optional[List[str]] = None,\n    area_unit: str = \"m2\",\n    length_unit: str = \"m\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Calculates geometric properties and adds them to the GeoDataFrame.\n\n    This function calculates various geometric properties of features in a\n    GeoDataFrame and adds them as new columns without modifying existing attributes.\n\n    Args:\n        data: GeoDataFrame containing vector features.\n        properties: List of geometric properties to calculate. Options include:\n            'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds',\n            'convex_hull_area', 'orientation', 'complexity', 'area_bbox',\n            'area_convex', 'area_filled', 'major_length', 'minor_length',\n            'eccentricity', 'diameter_areagth', 'extent', 'solidity',\n            'elongation'.\n            Defaults to ['area', 'length'] if None.\n        area_unit: String specifying the unit for area calculation ('m2', 'km2',\n            'ha'). Defaults to 'm2'.\n        length_unit: String specifying the unit for length calculation ('m', 'km').\n            Defaults to 'm'.\n\n    Returns:\n        geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added\n        geometric property columns.\n    \"\"\"\n    from shapely.ops import unary_union\n\n    if isinstance(data, str):\n        data = read_vector(data)\n\n    # Make a copy to avoid modifying the original\n    result = data.copy()\n\n    # Default properties to calculate\n    if properties is None:\n        properties = [\n            \"area\",\n            \"length\",\n            \"perimeter\",\n            \"convex_hull_area\",\n            \"orientation\",\n            \"complexity\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"diameter_area\",\n            \"extent\",\n            \"solidity\",\n            \"elongation\",\n        ]\n\n    # Make sure we're working with a GeoDataFrame with a valid CRS\n\n    if not isinstance(result, gpd.GeoDataFrame):\n        raise ValueError(\"Input must be a GeoDataFrame\")\n\n    if result.crs is None:\n        raise ValueError(\n            \"GeoDataFrame must have a defined coordinate reference system (CRS)\"\n        )\n\n    # Ensure we're working with a projected CRS for accurate measurements\n    if result.crs.is_geographic:\n        # Reproject to a suitable projected CRS for accurate measurements\n        result = result.to_crs(result.estimate_utm_crs())\n\n    # Basic area calculation with unit conversion\n    if \"area\" in properties:\n        # Calculate area (only for polygons)\n        result[\"area\"] = result.geometry.apply(\n            lambda geom: geom.area if isinstance(geom, (Polygon, MultiPolygon)) else 0\n        )\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area\"] = result[\"area\"] / 1_000_000  # m\u00b2 to km\u00b2\n            result.rename(columns={\"area\": \"area_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area\"] = result[\"area\"] / 10_000  # m\u00b2 to hectares\n            result.rename(columns={\"area\": \"area_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area\": \"area_m2\"}, inplace=True)\n\n    # Length calculation with unit conversion\n    if \"length\" in properties:\n        # Calculate length (works for lines and polygon boundaries)\n        result[\"length\"] = result.geometry.length\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"length\"] = result[\"length\"] / 1_000  # m to km\n            result.rename(columns={\"length\": \"length_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"length\": \"length_m\"}, inplace=True)\n\n    # Perimeter calculation (for polygons)\n    if \"perimeter\" in properties:\n        result[\"perimeter\"] = result.geometry.apply(\n            lambda geom: (\n                geom.boundary.length if isinstance(geom, (Polygon, MultiPolygon)) else 0\n            )\n        )\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"perimeter\"] = result[\"perimeter\"] / 1_000  # m to km\n            result.rename(columns={\"perimeter\": \"perimeter_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"perimeter\": \"perimeter_m\"}, inplace=True)\n\n    # Centroid coordinates\n    if \"centroid_x\" in properties or \"centroid_y\" in properties:\n        centroids = result.geometry.centroid\n\n        if \"centroid_x\" in properties:\n            result[\"centroid_x\"] = centroids.x\n\n        if \"centroid_y\" in properties:\n            result[\"centroid_y\"] = centroids.y\n\n    # Bounding box properties\n    if \"bounds\" in properties:\n        bounds = result.geometry.bounds\n        result[\"minx\"] = bounds.minx\n        result[\"miny\"] = bounds.miny\n        result[\"maxx\"] = bounds.maxx\n        result[\"maxy\"] = bounds.maxy\n\n    # Area of bounding box\n    if \"area_bbox\" in properties:\n        bounds = result.geometry.bounds\n        result[\"area_bbox\"] = (bounds.maxx - bounds.minx) * (bounds.maxy - bounds.miny)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 1_000_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 10_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_bbox\": \"area_bbox_m2\"}, inplace=True)\n\n    # Area of convex hull\n    if \"area_convex\" in properties or \"convex_hull_area\" in properties:\n        result[\"area_convex\"] = result.geometry.convex_hull.area\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 1_000_000\n            result.rename(columns={\"area_convex\": \"area_convex_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 10_000\n            result.rename(columns={\"area_convex\": \"area_convex_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_convex\": \"area_convex_m2\"}, inplace=True)\n\n        # For backward compatibility\n        if \"convex_hull_area\" in properties and \"area_convex\" not in properties:\n            result[\"convex_hull_area\"] = result[\"area_convex\"]\n            if area_unit == \"km2\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_km2\"}, inplace=True\n                )\n            elif area_unit == \"ha\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_ha\"}, inplace=True\n                )\n            else:\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_m2\"}, inplace=True\n                )\n\n    # Area of filled geometry (no holes)\n    if \"area_filled\" in properties:\n\n        def get_filled_area(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return 0\n\n            if isinstance(geom, MultiPolygon):\n                # For MultiPolygon, fill all constituent polygons\n                filled_polys = [Polygon(p.exterior) for p in geom.geoms]\n                return unary_union(filled_polys).area\n            else:\n                # For single Polygon, create a new one with just the exterior ring\n                return Polygon(geom.exterior).area\n\n        result[\"area_filled\"] = result.geometry.apply(get_filled_area)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 1_000_000\n            result.rename(columns={\"area_filled\": \"area_filled_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 10_000\n            result.rename(columns={\"area_filled\": \"area_filled_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_filled\": \"area_filled_m2\"}, inplace=True)\n\n    # Axes lengths, eccentricity, orientation, and elongation\n    if any(\n        p in properties\n        for p in [\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"orientation\",\n            \"elongation\",\n        ]\n    ):\n\n        def get_axes_properties(geom):\n            # Skip non-polygons\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return None, None, None, None, None\n\n            # Handle multipolygons by using the largest polygon\n            if isinstance(geom, MultiPolygon):\n                # Get the polygon with the largest area\n                geom = sorted(list(geom.geoms), key=lambda p: p.area, reverse=True)[0]\n\n            try:\n                # Get the minimum rotated rectangle\n                rect = geom.minimum_rotated_rectangle\n\n                # Extract coordinates\n                coords = list(rect.exterior.coords)[\n                    :-1\n                ]  # Remove the duplicated last point\n\n                if len(coords) &lt; 4:\n                    return None, None, None, None, None\n\n                # Calculate lengths of all four sides\n                sides = []\n                for i in range(len(coords)):\n                    p1 = coords[i]\n                    p2 = coords[(i + 1) % len(coords)]\n                    dx = p2[0] - p1[0]\n                    dy = p2[1] - p1[1]\n                    length = np.sqrt(dx**2 + dy**2)\n                    angle = np.degrees(np.arctan2(dy, dx)) % 180\n                    sides.append((length, angle, p1, p2))\n\n                # Group sides by length (allowing for small differences due to floating point precision)\n                # This ensures we correctly identify the rectangle's dimensions\n                sides_grouped = {}\n                tolerance = 1e-6  # Tolerance for length comparison\n\n                for s in sides:\n                    length, angle = s[0], s[1]\n                    matched = False\n\n                    for key in sides_grouped:\n                        if abs(length - key) &lt; tolerance:\n                            sides_grouped[key].append(s)\n                            matched = True\n                            break\n\n                    if not matched:\n                        sides_grouped[length] = [s]\n\n                # Get unique lengths (should be 2 for a rectangle, parallel sides have equal length)\n                unique_lengths = sorted(sides_grouped.keys(), reverse=True)\n\n                if len(unique_lengths) != 2:\n                    # If we don't get exactly 2 unique lengths, something is wrong with the rectangle\n                    # Fall back to simpler method using bounds\n                    bounds = rect.bounds\n                    width = bounds[2] - bounds[0]\n                    height = bounds[3] - bounds[1]\n                    major_length = max(width, height)\n                    minor_length = min(width, height)\n                    orientation = 0 if width &gt; height else 90\n                else:\n                    major_length = unique_lengths[0]\n                    minor_length = unique_lengths[1]\n                    # Get orientation from the major axis\n                    orientation = sides_grouped[major_length][0][1]\n\n                # Calculate eccentricity\n                if major_length &gt; 0:\n                    # Eccentricity for an ellipse: e = sqrt(1 - (b\u00b2/a\u00b2))\n                    # where a is the semi-major axis and b is the semi-minor axis\n                    eccentricity = np.sqrt(\n                        1 - ((minor_length / 2) ** 2 / (major_length / 2) ** 2)\n                    )\n                else:\n                    eccentricity = 0\n\n                # Calculate elongation (ratio of minor to major axis)\n                elongation = major_length / minor_length if major_length &gt; 0 else 1\n\n                return major_length, minor_length, eccentricity, orientation, elongation\n\n            except Exception as e:\n                # For debugging\n                # print(f\"Error calculating axes: {e}\")\n                return None, None, None, None, None\n\n        # Apply the function and split the results\n        axes_data = result.geometry.apply(get_axes_properties)\n\n        if \"major_length\" in properties:\n            result[\"major_length\"] = axes_data.apply(lambda x: x[0] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"major_length\"] = result[\"major_length\"] / 1_000\n                result.rename(columns={\"major_length\": \"major_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"major_length\": \"major_length_m\"}, inplace=True)\n\n        if \"minor_length\" in properties:\n            result[\"minor_length\"] = axes_data.apply(lambda x: x[1] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"minor_length\"] = result[\"minor_length\"] / 1_000\n                result.rename(columns={\"minor_length\": \"minor_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"minor_length\": \"minor_length_m\"}, inplace=True)\n\n        if \"eccentricity\" in properties:\n            result[\"eccentricity\"] = axes_data.apply(lambda x: x[2] if x else None)\n\n        if \"orientation\" in properties:\n            result[\"orientation\"] = axes_data.apply(lambda x: x[3] if x else None)\n\n        if \"elongation\" in properties:\n            result[\"elongation\"] = axes_data.apply(lambda x: x[4] if x else None)\n\n    # Equivalent diameter based on area\n    if \"diameter_areagth\" in properties:\n\n        def get_equivalent_diameter(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n            # Diameter of a circle with the same area: d = 2 * sqrt(A / \u03c0)\n            return 2 * np.sqrt(geom.area / np.pi)\n\n        result[\"diameter_areagth\"] = result.geometry.apply(get_equivalent_diameter)\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"diameter_areagth\"] = result[\"diameter_areagth\"] / 1_000\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_km\"},\n                inplace=True,\n            )\n        else:\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_m\"},\n                inplace=True,\n            )\n\n    # Extent (ratio of shape area to bounding box area)\n    if \"extent\" in properties:\n\n        def get_extent(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            bounds = geom.bounds\n            bbox_area = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])\n\n            if bbox_area &gt; 0:\n                return geom.area / bbox_area\n            return None\n\n        result[\"extent\"] = result.geometry.apply(get_extent)\n\n    # Solidity (ratio of shape area to convex hull area)\n    if \"solidity\" in properties:\n\n        def get_solidity(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            convex_hull_area = geom.convex_hull.area\n\n            if convex_hull_area &gt; 0:\n                return geom.area / convex_hull_area\n            return None\n\n        result[\"solidity\"] = result.geometry.apply(get_solidity)\n\n    # Complexity (ratio of perimeter to area)\n    if \"complexity\" in properties:\n\n        def calc_complexity(geom):\n            if isinstance(geom, (Polygon, MultiPolygon)) and geom.area &gt; 0:\n                # Shape index: P / (2 * sqrt(\u03c0 * A))\n                # Normalized to 1 for a circle, higher for more complex shapes\n                return geom.boundary.length / (2 * np.sqrt(np.pi * geom.area))\n            return None\n\n        result[\"complexity\"] = result.geometry.apply(calc_complexity)\n\n    return result\n</code></pre>"},{"location":"geoai/#geoai.geoai.analyze_vector_attributes","title":"<code>analyze_vector_attributes(vector_path, attribute_name)</code>","text":"<p>Analyze a specific attribute in a vector dataset and create a histogram.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to analyze</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing analysis results for the attribute</p> Source code in <code>geoai/utils.py</code> <pre><code>def analyze_vector_attributes(\n    vector_path: str, attribute_name: str\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Analyze a specific attribute in a vector dataset and create a histogram.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to analyze\n\n    Returns:\n        dict: Dictionary containing analysis results for the attribute\n    \"\"\"\n    try:\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return None\n\n        # Get the attribute series\n        attr = gdf[attribute_name]\n\n        # Perform different analyses based on data type\n        if pd.api.types.is_numeric_dtype(attr):\n            # Numeric attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"numeric\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"min\": attr.min(),\n                \"max\": attr.max(),\n                \"mean\": attr.mean(),\n                \"median\": attr.median(),\n                \"std\": attr.std(),\n                \"unique_values\": attr.nunique(),\n            }\n\n            # Create histogram\n            plt.figure(figsize=(10, 6))\n            plt.hist(attr.dropna(), bins=20, alpha=0.7, color=\"blue\")\n            plt.title(f\"Histogram of {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Frequency\")\n            plt.grid(True, alpha=0.3)\n            plt.show()\n\n        else:\n            # Categorical attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"categorical\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"unique_values\": attr.nunique(),\n                \"value_counts\": attr.value_counts().to_dict(),\n            }\n\n            # Create bar plot for top categories\n            top_n = min(10, attr.nunique())\n            plt.figure(figsize=(10, 6))\n            attr.value_counts().head(top_n).plot(kind=\"bar\", color=\"skyblue\")\n            plt.title(f\"Top {top_n} values for {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Count\")\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n\n        return analysis\n\n    except Exception as e:\n        print(f\"Error analyzing attribute: {str(e)}\")\n        return None\n</code></pre>"},{"location":"geoai/#geoai.geoai.batch_vector_to_raster","title":"<code>batch_vector_to_raster(vector_path, output_dir, attribute_field=None, reference_rasters=None, bounds_list=None, output_filename_pattern='{vector_name}_{index}', pixel_size=1.0, all_touched=False, fill_value=0, dtype=np.uint8, nodata=None)</code>","text":"<p>Batch convert vector data to multiple rasters based on different extents or reference rasters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output raster files.</p> required <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values.</p> <code>None</code> <code>reference_rasters</code> <code>list</code> <p>List of paths to reference rasters for dimensions, transform and CRS.</p> <code>None</code> <code>bounds_list</code> <code>list</code> <p>List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.</p> <code>None</code> <code>output_filename_pattern</code> <code>str</code> <p>Pattern for output filenames. Can include {vector_name} and {index} placeholders.</p> <code>'{vector_name}_{index}'</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size to use if reference_rasters not provided.</p> <code>1.0</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type of the output raster.</p> <code>uint8</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to the created raster files.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_vector_to_raster(\n    vector_path,\n    output_dir,\n    attribute_field=None,\n    reference_rasters=None,\n    bounds_list=None,\n    output_filename_pattern=\"{vector_name}_{index}\",\n    pixel_size=1.0,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n) -&gt; List[str]:\n    \"\"\"\n    Batch convert vector data to multiple rasters based on different extents or reference rasters.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_dir (str): Directory to save output raster files.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n        reference_rasters (list): List of paths to reference rasters for dimensions, transform and CRS.\n        bounds_list (list): List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.\n        output_filename_pattern (str): Pattern for output filenames.\n            Can include {vector_name} and {index} placeholders.\n        pixel_size (float or tuple): Pixel size to use if reference_rasters not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n\n    Returns:\n        List[str]: List of paths to the created raster files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load vector data if it's a path\n    if isinstance(vector_path, str):\n        gdf = gpd.read_file(vector_path)\n        vector_name = os.path.splitext(os.path.basename(vector_path))[0]\n    else:\n        gdf = vector_path\n        vector_name = \"vector\"\n\n    # Check input parameters\n    if reference_rasters is None and bounds_list is None:\n        raise ValueError(\"Either reference_rasters or bounds_list must be provided.\")\n\n    # Use reference_rasters if provided, otherwise use bounds_list\n    if reference_rasters is not None:\n        sources = reference_rasters\n        is_raster_reference = True\n    else:\n        sources = bounds_list\n        is_raster_reference = False\n\n    # Create output filenames\n    output_files = []\n\n    # Process each source (reference raster or bounds)\n    for i, source in enumerate(tqdm(sources, desc=\"Processing\")):\n        # Generate output filename\n        output_filename = output_filename_pattern.format(\n            vector_name=vector_name, index=i\n        )\n        if not output_filename.endswith(\".tif\"):\n            output_filename += \".tif\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        if is_raster_reference:\n            # Use reference raster\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                reference_raster=source,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n        else:\n            # Use bounds\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                bounds=source,\n                pixel_size=pixel_size,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n\n        output_files.append(output_path)\n\n    return output_files\n</code></pre>"},{"location":"geoai/#geoai.geoai.bbox_to_xy","title":"<code>bbox_to_xy(src_fp, coords, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.     Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright     While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[float]</code> <p>A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.</p> Source code in <code>geoai/utils.py</code> <pre><code>def bbox_to_xy(\n    src_fp: str, coords: List[float], coord_crs: str = \"epsg:4326\", **kwargs: Any\n) -&gt; List[float]:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n        Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n        While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n\n    Args:\n        src_fp (str): The source raster file path.\n        coords (list): A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]\n        coord_crs (str, optional): The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.\n    \"\"\"\n\n    if isinstance(coords, str):\n        gdf = gpd.read_file(coords)\n        coords = gdf.geometry.bounds.values.tolist()\n        if gdf.crs is not None:\n            coord_crs = f\"epsg:{gdf.crs.to_epsg()}\"\n    elif isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n    if isinstance(coords, dict):\n        import json\n\n        geojson = json.dumps(coords)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        coords = gdf.geometry.bounds.values.tolist()\n\n    elif not isinstance(coords, list):\n        raise ValueError(\"coords must be a list of coordinates.\")\n\n    if not isinstance(coords[0], list):\n        coords = [coords]\n\n    new_coords = []\n\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n\n        for coord in coords:\n            minx, miny, maxx, maxy = coord\n\n            if coord_crs != src.crs:\n                minx, miny = transform_coords(minx, miny, coord_crs, src.crs, **kwargs)\n                maxx, maxy = transform_coords(maxx, maxy, coord_crs, src.crs, **kwargs)\n\n                rows1, cols1 = rasterio.transform.rowcol(\n                    src.transform, minx, miny, **kwargs\n                )\n                rows2, cols2 = rasterio.transform.rowcol(\n                    src.transform, maxx, maxy, **kwargs\n                )\n\n                new_coords.append([cols1, rows1, cols2, rows2])\n\n            else:\n                new_coords.append([minx, miny, maxx, maxy])\n\n    result = []\n\n    for coord in new_coords:\n        minx, miny, maxx, maxy = coord\n\n        if (\n            minx &gt;= 0\n            and miny &gt;= 0\n            and maxx &gt;= 0\n            and maxy &gt;= 0\n            and minx &lt; width\n            and miny &lt; height\n            and maxx &lt; width\n            and maxy &lt; height\n        ):\n            # Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n            # While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n            result.append([minx, maxy, maxx, miny])\n\n    if len(result) == 0:\n        print(\"No valid pixel coordinates found.\")\n        return None\n    elif len(result) == 1:\n        return result[0]\n    elif len(result) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    return result\n</code></pre>"},{"location":"geoai/#geoai.geoai.boxes_to_vector","title":"<code>boxes_to_vector(coords, src_crs, dst_crs='EPSG:4326', output=None, **kwargs)</code>","text":"<p>Convert a list of bounding box coordinates to vector data.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].</p> required <code>src_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.</p> required <code>dst_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").</p> <code>'EPSG:4326'</code> <code>output</code> <code>str or None</code> <p>The full file path (including the directory and filename without the extension) where the vector data should be saved.                            If None (default), the function returns the GeoDataFrame without saving it to a file.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def boxes_to_vector(\n    coords: Union[List[List[float]], np.ndarray],\n    src_crs: str,\n    dst_crs: str = \"EPSG:4326\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a list of bounding box coordinates to vector data.\n\n    Args:\n        coords (list): A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].\n        src_crs (int or str): The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.\n        dst_crs (int or str, optional): The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").\n        output (str or None, optional): The full file path (including the directory and filename without the extension) where the vector data should be saved.\n                                       If None (default), the function returns the GeoDataFrame without saving it to a file.\n        **kwargs: Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.\n\n    Returns:\n        geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.\n    \"\"\"\n\n    from shapely.geometry import box\n\n    # Create a list of Shapely Polygon objects based on the provided coordinates\n    polygons = [box(*coord) for coord in coords]\n\n    # Create a GeoDataFrame with the Shapely Polygon objects\n    gdf = gpd.GeoDataFrame({\"geometry\": polygons}, crs=src_crs)\n\n    # Reproject the GeoDataFrame to the specified EPSG code\n    gdf_reprojected = gdf.to_crs(dst_crs)\n\n    if output is not None:\n        gdf_reprojected.to_file(output, **kwargs)\n    else:\n        return gdf_reprojected\n</code></pre>"},{"location":"geoai/#geoai.geoai.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.</p> Source code in <code>geoai/utils.py</code> <pre><code>def calc_stats(dataset, divide_by: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rasterio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"geoai/#geoai.geoai.clip_raster_by_bbox","title":"<code>clip_raster_by_bbox(input_raster, output_raster, bbox, bands=None, bbox_type='geo', bbox_crs=None)</code>","text":"<p>Clip a raster dataset using a bounding box and optionally select specific bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_raster</code> <code>str</code> <p>Path to the input raster file.</p> required <code>output_raster</code> <code>str</code> <p>Path where the clipped raster will be saved.</p> required <code>bbox</code> <code>tuple</code> <p>Bounding box coordinates either as:          - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"          - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"</p> required <code>bands</code> <code>list</code> <p>List of band indices to keep (1-based indexing).                    If None, all bands will be kept.</p> <code>None</code> <code>bbox_type</code> <code>str</code> <p>Type of bounding box coordinates. Either \"geo\" for                       geographic coordinates or \"pixel\" for row/column indices.                       Default is \"geo\".</p> <code>'geo'</code> <code>bbox_crs</code> <code>str or dict</code> <p>CRS of the bbox if different from the raster CRS.                              Can be provided as EPSG code (e.g., \"EPSG:4326\") or                              as a proj4 string. Only applies when bbox_type=\"geo\".                              If None, assumes bbox is in the same CRS as the raster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the clipped output raster.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>ValueError</code> <p>If the bbox is invalid, bands are out of range, or bbox_type is invalid.</p> <code>RuntimeError</code> <p>If the clipping operation fails.</p> <p>Examples:</p> <p>Clip using geographic coordinates in the same CRS as the raster</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n'clipped_geo.tif'\n</code></pre> <p>Clip using WGS84 coordinates when the raster is in a different CRS</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n...                     bbox_crs=\"EPSG:4326\")\n'clipped_wgs84.tif'\n</code></pre> <p>Clip using row/column indices</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n...                     bbox_type=\"pixel\")\n'clipped_pixel.tif'\n</code></pre> <p>Clip with band selection</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n...                     bands=[1, 3])\n'clipped_bands.tif'\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def clip_raster_by_bbox(\n    input_raster: str,\n    output_raster: str,\n    bbox: List[float],\n    bands: Optional[List[int]] = None,\n    bbox_type: str = \"geo\",\n    bbox_crs: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Clip a raster dataset using a bounding box and optionally select specific bands.\n\n    Args:\n        input_raster (str): Path to the input raster file.\n        output_raster (str): Path where the clipped raster will be saved.\n        bbox (tuple): Bounding box coordinates either as:\n                     - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"\n                     - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"\n        bands (list, optional): List of band indices to keep (1-based indexing).\n                               If None, all bands will be kept.\n        bbox_type (str, optional): Type of bounding box coordinates. Either \"geo\" for\n                                  geographic coordinates or \"pixel\" for row/column indices.\n                                  Default is \"geo\".\n        bbox_crs (str or dict, optional): CRS of the bbox if different from the raster CRS.\n                                         Can be provided as EPSG code (e.g., \"EPSG:4326\") or\n                                         as a proj4 string. Only applies when bbox_type=\"geo\".\n                                         If None, assumes bbox is in the same CRS as the raster.\n\n    Returns:\n        str: Path to the clipped output raster.\n\n    Raises:\n        ImportError: If required dependencies are not installed.\n        ValueError: If the bbox is invalid, bands are out of range, or bbox_type is invalid.\n        RuntimeError: If the clipping operation fails.\n\n    Examples:\n        Clip using geographic coordinates in the same CRS as the raster\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n        'clipped_geo.tif'\n\n        Clip using WGS84 coordinates when the raster is in a different CRS\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n        ...                     bbox_crs=\"EPSG:4326\")\n        'clipped_wgs84.tif'\n\n        Clip using row/column indices\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n        ...                     bbox_type=\"pixel\")\n        'clipped_pixel.tif'\n\n        Clip with band selection\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n        ...                     bands=[1, 3])\n        'clipped_bands.tif'\n    \"\"\"\n    from rasterio.transform import from_bounds\n    from rasterio.warp import transform_bounds\n\n    # Validate bbox_type\n    if bbox_type not in [\"geo\", \"pixel\"]:\n        raise ValueError(\"bbox_type must be either 'geo' or 'pixel'\")\n\n    # Validate bbox\n    if len(bbox) != 4:\n        raise ValueError(\"bbox must contain exactly 4 values\")\n\n    # Open the source raster\n    with rasterio.open(input_raster) as src:\n        # Get the source CRS\n        src_crs = src.crs\n\n        # Handle different bbox types\n        if bbox_type == \"geo\":\n            minx, miny, maxx, maxy = bbox\n\n            # Validate geographic bbox\n            if minx &gt;= maxx or miny &gt;= maxy:\n                raise ValueError(\n                    \"Invalid geographic bbox. Expected (minx, miny, maxx, maxy) where minx &lt; maxx and miny &lt; maxy\"\n                )\n\n            # If bbox_crs is provided and different from the source CRS, transform the bbox\n            if bbox_crs is not None and bbox_crs != src_crs:\n                try:\n                    # Transform bbox coordinates from bbox_crs to src_crs\n                    minx, miny, maxx, maxy = transform_bounds(\n                        bbox_crs, src_crs, minx, miny, maxx, maxy\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Failed to transform bbox from {bbox_crs} to {src_crs}: {str(e)}\"\n                    )\n\n            # Calculate the pixel window from geographic coordinates\n            window = src.window(minx, miny, maxx, maxy)\n\n            # Use the same bounds for the output transform\n            output_bounds = (minx, miny, maxx, maxy)\n\n        else:  # bbox_type == \"pixel\"\n            min_row, min_col, max_row, max_col = bbox\n\n            # Validate pixel bbox\n            if min_row &gt;= max_row or min_col &gt;= max_col:\n                raise ValueError(\n                    \"Invalid pixel bbox. Expected (min_row, min_col, max_row, max_col) where min_row &lt; max_row and min_col &lt; max_col\"\n                )\n\n            if (\n                min_row &lt; 0\n                or min_col &lt; 0\n                or max_row &gt; src.height\n                or max_col &gt; src.width\n            ):\n                raise ValueError(\n                    f\"Pixel indices out of bounds. Raster dimensions are {src.height} rows x {src.width} columns\"\n                )\n\n            # Create a window from pixel coordinates\n            window = Window(min_col, min_row, max_col - min_col, max_row - min_row)\n\n            # Calculate the geographic bounds for this window\n            window_transform = src.window_transform(window)\n            output_bounds = rasterio.transform.array_bounds(\n                window.height, window.width, window_transform\n            )\n            # Reorder to (minx, miny, maxx, maxy)\n            output_bounds = (\n                output_bounds[0],\n                output_bounds[1],\n                output_bounds[2],\n                output_bounds[3],\n            )\n\n        # Get window dimensions\n        window_width = int(window.width)\n        window_height = int(window.height)\n\n        # Check if the window is valid\n        if window_width &lt;= 0 or window_height &lt;= 0:\n            raise ValueError(\"Bounding box results in an empty window\")\n\n        # Handle band selection\n        if bands is None:\n            # Use all bands\n            bands_to_read = list(range(1, src.count + 1))\n        else:\n            # Validate band indices\n            if not all(1 &lt;= b &lt;= src.count for b in bands):\n                raise ValueError(f\"Band indices must be between 1 and {src.count}\")\n            bands_to_read = bands\n\n        # Calculate new transform for the clipped raster\n        new_transform = from_bounds(\n            output_bounds[0],\n            output_bounds[1],\n            output_bounds[2],\n            output_bounds[3],\n            window_width,\n            window_height,\n        )\n\n        # Create a metadata dictionary for the output\n        out_meta = src.meta.copy()\n        out_meta.update(\n            {\n                \"height\": window_height,\n                \"width\": window_width,\n                \"transform\": new_transform,\n                \"count\": len(bands_to_read),\n            }\n        )\n\n        # Read the data for the selected bands\n        data = []\n        for band_idx in bands_to_read:\n            band_data = src.read(band_idx, window=window)\n            data.append(band_data)\n\n        # Stack the bands into a single array\n        if len(data) &gt; 1:\n            clipped_data = np.stack(data)\n        else:\n            clipped_data = data[0][np.newaxis, :, :]\n\n        # Write the output raster\n        with rasterio.open(output_raster, \"w\", **out_meta) as dst:\n            dst.write(clipped_data)\n\n    return output_raster\n</code></pre>"},{"location":"geoai/#geoai.geoai.compute_class_weights","title":"<code>compute_class_weights(labels_dir, num_classes, ignore_index=None, custom_multipliers=None, max_weight=50.0, use_inverse_frequency=True)</code>","text":"<p>Compute class weights for imbalanced datasets with optional custom multipliers and maximum weight cap.</p> <p>Parameters:</p> Name Type Description Default <code>labels_dir</code> <code>str</code> <p>Directory containing label files</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> required <code>ignore_index</code> <code>Optional[int]</code> <p>Class index to ignore when computing weights (default: None)</p> <code>None</code> <code>custom_multipliers</code> <code>Optional[Dict[int, float]]</code> <p>Custom multipliers for specific classes after inverse frequency calculation. Format: {class_id: multiplier} Example: {1: 0.5, 7: 2.0} - reduce class 1 weight by half, double class 7 weight</p> <code>None</code> <code>max_weight</code> <code>float</code> <p>Maximum allowed weight value to prevent extreme values (default: 50.0)</p> <code>50.0</code> <code>use_inverse_frequency</code> <code>bool</code> <p>Whether to compute inverse frequency weights. - True (default): Compute inverse frequency weights, then apply custom multipliers - False: Use uniform weights (1.0) for all classes, then apply custom multipliers</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Tensor of class weights (num_classes,) with custom adjustments and maximum weight cap applied</p> Source code in <code>geoai/landcover_train.py</code> <pre><code>def compute_class_weights(\n    labels_dir: str,\n    num_classes: int,\n    ignore_index: Optional[int] = None,\n    custom_multipliers: Optional[Dict[int, float]] = None,\n    max_weight: float = 50.0,\n    use_inverse_frequency: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute class weights for imbalanced datasets with optional custom multipliers and maximum weight cap.\n\n    Args:\n        labels_dir: Directory containing label files\n        num_classes: Number of classes\n        ignore_index: Class index to ignore when computing weights (default: None)\n        custom_multipliers: Custom multipliers for specific classes after inverse frequency calculation.\n            Format: {class_id: multiplier}\n            Example: {1: 0.5, 7: 2.0} - reduce class 1 weight by half, double class 7 weight\n        max_weight: Maximum allowed weight value to prevent extreme values (default: 50.0)\n        use_inverse_frequency: Whether to compute inverse frequency weights.\n            - True (default): Compute inverse frequency weights, then apply custom multipliers\n            - False: Use uniform weights (1.0) for all classes, then apply custom multipliers\n\n    Returns:\n        Tensor of class weights (num_classes,) with custom adjustments and maximum weight cap applied\n    \"\"\"\n    import os\n    import rasterio\n    from collections import Counter\n\n    # Count pixels for each class\n    class_counts = Counter()\n    total_pixels = 0\n\n    # Get all label files\n    label_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n    label_files = [\n        os.path.join(labels_dir, f)\n        for f in os.listdir(labels_dir)\n        if f.lower().endswith(label_extensions)\n    ]\n\n    print(f\"Computing class weights from {len(label_files)} label files...\")\n\n    for label_file in label_files:\n        try:\n            with rasterio.open(label_file) as src:\n                label_data = src.read(1)\n                for class_id in range(num_classes):\n                    if ignore_index is not None and class_id == ignore_index:\n                        continue\n                    count = (label_data == class_id).sum()\n                    class_counts[class_id] += int(count)\n                    total_pixels += int(count)\n        except Exception as e:\n            print(f\"Warning: Could not read {label_file}: {e}\")\n            continue\n\n    if total_pixels == 0:\n        raise ValueError(\"No valid pixels found in label files\")\n\n    # Initialize weights\n    weights = torch.ones(num_classes)\n\n    if use_inverse_frequency:\n        # Compute inverse frequency weights\n        for class_id in range(num_classes):\n            if ignore_index is not None and class_id == ignore_index:\n                weights[class_id] = 0.0\n            elif class_counts[class_id] &gt; 0:\n                # Inverse frequency: total_pixels / class_pixels\n                weights[class_id] = total_pixels / class_counts[class_id]\n            else:\n                weights[class_id] = 0.0\n\n        # Normalize to have mean weight of 1.0\n        non_zero_weights = weights[weights &gt; 0]\n        if len(non_zero_weights) &gt; 0:\n            weights = weights / non_zero_weights.mean()\n    else:\n        # Use uniform weights (all 1.0)\n        for class_id in range(num_classes):\n            if ignore_index is not None and class_id == ignore_index:\n                weights[class_id] = 0.0\n\n    # Apply custom multipliers if provided\n    if custom_multipliers:\n        print(f\"\\n\ud83c\udfaf Applying custom multipliers: {custom_multipliers}\")\n        for class_id, multiplier in custom_multipliers.items():\n            if class_id &lt; 0 or class_id &gt;= num_classes:\n                print(f\"Warning: Invalid class_id {class_id}, skipping\")\n                continue\n\n            original_weight = weights[class_id].item()\n            weights[class_id] = weights[class_id] * multiplier\n            print(\n                f\"  Class {class_id}: {original_weight:.4f} \u00d7 {multiplier} = {weights[class_id].item():.4f}\"\n            )\n    else:\n        print(\"\\n\u2139\ufe0f  No custom multipliers provided, using computed weights as-is\")\n\n    # Apply maximum weight cap to prevent extreme values\n    weights_capped = False\n    print(f\"\\n\ud83d\udd12 Applying maximum weight cap of {max_weight}...\")\n    for class_id in range(num_classes):\n        if weights[class_id] &gt; max_weight:\n            print(\n                f\"  Class {class_id}: {weights[class_id].item():.4f} \u2192 {max_weight} (capped)\"\n            )\n            weights[class_id] = max_weight\n            weights_capped = True\n\n    if not weights_capped:\n        print(\"  No weights exceeded the cap\")\n\n    print(f\"\\nClass pixel counts: {dict(class_counts)}\")\n    print(f\"\\nFinal class weights:\")\n    for class_id in range(num_classes):\n        pixel_count = class_counts.get(class_id, 0)\n        percent = (pixel_count / total_pixels * 100) if total_pixels &gt; 0 else 0\n        print(\n            f\"  Class {class_id}: weight={weights[class_id].item():.4f}, \"\n            f\"pixels={pixel_count:,} ({percent:.2f}%)\"\n        )\n\n    if ignore_index is not None and 0 &lt;= ignore_index &lt; num_classes:\n        print(f\"\\n\u26a0\ufe0f  Note: Class {ignore_index} (ignore_index) has weight 0.0\")\n\n    return weights\n</code></pre>"},{"location":"geoai/#geoai.geoai.coords_to_xy","title":"<code>coords_to_xy(src_fp, coords, coord_crs='epsg:4326', return_out_of_bounds=False, **kwargs)</code>","text":"<p>Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>ndarray</code> <p>A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]     or [[[x1, y1]], [[x2, y2]], ...].</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>return_out_of_bounds</code> <code>bool</code> <p>Whether to return out-of-bounds coordinates. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D or 3D array of pixel coordinates in the same format as the input.</p> Source code in <code>geoai/utils.py</code> <pre><code>def coords_to_xy(\n    src_fp: str,\n    coords: np.ndarray,\n    coord_crs: str = \"epsg:4326\",\n    return_out_of_bounds: bool = False,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    \"\"\"Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        coords: A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]\n                or [[[x1, y1]], [[x2, y2]], ...].\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        return_out_of_bounds: Whether to return out-of-bounds coordinates. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A 2D or 3D array of pixel coordinates in the same format as the input.\n    \"\"\"\n    from rasterio.warp import transform as transform_coords\n\n    out_of_bounds = []\n    if isinstance(coords, np.ndarray):\n        input_is_3d = coords.ndim == 3  # Check if the input is a 3D array\n    else:\n        input_is_3d = False\n\n    # Flatten the 3D array to 2D if necessary\n    if input_is_3d:\n        original_shape = coords.shape  # Store the original shape\n        coords = coords.reshape(-1, 2)  # Flatten to 2D\n\n    # Convert ndarray to a list if necessary\n    if isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n\n    xs, ys = zip(*coords)\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n        if coord_crs != src.crs:\n            xs, ys = transform_coords(coord_crs, src.crs, xs, ys, **kwargs)\n        rows, cols = rasterio.transform.rowcol(src.transform, xs, ys, **kwargs)\n\n    result = [[col, row] for col, row in zip(cols, rows)]\n\n    output = []\n\n    for i, (x, y) in enumerate(result):\n        if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height:\n            output.append([x, y])\n        else:\n            out_of_bounds.append(i)\n\n    # Convert the output back to the original shape if input was 3D\n    output = np.array(output)\n    if input_is_3d:\n        output = output.reshape(original_shape)\n\n    # Handle cases where no valid pixel coordinates are found\n    if len(output) == 0:\n        print(\"No valid pixel coordinates found.\")\n    elif len(output) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    if return_out_of_bounds:\n        return output, out_of_bounds\n    else:\n        return output\n</code></pre>"},{"location":"geoai/#geoai.geoai.create_overview_image","title":"<code>create_overview_image(src, tile_coordinates, output_path, tile_size, stride, geojson_path=None)</code>","text":"<p>Create an overview image showing all tiles and their status, with optional GeoJSON export.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>DatasetReader</code> <p>The source raster dataset.</p> required <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>output_path</code> <code>str</code> <p>The path where the overview image will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels.</p> required <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Controls overlap between adjacent tiles.</p> required <code>geojson_path</code> <code>str</code> <p>If provided, exports the tile rectangles as GeoJSON to this path.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved overview image.</p> Source code in <code>geoai/utils.py</code> <pre><code>def create_overview_image(\n    src, tile_coordinates, output_path, tile_size, stride, geojson_path=None\n) -&gt; str:\n    \"\"\"Create an overview image showing all tiles and their status, with optional GeoJSON export.\n\n    Args:\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        output_path (str): The path where the overview image will be saved.\n        tile_size (int): The size of each tile in pixels.\n        stride (int): The stride between tiles in pixels. Controls overlap between adjacent tiles.\n        geojson_path (str, optional): If provided, exports the tile rectangles as GeoJSON to this path.\n\n    Returns:\n        str: Path to the saved overview image.\n    \"\"\"\n    # Read a reduced version of the source image\n    overview_scale = max(\n        1, int(max(src.width, src.height) / 2000)\n    )  # Scale to max ~2000px\n    overview_width = src.width // overview_scale\n    overview_height = src.height // overview_scale\n\n    # Read downsampled image\n    overview_data = src.read(\n        out_shape=(src.count, overview_height, overview_width),\n        resampling=rasterio.enums.Resampling.average,\n    )\n\n    # Create RGB image for display\n    if overview_data.shape[0] &gt;= 3:\n        rgb = np.moveaxis(overview_data[:3], 0, -1)\n    else:\n        # For single band, create grayscale RGB\n        rgb = np.stack([overview_data[0], overview_data[0], overview_data[0]], axis=-1)\n\n    # Normalize for display\n    for i in range(rgb.shape[-1]):\n        band = rgb[..., i]\n        non_zero = band[band &gt; 0]\n        if len(non_zero) &gt; 0:\n            p2, p98 = np.percentile(non_zero, (2, 98))\n            rgb[..., i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n\n    # Create figure\n    plt.figure(figsize=(12, 12))\n    plt.imshow(rgb)\n\n    # If GeoJSON export is requested, prepare GeoJSON structures\n    if geojson_path:\n        features = []\n\n    # Draw tile boundaries\n    for tile in tile_coordinates:\n        # Convert bounds to pixel coordinates in overview\n        bounds = tile[\"bounds\"]\n        # Calculate scaled pixel coordinates\n        x_min = int((tile[\"x\"]) / overview_scale)\n        y_min = int((tile[\"y\"]) / overview_scale)\n        width = int(tile_size / overview_scale)\n        height = int(tile_size / overview_scale)\n\n        # Draw rectangle\n        color = \"lime\" if tile[\"has_features\"] else \"red\"\n        rect = plt.Rectangle(\n            (x_min, y_min), width, height, fill=False, edgecolor=color, linewidth=0.5\n        )\n        plt.gca().add_patch(rect)\n\n        # Add tile number if not too crowded\n        if width &gt; 20 and height &gt; 20:\n            plt.text(\n                x_min + width / 2,\n                y_min + height / 2,\n                str(tile[\"index\"]),\n                color=\"white\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n            )\n\n        # Add to GeoJSON features if exporting\n        if geojson_path:\n            # Create a polygon from the bounds (already in geo-coordinates)\n            minx, miny, maxx, maxy = bounds\n            polygon = box(minx, miny, maxx, maxy)\n\n            # Calculate overlap with neighboring tiles\n            overlap = 0\n            if stride &lt; tile_size:\n                overlap = tile_size - stride\n\n            # Create a GeoJSON feature\n            feature = {\n                \"type\": \"Feature\",\n                \"geometry\": mapping(polygon),\n                \"properties\": {\n                    \"index\": tile[\"index\"],\n                    \"has_features\": tile[\"has_features\"],\n                    \"bounds_pixel\": [\n                        tile[\"x\"],\n                        tile[\"y\"],\n                        tile[\"x\"] + tile_size,\n                        tile[\"y\"] + tile_size,\n                    ],\n                    \"tile_size_px\": tile_size,\n                    \"stride_px\": stride,\n                    \"overlap_px\": overlap,\n                },\n            }\n\n            # Add any additional properties from the tile\n            for key, value in tile.items():\n                if key not in [\"x\", \"y\", \"index\", \"has_features\", \"bounds\"]:\n                    feature[\"properties\"][key] = value\n\n            features.append(feature)\n\n    plt.title(\"Tile Overview (Green = Contains Features, Red = Empty)\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"Overview image saved to {output_path}\")\n\n    # Export GeoJSON if requested\n    if geojson_path:\n        geojson_collection = {\n            \"type\": \"FeatureCollection\",\n            \"features\": features,\n            \"properties\": {\n                \"crs\": (\n                    src.crs.to_string()\n                    if hasattr(src.crs, \"to_string\")\n                    else str(src.crs)\n                ),\n                \"total_tiles\": len(features),\n                \"source_raster_dimensions\": [src.width, src.height],\n            },\n        }\n\n        # Save to file\n        with open(geojson_path, \"w\") as f:\n            json.dump(geojson_collection, f)\n\n        print(f\"GeoJSON saved to {geojson_path}\")\n\n    return output_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.create_split_map","title":"<code>create_split_map(left_layer='TERRAIN', right_layer='OpenTopoMap', left_args=None, right_args=None, left_array_args=None, right_array_args=None, zoom_control=True, fullscreen_control=True, layer_control=True, add_close_button=False, left_label=None, right_label=None, left_position='bottomleft', right_position='bottomright', widget_layout=None, draggable=True, center=[20, 0], zoom=2, height='600px', basemap=None, basemap_args=None, m=None, **kwargs)</code>","text":"<p>Adds split map.</p> <p>Parameters:</p> Name Type Description Default <code>left_layer</code> <code>str</code> <p>The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.</p> <code>'TERRAIN'</code> <code>right_layer</code> <code>str</code> <p>The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.</p> <code>'OpenTopoMap'</code> <code>left_args</code> <code>dict</code> <p>The arguments for the left tile layer. Defaults to {}.</p> <code>None</code> <code>right_args</code> <code>dict</code> <p>The arguments for the right tile layer. Defaults to {}.</p> <code>None</code> <code>left_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the left layer. Defaults to {}.</p> <code>None</code> <code>right_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the right layer. Defaults to {}.</p> <code>None</code> <code>zoom_control</code> <code>bool</code> <p>Whether to add zoom control. Defaults to True.</p> <code>True</code> <code>fullscreen_control</code> <code>bool</code> <p>Whether to add fullscreen control. Defaults to True.</p> <code>True</code> <code>layer_control</code> <code>bool</code> <p>Whether to add layer control. Defaults to True.</p> <code>True</code> <code>add_close_button</code> <code>bool</code> <p>Whether to add a close button. Defaults to False.</p> <code>False</code> <code>left_label</code> <code>str</code> <p>The label for the left layer. Defaults to None.</p> <code>None</code> <code>right_label</code> <code>str</code> <p>The label for the right layer. Defaults to None.</p> <code>None</code> <code>left_position</code> <code>str</code> <p>The position for the left label. Defaults to \"bottomleft\".</p> <code>'bottomleft'</code> <code>right_position</code> <code>str</code> <p>The position for the right label. Defaults to \"bottomright\".</p> <code>'bottomright'</code> <code>widget_layout</code> <code>dict</code> <p>The layout for the widget. Defaults to None.</p> <code>None</code> <code>draggable</code> <code>bool</code> <p>Whether the split map is draggable. Defaults to True.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def create_split_map(\n    left_layer: Optional[str] = \"TERRAIN\",\n    right_layer: Optional[str] = \"OpenTopoMap\",\n    left_args: Optional[dict] = None,\n    right_args: Optional[dict] = None,\n    left_array_args: Optional[dict] = None,\n    right_array_args: Optional[dict] = None,\n    zoom_control: Optional[bool] = True,\n    fullscreen_control: Optional[bool] = True,\n    layer_control: Optional[bool] = True,\n    add_close_button: Optional[bool] = False,\n    left_label: Optional[str] = None,\n    right_label: Optional[str] = None,\n    left_position: Optional[str] = \"bottomleft\",\n    right_position: Optional[str] = \"bottomright\",\n    widget_layout: Optional[dict] = None,\n    draggable: Optional[bool] = True,\n    center: Optional[List[float]] = [20, 0],\n    zoom: Optional[int] = 2,\n    height: Optional[int] = \"600px\",\n    basemap: Optional[str] = None,\n    basemap_args: Optional[Dict] = None,\n    m: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Adds split map.\n\n    Args:\n        left_layer (str, optional): The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.\n        right_layer (str, optional): The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.\n        left_args (dict, optional): The arguments for the left tile layer. Defaults to {}.\n        right_args (dict, optional): The arguments for the right tile layer. Defaults to {}.\n        left_array_args (dict, optional): The arguments for array_to_image for the left layer. Defaults to {}.\n        right_array_args (dict, optional): The arguments for array_to_image for the right layer. Defaults to {}.\n        zoom_control (bool, optional): Whether to add zoom control. Defaults to True.\n        fullscreen_control (bool, optional): Whether to add fullscreen control. Defaults to True.\n        layer_control (bool, optional): Whether to add layer control. Defaults to True.\n        add_close_button (bool, optional): Whether to add a close button. Defaults to False.\n        left_label (str, optional): The label for the left layer. Defaults to None.\n        right_label (str, optional): The label for the right layer. Defaults to None.\n        left_position (str, optional): The position for the left label. Defaults to \"bottomleft\".\n        right_position (str, optional): The position for the right label. Defaults to \"bottomright\".\n        widget_layout (dict, optional): The layout for the widget. Defaults to None.\n        draggable (bool, optional): Whether the split map is draggable. Defaults to True.\n    \"\"\"\n\n    if left_args is None:\n        left_args = {}\n\n    if right_args is None:\n        right_args = {}\n\n    if left_array_args is None:\n        left_array_args = {}\n\n    if right_array_args is None:\n        right_array_args = {}\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if m is None:\n        m = leafmap.Map(center=center, zoom=zoom, height=height, **kwargs)\n        m.clear_layers()\n    if isinstance(basemap, str):\n        if basemap.endswith(\".tif\"):\n            if basemap.startswith(\"http\"):\n                m.add_cog_layer(basemap, name=\"Basemap\", **basemap_args)\n            else:\n                m.add_raster(basemap, layer_name=\"Basemap\", **basemap_args)\n        else:\n            m.add_basemap(basemap)\n    m.split_map(\n        left_layer=left_layer,\n        right_layer=right_layer,\n        left_args=left_args,\n        right_args=right_args,\n        left_array_args=left_array_args,\n        right_array_args=right_array_args,\n        zoom_control=zoom_control,\n        fullscreen_control=fullscreen_control,\n        layer_control=layer_control,\n        add_close_button=add_close_button,\n        left_label=left_label,\n        right_label=right_label,\n        left_position=left_position,\n        right_position=right_position,\n        widget_layout=widget_layout,\n        draggable=draggable,\n    )\n\n    return m\n</code></pre>"},{"location":"geoai/#geoai.geoai.create_vector_data","title":"<code>create_vector_data(m=None, properties=None, time_format='%Y%m%dT%H%M%S', column_widths=(9, 3), map_height='600px', out_dir=None, filename_prefix='', file_ext='geojson', add_mapillary=False, style='photo', radius=5e-05, width=300, height=420, frame_border=0, **kwargs)</code>","text":"<p>Generates a widget-based interface for creating and managing vector data on a map.</p> <p>This function creates an interactive widget interface that allows users to draw features (points, lines, polygons) on a map, assign properties to these features, and export them as GeoJSON files. The interface includes a map, a sidebar for property management, and buttons for saving, exporting, and resetting the data.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Map</code> <p>An existing Map object. If not provided, a default map with basemaps and drawing controls will be created. Defaults to None.</p> <code>None</code> <code>properties</code> <code>Dict[str, List[Any]]</code> <p>A dictionary where keys are property names and values are lists of possible values for each property. These properties can be assigned to the drawn features. Defaults to None.</p> <code>None</code> <code>time_format</code> <code>str</code> <p>The format string for the timestamp used in the exported filename. Defaults to \"%Y%m%dT%H%M%S\".</p> <code>'%Y%m%dT%H%M%S'</code> <code>column_widths</code> <code>Optional[List[int]]</code> <p>A list of two integers specifying the relative widths of the map and sidebar columns. Defaults to (9, 3).</p> <code>(9, 3)</code> <code>map_height</code> <code>str</code> <p>The height of the map widget. Defaults to \"600px\".</p> <code>'600px'</code> <code>out_dir</code> <code>str</code> <p>The directory where the exported GeoJSON files will be saved. If not provided, the current working directory is used. Defaults to None.</p> <code>None</code> <code>filename_prefix</code> <code>str</code> <p>A prefix to be added to the exported filename. Defaults to \"\".</p> <code>''</code> <code>file_ext</code> <code>str</code> <p>The file extension for the exported file. Defaults to \"geojson\".</p> <code>'geojson'</code> <code>add_mapillary</code> <code>bool</code> <p>Whether to add a Mapillary image widget that displays the nearest image to the clicked point on the map. Defaults to False.</p> <code>False</code> <code>style</code> <code>str</code> <p>The style of the Mapillary image widget. Can be \"classic\", \"photo\", or \"split\". Defaults to \"photo\".</p> <code>'photo'</code> <code>radius</code> <code>float</code> <p>The radius (in degrees) used to search for the nearest Mapillary image. Defaults to 0.00005 degrees.</p> <code>5e-05</code> <code>width</code> <code>int</code> <p>The width of the Mapillary image widget. Defaults to 300.</p> <code>300</code> <code>height</code> <code>int</code> <p>The height of the Mapillary image widget. Defaults to 420.</p> <code>420</code> <code>frame_border</code> <code>int</code> <p>The width of the frame border for the Mapillary image widget. Defaults to 0.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that may be passed to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>widgets.VBox: A vertical box widget containing the map, sidebar, and control buttons.</p> Example <p>properties = { ...     \"Type\": [\"Residential\", \"Commercial\", \"Industrial\"], ...     \"Area\": [100, 200, 300], ... } widget = create_vector_data(properties=properties) display(widget)  # Display the widget in a Jupyter notebook</p> Source code in <code>geoai/geoai.py</code> <pre><code>def create_vector_data(\n    m: Optional[LeafMap] = None,\n    properties: Optional[Dict[str, List[Any]]] = None,\n    time_format: str = \"%Y%m%dT%H%M%S\",\n    column_widths: Optional[List[int]] = (9, 3),\n    map_height: str = \"600px\",\n    out_dir: Optional[str] = None,\n    filename_prefix: str = \"\",\n    file_ext: str = \"geojson\",\n    add_mapillary: bool = False,\n    style: str = \"photo\",\n    radius: float = 0.00005,\n    width: int = 300,\n    height: int = 420,\n    frame_border: int = 0,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Generates a widget-based interface for creating and managing vector data on a map.\n\n    This function creates an interactive widget interface that allows users to draw features\n    (points, lines, polygons) on a map, assign properties to these features, and export them\n    as GeoJSON files. The interface includes a map, a sidebar for property management, and\n    buttons for saving, exporting, and resetting the data.\n\n    Args:\n        m (Map, optional): An existing Map object. If not provided, a default map with\n            basemaps and drawing controls will be created. Defaults to None.\n        properties (Dict[str, List[Any]], optional): A dictionary where keys are property names\n            and values are lists of possible values for each property. These properties can be\n            assigned to the drawn features. Defaults to None.\n        time_format (str, optional): The format string for the timestamp used in the exported\n            filename. Defaults to \"%Y%m%dT%H%M%S\".\n        column_widths (Optional[List[int]], optional): A list of two integers specifying the\n            relative widths of the map and sidebar columns. Defaults to (9, 3).\n        map_height (str, optional): The height of the map widget. Defaults to \"600px\".\n        out_dir (str, optional): The directory where the exported GeoJSON files will be saved.\n            If not provided, the current working directory is used. Defaults to None.\n        filename_prefix (str, optional): A prefix to be added to the exported filename.\n            Defaults to \"\".\n        file_ext (str, optional): The file extension for the exported file. Defaults to \"geojson\".\n        add_mapillary (bool, optional): Whether to add a Mapillary image widget that displays the\n            nearest image to the clicked point on the map. Defaults to False.\n        style (str, optional): The style of the Mapillary image widget. Can be \"classic\", \"photo\",\n            or \"split\". Defaults to \"photo\".\n        radius (float, optional): The radius (in degrees) used to search for the nearest Mapillary\n            image. Defaults to 0.00005 degrees.\n        width (int, optional): The width of the Mapillary image widget. Defaults to 300.\n        height (int, optional): The height of the Mapillary image widget. Defaults to 420.\n        frame_border (int, optional): The width of the frame border for the Mapillary image widget.\n            Defaults to 0.\n        **kwargs (Any): Additional keyword arguments that may be passed to the function.\n\n    Returns:\n        widgets.VBox: A vertical box widget containing the map, sidebar, and control buttons.\n\n    Example:\n        &gt;&gt;&gt; properties = {\n        ...     \"Type\": [\"Residential\", \"Commercial\", \"Industrial\"],\n        ...     \"Area\": [100, 200, 300],\n        ... }\n        &gt;&gt;&gt; widget = create_vector_data(properties=properties)\n        &gt;&gt;&gt; display(widget)  # Display the widget in a Jupyter notebook\n    \"\"\"\n    return maplibregl.create_vector_data(\n        m=m,\n        properties=properties,\n        time_format=time_format,\n        column_widths=column_widths,\n        map_height=map_height,\n        out_dir=out_dir,\n        filename_prefix=filename_prefix,\n        file_ext=file_ext,\n        add_mapillary=add_mapillary,\n        style=style,\n        radius=radius,\n        width=width,\n        height=height,\n        frame_border=frame_border,\n        **kwargs,\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.dict_to_image","title":"<code>dict_to_image(data_dict, output=None, **kwargs)</code>","text":"<p>Convert a dictionary containing spatial data to a rasterio dataset or save it to a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>This function transforms a dictionary with CRS, bounding box, and image data into a rasterio DatasetReader using leafmap's array_to_image utility after first converting to a rioxarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing: - 'crs': A pyproj CRS object - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes   and optionally mint, maxt for temporal bounds - 'image': A tensor or array-like object with image data</p> required <code>output</code> <code>Optional[str]</code> <p>Optional path to save the image to a file. If not provided, the image will be returned as a rasterio DatasetReader object.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to leafmap.array_to_image. Common options include: - colormap: str, name of the colormap (e.g., 'viridis', 'terrain') - vmin: float, minimum value for colormap scaling - vmax: float, maximum value for colormap scaling</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Any]</code> <p>A rasterio DatasetReader object that can be used for visualization or</p> <code>Union[str, Any]</code> <p>further processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = dict_to_image(\n...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n...     colormap='terrain'\n... )\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n&gt;&gt;&gt; show(image, ax=ax)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_image(\n    data_dict: Dict[str, Any], output: Optional[str] = None, **kwargs: Any\n) -&gt; Union[str, Any]:\n    \"\"\"Convert a dictionary containing spatial data to a rasterio dataset or save it to\n    a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\".\n    It can be generated from a TorchGeo dataset sampler.\n\n    This function transforms a dictionary with CRS, bounding box, and image data\n    into a rasterio DatasetReader using leafmap's array_to_image utility after\n    first converting to a rioxarray DataArray.\n\n    Args:\n        data_dict: A dictionary containing:\n            - 'crs': A pyproj CRS object\n            - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes\n              and optionally mint, maxt for temporal bounds\n            - 'image': A tensor or array-like object with image data\n        output: Optional path to save the image to a file. If not provided, the image\n            will be returned as a rasterio DatasetReader object.\n        **kwargs: Additional keyword arguments to pass to leafmap.array_to_image.\n            Common options include:\n            - colormap: str, name of the colormap (e.g., 'viridis', 'terrain')\n            - vmin: float, minimum value for colormap scaling\n            - vmax: float, maximum value for colormap scaling\n\n    Returns:\n        A rasterio DatasetReader object that can be used for visualization or\n        further processing.\n\n    Examples:\n        &gt;&gt;&gt; image = dict_to_image(\n        ...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n        ...     colormap='terrain'\n        ... )\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n        &gt;&gt;&gt; show(image, ax=ax)\n    \"\"\"\n    da = dict_to_rioxarray(data_dict)\n\n    if output is not None:\n        out_dir = os.path.abspath(os.path.dirname(output))\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        da.rio.to_raster(output)\n        return output\n    else:\n        image = leafmap.array_to_image(da, **kwargs)\n        return image\n</code></pre>"},{"location":"geoai/#geoai.geoai.dict_to_rioxarray","title":"<code>dict_to_rioxarray(data_dict)</code>","text":"<p>Convert a dictionary to a xarray DataArray. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>The dictionary containing the data.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: The xarray DataArray.</p> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_rioxarray(data_dict: Dict) -&gt; xr.DataArray:\n    \"\"\"Convert a dictionary to a xarray DataArray. The dictionary should contain the\n    following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo\n    dataset sampler.\n\n    Args:\n        data_dict (Dict): The dictionary containing the data.\n\n    Returns:\n        xr.DataArray: The xarray DataArray.\n    \"\"\"\n\n    from collections import namedtuple\n\n    from affine import Affine\n\n    BoundingBox = namedtuple(\"BoundingBox\", [\"minx\", \"maxx\", \"miny\", \"maxy\"])\n\n    # Extract components from the dictionary\n    crs = data_dict[\"crs\"]\n    bounds = data_dict[\"bounds\"]\n    image_tensor = data_dict[\"image\"]\n\n    if hasattr(bounds, \"left\"):\n        bounds = BoundingBox(bounds.left, bounds.right, bounds.bottom, bounds.top)\n\n    # Convert tensor to numpy array if needed\n    if hasattr(image_tensor, \"numpy\"):\n        # For PyTorch tensors\n        image_array = image_tensor.numpy()\n    else:\n        # If it's already a numpy array or similar\n        image_array = np.array(image_tensor)\n\n    # Calculate pixel resolution\n    width = image_array.shape[2]  # Width is the size of the last dimension\n    height = image_array.shape[1]  # Height is the size of the middle dimension\n\n    res_x = (bounds.maxx - bounds.minx) / width\n    res_y = (bounds.maxy - bounds.miny) / height\n\n    # Create the transform matrix\n    transform = Affine(res_x, 0.0, bounds.minx, 0.0, -res_y, bounds.maxy)\n\n    # Create dimensions\n    x_coords = np.linspace(bounds.minx + res_x / 2, bounds.maxx - res_x / 2, width)\n    y_coords = np.linspace(bounds.maxy - res_y / 2, bounds.miny + res_y / 2, height)\n\n    # If time dimension exists in the bounds\n    if hasattr(bounds, \"mint\") and hasattr(bounds, \"maxt\"):\n        # Create a single time value or range if needed\n        t_coords = [\n            bounds.mint\n        ]  # Or np.linspace(bounds.mint, bounds.maxt, num_time_steps)\n\n        # Create DataArray with time dimension\n        dims = (\n            (\"band\", \"y\", \"x\")\n            if image_array.shape[0] &lt;= 10\n            else (\"time\", \"band\", \"y\", \"x\")\n        )\n\n        if dims[0] == \"band\":\n            # For multi-band single time\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"band\": np.arange(1, image_array.shape[0] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n        else:\n            # For multi-time multi-band\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"time\": t_coords,\n                    \"band\": np.arange(1, image_array.shape[1] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n    else:\n        # Create DataArray without time dimension\n        da = xr.DataArray(\n            image_array,\n            dims=(\"band\", \"y\", \"x\"),\n            coords={\n                \"band\": np.arange(1, image_array.shape[0] + 1),\n                \"y\": y_coords,\n                \"x\": x_coords,\n            },\n        )\n\n    # Set spatial attributes\n    da.rio.write_crs(crs, inplace=True)\n    da.rio.write_transform(transform, inplace=True)\n\n    return da\n</code></pre>"},{"location":"geoai/#geoai.geoai.download_file","title":"<code>download_file(url, output_path=None, overwrite=False, unzip=True)</code>","text":"<p>Download a file from a given URL with a progress bar. Optionally unzip the file if it's a ZIP archive.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>The path where the downloaded file will be saved. If not provided, the filename from the URL will be used.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Whether to unzip the file if it is a ZIP archive.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the downloaded file or the extracted directory.</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_file(\n    url: str,\n    output_path: Optional[str] = None,\n    overwrite: bool = False,\n    unzip: bool = True,\n) -&gt; str:\n    \"\"\"\n    Download a file from a given URL with a progress bar.\n    Optionally unzip the file if it's a ZIP archive.\n\n    Args:\n        url (str): The URL of the file to download.\n        output_path (str, optional): The path where the downloaded file will be saved.\n            If not provided, the filename from the URL will be used.\n        overwrite (bool, optional): Whether to overwrite the file if it already exists.\n        unzip (bool, optional): Whether to unzip the file if it is a ZIP archive.\n\n    Returns:\n        str: The path to the downloaded file or the extracted directory.\n    \"\"\"\n\n    import zipfile\n\n    from tqdm import tqdm\n\n    if output_path is None:\n        output_path = os.path.basename(url)\n\n    if os.path.exists(output_path) and not overwrite:\n        print(f\"File already exists: {output_path}\")\n    else:\n        # Download the file with a progress bar\n        response = requests.get(url, stream=True, timeout=50)\n        response.raise_for_status()\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        with (\n            open(output_path, \"wb\") as file,\n            tqdm(\n                desc=f\"Downloading {os.path.basename(output_path)}\",\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as progress_bar,\n        ):\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    file.write(chunk)\n                    progress_bar.update(len(chunk))\n\n    # If the file is a ZIP archive and unzip is True\n    if unzip and zipfile.is_zipfile(output_path):\n        extract_dir = os.path.splitext(output_path)[0]\n        if not os.path.exists(extract_dir) or overwrite:\n            with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n                zip_ref.extractall(extract_dir)\n            print(f\"Extracted to: {extract_dir}\")\n        return extract_dir\n\n    return output_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.download_model_from_hf","title":"<code>download_model_from_hf(model_path, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model file.</p> required <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded model file</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_model_from_hf(model_path: str, repo_id: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    from huggingface_hub import hf_hub_download\n\n    try:\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            print(\n                \"Repo is not specified, using default Hugging Face repo_id: giswqs/geoai\"\n            )\n            repo_id = \"giswqs/geoai\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"geoai/#geoai.geoai.edit_vector_data","title":"<code>edit_vector_data(m=None, filename=None, properties=None, time_format='%Y%m%dT%H%M%S', column_widths=(9, 3), map_height='600px', out_dir=None, filename_prefix='', file_ext='geojson', add_mapillary=False, style='photo', radius=5e-05, width=300, height=420, frame_border=0, controls=None, position='top-right', fit_bounds_options=None, **kwargs)</code>","text":"<p>Generates a widget-based interface for creating and managing vector data on a map.</p> <p>This function creates an interactive widget interface that allows users to draw features (points, lines, polygons) on a map, assign properties to these features, and export them as GeoJSON files. The interface includes a map, a sidebar for property management, and buttons for saving, exporting, and resetting the data.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Map</code> <p>An existing Map object. If not provided, a default map with basemaps and drawing controls will be created. Defaults to None.</p> <code>None</code> <code>filename</code> <code>str or GeoDataFrame</code> <p>The path to a GeoJSON file or a GeoDataFrame containing the vector data to be edited. Defaults to None.</p> <code>None</code> <code>properties</code> <code>Dict[str, List[Any]]</code> <p>A dictionary where keys are property names and values are lists of possible values for each property. These properties can be assigned to the drawn features. Defaults to None.</p> <code>None</code> <code>time_format</code> <code>str</code> <p>The format string for the timestamp used in the exported filename. Defaults to \"%Y%m%dT%H%M%S\".</p> <code>'%Y%m%dT%H%M%S'</code> <code>column_widths</code> <code>Optional[List[int]]</code> <p>A list of two integers specifying the relative widths of the map and sidebar columns. Defaults to (9, 3).</p> <code>(9, 3)</code> <code>map_height</code> <code>str</code> <p>The height of the map widget. Defaults to \"600px\".</p> <code>'600px'</code> <code>out_dir</code> <code>str</code> <p>The directory where the exported GeoJSON files will be saved. If not provided, the current working directory is used. Defaults to None.</p> <code>None</code> <code>filename_prefix</code> <code>str</code> <p>A prefix to be added to the exported filename. Defaults to \"\".</p> <code>''</code> <code>file_ext</code> <code>str</code> <p>The file extension for the exported file. Defaults to \"geojson\".</p> <code>'geojson'</code> <code>add_mapillary</code> <code>bool</code> <p>Whether to add a Mapillary image widget that displays the nearest image to the clicked point on the map. Defaults to False.</p> <code>False</code> <code>style</code> <code>str</code> <p>The style of the Mapillary image widget. Can be \"classic\", \"photo\", or \"split\". Defaults to \"photo\".</p> <code>'photo'</code> <code>radius</code> <code>float</code> <p>The radius (in degrees) used to search for the nearest Mapillary image. Defaults to 0.00005 degrees.</p> <code>5e-05</code> <code>width</code> <code>int</code> <p>The width of the Mapillary image widget. Defaults to 300.</p> <code>300</code> <code>height</code> <code>int</code> <p>The height of the Mapillary image widget. Defaults to 420.</p> <code>420</code> <code>frame_border</code> <code>int</code> <p>The width of the frame border for the Mapillary image widget. Defaults to 0.</p> <code>0</code> <code>controls</code> <code>Optional[List[str]]</code> <p>The drawing controls to be added to the map. Defaults to [\"point\", \"polygon\", \"line_string\", \"trash\"].</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the drawing controls on the map. Defaults to \"top-right\".</p> <code>'top-right'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that may be passed to the function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>widgets.VBox: A vertical box widget containing the map, sidebar, and control buttons.</p> Source code in <code>geoai/geoai.py</code> <pre><code>def edit_vector_data(\n    m: Optional[LeafMap] = None,\n    filename: str = None,\n    properties: Optional[Dict[str, List[Any]]] = None,\n    time_format: str = \"%Y%m%dT%H%M%S\",\n    column_widths: Optional[List[int]] = (9, 3),\n    map_height: str = \"600px\",\n    out_dir: Optional[str] = None,\n    filename_prefix: str = \"\",\n    file_ext: str = \"geojson\",\n    add_mapillary: bool = False,\n    style: str = \"photo\",\n    radius: float = 0.00005,\n    width: int = 300,\n    height: int = 420,\n    frame_border: int = 0,\n    controls: Optional[List[str]] = None,\n    position: str = \"top-right\",\n    fit_bounds_options: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Generates a widget-based interface for creating and managing vector data on a map.\n\n    This function creates an interactive widget interface that allows users to draw features\n    (points, lines, polygons) on a map, assign properties to these features, and export them\n    as GeoJSON files. The interface includes a map, a sidebar for property management, and\n    buttons for saving, exporting, and resetting the data.\n\n    Args:\n        m (Map, optional): An existing Map object. If not provided, a default map with\n            basemaps and drawing controls will be created. Defaults to None.\n        filename (str or gpd.GeoDataFrame): The path to a GeoJSON file or a GeoDataFrame\n            containing the vector data to be edited. Defaults to None.\n        properties (Dict[str, List[Any]], optional): A dictionary where keys are property names\n            and values are lists of possible values for each property. These properties can be\n            assigned to the drawn features. Defaults to None.\n        time_format (str, optional): The format string for the timestamp used in the exported\n            filename. Defaults to \"%Y%m%dT%H%M%S\".\n        column_widths (Optional[List[int]], optional): A list of two integers specifying the\n            relative widths of the map and sidebar columns. Defaults to (9, 3).\n        map_height (str, optional): The height of the map widget. Defaults to \"600px\".\n        out_dir (str, optional): The directory where the exported GeoJSON files will be saved.\n            If not provided, the current working directory is used. Defaults to None.\n        filename_prefix (str, optional): A prefix to be added to the exported filename.\n            Defaults to \"\".\n        file_ext (str, optional): The file extension for the exported file. Defaults to \"geojson\".\n        add_mapillary (bool, optional): Whether to add a Mapillary image widget that displays the\n            nearest image to the clicked point on the map. Defaults to False.\n        style (str, optional): The style of the Mapillary image widget. Can be \"classic\", \"photo\",\n            or \"split\". Defaults to \"photo\".\n        radius (float, optional): The radius (in degrees) used to search for the nearest Mapillary\n            image. Defaults to 0.00005 degrees.\n        width (int, optional): The width of the Mapillary image widget. Defaults to 300.\n        height (int, optional): The height of the Mapillary image widget. Defaults to 420.\n        frame_border (int, optional): The width of the frame border for the Mapillary image widget.\n            Defaults to 0.\n        controls (Optional[List[str]], optional): The drawing controls to be added to the map.\n            Defaults to [\"point\", \"polygon\", \"line_string\", \"trash\"].\n        position (str, optional): The position of the drawing controls on the map. Defaults to \"top-right\".\n        **kwargs (Any): Additional keyword arguments that may be passed to the function.\n\n    Returns:\n        widgets.VBox: A vertical box widget containing the map, sidebar, and control buttons.\n    \"\"\"\n    return maplibregl.edit_vector_data(\n        m=m,\n        filename=filename,\n        properties=properties,\n        time_format=time_format,\n        column_widths=column_widths,\n        map_height=map_height,\n        out_dir=out_dir,\n        filename_prefix=filename_prefix,\n        file_ext=file_ext,\n        add_mapillary=add_mapillary,\n        style=style,\n        radius=radius,\n        width=width,\n        height=height,\n        frame_border=frame_border,\n        controls=controls,\n        position=position,\n        fit_bounds_options=fit_bounds_options,\n        **kwargs,\n    )\n</code></pre>"},{"location":"geoai/#geoai.geoai.empty_cache","title":"<code>empty_cache()</code>","text":"<p>Empty the cache of the current device.</p> Source code in <code>geoai/utils.py</code> <pre><code>def empty_cache() -&gt; None:\n    \"\"\"Empty the cache of the current device.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n</code></pre>"},{"location":"geoai/#geoai.geoai.export_geotiff_tiles","title":"<code>export_geotiff_tiles(in_raster, out_folder, in_class_data, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False)</code>","text":"<p>Export georeferenced GeoTIFF tiles and labels from raster and classification data.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image</p> required <code>out_folder</code> <code>str</code> <p>Path to output folder</p> required <code>in_class_data</code> <code>str</code> <p>Path to classification data - can be vector file or raster</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles(\n    in_raster,\n    out_folder,\n    in_class_data,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n):\n    \"\"\"\n    Export georeferenced GeoTIFF tiles and labels from raster and classification data.\n\n    Args:\n        in_raster (str): Path to input raster image\n        out_folder (str): Path to output folder\n        in_class_data (str): Path to classification data - can be vector file or raster\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n    \"\"\"\n\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Create output directories\n    os.makedirs(out_folder, exist_ok=True)\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n    ann_dir = os.path.join(out_folder, \"annotations\")\n    os.makedirs(ann_dir, exist_ok=True)\n\n    # Determine if class data is raster or vector\n    is_class_data_raster = False\n    if isinstance(in_class_data, str):\n        file_ext = Path(in_class_data).suffix.lower()\n        # Common raster extensions\n        if file_ext in [\".tif\", \".tiff\", \".img\", \".jp2\", \".png\", \".bmp\", \".gif\"]:\n            try:\n                with rasterio.open(in_class_data) as src:\n                    is_class_data_raster = True\n                    if not quiet:\n                        print(f\"Detected in_class_data as raster: {in_class_data}\")\n                        print(f\"Raster CRS: {src.crs}\")\n                        print(f\"Raster dimensions: {src.width} x {src.height}\")\n            except Exception:\n                is_class_data_raster = False\n                if not quiet:\n                    print(f\"Unable to open {in_class_data} as raster, trying as vector\")\n\n    # Open the input raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Resolution: {src.res}\")\n            print(f\"  Bands: {src.count}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Calculate number of tiles\n        num_tiles_x = math.ceil((src.width - tile_size) / stride) + 1\n        num_tiles_y = math.ceil((src.height - tile_size) / stride) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        if max_tiles is None:\n            max_tiles = total_tiles\n\n        # Process classification data\n        class_to_id = {}\n\n        if is_class_data_raster:\n            # Load raster class data\n            with rasterio.open(in_class_data) as class_src:\n                # Check if raster CRS matches\n                if class_src.crs != src.crs:\n                    warnings.warn(\n                        f\"CRS mismatch: Class raster ({class_src.crs}) doesn't match input raster ({src.crs}). \"\n                        f\"Results may be misaligned.\"\n                    )\n\n                # Get unique values from raster\n                # Sample to avoid loading huge rasters\n                sample_data = class_src.read(\n                    1,\n                    out_shape=(\n                        1,\n                        min(class_src.height, 1000),\n                        min(class_src.width, 1000),\n                    ),\n                )\n\n                unique_classes = np.unique(sample_data)\n                unique_classes = unique_classes[\n                    unique_classes &gt; 0\n                ]  # Remove 0 as it's typically background\n\n                if not quiet:\n                    print(\n                        f\"Found {len(unique_classes)} unique classes in raster: {unique_classes}\"\n                    )\n\n                # Create class mapping\n                class_to_id = {int(cls): i + 1 for i, cls in enumerate(unique_classes)}\n        else:\n            # Load vector class data\n            try:\n                gdf = gpd.read_file(in_class_data)\n                if not quiet:\n                    print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n                    print(f\"Vector CRS: {gdf.crs}\")\n\n                # Always reproject to match raster CRS\n                if gdf.crs != src.crs:\n                    if not quiet:\n                        print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n                    gdf = gdf.to_crs(src.crs)\n\n                # Apply buffer if specified\n                if buffer_radius &gt; 0:\n                    gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n                    if not quiet:\n                        print(f\"Applied buffer of {buffer_radius} units\")\n\n                # Check if class_value_field exists\n                if class_value_field in gdf.columns:\n                    unique_classes = gdf[class_value_field].unique()\n                    if not quiet:\n                        print(\n                            f\"Found {len(unique_classes)} unique classes: {unique_classes}\"\n                        )\n                    # Create class mapping\n                    class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n                else:\n                    if not quiet:\n                        print(\n                            f\"WARNING: '{class_value_field}' not found in vector data. Using default class ID 1.\"\n                        )\n                    class_to_id = {1: 1}  # Default mapping\n            except Exception as e:\n                raise ValueError(f\"Error processing vector data: {e}\")\n\n        # Create progress bar\n        pbar = tqdm(\n            total=min(total_tiles, max_tiles),\n            desc=\"Generating tiles\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Track statistics for summary\n        stats = {\n            \"total_tiles\": 0,\n            \"tiles_with_features\": 0,\n            \"feature_pixels\": 0,\n            \"errors\": 0,\n            \"tile_coordinates\": [],  # For overview image\n        }\n\n        # Process tiles\n        tile_index = 0\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                if tile_index &gt;= max_tiles:\n                    break\n\n                # Calculate window coordinates\n                window_x = x * stride\n                window_y = y * stride\n\n                # Adjust for edge cases\n                if window_x + tile_size &gt; src.width:\n                    window_x = src.width - tile_size\n                if window_y + tile_size &gt; src.height:\n                    window_y = src.height - tile_size\n\n                # Define window\n                window = Window(window_x, window_y, tile_size, tile_size)\n\n                # Get window transform and bounds\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size * window_transform[0]  # Add width\n                miny = maxy + tile_size * window_transform[4]  # Add height\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Store tile coordinates for overview\n                if create_overview:\n                    stats[\"tile_coordinates\"].append(\n                        {\n                            \"index\": tile_index,\n                            \"x\": window_x,\n                            \"y\": window_y,\n                            \"bounds\": [minx, miny, maxx, maxy],\n                            \"has_features\": False,\n                        }\n                    )\n\n                # Create label mask\n                label_mask = np.zeros((tile_size, tile_size), dtype=np.uint8)\n                has_features = False\n\n                # Process classification data to create labels\n                if is_class_data_raster:\n                    # For raster class data\n                    with rasterio.open(in_class_data) as class_src:\n                        # Calculate window in class raster\n                        src_bounds = src.bounds\n                        class_bounds = class_src.bounds\n\n                        # Check if windows overlap\n                        if (\n                            src_bounds.left &gt; class_bounds.right\n                            or src_bounds.right &lt; class_bounds.left\n                            or src_bounds.bottom &gt; class_bounds.top\n                            or src_bounds.top &lt; class_bounds.bottom\n                        ):\n                            warnings.warn(\n                                \"Class raster and input raster do not overlap.\"\n                            )\n                        else:\n                            # Get corresponding window in class raster\n                            window_class = rasterio.windows.from_bounds(\n                                minx, miny, maxx, maxy, class_src.transform\n                            )\n\n                            # Read label data\n                            try:\n                                label_data = class_src.read(\n                                    1,\n                                    window=window_class,\n                                    boundless=True,\n                                    out_shape=(tile_size, tile_size),\n                                )\n\n                                # Remap class values if needed\n                                if class_to_id:\n                                    remapped_data = np.zeros_like(label_data)\n                                    for orig_val, new_val in class_to_id.items():\n                                        remapped_data[label_data == orig_val] = new_val\n                                    label_mask = remapped_data\n                                else:\n                                    label_mask = label_data\n\n                                # Check if we have any features\n                                if np.any(label_mask &gt; 0):\n                                    has_features = True\n                                    stats[\"feature_pixels\"] += np.count_nonzero(\n                                        label_mask\n                                    )\n                            except Exception as e:\n                                pbar.write(f\"Error reading class raster window: {e}\")\n                                stats[\"errors\"] += 1\n                else:\n                    # For vector class data\n                    # Find features that intersect with window\n                    window_features = gdf[gdf.intersects(window_bounds)]\n\n                    if len(window_features) &gt; 0:\n                        for idx, feature in window_features.iterrows():\n                            # Get class value\n                            if class_value_field in feature:\n                                class_val = feature[class_value_field]\n                                class_id = class_to_id.get(class_val, 1)\n                            else:\n                                class_id = 1\n\n                            # Get geometry in window coordinates\n                            geom = feature.geometry.intersection(window_bounds)\n                            if not geom.is_empty:\n                                try:\n                                    # Rasterize feature\n                                    feature_mask = features.rasterize(\n                                        [(geom, class_id)],\n                                        out_shape=(tile_size, tile_size),\n                                        transform=window_transform,\n                                        fill=0,\n                                        all_touched=all_touched,\n                                    )\n\n                                    # Add to label mask\n                                    label_mask = np.maximum(label_mask, feature_mask)\n\n                                    # Check if the feature was actually rasterized\n                                    if np.any(feature_mask):\n                                        has_features = True\n                                        if create_overview and tile_index &lt; len(\n                                            stats[\"tile_coordinates\"]\n                                        ):\n                                            stats[\"tile_coordinates\"][tile_index][\n                                                \"has_features\"\n                                            ] = True\n                                except Exception as e:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                    stats[\"errors\"] += 1\n\n                # Skip tile if no features and skip_empty_tiles is True\n                if skip_empty_tiles and not has_features:\n                    pbar.update(1)\n                    tile_index += 1\n                    continue\n\n                # Read image data\n                image_data = src.read(window=window)\n\n                # Export image as GeoTIFF\n                image_path = os.path.join(image_dir, f\"tile_{tile_index:06d}.tif\")\n\n                # Create profile for image GeoTIFF\n                image_profile = src.profile.copy()\n                image_profile.update(\n                    {\n                        \"height\": tile_size,\n                        \"width\": tile_size,\n                        \"count\": image_data.shape[0],\n                        \"transform\": window_transform,\n                    }\n                )\n\n                # Save image as GeoTIFF\n                try:\n                    with rasterio.open(image_path, \"w\", **image_profile) as dst:\n                        dst.write(image_data)\n                    stats[\"total_tiles\"] += 1\n                except Exception as e:\n                    pbar.write(f\"ERROR saving image GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size,\n                    \"width\": tile_size,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Export label as GeoTIFF\n                label_path = os.path.join(label_dir, f\"tile_{tile_index:06d}.tif\")\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask.astype(np.uint8), 1)\n\n                    if has_features:\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += np.count_nonzero(label_mask)\n                except Exception as e:\n                    pbar.write(f\"ERROR saving label GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create XML annotation for object detection if using vector class data\n                if (\n                    not is_class_data_raster\n                    and \"gdf\" in locals()\n                    and len(window_features) &gt; 0\n                ):\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = f\"tile_{tile_index:06d}.tif\"\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size)\n                    ET.SubElement(size, \"height\").text = str(tile_size)\n                    ET.SubElement(size, \"depth\").text = str(image_data.shape[0])\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    # Add objects\n                    for idx, feature in window_features.iterrows():\n                        # Get feature class\n                        if class_value_field in feature:\n                            class_val = feature[class_value_field]\n                        else:\n                            class_val = \"object\"\n\n                        # Get geometry bounds in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            # Get bounds in world coordinates\n                            minx_f, miny_f, maxx_f, maxy_f = geom.bounds\n\n                            # Convert to pixel coordinates\n                            col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                            col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                            # Ensure coordinates are within tile bounds\n                            xmin = max(0, min(tile_size, int(col_min)))\n                            ymin = max(0, min(tile_size, int(row_min)))\n                            xmax = max(0, min(tile_size, int(col_max)))\n                            ymax = max(0, min(tile_size, int(row_max)))\n\n                            # Only add if the box has non-zero area\n                            if xmax &gt; xmin and ymax &gt; ymin:\n                                obj = ET.SubElement(root, \"object\")\n                                ET.SubElement(obj, \"name\").text = str(class_val)\n                                ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                                bbox = ET.SubElement(obj, \"bndbox\")\n                                ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                                ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                                ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                                ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    tree = ET.ElementTree(root)\n                    xml_path = os.path.join(ann_dir, f\"tile_{tile_index:06d}.xml\")\n                    tree.write(xml_path)\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                tile_index += 1\n                if tile_index &gt;= max_tiles:\n                    break\n\n            if tile_index &gt;= max_tiles:\n                break\n\n        # Close progress bar\n        pbar.close()\n\n        # Create overview image if requested\n        if create_overview and stats[\"tile_coordinates\"]:\n            try:\n                create_overview_image(\n                    src,\n                    stats[\"tile_coordinates\"],\n                    os.path.join(out_folder, \"overview.png\"),\n                    tile_size,\n                    stride,\n                )\n            except Exception as e:\n                print(f\"Failed to create overview image: {e}\")\n\n        # Report results\n        if not quiet:\n            print(\"\\n------- Export Summary -------\")\n            print(f\"Total tiles exported: {stats['total_tiles']}\")\n            print(\n                f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n            )\n            if stats[\"tiles_with_features\"] &gt; 0:\n                print(\n                    f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n                )\n            if stats[\"errors\"] &gt; 0:\n                print(f\"Errors encountered: {stats['errors']}\")\n            print(f\"Output saved to: {out_folder}\")\n\n            # Verify georeference in a sample image and label\n            if stats[\"total_tiles\"] &gt; 0:\n                print(\"\\n------- Georeference Verification -------\")\n                sample_image = os.path.join(image_dir, f\"tile_0.tif\")\n                sample_label = os.path.join(label_dir, f\"tile_0.tif\")\n\n                if os.path.exists(sample_image):\n                    try:\n                        with rasterio.open(sample_image) as img:\n                            print(f\"Image CRS: {img.crs}\")\n                            print(f\"Image transform: {img.transform}\")\n                            print(\n                                f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                            )\n                            print(\n                                f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying image georeference: {e}\")\n\n                if os.path.exists(sample_label):\n                    try:\n                        with rasterio.open(sample_label) as lbl:\n                            print(f\"Label CRS: {lbl.crs}\")\n                            print(f\"Label transform: {lbl.transform}\")\n                            print(\n                                f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                            )\n                            print(\n                                f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying label georeference: {e}\")\n\n        # Return statistics dictionary for further processing if needed\n        return stats\n</code></pre>"},{"location":"geoai/#geoai.geoai.export_geotiff_tiles_batch","title":"<code>export_geotiff_tiles_batch(images_folder, masks_folder, output_folder, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False, image_extensions=None, mask_extensions=None)</code>","text":"<p>Export georeferenced GeoTIFF tiles from folders of images and masks.</p> <p>This function processes multiple image-mask pairs from input folders, generating tiles for each pair. All image tiles are saved to a single 'images' folder and all mask tiles to a single 'masks' folder.</p> <p>Images and masks are paired by their sorted order (alphabetically), not by filename matching. The number of images and masks must be equal.</p> <p>Parameters:</p> Name Type Description Default <code>images_folder</code> <code>str</code> <p>Path to folder containing raster images</p> required <code>masks_folder</code> <code>str</code> <p>Path to folder containing classification masks/vectors</p> required <code>output_folder</code> <code>str</code> <p>Path to output folder</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process per image (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> <code>image_extensions</code> <code>list</code> <p>List of image file extensions to process (default: common raster formats)</p> <code>None</code> <code>mask_extensions</code> <code>list</code> <p>List of mask file extensions to process (default: common raster/vector formats)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing batch processing statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images or masks found, or if counts don't match</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles_batch(\n    images_folder,\n    masks_folder,\n    output_folder,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n    image_extensions=None,\n    mask_extensions=None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Export georeferenced GeoTIFF tiles from folders of images and masks.\n\n    This function processes multiple image-mask pairs from input folders,\n    generating tiles for each pair. All image tiles are saved to a single\n    'images' folder and all mask tiles to a single 'masks' folder.\n\n    Images and masks are paired by their sorted order (alphabetically), not by\n    filename matching. The number of images and masks must be equal.\n\n    Args:\n        images_folder (str): Path to folder containing raster images\n        masks_folder (str): Path to folder containing classification masks/vectors\n        output_folder (str): Path to output folder\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process per image (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n        image_extensions (list): List of image file extensions to process (default: common raster formats)\n        mask_extensions (list): List of mask file extensions to process (default: common raster/vector formats)\n\n    Returns:\n        Dict[str, Any]: Dictionary containing batch processing statistics\n\n    Raises:\n        ValueError: If no images or masks found, or if counts don't match\n    \"\"\"\n\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Default extensions if not provided\n    if image_extensions is None:\n        image_extensions = [\".tif\", \".tiff\", \".jpg\", \".jpeg\", \".png\", \".jp2\", \".img\"]\n    if mask_extensions is None:\n        mask_extensions = [\n            \".tif\",\n            \".tiff\",\n            \".jpg\",\n            \".jpeg\",\n            \".png\",\n            \".jp2\",\n            \".img\",\n            \".shp\",\n            \".geojson\",\n            \".gpkg\",\n            \".geoparquet\",\n            \".json\",\n        ]\n\n    # Convert extensions to lowercase for comparison\n    image_extensions = [ext.lower() for ext in image_extensions]\n    mask_extensions = [ext.lower() for ext in mask_extensions]\n\n    # Create output folder structure\n    os.makedirs(output_folder, exist_ok=True)\n    output_images_dir = os.path.join(output_folder, \"images\")\n    output_masks_dir = os.path.join(output_folder, \"masks\")\n    os.makedirs(output_images_dir, exist_ok=True)\n    os.makedirs(output_masks_dir, exist_ok=True)\n\n    # Get list of image files\n    image_files = []\n    for ext in image_extensions:\n        pattern = os.path.join(images_folder, f\"*{ext}\")\n        image_files.extend(glob.glob(pattern))\n\n    # Get list of mask files\n    mask_files = []\n    for ext in mask_extensions:\n        pattern = os.path.join(masks_folder, f\"*{ext}\")\n        mask_files.extend(glob.glob(pattern))\n\n    # Sort files for consistent processing\n    image_files.sort()\n    mask_files.sort()\n\n    if not image_files:\n        raise ValueError(\n            f\"No image files found in {images_folder} with extensions {image_extensions}\"\n        )\n\n    if not mask_files:\n        raise ValueError(\n            f\"No mask files found in {masks_folder} with extensions {mask_extensions}\"\n        )\n\n    if len(image_files) != len(mask_files):\n        raise ValueError(\n            f\"Number of image files ({len(image_files)}) does not match number of mask files ({len(mask_files)})\"\n        )\n\n    # Initialize batch statistics\n    batch_stats = {\n        \"total_image_pairs\": 0,\n        \"processed_pairs\": 0,\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"errors\": 0,\n        \"processed_files\": [],\n        \"failed_files\": [],\n    }\n\n    if not quiet:\n        print(\n            f\"Found {len(image_files)} image files and {len(mask_files)} mask files to process\"\n        )\n        print(f\"Processing batch from {images_folder} and {masks_folder}\")\n        print(f\"Output folder: {output_folder}\")\n        print(\"-\" * 60)\n\n    # Global tile counter for unique naming\n    global_tile_counter = 0\n\n    # Process each image-mask pair by sorted order\n    for idx, (image_file, mask_file) in enumerate(\n        tqdm(\n            zip(image_files, mask_files),\n            desc=\"Processing image pairs\",\n            disable=quiet,\n            total=len(image_files),\n        )\n    ):\n        batch_stats[\"total_image_pairs\"] += 1\n\n        # Get base filename without extension for naming (use image filename)\n        base_name = os.path.splitext(os.path.basename(image_file))[0]\n\n        try:\n            if not quiet:\n                print(f\"\\nProcessing: {base_name}\")\n                print(f\"  Image: {os.path.basename(image_file)}\")\n                print(f\"  Mask: {os.path.basename(mask_file)}\")\n\n            # Process the image-mask pair manually to get direct control over tile saving\n            tiles_generated = _process_image_mask_pair(\n                image_file=image_file,\n                mask_file=mask_file,\n                base_name=base_name,\n                output_images_dir=output_images_dir,\n                output_masks_dir=output_masks_dir,\n                global_tile_counter=global_tile_counter,\n                tile_size=tile_size,\n                stride=stride,\n                class_value_field=class_value_field,\n                buffer_radius=buffer_radius,\n                max_tiles=max_tiles,\n                all_touched=all_touched,\n                skip_empty_tiles=skip_empty_tiles,\n                quiet=quiet,\n            )\n\n            # Update counters\n            global_tile_counter += tiles_generated[\"total_tiles\"]\n\n            # Update batch statistics\n            batch_stats[\"processed_pairs\"] += 1\n            batch_stats[\"total_tiles\"] += tiles_generated[\"total_tiles\"]\n            batch_stats[\"tiles_with_features\"] += tiles_generated[\"tiles_with_features\"]\n            batch_stats[\"errors\"] += tiles_generated[\"errors\"]\n\n            batch_stats[\"processed_files\"].append(\n                {\n                    \"image\": image_file,\n                    \"mask\": mask_file,\n                    \"base_name\": base_name,\n                    \"tiles_generated\": tiles_generated[\"total_tiles\"],\n                    \"tiles_with_features\": tiles_generated[\"tiles_with_features\"],\n                }\n            )\n\n        except Exception as e:\n            if not quiet:\n                print(f\"ERROR processing {base_name}: {e}\")\n            batch_stats[\"failed_files\"].append(\n                {\"image\": image_file, \"mask\": mask_file, \"error\": str(e)}\n            )\n            batch_stats[\"errors\"] += 1\n\n    # Print batch summary\n    if not quiet:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"BATCH PROCESSING SUMMARY\")\n        print(\"=\" * 60)\n        print(f\"Total image pairs found: {batch_stats['total_image_pairs']}\")\n        print(f\"Successfully processed: {batch_stats['processed_pairs']}\")\n        print(f\"Failed to process: {len(batch_stats['failed_files'])}\")\n        print(f\"Total tiles generated: {batch_stats['total_tiles']}\")\n        print(f\"Tiles with features: {batch_stats['tiles_with_features']}\")\n\n        if batch_stats[\"total_tiles\"] &gt; 0:\n            feature_percentage = (\n                batch_stats[\"tiles_with_features\"] / batch_stats[\"total_tiles\"]\n            ) * 100\n            print(f\"Feature percentage: {feature_percentage:.1f}%\")\n\n        if batch_stats[\"errors\"] &gt; 0:\n            print(f\"Total errors: {batch_stats['errors']}\")\n\n        print(f\"Output saved to: {output_folder}\")\n        print(f\"  Images: {output_images_dir}\")\n        print(f\"  Masks: {output_masks_dir}\")\n\n        # List failed files if any\n        if batch_stats[\"failed_files\"]:\n            print(f\"\\nFailed files:\")\n            for failed in batch_stats[\"failed_files\"]:\n                print(f\"  - {os.path.basename(failed['image'])}: {failed['error']}\")\n\n    return batch_stats\n</code></pre>"},{"location":"geoai/#geoai.geoai.export_landcover_tiles","title":"<code>export_landcover_tiles(in_raster, out_folder, in_class_data=None, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False, min_feature_ratio=False, metadata_format='PASCAL_VOC')</code>","text":"<p>Export GeoTIFF tiles optimized for landcover classification training.</p> <p>This function extends the base export_geotiff_tiles with enhanced filtering capabilities specifically designed for discrete landcover classification. It can filter out tiles dominated by background pixels to improve training data quality and reduce dataset size.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster (image to tile)</p> required <code>out_folder</code> <code>str</code> <p>Output directory for tiles</p> required <code>in_class_data</code> <code>Optional[Union[str, GeoDataFrame]]</code> <p>Path to vector mask or GeoDataFrame (optional for image-only export)</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of output tiles in pixels (default: 256)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Stride for sliding window (default: 128)</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field name containing class values (default: \"class\")</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer radius around features in pixels (default: 0)</p> <code>0</code> <code>max_tiles</code> <code>Optional[int]</code> <p>Maximum number of tiles to export (default: None)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress progress output (default: False)</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Include pixels touched by geometry (default: True)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Create overview image showing tile locations (default: False)</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>Skip tiles with no features (default: False)</p> <code>False</code> <code>min_feature_ratio</code> <code>Union[bool, float]</code> <p>Minimum ratio of non-background pixels required to keep tile - False: Disable ratio filtering (default) - 0.0-1.0: Minimum ratio threshold (e.g., 0.1 = 10% features required)</p> <code>False</code> <code>metadata_format</code> <code>str</code> <p>Annotation format (\"PASCAL_VOC\" or \"YOLO\")</p> <code>'PASCAL_VOC'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - tiles_exported: Number of tiles successfully exported - tiles_skipped_empty: Number of completely empty tiles skipped - tiles_skipped_ratio: Number of tiles filtered by min_feature_ratio - output_dirs: Dictionary with paths to images and labels directories</p> <p>Examples:</p>"},{"location":"geoai/#geoai.geoai.export_landcover_tiles--original-behavior-no-filtering","title":"Original behavior (no filtering)","text":"<p>export_landcover_tiles(     \"input.tif\",     \"output\",     \"mask.shp\",     skip_empty_tiles=True )</p>"},{"location":"geoai/#geoai.geoai.export_landcover_tiles--light-filtering-keep-tiles-with-5-features","title":"Light filtering (keep tiles with \u22655% features)","text":"<p>export_landcover_tiles(     \"input.tif\",     \"output\",     \"mask.shp\",     skip_empty_tiles=True,     min_feature_ratio=0.05 )</p>"},{"location":"geoai/#geoai.geoai.export_landcover_tiles--moderate-filtering-keep-tiles-with-15-features","title":"Moderate filtering (keep tiles with \u226515% features)","text":"<p>export_landcover_tiles(     \"input.tif\",     \"output\",     \"mask.shp\",     skip_empty_tiles=True,     min_feature_ratio=0.15 )</p> Note <p>This function is designed for discrete landcover classification where class 0 typically represents background/no data. The min_feature_ratio parameter counts non-zero pixels as \"features\".</p> Source code in <code>geoai/landcover_utils.py</code> <pre><code>def export_landcover_tiles(\n    in_raster: str,\n    out_folder: str,\n    in_class_data: Optional[Union[str, gpd.GeoDataFrame]] = None,\n    tile_size: int = 256,\n    stride: int = 128,\n    class_value_field: str = \"class\",\n    buffer_radius: float = 0,\n    max_tiles: Optional[int] = None,\n    quiet: bool = False,\n    all_touched: bool = True,\n    create_overview: bool = False,\n    skip_empty_tiles: bool = False,\n    min_feature_ratio: Union[bool, float] = False,\n    metadata_format: str = \"PASCAL_VOC\",\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Export GeoTIFF tiles optimized for landcover classification training.\n\n    This function extends the base export_geotiff_tiles with enhanced filtering\n    capabilities specifically designed for discrete landcover classification.\n    It can filter out tiles dominated by background pixels to improve training\n    data quality and reduce dataset size.\n\n    Args:\n        in_raster: Path to input raster (image to tile)\n        out_folder: Output directory for tiles\n        in_class_data: Path to vector mask or GeoDataFrame (optional for image-only export)\n        tile_size: Size of output tiles in pixels (default: 256)\n        stride: Stride for sliding window (default: 128)\n        class_value_field: Field name containing class values (default: \"class\")\n        buffer_radius: Buffer radius around features in pixels (default: 0)\n        max_tiles: Maximum number of tiles to export (default: None)\n        quiet: Suppress progress output (default: False)\n        all_touched: Include pixels touched by geometry (default: True)\n        create_overview: Create overview image showing tile locations (default: False)\n        skip_empty_tiles: Skip tiles with no features (default: False)\n        min_feature_ratio: Minimum ratio of non-background pixels required to keep tile\n            - False: Disable ratio filtering (default)\n            - 0.0-1.0: Minimum ratio threshold (e.g., 0.1 = 10% features required)\n        metadata_format: Annotation format (\"PASCAL_VOC\" or \"YOLO\")\n\n    Returns:\n        Dictionary containing:\n            - tiles_exported: Number of tiles successfully exported\n            - tiles_skipped_empty: Number of completely empty tiles skipped\n            - tiles_skipped_ratio: Number of tiles filtered by min_feature_ratio\n            - output_dirs: Dictionary with paths to images and labels directories\n\n    Examples:\n        # Original behavior (no filtering)\n        export_landcover_tiles(\n            \"input.tif\",\n            \"output\",\n            \"mask.shp\",\n            skip_empty_tiles=True\n        )\n\n        # Light filtering (keep tiles with \u22655% features)\n        export_landcover_tiles(\n            \"input.tif\",\n            \"output\",\n            \"mask.shp\",\n            skip_empty_tiles=True,\n            min_feature_ratio=0.05\n        )\n\n        # Moderate filtering (keep tiles with \u226515% features)\n        export_landcover_tiles(\n            \"input.tif\",\n            \"output\",\n            \"mask.shp\",\n            skip_empty_tiles=True,\n            min_feature_ratio=0.15\n        )\n\n    Note:\n        This function is designed for discrete landcover classification where\n        class 0 typically represents background/no data. The min_feature_ratio\n        parameter counts non-zero pixels as \"features\".\n    \"\"\"\n\n    # Validate min_feature_ratio parameter\n    if min_feature_ratio is not False:\n        if not isinstance(min_feature_ratio, (int, float)):\n            warnings.warn(\n                f\"min_feature_ratio must be a number between 0.0 and 1.0, got {type(min_feature_ratio)}. \"\n                \"Disabling ratio filtering.\"\n            )\n            min_feature_ratio = False\n        elif not (0.0 &lt;= min_feature_ratio &lt;= 1.0):\n            warnings.warn(\n                f\"min_feature_ratio must be between 0.0 and 1.0, got {min_feature_ratio}. \"\n                \"Disabling ratio filtering.\"\n            )\n            min_feature_ratio = False\n\n    # Create output directories\n    out_folder = Path(out_folder)\n    out_folder.mkdir(parents=True, exist_ok=True)\n\n    images_dir = out_folder / \"images\"\n    labels_dir = out_folder / \"labels\"\n    images_dir.mkdir(exist_ok=True)\n    labels_dir.mkdir(exist_ok=True)\n\n    if metadata_format == \"PASCAL_VOC\":\n        ann_dir = out_folder / \"annotations\"\n        ann_dir.mkdir(exist_ok=True)\n\n    # Initialize statistics\n    stats = {\n        \"tiles_exported\": 0,\n        \"tiles_skipped_empty\": 0,\n        \"tiles_skipped_ratio\": 0,\n        \"output_dirs\": {\"images\": str(images_dir), \"labels\": str(labels_dir)},\n    }\n\n    # Open raster\n    with rasterio.open(in_raster) as src:\n        height, width = src.shape\n\n        # Detect if in_class_data is raster or vector\n        is_class_data_raster = False\n        class_src = None\n        gdf = None\n        mask_array = None\n\n        if in_class_data is not None:\n            if isinstance(in_class_data, str):\n                file_ext = Path(in_class_data).suffix.lower()\n                if file_ext in [\n                    \".tif\",\n                    \".tiff\",\n                    \".img\",\n                    \".jp2\",\n                    \".png\",\n                    \".bmp\",\n                    \".gif\",\n                ]:\n                    try:\n                        # Try to open as raster\n                        class_src = rasterio.open(in_class_data)\n                        is_class_data_raster = True\n\n                        # Verify CRS match\n                        if class_src.crs != src.crs:\n                            if not quiet:\n                                print(\n                                    f\"Warning: CRS mismatch between image ({src.crs}) and mask ({class_src.crs})\"\n                                )\n                    except Exception as e:\n                        is_class_data_raster = False\n                        if not quiet:\n                            print(f\"Could not open as raster, trying vector: {e}\")\n\n                # If not raster or raster open failed, try vector\n                if not is_class_data_raster:\n                    gdf = gpd.read_file(in_class_data)\n\n                    # Reproject if needed\n                    if gdf.crs != src.crs:\n                        if not quiet:\n                            print(f\"Reprojecting mask from {gdf.crs} to {src.crs}\")\n                        gdf = gdf.to_crs(src.crs)\n\n                    # Apply buffer if requested\n                    if buffer_radius &gt; 0:\n                        gdf.geometry = gdf.geometry.buffer(buffer_radius)\n\n                    # For vector data, rasterize entire mask up front for efficiency\n                    shapes = [\n                        (geom, value)\n                        for geom, value in zip(gdf.geometry, gdf[class_value_field])\n                    ]\n                    mask_array = features.rasterize(\n                        shapes,\n                        out_shape=(height, width),\n                        transform=src.transform,\n                        all_touched=all_touched,\n                        fill=0,\n                        dtype=np.uint8,\n                    )\n            else:\n                # Assume GeoDataFrame passed directly\n                gdf = in_class_data\n\n                # Reproject if needed\n                if gdf.crs != src.crs:\n                    if not quiet:\n                        print(f\"Reprojecting mask from {gdf.crs} to {src.crs}\")\n                    gdf = gdf.to_crs(src.crs)\n\n                # Apply buffer if requested\n                if buffer_radius &gt; 0:\n                    gdf.geometry = gdf.geometry.buffer(buffer_radius)\n\n                # Rasterize entire mask up front\n                shapes = [\n                    (geom, value)\n                    for geom, value in zip(gdf.geometry, gdf[class_value_field])\n                ]\n                mask_array = features.rasterize(\n                    shapes,\n                    out_shape=(height, width),\n                    transform=src.transform,\n                    all_touched=all_touched,\n                    fill=0,\n                    dtype=np.uint8,\n                )\n\n        # Calculate tile positions\n        tile_positions = []\n        for y in range(0, height - tile_size + 1, stride):\n            for x in range(0, width - tile_size + 1, stride):\n                tile_positions.append((x, y))\n\n        if max_tiles:\n            tile_positions = tile_positions[:max_tiles]\n\n        # Process tiles\n        pbar = tqdm(tile_positions, desc=\"Exporting tiles\", disable=quiet)\n\n        for tile_idx, (x, y) in enumerate(pbar):\n            window = Window(x, y, tile_size, tile_size)\n\n            # Read image tile\n            image_tile = src.read(window=window)\n\n            # Read mask tile based on data type\n            mask_tile = None\n            has_features = False\n\n            if is_class_data_raster and class_src is not None:\n                # For raster masks, read directly from the raster source\n                # Get window transform and bounds\n                window_transform = src.window_transform(window)\n                minx = window_transform[2]\n                maxy = window_transform[5]\n                maxx = minx + tile_size * window_transform[0]\n                miny = maxy + tile_size * window_transform[4]\n\n                # Get corresponding window in class raster\n                window_class = rasterio.windows.from_bounds(\n                    minx, miny, maxx, maxy, class_src.transform\n                )\n\n                try:\n                    # Read label data from raster\n                    mask_tile = class_src.read(\n                        1,\n                        window=window_class,\n                        boundless=True,\n                        out_shape=(tile_size, tile_size),\n                    )\n\n                    # Check if tile has features\n                    has_features = np.any(mask_tile &gt; 0)\n                except Exception as e:\n                    if not quiet:\n                        pbar.write(f\"Error reading mask tile at ({x}, {y}): {e}\")\n                    continue\n\n            elif mask_array is not None:\n                # For vector masks (pre-rasterized)\n                mask_tile = mask_array[y : y + tile_size, x : x + tile_size]\n                has_features = np.any(mask_tile &gt; 0)\n\n            # Skip empty tiles if requested\n            if skip_empty_tiles and not has_features:\n                stats[\"tiles_skipped_empty\"] += 1\n                continue\n\n            # Apply min_feature_ratio filtering if enabled\n            if skip_empty_tiles and has_features and min_feature_ratio is not False:\n                # Calculate ratio of non-background pixels\n                total_pixels = mask_tile.size\n                feature_pixels = np.sum(mask_tile &gt; 0)\n                feature_ratio = feature_pixels / total_pixels\n\n                # Skip tile if below threshold\n                if feature_ratio &lt; min_feature_ratio:\n                    stats[\"tiles_skipped_ratio\"] += 1\n                    continue\n\n            # Save image tile\n            tile_name = f\"tile_{tile_idx:06d}.tif\"\n            image_path = images_dir / tile_name\n\n            # Get transform for this tile\n            tile_transform = src.window_transform(window)\n\n            # Write image\n            with rasterio.open(\n                image_path,\n                \"w\",\n                driver=\"GTiff\",\n                height=tile_size,\n                width=tile_size,\n                count=src.count,\n                dtype=src.dtypes[0],\n                crs=src.crs,\n                transform=tile_transform,\n                compress=\"lzw\",\n            ) as dst:\n                dst.write(image_tile)\n\n            # Save mask tile if available\n            if mask_tile is not None:\n                mask_path = labels_dir / tile_name\n                with rasterio.open(\n                    mask_path,\n                    \"w\",\n                    driver=\"GTiff\",\n                    height=tile_size,\n                    width=tile_size,\n                    count=1,\n                    dtype=np.uint8,\n                    crs=src.crs,\n                    transform=tile_transform,\n                    compress=\"lzw\",\n                ) as dst:\n                    dst.write(mask_tile, 1)\n\n            stats[\"tiles_exported\"] += 1\n\n            # Update progress bar description with selection count\n            if not quiet:\n                pbar.set_description(\n                    f\"Exporting tiles ({stats['tiles_exported']}/{tile_idx + 1})\"\n                )\n\n    # Close raster class source if opened\n    if class_src is not None:\n        class_src.close()\n\n    # Print summary\n    if not quiet:\n        print(f\"\\n{'='*60}\")\n        print(\"TILE EXPORT SUMMARY\")\n        print(f\"{'='*60}\")\n        print(f\"Tiles exported: {stats['tiles_exported']}/{len(tile_positions)}\")\n        if skip_empty_tiles:\n            print(f\"Tiles skipped (empty): {stats['tiles_skipped_empty']}\")\n        if min_feature_ratio is not False:\n            print(\n                f\"Tiles skipped (low feature ratio &lt; {min_feature_ratio}): {stats['tiles_skipped_ratio']}\"\n            )\n        print(f\"\\nOutput directories:\")\n        print(f\"  Images: {stats['output_dirs']['images']}\")\n        print(f\"  Labels: {stats['output_dirs']['labels']}\")\n        print(f\"{'='*60}\\n\")\n\n    return stats\n</code></pre>"},{"location":"geoai/#geoai.geoai.export_tiles_to_geojson","title":"<code>export_tiles_to_geojson(tile_coordinates, src, output_path, tile_size=None, stride=None)</code>","text":"<p>Export tile rectangles directly to GeoJSON without creating an overview image.</p> <p>Parameters:</p> Name Type Description Default <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>src</code> <code>DatasetReader</code> <p>The source raster dataset.</p> required <code>output_path</code> <code>str</code> <p>The path where the GeoJSON will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels. Only needed if not in tile_coordinates.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Used to calculate overlaps between tiles.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved GeoJSON file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_tiles_to_geojson(\n    tile_coordinates, src, output_path, tile_size=None, stride=None\n) -&gt; str:\n    \"\"\"\n    Export tile rectangles directly to GeoJSON without creating an overview image.\n\n    Args:\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        output_path (str): The path where the GeoJSON will be saved.\n        tile_size (int, optional): The size of each tile in pixels. Only needed if not in tile_coordinates.\n        stride (int, optional): The stride between tiles in pixels. Used to calculate overlaps between tiles.\n\n    Returns:\n        str: Path to the saved GeoJSON file.\n    \"\"\"\n    features = []\n\n    for tile in tile_coordinates:\n        # Get the size from the tile or use the provided parameter\n        tile_width = tile.get(\"width\", tile.get(\"size\", tile_size))\n        tile_height = tile.get(\"height\", tile.get(\"size\", tile_size))\n\n        if tile_width is None or tile_height is None:\n            raise ValueError(\n                \"Tile size not found in tile data and no tile_size parameter provided\"\n            )\n\n        # Get bounds from the tile\n        if \"bounds\" in tile:\n            # If bounds are already in geo coordinates\n            minx, miny, maxx, maxy = tile[\"bounds\"]\n        else:\n            # Try to calculate bounds from transform if available\n            if hasattr(src, \"transform\"):\n                # Convert pixel coordinates to geo coordinates\n                window_transform = src.transform\n                x, y = tile[\"x\"], tile[\"y\"]\n                minx = window_transform[2] + x * window_transform[0]\n                maxy = window_transform[5] + y * window_transform[4]\n                maxx = minx + tile_width * window_transform[0]\n                miny = maxy + tile_height * window_transform[4]\n            else:\n                raise ValueError(\n                    \"Cannot determine bounds. Neither 'bounds' in tile nor transform in src.\"\n                )\n\n        # Calculate overlap with neighboring tiles if stride is provided\n        overlap = 0\n        if stride is not None and stride &lt; tile_width:\n            overlap = tile_width - stride\n\n        # Create a polygon from the bounds\n        polygon = box(minx, miny, maxx, maxy)\n\n        # Create a GeoJSON feature\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": mapping(polygon),\n            \"properties\": {\n                \"index\": tile[\"index\"],\n                \"has_features\": tile.get(\"has_features\", False),\n                \"tile_width_px\": tile_width,\n                \"tile_height_px\": tile_height,\n            },\n        }\n\n        # Add overlap information if stride is provided\n        if stride is not None:\n            feature[\"properties\"][\"stride_px\"] = stride\n            feature[\"properties\"][\"overlap_px\"] = overlap\n\n        # Add additional properties from the tile\n        for key, value in tile.items():\n            if key not in [\"bounds\", \"geometry\"]:\n                feature[\"properties\"][key] = value\n\n        features.append(feature)\n\n    # Create the GeoJSON collection\n    geojson_collection = {\n        \"type\": \"FeatureCollection\",\n        \"features\": features,\n        \"properties\": {\n            \"crs\": (\n                src.crs.to_string() if hasattr(src.crs, \"to_string\") else str(src.crs)\n            ),\n            \"total_tiles\": len(features),\n            \"source_raster_dimensions\": (\n                [src.width, src.height] if hasattr(src, \"width\") else None\n            ),\n        },\n    }\n\n    # Create directory if it doesn't exist\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)) or \".\", exist_ok=True)\n\n    # Save to file\n    with open(output_path, \"w\") as f:\n        json.dump(geojson_collection, f)\n\n    print(f\"GeoJSON saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.export_training_data","title":"<code>export_training_data(in_raster, out_folder, in_class_data, image_chip_format='GEOTIFF', tile_size_x=256, tile_size_y=256, stride_x=None, stride_y=None, output_nofeature_tiles=True, metadata_format='PASCAL_VOC', start_index=0, class_value_field='class', buffer_radius=0, in_mask_polygons=None, rotation_angle=0, reference_system=None, blacken_around_feature=False, crop_mode='FIXED_SIZE', in_raster2=None, in_instance_data=None, instance_class_value_field=None, min_polygon_overlap_ratio=0.0, all_touched=True, save_geotiff=True, quiet=False)</code>","text":"<p>Export training data for deep learning using TorchGeo with progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image.</p> required <code>out_folder</code> <code>str</code> <p>Output folder path where chips and labels will be saved.</p> required <code>in_class_data</code> <code>str</code> <p>Path to vector file containing class polygons.</p> required <code>image_chip_format</code> <code>str</code> <p>Output image format (PNG, JPEG, TIFF, GEOTIFF).</p> <code>'GEOTIFF'</code> <code>tile_size_x</code> <code>int</code> <p>Width of image chips in pixels.</p> <code>256</code> <code>tile_size_y</code> <code>int</code> <p>Height of image chips in pixels.</p> <code>256</code> <code>stride_x</code> <code>int</code> <p>Horizontal stride between chips. If None, uses tile_size_x.</p> <code>None</code> <code>stride_y</code> <code>int</code> <p>Vertical stride between chips. If None, uses tile_size_y.</p> <code>None</code> <code>output_nofeature_tiles</code> <code>bool</code> <p>Whether to export chips without features.</p> <code>True</code> <code>metadata_format</code> <code>str</code> <p>Output metadata format (PASCAL_VOC, KITTI, COCO).</p> <code>'PASCAL_VOC'</code> <code>start_index</code> <code>int</code> <p>Starting index for chip filenames.</p> <code>0</code> <code>class_value_field</code> <code>str</code> <p>Field name in in_class_data containing class values.</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer radius around features (in CRS units).</p> <code>0</code> <code>in_mask_polygons</code> <code>str</code> <p>Path to vector file containing mask polygons.</p> <code>None</code> <code>rotation_angle</code> <code>float</code> <p>Rotation angle in degrees.</p> <code>0</code> <code>reference_system</code> <code>str</code> <p>Reference system code.</p> <code>None</code> <code>blacken_around_feature</code> <code>bool</code> <p>Whether to mask areas outside of features.</p> <code>False</code> <code>crop_mode</code> <code>str</code> <p>Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).</p> <code>'FIXED_SIZE'</code> <code>in_raster2</code> <code>str</code> <p>Path to secondary raster image.</p> <code>None</code> <code>in_instance_data</code> <code>str</code> <p>Path to vector file containing instance polygons.</p> <code>None</code> <code>instance_class_value_field</code> <code>str</code> <p>Field name in in_instance_data for instance classes.</p> <code>None</code> <code>min_polygon_overlap_ratio</code> <code>float</code> <p>Minimum overlap ratio for polygons.</p> <code>0.0</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization.</p> <code>True</code> <code>save_geotiff</code> <code>bool</code> <p>Whether to save as GeoTIFF with georeferencing.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>If True, suppress most output messages.</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_training_data(\n    in_raster,\n    out_folder,\n    in_class_data,\n    image_chip_format=\"GEOTIFF\",\n    tile_size_x=256,\n    tile_size_y=256,\n    stride_x=None,\n    stride_y=None,\n    output_nofeature_tiles=True,\n    metadata_format=\"PASCAL_VOC\",\n    start_index=0,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    in_mask_polygons=None,\n    rotation_angle=0,\n    reference_system=None,\n    blacken_around_feature=False,\n    crop_mode=\"FIXED_SIZE\",  # Implemented but not fully used yet\n    in_raster2=None,\n    in_instance_data=None,\n    instance_class_value_field=None,  # Implemented but not fully used yet\n    min_polygon_overlap_ratio=0.0,\n    all_touched=True,\n    save_geotiff=True,\n    quiet=False,\n):\n    \"\"\"\n    Export training data for deep learning using TorchGeo with progress bar.\n\n    Args:\n        in_raster (str): Path to input raster image.\n        out_folder (str): Output folder path where chips and labels will be saved.\n        in_class_data (str): Path to vector file containing class polygons.\n        image_chip_format (str): Output image format (PNG, JPEG, TIFF, GEOTIFF).\n        tile_size_x (int): Width of image chips in pixels.\n        tile_size_y (int): Height of image chips in pixels.\n        stride_x (int): Horizontal stride between chips. If None, uses tile_size_x.\n        stride_y (int): Vertical stride between chips. If None, uses tile_size_y.\n        output_nofeature_tiles (bool): Whether to export chips without features.\n        metadata_format (str): Output metadata format (PASCAL_VOC, KITTI, COCO).\n        start_index (int): Starting index for chip filenames.\n        class_value_field (str): Field name in in_class_data containing class values.\n        buffer_radius (float): Buffer radius around features (in CRS units).\n        in_mask_polygons (str): Path to vector file containing mask polygons.\n        rotation_angle (float): Rotation angle in degrees.\n        reference_system (str): Reference system code.\n        blacken_around_feature (bool): Whether to mask areas outside of features.\n        crop_mode (str): Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).\n        in_raster2 (str): Path to secondary raster image.\n        in_instance_data (str): Path to vector file containing instance polygons.\n        instance_class_value_field (str): Field name in in_instance_data for instance classes.\n        min_polygon_overlap_ratio (float): Minimum overlap ratio for polygons.\n        all_touched (bool): Whether to use all_touched=True in rasterization.\n        save_geotiff (bool): Whether to save as GeoTIFF with georeferencing.\n        quiet (bool): If True, suppress most output messages.\n    \"\"\"\n    # Create output directories\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n\n    # Define annotation directories based on metadata format\n    if metadata_format == \"PASCAL_VOC\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n    elif metadata_format == \"COCO\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n        # Initialize COCO annotations dictionary\n        coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # Initialize statistics dictionary\n    stats = {\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"feature_pixels\": 0,\n        \"errors\": 0,\n    }\n\n    # Open raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Set defaults for stride if not provided\n        if stride_x is None:\n            stride_x = tile_size_x\n        if stride_y is None:\n            stride_y = tile_size_y\n\n        # Calculate number of tiles in x and y directions\n        num_tiles_x = math.ceil((src.width - tile_size_x) / stride_x) + 1\n        num_tiles_y = math.ceil((src.height - tile_size_y) / stride_y) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        # Read class data\n        gdf = gpd.read_file(in_class_data)\n        if not quiet:\n            print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n            print(f\"Available columns: {gdf.columns.tolist()}\")\n            print(f\"GeoJSON CRS: {gdf.crs}\")\n\n        # Check if class_value_field exists\n        if class_value_field not in gdf.columns:\n            if not quiet:\n                print(\n                    f\"WARNING: '{class_value_field}' field not found in the input data. Using default class value 1.\"\n                )\n            # Add a default class column\n            gdf[class_value_field] = 1\n            unique_classes = [1]\n        else:\n            # Print unique classes for debugging\n            unique_classes = gdf[class_value_field].unique()\n            if not quiet:\n                print(f\"Found {len(unique_classes)} unique classes: {unique_classes}\")\n\n        # CRITICAL: Always reproject to match raster CRS to ensure proper alignment\n        if gdf.crs != src.crs:\n            if not quiet:\n                print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n            gdf = gdf.to_crs(src.crs)\n        elif reference_system and gdf.crs != reference_system:\n            if not quiet:\n                print(\n                    f\"Reprojecting features to specified reference system {reference_system}\"\n                )\n            gdf = gdf.to_crs(reference_system)\n\n        # Check overlap between raster and vector data\n        raster_bounds = box(*src.bounds)\n        vector_bounds = box(*gdf.total_bounds)\n        if not raster_bounds.intersects(vector_bounds):\n            if not quiet:\n                print(\n                    \"WARNING: The vector data doesn't intersect with the raster extent!\"\n                )\n                print(f\"Raster bounds: {src.bounds}\")\n                print(f\"Vector bounds: {gdf.total_bounds}\")\n        else:\n            overlap = (\n                raster_bounds.intersection(vector_bounds).area / vector_bounds.area\n            )\n            if not quiet:\n                print(f\"Overlap between raster and vector: {overlap:.2%}\")\n\n        # Apply buffer if specified\n        if buffer_radius &gt; 0:\n            gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n\n        # Initialize class mapping (ensure all classes are mapped to non-zero values)\n        class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n\n        # Store category info for COCO format\n        if metadata_format == \"COCO\":\n            for cls_val in unique_classes:\n                coco_annotations[\"categories\"].append(\n                    {\n                        \"id\": class_to_id[cls_val],\n                        \"name\": str(cls_val),\n                        \"supercategory\": \"object\",\n                    }\n                )\n\n        # Load mask polygons if provided\n        mask_gdf = None\n        if in_mask_polygons:\n            mask_gdf = gpd.read_file(in_mask_polygons)\n            if reference_system:\n                mask_gdf = mask_gdf.to_crs(reference_system)\n            elif mask_gdf.crs != src.crs:\n                mask_gdf = mask_gdf.to_crs(src.crs)\n\n        # Process instance data if provided\n        instance_gdf = None\n        if in_instance_data:\n            instance_gdf = gpd.read_file(in_instance_data)\n            if reference_system:\n                instance_gdf = instance_gdf.to_crs(reference_system)\n            elif instance_gdf.crs != src.crs:\n                instance_gdf = instance_gdf.to_crs(src.crs)\n\n        # Load secondary raster if provided\n        src2 = None\n        if in_raster2:\n            src2 = rasterio.open(in_raster2)\n\n        # Set up augmentation if rotation is specified\n        augmentation = None\n        if rotation_angle != 0:\n            # Fixed: Added data_keys parameter to AugmentationSequential\n            augmentation = torchgeo.transforms.AugmentationSequential(\n                torch.nn.ModuleList([RandomRotation(rotation_angle)]),\n                data_keys=[\"image\"],  # Add data_keys parameter\n            )\n\n        # Initialize annotation ID for COCO format\n        ann_id = 0\n\n        # Create progress bar\n        pbar = tqdm(\n            total=total_tiles,\n            desc=f\"Generating tiles (with features: 0)\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Generate tiles\n        chip_index = start_index\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                # Calculate window coordinates\n                window_x = x * stride_x\n                window_y = y * stride_y\n\n                # Adjust for edge cases\n                if window_x + tile_size_x &gt; src.width:\n                    window_x = src.width - tile_size_x\n                if window_y + tile_size_y &gt; src.height:\n                    window_y = src.height - tile_size_y\n\n                # Adjust window based on crop_mode\n                if crop_mode == \"CENTERED_ON_FEATURE\" and len(gdf) &gt; 0:\n                    # Find the nearest feature to the center of this window\n                    window_center_x = window_x + tile_size_x // 2\n                    window_center_y = window_y + tile_size_y // 2\n\n                    # Convert center to world coordinates\n                    center_x, center_y = src.xy(window_center_y, window_center_x)\n                    center_point = gpd.points_from_xy([center_x], [center_y])[0]\n\n                    # Find nearest feature\n                    distances = gdf.geometry.distance(center_point)\n                    nearest_idx = distances.idxmin()\n                    nearest_feature = gdf.iloc[nearest_idx]\n\n                    # Get centroid of nearest feature\n                    feature_centroid = nearest_feature.geometry.centroid\n\n                    # Convert feature centroid to pixel coordinates\n                    feature_row, feature_col = src.index(\n                        feature_centroid.x, feature_centroid.y\n                    )\n\n                    # Adjust window to center on feature\n                    window_x = max(\n                        0, min(src.width - tile_size_x, feature_col - tile_size_x // 2)\n                    )\n                    window_y = max(\n                        0, min(src.height - tile_size_y, feature_row - tile_size_y // 2)\n                    )\n\n                # Define window\n                window = Window(window_x, window_y, tile_size_x, tile_size_y)\n\n                # Get window transform and bounds in source CRS\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds more explicitly and accurately\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size_x * window_transform[0]  # Add width\n                miny = (\n                    maxy + tile_size_y * window_transform[4]\n                )  # Add height (note: transform[4] is typically negative)\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Apply rotation if specified\n                if rotation_angle != 0:\n                    window_bounds = rotate(\n                        window_bounds, rotation_angle, origin=\"center\"\n                    )\n\n                # Find features that intersect with window\n                window_features = gdf[gdf.intersects(window_bounds)]\n\n                # Process instance data if provided\n                window_instances = None\n                if instance_gdf is not None and instance_class_value_field is not None:\n                    window_instances = instance_gdf[\n                        instance_gdf.intersects(window_bounds)\n                    ]\n                    if len(window_instances) &gt; 0:\n                        if not quiet:\n                            pbar.write(\n                                f\"Found {len(window_instances)} instances in tile {chip_index}\"\n                            )\n\n                # Skip if no features and output_nofeature_tiles is False\n                if not output_nofeature_tiles and len(window_features) == 0:\n                    pbar.update(1)  # Still update progress bar\n                    continue\n\n                # Check polygon overlap ratio if specified\n                if min_polygon_overlap_ratio &gt; 0 and len(window_features) &gt; 0:\n                    valid_features = []\n                    for _, feature in window_features.iterrows():\n                        overlap_ratio = (\n                            feature.geometry.intersection(window_bounds).area\n                            / feature.geometry.area\n                        )\n                        if overlap_ratio &gt;= min_polygon_overlap_ratio:\n                            valid_features.append(feature)\n\n                    if len(valid_features) &gt; 0:\n                        window_features = gpd.GeoDataFrame(valid_features)\n                    elif not output_nofeature_tiles:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Apply mask if provided\n                if mask_gdf is not None:\n                    mask_features = mask_gdf[mask_gdf.intersects(window_bounds)]\n                    if len(mask_features) == 0:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Read image data - keep original for GeoTIFF export\n                orig_image_data = src.read(window=window)\n\n                # Create a copy for processing\n                image_data = orig_image_data.copy().astype(np.float32)\n\n                # Normalize image data for processing\n                for band in range(image_data.shape[0]):\n                    band_min, band_max = np.percentile(image_data[band], (1, 99))\n                    if band_max &gt; band_min:\n                        image_data[band] = np.clip(\n                            (image_data[band] - band_min) / (band_max - band_min), 0, 1\n                        )\n\n                # Read secondary image data if provided\n                if src2:\n                    image_data2 = src2.read(window=window)\n                    # Stack the two images\n                    image_data = np.vstack((image_data, image_data2))\n\n                # Apply blacken_around_feature if needed\n                if blacken_around_feature and len(window_features) &gt; 0:\n                    mask = np.zeros((tile_size_y, tile_size_x), dtype=bool)\n                    for _, feature in window_features.iterrows():\n                        # Project feature to pixel coordinates\n                        feature_pixels = features.rasterize(\n                            [(feature.geometry, 1)],\n                            out_shape=(tile_size_y, tile_size_x),\n                            transform=window_transform,\n                        )\n                        mask = np.logical_or(mask, feature_pixels.astype(bool))\n\n                    # Apply mask to image\n                    for band in range(image_data.shape[0]):\n                        temp = image_data[band, :, :]\n                        temp[~mask] = 0\n                        image_data[band, :, :] = temp\n\n                # Apply rotation if specified\n                if augmentation:\n                    # Convert to torch tensor for augmentation\n                    image_tensor = torch.from_numpy(image_data).unsqueeze(\n                        0\n                    )  # Add batch dimension\n                    # Apply augmentation with proper data format\n                    augmented = augmentation({\"image\": image_tensor})\n                    image_data = (\n                        augmented[\"image\"].squeeze(0).numpy()\n                    )  # Remove batch dimension\n\n                # Create a processed version for regular image formats\n                processed_image = (image_data * 255).astype(np.uint8)\n\n                # Create label mask\n                label_mask = np.zeros((tile_size_y, tile_size_x), dtype=np.uint8)\n                has_features = False\n\n                if len(window_features) &gt; 0:\n                    for idx, feature in window_features.iterrows():\n                        # Get class value\n                        class_val = (\n                            feature[class_value_field]\n                            if class_value_field in feature\n                            else 1\n                        )\n                        if isinstance(class_val, str):\n                            # If class is a string, use its position in the unique classes list\n                            class_id = class_to_id.get(class_val, 1)\n                        else:\n                            # If class is already a number, use it directly\n                            class_id = int(class_val) if class_val &gt; 0 else 1\n\n                        # Get the geometry in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            try:\n                                # Rasterize the feature\n                                feature_mask = features.rasterize(\n                                    [(geom, class_id)],\n                                    out_shape=(tile_size_y, tile_size_x),\n                                    transform=window_transform,\n                                    fill=0,\n                                    all_touched=all_touched,\n                                )\n\n                                # Update mask with higher class values taking precedence\n                                label_mask = np.maximum(label_mask, feature_mask)\n\n                                # Check if any pixels were added\n                                if np.any(feature_mask):\n                                    has_features = True\n                            except Exception as e:\n                                if not quiet:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                stats[\"errors\"] += 1\n\n                # Save as GeoTIFF if requested\n                if save_geotiff or image_chip_format.upper() in [\n                    \"TIFF\",\n                    \"TIF\",\n                    \"GEOTIFF\",\n                ]:\n                    # Standardize extension to .tif for GeoTIFF files\n                    image_filename = f\"tile_{chip_index:06d}.tif\"\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create profile for the GeoTIFF\n                    profile = src.profile.copy()\n                    profile.update(\n                        {\n                            \"height\": tile_size_y,\n                            \"width\": tile_size_x,\n                            \"count\": orig_image_data.shape[0],\n                            \"transform\": window_transform,\n                        }\n                    )\n\n                    # Save the GeoTIFF with original data\n                    try:\n                        with rasterio.open(image_path, \"w\", **profile) as dst:\n                            dst.write(orig_image_data)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving image GeoTIFF for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n                else:\n                    # For non-GeoTIFF formats, use PIL to save the image\n                    image_filename = (\n                        f\"tile_{chip_index:06d}.{image_chip_format.lower()}\"\n                    )\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create PIL image for saving\n                    if processed_image.shape[0] == 1:\n                        img = Image.fromarray(processed_image[0])\n                    elif processed_image.shape[0] == 3:\n                        # For RGB, need to transpose and make sure it's the right data type\n                        rgb_data = np.transpose(processed_image, (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n                    else:\n                        # For multiband images, save only RGB or first three bands\n                        rgb_data = np.transpose(processed_image[:3], (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n\n                    # Save image\n                    try:\n                        img.save(image_path)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(f\"ERROR saving image for tile {chip_index}: {e}\")\n                        stats[\"errors\"] += 1\n\n                # Save label as GeoTIFF\n                label_filename = f\"tile_{chip_index:06d}.tif\"\n                label_path = os.path.join(label_dir, label_filename)\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size_y,\n                    \"width\": tile_size_x,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Save label GeoTIFF\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask, 1)\n\n                    if has_features:\n                        pixel_count = np.count_nonzero(label_mask)\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += pixel_count\n                except Exception as e:\n                    if not quiet:\n                        pbar.write(f\"ERROR saving label for tile {chip_index}: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Also save a PNG version for easy visualization if requested\n                if metadata_format == \"PASCAL_VOC\":\n                    try:\n                        # Ensure correct data type for PIL\n                        png_label = label_mask.astype(np.uint8)\n                        label_img = Image.fromarray(png_label)\n                        label_png_path = os.path.join(\n                            label_dir, f\"tile_{chip_index:06d}.png\"\n                        )\n                        label_img.save(label_png_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving PNG label for tile {chip_index}: {e}\"\n                            )\n                            pbar.write(\n                                f\"  Label mask shape: {label_mask.shape}, dtype: {label_mask.dtype}\"\n                            )\n                            # Try again with explicit conversion\n                            try:\n                                # Alternative approach for problematic arrays\n                                png_data = np.zeros(\n                                    (tile_size_y, tile_size_x), dtype=np.uint8\n                                )\n                                np.copyto(png_data, label_mask, casting=\"unsafe\")\n                                label_img = Image.fromarray(png_data)\n                                label_img.save(label_png_path)\n                                pbar.write(\n                                    f\"  Succeeded using alternative conversion method\"\n                                )\n                            except Exception as e2:\n                                pbar.write(f\"  Second attempt also failed: {e2}\")\n                                stats[\"errors\"] += 1\n\n                # Generate annotations\n                if metadata_format == \"PASCAL_VOC\" and len(window_features) &gt; 0:\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = image_filename\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size_x)\n                    ET.SubElement(size, \"height\").text = str(tile_size_y)\n                    ET.SubElement(size, \"depth\").text = str(min(image_data.shape[0], 3))\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    for _, feature in window_features.iterrows():\n                        # Convert feature geometry to pixel coordinates\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        obj = ET.SubElement(root, \"object\")\n                        ET.SubElement(obj, \"name\").text = str(\n                            feature[class_value_field]\n                        )\n                        ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                        bbox = ET.SubElement(obj, \"bndbox\")\n                        ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                        ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                        ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                        ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    try:\n                        tree = ET.ElementTree(root)\n                        xml_path = os.path.join(ann_dir, f\"tile_{chip_index:06d}.xml\")\n                        tree.write(xml_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving XML annotation for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n\n                elif metadata_format == \"COCO\" and len(window_features) &gt; 0:\n                    # Add image info\n                    image_id = chip_index\n                    coco_annotations[\"images\"].append(\n                        {\n                            \"id\": image_id,\n                            \"file_name\": image_filename,\n                            \"width\": tile_size_x,\n                            \"height\": tile_size_y,\n                            \"crs\": str(src.crs),\n                            \"transform\": str(window_transform),\n                        }\n                    )\n\n                    # Add annotations for each feature\n                    for _, feature in window_features.iterrows():\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        width = xmax - xmin\n                        height = ymax - ymin\n\n                        # Add annotation\n                        ann_id += 1\n                        category_id = class_to_id[feature[class_value_field]]\n\n                        coco_annotations[\"annotations\"].append(\n                            {\n                                \"id\": ann_id,\n                                \"image_id\": image_id,\n                                \"category_id\": category_id,\n                                \"bbox\": [xmin, ymin, width, height],\n                                \"area\": width * height,\n                                \"iscrowd\": 0,\n                            }\n                        )\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                chip_index += 1\n\n        # Close progress bar\n        pbar.close()\n\n        # Save COCO annotations if applicable\n        if metadata_format == \"COCO\":\n            try:\n                with open(os.path.join(ann_dir, \"instances.json\"), \"w\") as f:\n                    json.dump(coco_annotations, f)\n            except Exception as e:\n                if not quiet:\n                    print(f\"ERROR saving COCO annotations: {e}\")\n                stats[\"errors\"] += 1\n\n        # Close secondary raster if opened\n        if src2:\n            src2.close()\n\n    # Print summary\n    if not quiet:\n        print(\"\\n------- Export Summary -------\")\n        print(f\"Total tiles exported: {stats['total_tiles']}\")\n        print(\n            f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n        )\n        if stats[\"tiles_with_features\"] &gt; 0:\n            print(\n                f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n            )\n        if stats[\"errors\"] &gt; 0:\n            print(f\"Errors encountered: {stats['errors']}\")\n        print(f\"Output saved to: {out_folder}\")\n\n        # Verify georeference in a sample image and label\n        if stats[\"total_tiles\"] &gt; 0:\n            print(\"\\n------- Georeference Verification -------\")\n            sample_image = os.path.join(image_dir, f\"tile_{start_index}.tif\")\n            sample_label = os.path.join(label_dir, f\"tile_{start_index}.tif\")\n\n            if os.path.exists(sample_image):\n                try:\n                    with rasterio.open(sample_image) as img:\n                        print(f\"Image CRS: {img.crs}\")\n                        print(f\"Image transform: {img.transform}\")\n                        print(\n                            f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                        )\n                        print(\n                            f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying image georeference: {e}\")\n\n            if os.path.exists(sample_label):\n                try:\n                    with rasterio.open(sample_label) as lbl:\n                        print(f\"Label CRS: {lbl.crs}\")\n                        print(f\"Label transform: {lbl.transform}\")\n                        print(\n                            f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                        )\n                        print(\n                            f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying label georeference: {e}\")\n\n    # Return statistics\n    return stats, out_folder\n</code></pre>"},{"location":"geoai/#geoai.geoai.geojson_to_coords","title":"<code>geojson_to_coords(geojson, src_crs='epsg:4326', dst_crs='epsg:4326')</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>str | dict</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>geoai/utils.py</code> <pre><code>def geojson_to_coords(\n    geojson: str, src_crs: str = \"epsg:4326\", dst_crs: str = \"epsg:4326\"\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.\n\n    Args:\n        geojson (str | dict): The geojson file path or a dictionary of feature collection.\n        src_crs (str, optional): The source CRS. Defaults to \"epsg:4326\".\n        dst_crs (str, optional): The destination CRS. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n\n    import json\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    if isinstance(geojson, dict):\n        geojson = json.dumps(geojson)\n    gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n    centroids = gdf.geometry.centroid\n    centroid_list = [[point.x, point.y] for point in centroids]\n    if src_crs != dst_crs:\n        centroid_list = transform_coords(\n            [x[0] for x in centroid_list],\n            [x[1] for x in centroid_list],\n            src_crs,\n            dst_crs,\n        )\n        centroid_list = [[x, y] for x, y in zip(centroid_list[0], centroid_list[1])]\n    return centroid_list\n</code></pre>"},{"location":"geoai/#geoai.geoai.geojson_to_xy","title":"<code>geojson_to_xy(src_fp, geojson, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>geojson</code> <code>str</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>geoai/utils.py</code> <pre><code>def geojson_to_xy(\n    src_fp: str, geojson: str, coord_crs: str = \"epsg:4326\", **kwargs: Any\n) -&gt; List[List[float]]:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        geojson: The geojson file path or a dictionary of feature collection.\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    with rasterio.open(src_fp) as src:\n        src_crs = src.crs\n    coords = geojson_to_coords(geojson, coord_crs, src_crs)\n    return coords_to_xy(src_fp, coords, src_crs, **kwargs)\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_device","title":"<code>get_device()</code>","text":"<p>Returns the best available device for deep learning in the order: CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"\n    Returns the best available device for deep learning in the order:\n    CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_landcover_loss_function","title":"<code>get_landcover_loss_function(loss_name='crossentropy', num_classes=2, ignore_index=None, class_weights=None, use_class_weights=False, focal_alpha=1.0, focal_gamma=2.0, device=None)</code>","text":"<p>Get loss function configured for landcover classification.</p> <p>Parameters:</p> Name Type Description Default <code>loss_name</code> <code>str</code> <p>Name of loss function (\"crossentropy\", \"focal\", \"dice\", \"combo\")</p> <code>'crossentropy'</code> <code>num_classes</code> <code>int</code> <p>Number of classes</p> <code>2</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Class index to ignore (default: None for no ignoring)</p> <code>None</code> <code>class_weights</code> <code>Optional[Tensor]</code> <p>Manual class weights tensor</p> <code>None</code> <code>use_class_weights</code> <code>bool</code> <p>Whether to use class weights</p> <code>False</code> <code>focal_alpha</code> <code>float</code> <p>Alpha parameter for focal loss</p> <code>1.0</code> <code>focal_gamma</code> <code>float</code> <p>Gamma parameter for focal loss</p> <code>2.0</code> <code>device</code> <code>Optional[device]</code> <p>Device to place loss function on</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>Configured loss function</p> Source code in <code>geoai/landcover_train.py</code> <pre><code>def get_landcover_loss_function(\n    loss_name: str = \"crossentropy\",\n    num_classes: int = 2,\n    ignore_index: Optional[int] = None,\n    class_weights: Optional[torch.Tensor] = None,\n    use_class_weights: bool = False,\n    focal_alpha: float = 1.0,\n    focal_gamma: float = 2.0,\n    device: Optional[torch.device] = None,\n) -&gt; nn.Module:\n    \"\"\"\n    Get loss function configured for landcover classification.\n\n    Args:\n        loss_name: Name of loss function (\"crossentropy\", \"focal\", \"dice\", \"combo\")\n        num_classes: Number of classes\n        ignore_index: Class index to ignore (default: None for no ignoring)\n        class_weights: Manual class weights tensor\n        use_class_weights: Whether to use class weights\n        focal_alpha: Alpha parameter for focal loss\n        focal_gamma: Gamma parameter for focal loss\n        device: Device to place loss function on\n\n    Returns:\n        Configured loss function\n    \"\"\"\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    loss_name = loss_name.lower()\n\n    if loss_name == \"crossentropy\":\n        weights = class_weights if use_class_weights else None\n        if weights is not None:\n            weights = weights.to(device)\n\n        return LandcoverCrossEntropyLoss(\n            weight=weights,\n            ignore_index=ignore_index,\n            reduction=\"mean\",\n        )\n\n    elif loss_name == \"focal\":\n        weights = class_weights if use_class_weights else None\n        if weights is not None:\n            weights = weights.to(device)\n\n        # Use -100 as default ignore_index for compatibility\n        idx = ignore_index if ignore_index is not None else -100\n\n        return FocalLoss(\n            alpha=focal_alpha,\n            gamma=focal_gamma,\n            ignore_index=idx,\n            reduction=\"mean\",\n            weight=weights,\n        )\n\n    else:\n        # Fall back to standard PyTorch loss\n        weights = class_weights if use_class_weights else None\n        if weights is not None:\n            weights = weights.to(device)\n\n        # Use -100 as default ignore_index for compatibility\n        idx = ignore_index if ignore_index is not None else -100\n\n        return nn.CrossEntropyLoss(\n            weight=weights,\n            ignore_index=idx,\n            reduction=\"mean\",\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_model_config","title":"<code>get_model_config(model_id)</code>","text":"<p>Get the model configuration for a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The Hugging Face model ID.</p> required <p>Returns:</p> Type Description <code>PretrainedConfig</code> <p>transformers.configuration_utils.PretrainedConfig: The model configuration.</p> Source code in <code>geoai/hf.py</code> <pre><code>def get_model_config(\n    model_id: str,\n) -&gt; \"transformers.configuration_utils.PretrainedConfig\":\n    \"\"\"\n    Get the model configuration for a Hugging Face model.\n\n    Args:\n        model_id (str): The Hugging Face model ID.\n\n    Returns:\n        transformers.configuration_utils.PretrainedConfig: The model configuration.\n    \"\"\"\n    return AutoConfig.from_pretrained(model_id)\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_model_input_channels","title":"<code>get_model_input_channels(model_id)</code>","text":"<p>Check the number of input channels supported by a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The Hugging Face model ID.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of input channels the model accepts.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unable to determine the number of input channels.</p> Source code in <code>geoai/hf.py</code> <pre><code>def get_model_input_channels(model_id: str) -&gt; int:\n    \"\"\"\n    Check the number of input channels supported by a Hugging Face model.\n\n    Args:\n        model_id (str): The Hugging Face model ID.\n\n    Returns:\n        int: The number of input channels the model accepts.\n\n    Raises:\n        ValueError: If unable to determine the number of input channels.\n    \"\"\"\n    # Load the model configuration\n    config = AutoConfig.from_pretrained(model_id)\n\n    # For Mask2Former models\n    if hasattr(config, \"backbone_config\"):\n        if hasattr(config.backbone_config, \"num_channels\"):\n            return config.backbone_config.num_channels\n\n    # Try to load the model and inspect its architecture\n    try:\n        model = AutoModelForMaskedImageModeling.from_pretrained(model_id)\n\n        # For Swin Transformer-based models like Mask2Former\n        if hasattr(model, \"backbone\") and hasattr(model.backbone, \"embeddings\"):\n            if hasattr(model.backbone.embeddings, \"patch_embeddings\"):\n                # Swin models typically have patch embeddings that indicate channel count\n                return model.backbone.embeddings.patch_embeddings.in_channels\n    except Exception as e:\n        print(f\"Couldn't inspect model architecture: {e}\")\n\n    # Default for most vision models\n    return 3\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_raster_info","title":"<code>get_raster_info(raster_path)</code>","text":"<p>Display basic information about a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing the basic information about the raster</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info(raster_path: str) -&gt; Dict[str, Any]:\n    \"\"\"Display basic information about a raster dataset.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster\n    \"\"\"\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Get basic metadata\n        info = {\n            \"driver\": src.driver,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"count\": src.count,\n            \"dtype\": src.dtypes[0],\n            \"crs\": src.crs.to_string() if src.crs else \"No CRS defined\",\n            \"transform\": src.transform,\n            \"bounds\": src.bounds,\n            \"resolution\": (src.transform[0], -src.transform[4]),\n            \"nodata\": src.nodata,\n        }\n\n        # Calculate statistics for each band\n        stats = []\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n            band_stats = {\n                \"band\": i,\n                \"min\": float(band.min()),\n                \"max\": float(band.max()),\n                \"mean\": float(band.mean()),\n                \"std\": float(band.std()),\n            }\n            stats.append(band_stats)\n\n        info[\"band_stats\"] = stats\n\n    return info\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_raster_info_gdal","title":"<code>get_raster_info_gdal(raster_path)</code>","text":"<p>Get basic information about a raster dataset using GDAL.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the raster, or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info_gdal(raster_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get basic information about a raster dataset using GDAL.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster,\n            or None if the file cannot be opened\n    \"\"\"\n\n    from osgeo import gdal\n\n    # Open the dataset\n    ds = gdal.Open(raster_path)\n    if ds is None:\n        print(f\"Error: Could not open {raster_path}\")\n        return None\n\n    # Get basic information\n    info = {\n        \"driver\": ds.GetDriver().ShortName,\n        \"width\": ds.RasterXSize,\n        \"height\": ds.RasterYSize,\n        \"count\": ds.RasterCount,\n        \"projection\": ds.GetProjection(),\n        \"geotransform\": ds.GetGeoTransform(),\n    }\n\n    # Calculate resolution\n    gt = ds.GetGeoTransform()\n    if gt:\n        info[\"resolution\"] = (abs(gt[1]), abs(gt[5]))\n        info[\"origin\"] = (gt[0], gt[3])\n\n    # Get band information\n    bands_info = []\n    for i in range(1, ds.RasterCount + 1):\n        band = ds.GetRasterBand(i)\n        stats = band.GetStatistics(True, True)\n        band_info = {\n            \"band\": i,\n            \"datatype\": gdal.GetDataTypeName(band.DataType),\n            \"min\": stats[0],\n            \"max\": stats[1],\n            \"mean\": stats[2],\n            \"std\": stats[3],\n            \"nodata\": band.GetNoDataValue(),\n        }\n        bands_info.append(band_info)\n\n    info[\"bands\"] = bands_info\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_raster_resolution","title":"<code>get_raster_resolution(image_path)</code>","text":"<p>Get pixel resolution from the raster using rasterio.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>The path to the raster image.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>A tuple of (x resolution, y resolution).</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_resolution(image_path: str) -&gt; Tuple[float, float]:\n    \"\"\"Get pixel resolution from the raster using rasterio.\n\n    Args:\n        image_path: The path to the raster image.\n\n    Returns:\n        A tuple of (x resolution, y resolution).\n    \"\"\"\n    with rasterio.open(image_path) as src:\n        res = src.res\n    return res\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_raster_stats","title":"<code>get_raster_stats(raster_path, divide_by=1.0)</code>","text":"<p>Calculate statistics for each band in a raster dataset.</p> <p>This function computes min, max, mean, and standard deviation values for each band in the provided raster, returning results in a dictionary with lists for each statistic type.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>divide_by</code> <code>float</code> <p>Value to divide pixel values by. Defaults to 1.0, which keeps the original pixel</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing lists of statistics with keys: - 'min': List of minimum values for each band - 'max': List of maximum values for each band - 'mean': List of mean values for each band - 'std': List of standard deviation values for each band</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_stats(raster_path: str, divide_by: float = 1.0) -&gt; Dict[str, Any]:\n    \"\"\"Calculate statistics for each band in a raster dataset.\n\n    This function computes min, max, mean, and standard deviation values\n    for each band in the provided raster, returning results in a dictionary\n    with lists for each statistic type.\n\n    Args:\n        raster_path (str): Path to the raster file\n        divide_by (float, optional): Value to divide pixel values by.\n            Defaults to 1.0, which keeps the original pixel\n\n    Returns:\n        dict: Dictionary containing lists of statistics with keys:\n            - 'min': List of minimum values for each band\n            - 'max': List of maximum values for each band\n            - 'mean': List of mean values for each band\n            - 'std': List of standard deviation values for each band\n    \"\"\"\n    # Initialize the results dictionary with empty lists\n    stats = {\"min\": [], \"max\": [], \"mean\": [], \"std\": []}\n\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Calculate statistics for each band\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n\n            # Append statistics for this band to each list\n            stats[\"min\"].append(float(band.min()) / divide_by)\n            stats[\"max\"].append(float(band.max()) / divide_by)\n            stats[\"mean\"].append(float(band.mean()) / divide_by)\n            stats[\"std\"].append(float(band.std()) / divide_by)\n\n    return stats\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_vector_info","title":"<code>get_vector_info(vector_path)</code>","text":"<p>Display basic information about a vector dataset using GeoPandas.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the vector dataset</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info(vector_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Display basic information about a vector dataset using GeoPandas.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset\n    \"\"\"\n    # Open the vector dataset\n    gdf = (\n        gpd.read_parquet(vector_path)\n        if vector_path.endswith(\".parquet\")\n        else gpd.read_file(vector_path)\n    )\n\n    # Get basic metadata\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": os.path.splitext(vector_path)[1][1:].upper(),  # Format from extension\n        \"feature_count\": len(gdf),\n        \"crs\": str(gdf.crs),\n        \"geometry_type\": str(gdf.geom_type.value_counts().to_dict()),\n        \"attribute_count\": len(gdf.columns) - 1,  # Subtract the geometry column\n        \"attribute_names\": list(gdf.columns[gdf.columns != \"geometry\"]),\n        \"bounds\": gdf.total_bounds.tolist(),\n    }\n\n    # Add statistics about numeric attributes\n    numeric_columns = gdf.select_dtypes(include=[\"number\"]).columns\n    attribute_stats = {}\n    for col in numeric_columns:\n        if col != \"geometry\":\n            attribute_stats[col] = {\n                \"min\": gdf[col].min(),\n                \"max\": gdf[col].max(),\n                \"mean\": gdf[col].mean(),\n                \"std\": gdf[col].std(),\n                \"null_count\": gdf[col].isna().sum(),\n            }\n\n    info[\"attribute_stats\"] = attribute_stats\n\n    return info\n</code></pre>"},{"location":"geoai/#geoai.geoai.get_vector_info_ogr","title":"<code>get_vector_info_ogr(vector_path)</code>","text":"<p>Get basic information about a vector dataset using OGR.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the vector dataset, or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info_ogr(vector_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get basic information about a vector dataset using OGR.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset,\n            or None if the file cannot be opened\n    \"\"\"\n    from osgeo import ogr\n\n    # Register all OGR drivers\n    ogr.RegisterAll()\n\n    # Open the dataset\n    ds = ogr.Open(vector_path)\n    if ds is None:\n        print(f\"Error: Could not open {vector_path}\")\n        return None\n\n    # Basic dataset information\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": ds.GetDriver().GetName(),\n        \"layer_count\": ds.GetLayerCount(),\n        \"layers\": [],\n    }\n\n    # Extract information for each layer\n    for i in range(ds.GetLayerCount()):\n        layer = ds.GetLayer(i)\n        layer_info = {\n            \"name\": layer.GetName(),\n            \"feature_count\": layer.GetFeatureCount(),\n            \"geometry_type\": ogr.GeometryTypeToName(layer.GetGeomType()),\n            \"spatial_ref\": (\n                layer.GetSpatialRef().ExportToWkt() if layer.GetSpatialRef() else \"None\"\n            ),\n            \"extent\": layer.GetExtent(),\n            \"fields\": [],\n        }\n\n        # Get field information\n        defn = layer.GetLayerDefn()\n        for j in range(defn.GetFieldCount()):\n            field_defn = defn.GetFieldDefn(j)\n            field_info = {\n                \"name\": field_defn.GetName(),\n                \"type\": field_defn.GetTypeName(),\n                \"width\": field_defn.GetWidth(),\n                \"precision\": field_defn.GetPrecision(),\n            }\n            layer_info[\"fields\"].append(field_info)\n\n        info[\"layers\"].append(layer_info)\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"geoai/#geoai.geoai.hybrid_regularization","title":"<code>hybrid_regularization(building_polygons)</code>","text":"<p>A comprehensive hybrid approach to building footprint regularization.</p> <p>Applies different strategies based on building characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def hybrid_regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    A comprehensive hybrid approach to building footprint regularization.\n\n    Applies different strategies based on building characteristics.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Use minimum_rotated_rectangle instead of oriented_envelope\n    try:\n        from shapely.minimum_rotated_rectangle import minimum_rotated_rectangle\n    except ImportError:\n        # For older Shapely versions\n        def minimum_rotated_rectangle(geom):\n            \"\"\"Calculate the minimum rotated rectangle for a geometry\"\"\"\n            # For older Shapely versions, implement a simple version\n            return geom.minimum_rotated_rectangle\n\n    # Determine input type for correct return\n    is_gdf = isinstance(building_polygons, gpd.GeoDataFrame)\n\n    # Extract geometries if GeoDataFrame\n    if is_gdf:\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # 1. Analyze building characteristics\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Calculate shape complexity metrics\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Calculate dominant angle\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        segment_angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Weight angles by segment length\n        hist, bins = np.histogram(\n            segment_angles % 180, bins=36, range=(0, 180), weights=segment_lengths\n        )\n        bin_centers = (bins[:-1] + bins[1:]) / 2\n        dominant_angle = bin_centers[np.argmax(hist)]\n\n        # Check if building is close to orthogonal\n        is_orthogonal = min(dominant_angle % 45, 45 - (dominant_angle % 45)) &lt; 5\n\n        # 2. Apply appropriate regularization strategy\n        if complexity &gt; 1.5:\n            # Complex buildings: use minimum rotated rectangle\n            result = minimum_rotated_rectangle(building)\n        elif is_orthogonal:\n            # Near-orthogonal buildings: orthogonalize in place\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create orthogonal hull in rotated space\n            bounds = rotated.bounds\n            ortho_hull = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            result = rotate(ortho_hull, dominant_angle, origin=\"centroid\")\n        else:\n            # Diagonal buildings: use custom approach for diagonal buildings\n            # Rotate to align with axes\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Simplify in rotated space\n            simplified = rotated.simplify(0.3, preserve_topology=True)\n\n            # Get the bounds in rotated space\n            bounds = simplified.bounds\n            min_x, min_y, max_x, max_y = bounds\n\n            # Create a rectangular hull in rotated space\n            rect_poly = Polygon(\n                [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]\n            )\n\n            # Rotate back to original orientation\n            result = rotate(rect_poly, dominant_angle, origin=\"centroid\")\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"geoai/#geoai.geoai.image_segmentation","title":"<code>image_segmentation(tif_path, output_path, labels_to_extract=None, dtype='uint8', model_name=None, segmenter_args=None, **kwargs)</code>","text":"<p>Segments an image with a Hugging Face segmentation model and saves the results as a single georeferenced image where each class has a unique integer value.</p> <p>Parameters:</p> Name Type Description Default <code>tif_path</code> <code>str</code> <p>Path to the input georeferenced TIF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output georeferenced segmentation will be saved.</p> required <code>labels_to_extract</code> <code>list</code> <p>List of labels to extract. If None, extracts all labels.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type to use for the output mask. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>model_name</code> <code>str</code> <p>Name of the Hugging Face model to use for segmentation, such as \"facebook/mask2former-swin-large-cityscapes-semantic\". Defaults to None. See https://huggingface.co/models?pipeline_tag=image-segmentation&amp;sort=trending for options.</p> <code>None</code> <code>segmenter_args</code> <code>dict</code> <p>Additional arguments to pass to the segmenter. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the segmentation pipeline</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>str</code> <p>(Path to saved image, dictionary mapping label names to their assigned values, dictionary mapping label names to confidence scores)</p> Source code in <code>geoai/hf.py</code> <pre><code>def image_segmentation(\n    tif_path: str,\n    output_path: str,\n    labels_to_extract: Optional[List[str]] = None,\n    dtype: str = \"uint8\",\n    model_name: Optional[str] = None,\n    segmenter_args: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Segments an image with a Hugging Face segmentation model and saves the results\n    as a single georeferenced image where each class has a unique integer value.\n\n    Args:\n        tif_path (str): Path to the input georeferenced TIF file.\n        output_path (str): Path where the output georeferenced segmentation will be saved.\n        labels_to_extract (list, optional): List of labels to extract. If None, extracts all labels.\n        dtype (str, optional): Data type to use for the output mask. Defaults to \"uint8\".\n        model_name (str, optional): Name of the Hugging Face model to use for segmentation,\n            such as \"facebook/mask2former-swin-large-cityscapes-semantic\". Defaults to None.\n            See https://huggingface.co/models?pipeline_tag=image-segmentation&amp;sort=trending for options.\n        segmenter_args (dict, optional): Additional arguments to pass to the segmenter.\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the segmentation pipeline\n\n    Returns:\n        tuple: (Path to saved image, dictionary mapping label names to their assigned values,\n            dictionary mapping label names to confidence scores)\n    \"\"\"\n    # Load the original georeferenced image to extract metadata\n    with rasterio.open(tif_path) as src:\n        # Save the metadata for later use\n        meta = src.meta.copy()\n        # Get the dimensions\n        height = src.height\n        width = src.width\n        # Get the transform and CRS for georeferencing\n        # transform = src.transform\n        # crs = src.crs\n\n    # Initialize the segmentation pipeline\n    if model_name is None:\n        model_name = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n\n    kwargs[\"task\"] = \"image-segmentation\"\n\n    segmenter = pipeline(model=model_name, **kwargs)\n\n    # Run the segmentation on the GeoTIFF\n    if segmenter_args is None:\n        segmenter_args = {}\n\n    segments = segmenter(tif_path, **segmenter_args)\n\n    # If no specific labels are requested, extract all available ones\n    if labels_to_extract is None:\n        labels_to_extract = [segment[\"label\"] for segment in segments]\n\n    # Create an empty mask to hold all the labels\n    # Using uint8 for up to 255 classes, switch to uint16 for more\n    combined_mask = np.zeros((height, width), dtype=np.uint8)\n\n    # Create a dictionary to map labels to values and store scores\n    label_to_value = {}\n    label_to_score = {}\n\n    # Process each segment we want to keep\n    for i, segment in enumerate(\n        [s for s in segments if s[\"label\"] in labels_to_extract]\n    ):\n        # Assign a unique value to each label (starting from 1)\n        value = i + 1\n        label = segment[\"label\"]\n        score = segment[\"score\"]\n\n        label_to_value[label] = value\n        label_to_score[label] = score\n\n        # Convert PIL image to numpy array\n        mask = np.array(segment[\"mask\"])\n\n        # Apply a threshold if it's a probability mask (not binary)\n        if mask.dtype == float:\n            mask = (mask &gt; 0.5).astype(np.uint8)\n\n        # Resize if needed to match original dimensions\n        if mask.shape != (height, width):\n            mask_img = Image.fromarray(mask)\n            mask_img = mask_img.resize((width, height))\n            mask = np.array(mask_img)\n\n        # Add this class to the combined mask\n        # Only overwrite if the pixel isn't already assigned to another class\n        # This handles overlapping segments by giving priority to earlier segments\n        combined_mask = np.where(\n            (mask &gt; 0) &amp; (combined_mask == 0), value, combined_mask\n        )\n\n    # Update metadata for the output raster\n    meta.update(\n        {\n            \"count\": 1,  # One band for the mask\n            \"dtype\": dtype,  # Use uint8 for up to 255 classes\n            \"nodata\": 0,  # 0 represents no class\n        }\n    )\n\n    # Save the mask as a new georeferenced GeoTIFF\n    with rasterio.open(output_path, \"w\", **meta) as dst:\n        dst.write(combined_mask[np.newaxis, :, :])  # Add channel dimension\n\n    # Create a CSV colormap file with scores included\n    csv_path = os.path.splitext(output_path)[0] + \"_colormap.csv\"\n    with open(csv_path, \"w\", newline=\"\") as csvfile:\n        fieldnames = [\"ClassValue\", \"ClassName\", \"ConfidenceScore\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for label, value in label_to_value.items():\n            writer.writerow(\n                {\n                    \"ClassValue\": value,\n                    \"ClassName\": label,\n                    \"ConfidenceScore\": f\"{label_to_score[label]:.4f}\",\n                }\n            )\n\n    return output_path, label_to_value, label_to_score\n</code></pre>"},{"location":"geoai/#geoai.geoai.inspect_pth_file","title":"<code>inspect_pth_file(pth_path)</code>","text":"<p>Inspect a PyTorch .pth model file to determine its architecture.</p> <p>Parameters:</p> Name Type Description Default <code>pth_path</code> <code>str</code> <p>Path to the .pth file to inspect</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Information about the model architecture</p> Source code in <code>geoai/utils.py</code> <pre><code>def inspect_pth_file(pth_path: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Inspect a PyTorch .pth model file to determine its architecture.\n\n    Args:\n        pth_path: Path to the .pth file to inspect\n\n    Returns:\n        Information about the model architecture\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(pth_path):\n        print(f\"Error: File {pth_path} not found\")\n        return\n\n    # Load the checkpoint\n    try:\n        checkpoint = torch.load(pth_path, map_location=torch.device(\"cpu\"))\n        print(f\"\\n{'='*50}\")\n        print(f\"Inspecting model file: {pth_path}\")\n        print(f\"{'='*50}\\n\")\n\n        # Check if it's a state_dict or a complete model\n        if isinstance(checkpoint, OrderedDict) or isinstance(checkpoint, dict):\n            if \"state_dict\" in checkpoint:\n                print(\"Found 'state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"state_dict\"]\n            elif \"model_state_dict\" in checkpoint:\n                print(\"Found 'model_state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"model_state_dict\"]\n            else:\n                print(\"Assuming file contains a direct state_dict.\")\n                state_dict = checkpoint\n\n            # Print the keys in the checkpoint\n            print(\"\\nCheckpoint contains the following keys:\")\n            for key in checkpoint.keys():\n                if isinstance(checkpoint[key], dict):\n                    print(f\"- {key} (dictionary with {len(checkpoint[key])} items)\")\n                elif isinstance(checkpoint[key], (torch.Tensor, list, tuple)):\n                    print(\n                        f\"- {key} (shape/size: {len(checkpoint[key]) if isinstance(checkpoint[key], (list, tuple)) else checkpoint[key].shape})\"\n                    )\n                else:\n                    print(f\"- {key} ({type(checkpoint[key]).__name__})\")\n\n            # Try to infer the model architecture from the state_dict keys\n            print(\"\\nAnalyzing model architecture from state_dict...\")\n\n            # Extract layer keys for analysis\n            layer_keys = list(state_dict.keys())\n\n            # Print the first few layer keys to understand naming pattern\n            print(\"\\nFirst 10 layer names in state_dict:\")\n            for i, key in enumerate(layer_keys[:10]):\n                shape = state_dict[key].shape\n                print(f\"- {key} (shape: {shape})\")\n\n            # Look for architecture indicators in the keys\n            architecture_indicators = {\n                \"conv\": 0,\n                \"bn\": 0,\n                \"layer\": 0,\n                \"fc\": 0,\n                \"backbone\": 0,\n                \"encoder\": 0,\n                \"decoder\": 0,\n                \"unet\": 0,\n                \"resnet\": 0,\n                \"classifier\": 0,\n                \"deeplab\": 0,\n                \"fcn\": 0,\n            }\n\n            for key in layer_keys:\n                for indicator in architecture_indicators:\n                    if indicator in key.lower():\n                        architecture_indicators[indicator] += 1\n\n            print(\"\\nArchitecture indicators found in layer names:\")\n            for indicator, count in architecture_indicators.items():\n                if count &gt; 0:\n                    print(f\"- '{indicator}' appears {count} times\")\n\n            # Count total parameters\n            total_params = sum(p.numel() for p in state_dict.values())\n            print(f\"\\nTotal parameters: {total_params:,}\")\n\n            # Try to load the model with different architectures\n            print(\"\\nAttempting to match with common architectures...\")\n\n            # Try to identify if it's a segmentation model\n            if any(\"out\" in k or \"classifier\" in k for k in layer_keys):\n                print(\"Model appears to be a segmentation model.\")\n\n                # Check if it might be a UNet\n                if (\n                    architecture_indicators[\"encoder\"] &gt; 0\n                    and architecture_indicators[\"decoder\"] &gt; 0\n                ):\n                    print(\n                        \"Architecture seems to be a UNet-based model with encoder-decoder structure.\"\n                    )\n                # Check for FCN or DeepLab indicators\n                elif architecture_indicators[\"fcn\"] &gt; 0:\n                    print(\n                        \"Architecture seems to be FCN-based (Fully Convolutional Network).\"\n                    )\n                elif architecture_indicators[\"deeplab\"] &gt; 0:\n                    print(\"Architecture seems to be DeepLab-based.\")\n                elif architecture_indicators[\"backbone\"] &gt; 0:\n                    print(\n                        \"Model has a backbone architecture, likely a modern segmentation model.\"\n                    )\n\n            # Try to infer output classes from the final layer\n            output_layer_keys = [\n                k for k in layer_keys if \"classifier\" in k or k.endswith(\".out.weight\")\n            ]\n            if output_layer_keys:\n                output_shape = state_dict[output_layer_keys[0]].shape\n                if len(output_shape) &gt;= 2:\n                    num_classes = output_shape[0]\n                    print(f\"\\nModel likely has {num_classes} output classes.\")\n\n            print(\"\\nSUMMARY:\")\n            print(\"The model appears to be\", end=\" \")\n            if architecture_indicators[\"unet\"] &gt; 0:\n                print(\"a UNet architecture.\", end=\" \")\n            elif architecture_indicators[\"fcn\"] &gt; 0:\n                print(\"an FCN architecture.\", end=\" \")\n            elif architecture_indicators[\"deeplab\"] &gt; 0:\n                print(\"a DeepLab architecture.\", end=\" \")\n            elif architecture_indicators[\"resnet\"] &gt; 0:\n                print(\"ResNet-based.\", end=\" \")\n            else:\n                print(\"a custom architecture.\", end=\" \")\n\n            # Try to load with common models\n            try_common_architectures(state_dict)\n\n        else:\n            print(\n                \"The file contains an entire model object rather than just a state dictionary.\"\n            )\n            # If it's a complete model, we can directly examine its architecture\n            print(checkpoint)\n\n    except Exception as e:\n        print(f\"Error loading the model file: {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>geoai/utils.py</code> <pre><code>def install_package(package: Union[str, List[str]]) -&gt; None:\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n    elif isinstance(package, list):\n        packages = package\n    else:\n        raise ValueError(\"The package argument must be a string or a list of strings.\")\n\n    for package in packages:\n        if package.startswith(\"https\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"geoai/#geoai.geoai.landcover_iou","title":"<code>landcover_iou(pred, target, num_classes, ignore_index=None, smooth=1e-06, mode='mean', boundary_weight_map=None)</code>","text":"<p>Calculate IoU for landcover classification with multiple weighting options.</p> <p>Supports three IoU calculation modes: 1. \"mean\": Simple mean IoU across all classes 2. \"perclass_frequency\": Weight by per-class pixel frequency 3. \"boundary_weighted\": Weight by distance to class boundaries</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted classes (N, H, W) or logits (N, C, H, W)</p> required <code>target</code> <code>Tensor</code> <p>Ground truth (N, H, W)</p> required <code>num_classes</code> <code>int</code> <p>Number of classes</p> required <code>ignore_index</code> <code>Optional[int]</code> <p>Class index to ignore (default: None)</p> <code>None</code> <code>smooth</code> <code>float</code> <p>Smoothing factor to avoid division by zero</p> <code>1e-06</code> <code>mode</code> <code>str</code> <p>IoU calculation mode (\"mean\", \"perclass_frequency\", \"boundary_weighted\")</p> <code>'mean'</code> <code>boundary_weight_map</code> <code>Optional[Tensor]</code> <p>Optional boundary weights (N, H, W)</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[float, Tuple[float, List[float], List[int]]]</code> <p>If mode == \"mean\": float (mean IoU)</p> <code>Union[float, Tuple[float, List[float], List[int]]]</code> <p>If mode == \"perclass_frequency\": tuple (weighted IoU, per-class IoUs, class counts)</p> <code>Union[float, Tuple[float, List[float], List[int]]]</code> <p>If mode == \"boundary_weighted\": float (boundary-weighted IoU)</p> Source code in <code>geoai/landcover_train.py</code> <pre><code>def landcover_iou(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    num_classes: int,\n    ignore_index: Optional[int] = None,\n    smooth: float = 1e-6,\n    mode: str = \"mean\",\n    boundary_weight_map: Optional[torch.Tensor] = None,\n) -&gt; Union[float, Tuple[float, List[float], List[int]]]:\n    \"\"\"\n    Calculate IoU for landcover classification with multiple weighting options.\n\n    Supports three IoU calculation modes:\n    1. \"mean\": Simple mean IoU across all classes\n    2. \"perclass_frequency\": Weight by per-class pixel frequency\n    3. \"boundary_weighted\": Weight by distance to class boundaries\n\n    Args:\n        pred: Predicted classes (N, H, W) or logits (N, C, H, W)\n        target: Ground truth (N, H, W)\n        num_classes: Number of classes\n        ignore_index: Class index to ignore (default: None)\n        smooth: Smoothing factor to avoid division by zero\n        mode: IoU calculation mode (\"mean\", \"perclass_frequency\", \"boundary_weighted\")\n        boundary_weight_map: Optional boundary weights (N, H, W)\n\n    Returns:\n        If mode == \"mean\": float (mean IoU)\n        If mode == \"perclass_frequency\": tuple (weighted IoU, per-class IoUs, class counts)\n        If mode == \"boundary_weighted\": float (boundary-weighted IoU)\n    \"\"\"\n\n    # Convert logits to class predictions if needed\n    if pred.dim() == 4:\n        pred = torch.argmax(pred, dim=1)\n\n    # Ensure correct shape\n    assert (\n        pred.shape == target.shape\n    ), f\"Shape mismatch: pred {pred.shape}, target {target.shape}\"\n\n    # Create mask for valid pixels\n    if ignore_index is not None:\n        valid_mask = target != ignore_index\n    else:\n        valid_mask = torch.ones_like(target, dtype=torch.bool)\n\n    # Simple mean IoU\n    if mode == \"mean\":\n        ious = []\n        for cls in range(num_classes):\n            if ignore_index is not None and cls == ignore_index:\n                continue\n\n            pred_cls = (pred == cls) &amp; valid_mask\n            target_cls = (target == cls) &amp; valid_mask\n\n            intersection = (pred_cls &amp; target_cls).sum().float()\n            union = (pred_cls | target_cls).sum().float()\n\n            if union &gt; 0:\n                iou = (intersection + smooth) / (union + smooth)\n                ious.append(iou.item())\n\n        return sum(ious) / len(ious) if ious else 0.0\n\n    # Per-class frequency weighted IoU\n    elif mode == \"perclass_frequency\":\n        ious = []\n        class_counts = []\n\n        # Filter out ignore_index from target\n        if ignore_index is not None:\n            target_filtered = target[valid_mask]\n            pred_filtered = pred[valid_mask]\n        else:\n            target_filtered = target.view(-1)\n            pred_filtered = pred.view(-1)\n\n        total_valid_pixels = target_filtered.numel()\n\n        for cls in range(num_classes):\n            if ignore_index is not None and cls == ignore_index:\n                continue\n\n            pred_cls = pred_filtered == cls\n            target_cls = target_filtered == cls\n\n            intersection = (pred_cls &amp; target_cls).sum().float()\n            union = (pred_cls | target_cls).sum().float()\n\n            class_pixel_count = target_cls.sum().item()\n\n            if union &gt; 0:\n                iou = (intersection + smooth) / (union + smooth)\n                ious.append(iou.item())\n                class_counts.append(class_pixel_count)\n            else:\n                ious.append(0.0)\n                class_counts.append(0)\n\n        # Calculate frequency-weighted IoU\n        if sum(class_counts) &gt; 0:\n            weights = [count / total_valid_pixels for count in class_counts]\n            weighted_iou = sum(iou * weight for iou, weight in zip(ious, weights))\n        else:\n            weighted_iou = 0.0\n\n        return weighted_iou, ious, class_counts\n\n    # Boundary-weighted IoU\n    elif mode == \"boundary_weighted\":\n        if boundary_weight_map is None:\n            raise ValueError(\"boundary_weight_map required for boundary_weighted mode\")\n\n        ious = []\n        weights = []\n\n        for cls in range(num_classes):\n            if ignore_index is not None and cls == ignore_index:\n                continue\n\n            pred_cls = (pred == cls) &amp; valid_mask\n            target_cls = (target == cls) &amp; valid_mask\n\n            # Weight by boundary map\n            weighted_intersection = (\n                pred_cls &amp; target_cls\n            ).float() * boundary_weight_map\n            weighted_union = (pred_cls | target_cls).float() * boundary_weight_map\n\n            intersection_sum = weighted_intersection.sum()\n            union_sum = weighted_union.sum()\n\n            if union_sum &gt; 0:\n                iou = (intersection_sum + smooth) / (union_sum + smooth)\n                weight = union_sum.item()\n                ious.append(iou.item())\n                weights.append(weight)\n\n        if sum(weights) &gt; 0:\n            weighted_iou = sum(iou * w for iou, w in zip(ious, weights)) / sum(weights)\n        else:\n            weighted_iou = 0.0\n\n        return weighted_iou\n\n    else:\n        raise ValueError(\n            f\"Unknown mode: {mode}. Use 'mean', 'perclass_frequency', or 'boundary_weighted'\"\n        )\n</code></pre>"},{"location":"geoai/#geoai.geoai.mask_generation","title":"<code>mask_generation(input_path, output_mask_path, output_csv_path, model='facebook/sam-vit-base', confidence_threshold=0.5, points_per_side=32, crop_size=None, batch_size=1, band_indices=None, min_object_size=0, generator_kwargs=None, **kwargs)</code>","text":"<p>Process a GeoTIFF using SAM mask generation and save results as a GeoTIFF and CSV.</p> <p>The function reads a GeoTIFF image, applies the SAM mask generator from the Hugging Face transformers pipeline, rasterizes the resulting masks to create a labeled mask GeoTIFF, and saves mask scores and geometries to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF image.</p> required <code>output_mask_path</code> <code>str</code> <p>Path where the output mask GeoTIFF will be saved.</p> required <code>output_csv_path</code> <code>str</code> <p>Path where the mask scores CSV will be saved.</p> required <code>model</code> <code>str</code> <p>HuggingFace model checkpoint for the SAM model.</p> <code>'facebook/sam-vit-base'</code> <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence score for masks to be included.</p> <code>0.5</code> <code>points_per_side</code> <code>int</code> <p>Number of points to sample along each side of the image.</p> <code>32</code> <code>crop_size</code> <code>Optional[int]</code> <p>Size of image crops for processing. If None, process the full image.</p> <code>None</code> <code>band_indices</code> <code>Optional[List[int]]</code> <p>List of band indices to use. If None, use all bands.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>1</code> <code>min_object_size</code> <code>int</code> <p>Minimum size in pixels for objects to be included. Smaller masks will be filtered out.</p> <code>0</code> <code>generator_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the mask generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple containing the paths to the saved mask GeoTIFF and CSV file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input file cannot be opened or processed.</p> <code>RuntimeError</code> <p>If mask generation fails.</p> Source code in <code>geoai/hf.py</code> <pre><code>def mask_generation(\n    input_path: str,\n    output_mask_path: str,\n    output_csv_path: str,\n    model: str = \"facebook/sam-vit-base\",\n    confidence_threshold: float = 0.5,\n    points_per_side: int = 32,\n    crop_size: Optional[int] = None,\n    batch_size: int = 1,\n    band_indices: Optional[List[int]] = None,\n    min_object_size: int = 0,\n    generator_kwargs: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Process a GeoTIFF using SAM mask generation and save results as a GeoTIFF and CSV.\n\n    The function reads a GeoTIFF image, applies the SAM mask generator from the\n    Hugging Face transformers pipeline, rasterizes the resulting masks to create\n    a labeled mask GeoTIFF, and saves mask scores and geometries to a CSV file.\n\n    Args:\n        input_path: Path to the input GeoTIFF image.\n        output_mask_path: Path where the output mask GeoTIFF will be saved.\n        output_csv_path: Path where the mask scores CSV will be saved.\n        model: HuggingFace model checkpoint for the SAM model.\n        confidence_threshold: Minimum confidence score for masks to be included.\n        points_per_side: Number of points to sample along each side of the image.\n        crop_size: Size of image crops for processing. If None, process the full image.\n        band_indices: List of band indices to use. If None, use all bands.\n        batch_size: Batch size for inference.\n        min_object_size: Minimum size in pixels for objects to be included. Smaller masks will be filtered out.\n        generator_kwargs: Additional keyword arguments to pass to the mask generator.\n\n    Returns:\n        Tuple containing the paths to the saved mask GeoTIFF and CSV file.\n\n    Raises:\n        ValueError: If the input file cannot be opened or processed.\n        RuntimeError: If mask generation fails.\n    \"\"\"\n    # Set up the mask generator\n    print(\"Setting up mask generator...\")\n    mask_generator = pipeline(model=model, task=\"mask-generation\", **kwargs)\n\n    # Open the GeoTIFF file\n    try:\n        print(f\"Reading input GeoTIFF: {input_path}\")\n        with rasterio.open(input_path) as src:\n            # Read metadata\n            profile = src.profile\n            # transform = src.transform\n            # crs = src.crs\n\n            # Read the image data\n            if band_indices is not None:\n                print(f\"Using specified bands: {band_indices}\")\n                image_data = np.stack([src.read(i + 1) for i in band_indices])\n            else:\n                print(\"Using all bands\")\n                image_data = src.read()\n\n            # Handle image with more than 3 bands (convert to RGB for visualization)\n            if image_data.shape[0] &gt; 3:\n                print(\n                    f\"Converting {image_data.shape[0]} bands to RGB (using first 3 bands)\"\n                )\n                # Select first three bands or perform other band combination\n                image_data = image_data[:3]\n            elif image_data.shape[0] == 1:\n                print(\"Duplicating single band to create 3-band image\")\n                # Duplicate single band to create a 3-band image\n                image_data = np.vstack([image_data] * 3)\n\n            # Transpose to HWC format for the model\n            image_data = np.transpose(image_data, (1, 2, 0))\n\n            # Normalize the image if needed\n            if image_data.dtype != np.uint8:\n                print(f\"Normalizing image from {image_data.dtype} to uint8\")\n                image_data = (image_data / image_data.max() * 255).astype(np.uint8)\n    except Exception as e:\n        raise ValueError(f\"Failed to open or process input GeoTIFF: {e}\")\n\n    # Process the image with the mask generator\n    try:\n        # Convert numpy array to PIL Image for the pipeline\n        # Ensure the array is in the right format (HWC and uint8)\n        if image_data.dtype != np.uint8:\n            image_data = (image_data / image_data.max() * 255).astype(np.uint8)\n\n        # Create a PIL Image from the numpy array\n        print(\"Converting to PIL Image for mask generation\")\n        pil_image = Image.fromarray(image_data)\n\n        # Use the SAM pipeline for mask generation\n        if generator_kwargs is None:\n            generator_kwargs = {}\n\n        print(\"Running mask generation...\")\n        mask_results = mask_generator(\n            pil_image,\n            points_per_side=points_per_side,\n            crop_n_points_downscale_factor=1 if crop_size is None else 2,\n            point_grids=None,\n            pred_iou_thresh=confidence_threshold,\n            stability_score_thresh=confidence_threshold,\n            crops_n_layers=0 if crop_size is None else 1,\n            crop_overlap_ratio=0.5,\n            batch_size=batch_size,\n            **generator_kwargs,\n        )\n\n        print(\n            f\"Number of initial masks: {len(mask_results['masks']) if isinstance(mask_results, dict) and 'masks' in mask_results else len(mask_results)}\"\n        )\n\n    except Exception as e:\n        raise RuntimeError(f\"Mask generation failed: {e}\")\n\n    # Create a mask raster with unique IDs for each mask\n    mask_raster = np.zeros((image_data.shape[0], image_data.shape[1]), dtype=np.uint32)\n    mask_records = []\n\n    # Process each mask based on the structure of mask_results\n    if (\n        isinstance(mask_results, dict)\n        and \"masks\" in mask_results\n        and \"scores\" in mask_results\n    ):\n        # Handle dictionary with 'masks' and 'scores' lists\n        print(\"Processing masks...\")\n        total_masks = len(mask_results[\"masks\"])\n\n        # Create progress bar\n        for i, (mask_data, score) in enumerate(\n            tqdm(\n                zip(mask_results[\"masks\"], mask_results[\"scores\"]),\n                total=total_masks,\n                desc=\"Processing masks\",\n            )\n        ):\n            mask_id = i + 1  # Start IDs at 1\n\n            # Convert to numpy if not already\n            if not isinstance(mask_data, np.ndarray):\n                # Try to convert from tensor or other format if needed\n                try:\n                    mask_data = np.array(mask_data)\n                except:\n                    print(f\"Could not convert mask at index {i} to numpy array\")\n                    continue\n\n            mask_binary = mask_data.astype(bool)\n            area_pixels = np.sum(mask_binary)\n\n            # Skip if mask is smaller than the minimum size\n            if area_pixels &lt; min_object_size:\n                continue\n\n            # Add the mask to the raster with a unique ID\n            mask_raster[mask_binary] = mask_id\n\n            # Create a record for the CSV - without geometry calculation\n            mask_records.append(\n                {\"mask_id\": mask_id, \"score\": float(score), \"area_pixels\": area_pixels}\n            )\n    elif isinstance(mask_results, list):\n        # Handle list of dictionaries format (SAM original format)\n        print(\"Processing masks...\")\n        total_masks = len(mask_results)\n\n        # Create progress bar\n        for i, mask_result in enumerate(tqdm(mask_results, desc=\"Processing masks\")):\n            mask_id = i + 1  # Start IDs at 1\n\n            # Try different possible key names for masks and scores\n            mask_data = None\n            score = None\n\n            if isinstance(mask_result, dict):\n                # Try to find mask data\n                if \"segmentation\" in mask_result:\n                    mask_data = mask_result[\"segmentation\"]\n                elif \"mask\" in mask_result:\n                    mask_data = mask_result[\"mask\"]\n\n                # Try to find score\n                if \"score\" in mask_result:\n                    score = mask_result[\"score\"]\n                elif \"predicted_iou\" in mask_result:\n                    score = mask_result[\"predicted_iou\"]\n                elif \"stability_score\" in mask_result:\n                    score = mask_result[\"stability_score\"]\n                else:\n                    score = 1.0  # Default score if none found\n            else:\n                # If mask_result is not a dict, it might be the mask directly\n                try:\n                    mask_data = np.array(mask_result)\n                    score = 1.0  # Default score\n                except:\n                    print(f\"Could not process mask at index {i}\")\n                    continue\n\n            if mask_data is not None:\n                # Convert to numpy if not already\n                if not isinstance(mask_data, np.ndarray):\n                    try:\n                        mask_data = np.array(mask_data)\n                    except:\n                        print(f\"Could not convert mask at index {i} to numpy array\")\n                        continue\n\n                mask_binary = mask_data.astype(bool)\n                area_pixels = np.sum(mask_binary)\n\n                # Skip if mask is smaller than the minimum size\n                if area_pixels &lt; min_object_size:\n                    continue\n\n                # Add the mask to the raster with a unique ID\n                mask_raster[mask_binary] = mask_id\n\n                # Create a record for the CSV - without geometry calculation\n                mask_records.append(\n                    {\n                        \"mask_id\": mask_id,\n                        \"score\": float(score),\n                        \"area_pixels\": area_pixels,\n                    }\n                )\n    else:\n        # If we couldn't figure out the format, raise an error\n        raise ValueError(f\"Unexpected format for mask_results: {type(mask_results)}\")\n\n    print(f\"Number of final masks (after size filtering): {len(mask_records)}\")\n\n    # Save the mask raster as a GeoTIFF\n    print(f\"Saving mask GeoTIFF to {output_mask_path}\")\n    output_profile = profile.copy()\n    output_profile.update(dtype=rasterio.uint32, count=1, compress=\"lzw\", nodata=0)\n\n    with rasterio.open(output_mask_path, \"w\", **output_profile) as dst:\n        dst.write(mask_raster.astype(rasterio.uint32), 1)\n\n    # Save the mask data as a CSV\n    print(f\"Saving mask metadata to {output_csv_path}\")\n    mask_df = pd.DataFrame(mask_records)\n    mask_df.to_csv(output_csv_path, index=False)\n\n    print(\"Processing complete!\")\n    return output_mask_path, output_csv_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=1.0, mask_threshold=0.5, min_object_area=100, max_object_area=None, nms_iou_threshold=0.5)</code>","text":"<p>Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <code>str</code> <p>Path to the building masks GeoTIFF</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoJSON (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>1.0</code> <code>mask_threshold</code> <code>float</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>0.5</code> <code>min_object_area</code> <code>int</code> <p>Minimum area in pixels to keep a building (default: self.min_object_area)</p> <code>100</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Maximum area in pixels to keep a building (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>GeoDataFrame with building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def masks_to_vector(\n    mask_path: str,\n    output_path: Optional[str] = None,\n    simplify_tolerance: float = 1.0,\n    mask_threshold: float = 0.5,\n    min_object_area: int = 100,\n    max_object_area: Optional[int] = None,\n    nms_iou_threshold: float = 0.5,\n) -&gt; Any:\n    \"\"\"\n    Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.\n\n    Args:\n        mask_path: Path to the building masks GeoTIFF\n        output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep a building (default: self.min_object_area)\n        max_object_area: Maximum area in pixels to keep a building (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n\n    Returns:\n        Any: GeoDataFrame with building footprints\n    \"\"\"\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min building area: {min_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(f\"Found {num_labels-1} potential buildings\")  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this building\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this building\n            building_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                building_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Building class\n            },\n            crs=crs,\n        )\n\n        def filter_overlapping_polygons(gdf, **kwargs):\n            \"\"\"\n            Filter overlapping polygons using non-maximum suppression.\n\n            Args:\n                gdf: GeoDataFrame with polygons\n                **kwargs: Optional parameters:\n                    nms_iou_threshold: IoU threshold for filtering\n\n            Returns:\n                Filtered GeoDataFrame\n            \"\"\"\n            if len(gdf) &lt;= 1:\n                return gdf\n\n            # Get parameters from kwargs or use instance defaults\n            iou_threshold = kwargs.get(\"nms_iou_threshold\", nms_iou_threshold)\n\n            # Sort by confidence\n            gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n            # Fix any invalid geometries\n            gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n                lambda geom: geom.buffer(0) if not geom.is_valid else geom\n            )\n\n            keep_indices = []\n            polygons = gdf.geometry.values\n\n            for i in range(len(polygons)):\n                if i in keep_indices:\n                    continue\n\n                keep = True\n                for j in keep_indices:\n                    # Skip invalid geometries\n                    if not polygons[i].is_valid or not polygons[j].is_valid:\n                        continue\n\n                    # Calculate IoU\n                    try:\n                        intersection = polygons[i].intersection(polygons[j]).area\n                        union = polygons[i].area + polygons[j].area - intersection\n                        iou = intersection / union if union &gt; 0 else 0\n\n                        if iou &gt; iou_threshold:\n                            keep = False\n                            break\n                    except Exception:\n                        # Skip on topology exceptions\n                        continue\n\n                if keep:\n                    keep_indices.append(i)\n\n            return gdf.iloc[keep_indices]\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        print(f\"Final building count after filtering: {len(gdf)}\")\n\n        # Save to file\n        if output_path is not None:\n            gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.mosaic_geotiffs","title":"<code>mosaic_geotiffs(input_dir, output_file, mask_file=None)</code>","text":"<p>Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).</p> <p>This function identifies all GeoTIFF files in the specified directory, creates a seamless mosaic with proper handling of nodata values, and saves as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output will be clipped to the extent of the mask.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to the directory containing GeoTIFF files.</p> required <code>output_file</code> <code>str</code> <p>Path to the output Cloud Optimized GeoTIFF file.</p> required <code>mask_file</code> <code>str</code> <p>Path to a mask file to clip the output. If provided, the output will be clipped to the extent of this mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>None</code> <p>True if the mosaic was created successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\nTrue\n&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\nTrue\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def mosaic_geotiffs(\n    input_dir: str, output_file: str, mask_file: Optional[str] = None\n) -&gt; None:\n    \"\"\"Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).\n\n    This function identifies all GeoTIFF files in the specified directory,\n    creates a seamless mosaic with proper handling of nodata values, and saves\n    as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output\n    will be clipped to the extent of the mask.\n\n    Args:\n        input_dir (str): Path to the directory containing GeoTIFF files.\n        output_file (str): Path to the output Cloud Optimized GeoTIFF file.\n        mask_file (str, optional): Path to a mask file to clip the output.\n            If provided, the output will be clipped to the extent of this mask.\n            Defaults to None.\n\n    Returns:\n        bool: True if the mosaic was created successfully, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\n        True\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\n        True\n    \"\"\"\n    import glob\n\n    from osgeo import gdal\n\n    gdal.UseExceptions()\n    # Get all tif files in the directory\n    tif_files = glob.glob(os.path.join(input_dir, \"*.tif\"))\n\n    if not tif_files:\n        print(\"No GeoTIFF files found in the specified directory.\")\n        return False\n\n    # Analyze the first input file to determine compression and nodata settings\n    ds = gdal.Open(tif_files[0])\n    if ds is None:\n        print(f\"Unable to open {tif_files[0]}\")\n        return False\n\n    # Get driver metadata from the first file\n    driver = ds.GetDriver()\n    creation_options = []\n\n    # Check compression type\n    metadata = ds.GetMetadata(\"IMAGE_STRUCTURE\")\n    if \"COMPRESSION\" in metadata:\n        compression = metadata[\"COMPRESSION\"]\n        creation_options.append(f\"COMPRESS={compression}\")\n    else:\n        # Default compression if none detected\n        creation_options.append(\"COMPRESS=LZW\")\n\n    # Add COG-specific creation options\n    creation_options.extend([\"TILED=YES\", \"BLOCKXSIZE=512\", \"BLOCKYSIZE=512\"])\n\n    # Check for nodata value in the first band of the first file\n    band = ds.GetRasterBand(1)\n    has_nodata = band.GetNoDataValue() is not None\n    nodata_value = band.GetNoDataValue() if has_nodata else None\n\n    # Close the dataset\n    ds = None\n\n    # Create a temporary VRT (Virtual Dataset)\n    vrt_path = os.path.join(input_dir, \"temp_mosaic.vrt\")\n\n    # Build VRT from input files with proper nodata handling\n    vrt_options = gdal.BuildVRTOptions(\n        resampleAlg=\"nearest\",\n        srcNodata=nodata_value if has_nodata else None,\n        VRTNodata=nodata_value if has_nodata else None,\n    )\n    vrt_dataset = gdal.BuildVRT(vrt_path, tif_files, options=vrt_options)\n\n    # Close the VRT dataset to flush it to disk\n    vrt_dataset = None\n\n    # Create temp mosaic\n    temp_mosaic = output_file + \".temp.tif\"\n\n    # Convert VRT to GeoTIFF with the same compression as input\n    translate_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=creation_options,\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(temp_mosaic, vrt_path, options=translate_options)\n\n    # Apply mask if provided\n    if mask_file and os.path.exists(mask_file):\n        print(f\"Clipping mosaic to mask: {mask_file}\")\n\n        # Create a temporary clipped file\n        clipped_mosaic = output_file + \".clipped.tif\"\n\n        # Open mask file\n        mask_ds = gdal.Open(mask_file)\n        if mask_ds is None:\n            print(f\"Unable to open mask file: {mask_file}\")\n            # Continue without clipping\n        else:\n            # Get mask extent\n            mask_geotransform = mask_ds.GetGeoTransform()\n            mask_projection = mask_ds.GetProjection()\n            mask_ulx = mask_geotransform[0]\n            mask_uly = mask_geotransform[3]\n            mask_lrx = mask_ulx + (mask_geotransform[1] * mask_ds.RasterXSize)\n            mask_lry = mask_uly + (mask_geotransform[5] * mask_ds.RasterYSize)\n\n            # Close mask dataset\n            mask_ds = None\n\n            # Use warp options to clip\n            warp_options = gdal.WarpOptions(\n                format=\"GTiff\",\n                outputBounds=[mask_ulx, mask_lry, mask_lrx, mask_uly],\n                dstSRS=mask_projection,\n                creationOptions=creation_options,\n                srcNodata=nodata_value if has_nodata else None,\n                dstNodata=nodata_value if has_nodata else None,\n            )\n\n            # Apply clipping\n            gdal.Warp(clipped_mosaic, temp_mosaic, options=warp_options)\n\n            # Remove the unclipped temp mosaic and use the clipped one\n            os.remove(temp_mosaic)\n            temp_mosaic = clipped_mosaic\n\n    # Create internal overviews for the temp mosaic\n    ds = gdal.Open(temp_mosaic, gdal.GA_Update)\n    overview_list = [2, 4, 8, 16, 32]\n    ds.BuildOverviews(\"NEAREST\", overview_list)\n    ds = None  # Close the dataset to ensure overviews are written\n\n    # Convert the temp mosaic to a proper COG\n    cog_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=[\n            \"TILED=YES\",\n            \"COPY_SRC_OVERVIEWS=YES\",\n            \"COMPRESS=DEFLATE\",\n            \"PREDICTOR=2\",\n            \"BLOCKXSIZE=512\",\n            \"BLOCKYSIZE=512\",\n        ],\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(output_file, temp_mosaic, options=cog_options)\n\n    # Clean up temporary files\n    if os.path.exists(vrt_path):\n        os.remove(vrt_path)\n    if os.path.exists(temp_mosaic):\n        os.remove(temp_mosaic)\n\n    print(f\"Cloud Optimized GeoTIFF mosaic created successfully: {output_file}\")\n    return True\n</code></pre>"},{"location":"geoai/#geoai.geoai.orthogonalize","title":"<code>orthogonalize(input_path, output_path=None, epsilon=0.2, min_area=10, min_segments=4, area_tolerance=0.7, detect_triangles=True)</code>","text":"<p>Orthogonalizes object masks in a GeoTIFF file.</p> <p>This function reads a GeoTIFF containing object masks (binary or labeled regions), converts the raster masks to vector polygons, applies orthogonalization to each polygon, and optionally writes the result to a GeoJSON file. The source code is adapted from the Solar Panel Detection algorithm by Esri. See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284. Credits to Esri for the original code.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output GeoJSON file. If None, no file is saved.</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>Simplification tolerance for the Douglas-Peucker algorithm. Higher values result in more simplification. Default is 0.2.</p> <code>0.2</code> <code>min_area</code> <code>float</code> <p>Minimum area of polygons to process (smaller ones are kept as-is).</p> <code>10</code> <code>min_segments</code> <code>int</code> <p>Minimum number of segments to keep after simplification. Default is 4 (for rectangular shapes).</p> <code>4</code> <code>area_tolerance</code> <code>float</code> <p>Allowed ratio of area change. Values less than 1.0 restrict area change. Default is 0.7 (allows reduction to 70% of original area).</p> <code>0.7</code> <code>detect_triangles</code> <code>bool</code> <p>If True, performs additional check to avoid creating triangular shapes.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A GeoDataFrame containing the orthogonalized features.</p> Source code in <code>geoai/utils.py</code> <pre><code>def orthogonalize(\n    input_path,\n    output_path=None,\n    epsilon=0.2,\n    min_area=10,\n    min_segments=4,\n    area_tolerance=0.7,\n    detect_triangles=True,\n) -&gt; Any:\n    \"\"\"\n    Orthogonalizes object masks in a GeoTIFF file.\n\n    This function reads a GeoTIFF containing object masks (binary or labeled regions),\n    converts the raster masks to vector polygons, applies orthogonalization to each polygon,\n    and optionally writes the result to a GeoJSON file.\n    The source code is adapted from the Solar Panel Detection algorithm by Esri.\n    See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284.\n    Credits to Esri for the original code.\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str, optional): Path to save the output GeoJSON file. If None, no file is saved.\n        epsilon (float, optional): Simplification tolerance for the Douglas-Peucker algorithm.\n            Higher values result in more simplification. Default is 0.2.\n        min_area (float, optional): Minimum area of polygons to process (smaller ones are kept as-is).\n        min_segments (int, optional): Minimum number of segments to keep after simplification.\n            Default is 4 (for rectangular shapes).\n        area_tolerance (float, optional): Allowed ratio of area change. Values less than 1.0 restrict\n            area change. Default is 0.7 (allows reduction to 70% of original area).\n        detect_triangles (bool, optional): If True, performs additional check to avoid creating triangular shapes.\n\n    Returns:\n        Any: A GeoDataFrame containing the orthogonalized features.\n    \"\"\"\n\n    from functools import partial\n\n    def orthogonalize_ring(ring, epsilon=0.2, min_segments=4):\n        \"\"\"\n        Orthogonalizes a ring (list of coordinates).\n\n        Args:\n            ring (list): List of [x, y] coordinates forming a ring\n            epsilon (float, optional): Simplification tolerance\n            min_segments (int, optional): Minimum number of segments to keep\n\n        Returns:\n            list: Orthogonalized list of coordinates\n        \"\"\"\n        if len(ring) &lt;= 3:\n            return ring\n\n        # Convert to numpy array\n        ring_arr = np.array(ring)\n\n        # Get orientation\n        angle = math.degrees(get_orientation(ring_arr))\n\n        # Simplify using Ramer-Douglas-Peucker algorithm\n        ring_arr = simplify(ring_arr, eps=epsilon)\n\n        # If simplified too much, adjust epsilon to maintain minimum segments\n        if len(ring_arr) &lt; min_segments:\n            # Try with smaller epsilon until we get at least min_segments points\n            for adjust_factor in [0.75, 0.5, 0.25, 0.1]:\n                test_arr = simplify(np.array(ring), eps=epsilon * adjust_factor)\n                if len(test_arr) &gt;= min_segments:\n                    ring_arr = test_arr\n                    break\n\n        # Convert to dataframe for processing\n        df = to_dataframe(ring_arr)\n\n        # Add orientation information\n        add_orientation(df, angle)\n\n        # Align segments to orthogonal directions\n        df = align(df)\n\n        # Merge collinear line segments\n        df = merge_lines(df)\n\n        if len(df) == 0:\n            return ring\n\n        # If we have a triangle-like result (3 segments or less), return the original shape\n        if len(df) &lt;= 3:\n            return ring\n\n        # Join the orthogonalized segments back into a ring\n        joined_ring = join_ring(df)\n\n        # If the join operation didn't produce a valid ring, return the original\n        if len(joined_ring) == 0 or len(joined_ring[0]) &lt; 3:\n            return ring\n\n        # Enhanced validation: check for triangular result and geometric validity\n        result_coords = joined_ring[0]\n\n        # If result has 3 or fewer points (triangle), use original\n        if len(result_coords) &lt;= 3:  # 2 points + closing point (degenerate)\n            return ring\n\n        # Additional validation: check for degenerate geometry\n        # Calculate area ratio to detect if the shape got severely distorted\n        def calculate_polygon_area(coords):\n            if len(coords) &lt; 3:\n                return 0\n            area = 0\n            n = len(coords)\n            for i in range(n):\n                j = (i + 1) % n\n                area += coords[i][0] * coords[j][1]\n                area -= coords[j][0] * coords[i][1]\n            return abs(area) / 2\n\n        original_area = calculate_polygon_area(ring)\n        result_area = calculate_polygon_area(result_coords)\n\n        # If the area changed dramatically (more than 30% shrinkage or 300% growth), use original\n        if original_area &gt; 0 and result_area &gt; 0:\n            area_ratio = result_area / original_area\n            if area_ratio &lt; 0.3 or area_ratio &gt; 3.0:\n                return ring\n\n        # Check for triangular spikes and problematic artifacts\n        very_acute_angle_count = 0\n        triangular_spike_detected = False\n\n        for i in range(len(result_coords) - 1):  # -1 to exclude closing point\n            p1 = result_coords[i - 1]\n            p2 = result_coords[i]\n            p3 = result_coords[(i + 1) % (len(result_coords) - 1)]\n\n            # Calculate angle at p2\n            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n\n            v1_norm = np.linalg.norm(v1)\n            v2_norm = np.linalg.norm(v2)\n\n            if v1_norm &gt; 0 and v2_norm &gt; 0:\n                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                cos_angle = np.clip(cos_angle, -1, 1)\n                angle = np.arccos(cos_angle)\n\n                # Count very acute angles (&lt; 20 degrees) - these are likely spikes\n                if angle &lt; np.pi / 9:  # 20 degrees\n                    very_acute_angle_count += 1\n                    # If it's very acute with short sides, it's definitely a spike\n                    if v1_norm &lt; 5 or v2_norm &lt; 5:\n                        triangular_spike_detected = True\n\n        # Check for excessively long edges that might be artifacts\n        edge_lengths = []\n        for i in range(len(result_coords) - 1):\n            edge_len = np.sqrt(\n                (result_coords[i + 1][0] - result_coords[i][0]) ** 2\n                + (result_coords[i + 1][1] - result_coords[i][1]) ** 2\n            )\n            edge_lengths.append(edge_len)\n\n        excessive_edge_detected = False\n        if len(edge_lengths) &gt; 0:\n            avg_edge_length = np.mean(edge_lengths)\n            max_edge_length = np.max(edge_lengths)\n            # Only reject if edge is extremely disproportionate (8x average)\n            if max_edge_length &gt; avg_edge_length * 8:\n                excessive_edge_detected = True\n\n        # Check for triangular artifacts by detecting spikes that extend beyond bounds\n        # Calculate original bounds\n        orig_xs = [p[0] for p in ring]\n        orig_ys = [p[1] for p in ring]\n        orig_min_x, orig_max_x = min(orig_xs), max(orig_xs)\n        orig_min_y, orig_max_y = min(orig_ys), max(orig_ys)\n        orig_width = orig_max_x - orig_min_x\n        orig_height = orig_max_y - orig_min_y\n\n        # Calculate result bounds\n        result_xs = [p[0] for p in result_coords]\n        result_ys = [p[1] for p in result_coords]\n        result_min_x, result_max_x = min(result_xs), max(result_xs)\n        result_min_y, result_max_y = min(result_ys), max(result_ys)\n\n        # Stricter bounds checking to catch triangular artifacts\n        bounds_extension_detected = False\n        # More conservative: only allow 10% extension\n        tolerance_x = max(orig_width * 0.1, 1.0)  # 10% tolerance, at least 1 unit\n        tolerance_y = max(orig_height * 0.1, 1.0)  # 10% tolerance, at least 1 unit\n\n        if (\n            result_min_x &lt; orig_min_x - tolerance_x\n            or result_max_x &gt; orig_max_x + tolerance_x\n            or result_min_y &lt; orig_min_y - tolerance_y\n            or result_max_y &gt; orig_max_y + tolerance_y\n        ):\n            bounds_extension_detected = True\n\n        # Reject if we detect triangular spikes, excessive edges, or bounds violations\n        if (\n            triangular_spike_detected\n            or very_acute_angle_count &gt; 2  # Multiple very acute angles\n            or excessive_edge_detected\n            or bounds_extension_detected\n        ):  # Any significant bounds extension\n            return ring\n\n        # Convert back to a list and ensure it's closed\n        result = joined_ring[0].tolist()\n        if len(result) &gt; 0 and (result[0] != result[-1]):\n            result.append(result[0])\n\n        return result\n\n    def vectorize_mask(mask, transform):\n        \"\"\"\n        Converts a binary mask to vector polygons.\n\n        Args:\n            mask (numpy.ndarray): Binary mask where non-zero values represent objects\n            transform (rasterio.transform.Affine): Affine transformation matrix\n\n        Returns:\n            list: List of GeoJSON features\n        \"\"\"\n        shapes = features.shapes(mask, transform=transform)\n        features_list = []\n\n        for shape, value in shapes:\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                features_list.append(\n                    {\n                        \"type\": \"Feature\",\n                        \"properties\": {\"value\": int(value)},\n                        \"geometry\": shape,\n                    }\n                )\n\n        return features_list\n\n    def rasterize_features(features, shape, transform, dtype=np.uint8):\n        \"\"\"\n        Converts vector features back to a raster mask.\n\n        Args:\n            features (list): List of GeoJSON features\n            shape (tuple): Shape of the output raster (height, width)\n            transform (rasterio.transform.Affine): Affine transformation matrix\n            dtype (numpy.dtype, optional): Data type of the output raster\n\n        Returns:\n            numpy.ndarray: Rasterized mask\n        \"\"\"\n        mask = features.rasterize(\n            [\n                (feature[\"geometry\"], feature[\"properties\"][\"value\"])\n                for feature in features\n            ],\n            out_shape=shape,\n            transform=transform,\n            fill=0,\n            dtype=dtype,\n        )\n\n        return mask\n\n    # The following helper functions are from the original code\n    def get_orientation(contour):\n        \"\"\"\n        Calculate the orientation angle of a contour.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            float: Orientation angle in radians\n        \"\"\"\n        box = cv2.minAreaRect(contour.astype(int))\n        (cx, cy), (w, h), angle = box\n        return math.radians(angle)\n\n    def simplify(contour, eps=0.2):\n        \"\"\"\n        Simplify a contour using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n            eps (float, optional): Epsilon value for simplification\n\n        Returns:\n            numpy.ndarray: Simplified contour\n        \"\"\"\n        return rdp(contour, epsilon=eps)\n\n    def to_dataframe(ring):\n        \"\"\"\n        Convert a ring to a pandas DataFrame with line segment information.\n\n        Args:\n            ring (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            pandas.DataFrame: DataFrame with line segment information\n        \"\"\"\n        df = pd.DataFrame(ring, columns=[\"x1\", \"y1\"])\n        df[\"x2\"] = df[\"x1\"].shift(-1)\n        df[\"y2\"] = df[\"y1\"].shift(-1)\n        df.dropna(inplace=True)\n        df[\"angle_atan\"] = np.arctan2((df[\"y2\"] - df[\"y1\"]), (df[\"x2\"] - df[\"x1\"]))\n        df[\"angle_atan_deg\"] = df[\"angle_atan\"] * 57.2958\n        df[\"len\"] = np.sqrt((df[\"y2\"] - df[\"y1\"]) ** 2 + (df[\"x2\"] - df[\"x1\"]) ** 2)\n        df[\"cx\"] = (df[\"x2\"] + df[\"x1\"]) / 2.0\n        df[\"cy\"] = (df[\"y2\"] + df[\"y1\"]) / 2.0\n        return df\n\n    def add_orientation(df, angle):\n        \"\"\"\n        Add orientation information to the DataFrame.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n            angle (float): Orientation angle in degrees\n\n        Returns:\n            None: Modifies the DataFrame in-place\n        \"\"\"\n        rtangle = angle + 90\n        is_parallel = (\n            (df[\"angle_atan_deg\"] &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] &lt; (angle + 45))\n        ) | (\n            (df[\"angle_atan_deg\"] + 180 &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] + 180 &lt; (angle + 45))\n        )\n        df[\"angle\"] = math.radians(angle)\n        df[\"angle\"] = df[\"angle\"].where(is_parallel, math.radians(rtangle))\n\n    def align(df):\n        \"\"\"\n        Align line segments to their nearest orthogonal direction.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n\n        Returns:\n            pandas.DataFrame: DataFrame with aligned line segments\n        \"\"\"\n        # Handle edge case with empty dataframe\n        if len(df) == 0:\n            return df.copy()\n\n        df_clone = df.copy()\n\n        # Ensure angle column exists and has valid values\n        if \"angle\" not in df_clone.columns or df_clone[\"angle\"].isna().any():\n            # If angle data is missing, add default angles based on atan2\n            df_clone[\"angle\"] = df_clone[\"angle_atan\"]\n\n        # Ensure length and center point data is valid\n        if \"len\" not in df_clone.columns or df_clone[\"len\"].isna().any():\n            # Recalculate lengths if missing\n            df_clone[\"len\"] = np.sqrt(\n                (df_clone[\"x2\"] - df_clone[\"x1\"]) ** 2\n                + (df_clone[\"y2\"] - df_clone[\"y1\"]) ** 2\n            )\n\n        if \"cx\" not in df_clone.columns or df_clone[\"cx\"].isna().any():\n            df_clone[\"cx\"] = (df_clone[\"x1\"] + df_clone[\"x2\"]) / 2.0\n\n        if \"cy\" not in df_clone.columns or df_clone[\"cy\"].isna().any():\n            df_clone[\"cy\"] = (df_clone[\"y1\"] + df_clone[\"y2\"]) / 2.0\n\n        # Apply orthogonal alignment\n        df_clone[\"x1\"] = df_clone[\"cx\"] - ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"x2\"] = df_clone[\"cx\"] + ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"y1\"] = df_clone[\"cy\"] - ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n        df_clone[\"y2\"] = df_clone[\"cy\"] + ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n\n        return df_clone\n\n    def merge_lines(df_aligned):\n        \"\"\"\n        Merge collinear line segments.\n\n        Args:\n            df_aligned (pandas.DataFrame): DataFrame with aligned line segments\n\n        Returns:\n            pandas.DataFrame: DataFrame with merged line segments\n        \"\"\"\n        ortho_lines = []\n        groups = df_aligned.groupby(\n            (df_aligned[\"angle\"].shift() != df_aligned[\"angle\"]).cumsum()\n        )\n        for x, y in groups:\n            group_cx = (y[\"cx\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            group_cy = (y[\"cy\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            cumlen = y[\"len\"].sum()\n\n            ortho_lines.append((group_cx, group_cy, cumlen, y[\"angle\"].iloc[0]))\n\n        ortho_list = []\n        for cx, cy, length, rot_angle in ortho_lines:\n            X1 = cx - (length / 2) * math.cos(rot_angle)\n            X2 = cx + (length / 2) * math.cos(rot_angle)\n            Y1 = cy - (length / 2) * math.sin(rot_angle)\n            Y2 = cy + (length / 2) * math.sin(rot_angle)\n\n            ortho_list.append(\n                {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": length,\n                    \"cx\": cx,\n                    \"cy\": cy,\n                    \"angle\": rot_angle,\n                }\n            )\n\n        # Improved fix: Prevent merging that would create triangular or problematic shapes\n        if (\n            len(ortho_list) &gt; 3 and ortho_list[0][\"angle\"] == ortho_list[-1][\"angle\"]\n        ):  # join first and last segment if they're in same direction\n            # Check if merging would result in 3 or 4 segments (potentially triangular)\n            resulting_segments = len(ortho_list) - 1\n            if resulting_segments &lt;= 4:\n                # For very small polygons, be extra cautious about merging\n                # Calculate the spatial relationship between first and last segments\n                first_center = np.array([ortho_list[0][\"cx\"], ortho_list[0][\"cy\"]])\n                last_center = np.array([ortho_list[-1][\"cx\"], ortho_list[-1][\"cy\"]])\n                center_distance = np.linalg.norm(first_center - last_center)\n\n                # Get average segment length for comparison\n                avg_length = sum(seg[\"len\"] for seg in ortho_list) / len(ortho_list)\n\n                # Only merge if segments are close enough and it won't create degenerate shapes\n                if center_distance &gt; avg_length * 1.5:\n                    # Skip merging - segments are too far apart\n                    pass\n                else:\n                    # Proceed with merging only for well-connected segments\n                    totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n                    merge_cx = (\n                        (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                        + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n                    ) / totlen\n\n                    merge_cy = (\n                        (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                        + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n                    ) / totlen\n\n                    rot_angle = ortho_list[0][\"angle\"]\n                    X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n                    X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n                    Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n                    Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n                    ortho_list[-1] = {\n                        \"x1\": X1,\n                        \"y1\": Y1,\n                        \"x2\": X2,\n                        \"y2\": Y2,\n                        \"len\": totlen,\n                        \"cx\": merge_cx,\n                        \"cy\": merge_cy,\n                        \"angle\": rot_angle,\n                    }\n                    ortho_list = ortho_list[1:]\n            else:\n                # For larger polygons, proceed with standard merging\n                totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n                merge_cx = (\n                    (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                    + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n                ) / totlen\n\n                merge_cy = (\n                    (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                    + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n                ) / totlen\n\n                rot_angle = ortho_list[0][\"angle\"]\n                X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n                X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n                Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n                Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n                ortho_list[-1] = {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": totlen,\n                    \"cx\": merge_cx,\n                    \"cy\": merge_cy,\n                    \"angle\": rot_angle,\n                }\n                ortho_list = ortho_list[1:]\n        ortho_df = pd.DataFrame(ortho_list)\n        return ortho_df\n\n    def find_intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n        \"\"\"\n        Find the intersection point of two line segments.\n\n        Args:\n            x1, y1, x2, y2: Coordinates of the first line segment\n            x3, y3, x4, y4: Coordinates of the second line segment\n\n        Returns:\n            list: [x, y] coordinates of the intersection point\n\n        Raises:\n            ZeroDivisionError: If the lines are parallel or collinear\n        \"\"\"\n        # Calculate the denominator of the intersection formula\n        denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n\n        # Check if lines are parallel or collinear (denominator close to zero)\n        if abs(denominator) &lt; 1e-10:\n            raise ZeroDivisionError(\"Lines are parallel or collinear\")\n\n        px = (\n            (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n        py = (\n            (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n\n        # Check if the intersection point is within a reasonable distance\n        # from both line segments to avoid extreme extrapolation\n        def point_on_segment(x, y, x1, y1, x2, y2, tolerance=2.0):\n            # Check if point (x,y) is near the line segment from (x1,y1) to (x2,y2)\n            # First check if it's near the infinite line\n            line_len = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n            if line_len &lt; 1e-10:\n                return np.sqrt((x - x1) ** 2 + (y - y1) ** 2) &lt;= tolerance\n\n            t = ((x - x1) * (x2 - x1) + (y - y1) * (y2 - y1)) / (line_len**2)\n\n            # Check distance to the infinite line\n            proj_x = x1 + t * (x2 - x1)\n            proj_y = y1 + t * (y2 - y1)\n            dist_to_line = np.sqrt((x - proj_x) ** 2 + (y - proj_y) ** 2)\n\n            # Check if the projection is near the segment, not just the infinite line\n            if t &lt; -tolerance or t &gt; 1 + tolerance:\n                # If far from the segment, compute distance to the nearest endpoint\n                dist_to_start = np.sqrt((x - x1) ** 2 + (y - y1) ** 2)\n                dist_to_end = np.sqrt((x - x2) ** 2 + (y - y2) ** 2)\n                return min(dist_to_start, dist_to_end) &lt;= tolerance * 2\n\n            return dist_to_line &lt;= tolerance\n\n        # Check if intersection is reasonably close to both line segments\n        if not (\n            point_on_segment(px, py, x1, y1, x2, y2)\n            and point_on_segment(px, py, x3, y3, x4, y4)\n        ):\n            # If intersection is far from segments, it's probably extrapolating too much\n            raise ValueError(\"Intersection point too far from line segments\")\n\n        return [px, py]\n\n    def join_ring(merged_df):\n        \"\"\"\n        Join line segments to form a closed ring.\n\n        Args:\n            merged_df (pandas.DataFrame): DataFrame with merged line segments\n\n        Returns:\n            numpy.ndarray: Array of shape (1, n, 2) containing the ring coordinates\n        \"\"\"\n        # Handle edge cases\n        if len(merged_df) &lt; 3:\n            # Not enough segments to form a valid polygon\n            return np.array([[]])\n\n        ring = []\n\n        # Find intersections between adjacent line segments\n        for i in range(len(merged_df) - 1):\n            x1, y1, x2, y2, *_ = merged_df.iloc[i]\n            x3, y3, x4, y4, *_ = merged_df.iloc[i + 1]\n\n            try:\n                intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n                # Check if the intersection point is too far from either line segment\n                # This helps prevent extending edges beyond reasonable bounds\n                dist_to_seg1 = min(\n                    np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                    np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n                )\n                dist_to_seg2 = min(\n                    np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                    np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n                )\n\n                # Use the maximum of line segment lengths as a reference\n                max_len = max(\n                    np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                    np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n                )\n\n                # Improved intersection validation\n                # Calculate angle between segments to detect sharp corners\n                v1 = np.array([x2 - x1, y2 - y1])\n                v2 = np.array([x4 - x3, y4 - y3])\n                v1_norm = np.linalg.norm(v1)\n                v2_norm = np.linalg.norm(v2)\n\n                if v1_norm &gt; 0 and v2_norm &gt; 0:\n                    cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                    cos_angle = np.clip(cos_angle, -1, 1)\n                    angle = np.arccos(cos_angle)\n\n                    # Check for very sharp angles that could create triangular artifacts\n                    is_sharp_angle = (\n                        angle &lt; np.pi / 6 or angle &gt; 5 * np.pi / 6\n                    )  # &lt;30\u00b0 or &gt;150\u00b0\n                else:\n                    is_sharp_angle = False\n\n                # Determine whether to use intersection or segment endpoint\n                if (\n                    dist_to_seg1 &gt; max_len * 0.5\n                    or dist_to_seg2 &gt; max_len * 0.5\n                    or is_sharp_angle\n                ):\n                    # Use a more conservative approach for problematic intersections\n                    # Use the closer endpoint between segments\n                    dist_x2_to_seg2 = min(\n                        np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2),\n                        np.sqrt((x2 - x4) ** 2 + (y2 - y4) ** 2),\n                    )\n                    dist_x3_to_seg1 = min(\n                        np.sqrt((x3 - x1) ** 2 + (y3 - y1) ** 2),\n                        np.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2),\n                    )\n\n                    if dist_x2_to_seg2 &lt;= dist_x3_to_seg1:\n                        ring.append([x2, y2])\n                    else:\n                        ring.append([x3, y3])\n                else:\n                    ring.append(intersection)\n            except Exception:\n                # If intersection calculation fails, use the endpoint of the first segment\n                ring.append([x2, y2])\n\n        # Connect last segment with first segment\n        x1, y1, x2, y2, *_ = merged_df.iloc[-1]\n        x3, y3, x4, y4, *_ = merged_df.iloc[0]\n\n        try:\n            intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n            # Check if the intersection point is too far from either line segment\n            dist_to_seg1 = min(\n                np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n            )\n            dist_to_seg2 = min(\n                np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n            )\n\n            max_len = max(\n                np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n            )\n\n            # Apply same sharp angle detection for closing segment\n            v1 = np.array([x2 - x1, y2 - y1])\n            v2 = np.array([x4 - x3, y4 - y3])\n            v1_norm = np.linalg.norm(v1)\n            v2_norm = np.linalg.norm(v2)\n\n            if v1_norm &gt; 0 and v2_norm &gt; 0:\n                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                cos_angle = np.clip(cos_angle, -1, 1)\n                angle = np.arccos(cos_angle)\n                is_sharp_angle = angle &lt; np.pi / 6 or angle &gt; 5 * np.pi / 6\n            else:\n                is_sharp_angle = False\n\n            if (\n                dist_to_seg1 &gt; max_len * 0.5\n                or dist_to_seg2 &gt; max_len * 0.5\n                or is_sharp_angle\n            ):\n                # Use conservative approach for closing segment\n                dist_x2_to_seg2 = min(\n                    np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2),\n                    np.sqrt((x2 - x4) ** 2 + (y2 - y4) ** 2),\n                )\n                dist_x3_to_seg1 = min(\n                    np.sqrt((x3 - x1) ** 2 + (y3 - y1) ** 2),\n                    np.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2),\n                )\n\n                if dist_x2_to_seg2 &lt;= dist_x3_to_seg1:\n                    ring.append([x2, y2])\n                else:\n                    ring.append([x3, y3])\n            else:\n                ring.append(intersection)\n        except Exception:\n            # If intersection calculation fails, use the endpoint of the last segment\n            ring.append([x2, y2])\n\n        # Ensure the ring is closed\n        if len(ring) &gt; 0 and (ring[0][0] != ring[-1][0] or ring[0][1] != ring[-1][1]):\n            ring.append(ring[0])\n\n        return np.array([ring])\n\n    def rdp(M, epsilon=0, dist=None, algo=\"iter\", return_mask=False):\n        \"\"\"\n        Simplifies a given array of points using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float, optional): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            algo (str, optional): Algorithm to use ('iter' or 'rec')\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray or list: Simplified points or mask\n        \"\"\"\n        if dist is None:\n            dist = pldist\n\n        if algo == \"iter\":\n            algo = partial(rdp_iter, return_mask=return_mask)\n        elif algo == \"rec\":\n            if return_mask:\n                raise NotImplementedError(\n                    'return_mask=True not supported with algo=\"rec\"'\n                )\n            algo = rdp_rec\n\n        if \"numpy\" in str(type(M)):\n            return algo(M, epsilon, dist)\n\n        return algo(np.array(M), epsilon, dist).tolist()\n\n    def pldist(point, start, end):\n        \"\"\"\n        Calculates the distance from 'point' to the line given by 'start' and 'end'.\n\n        Args:\n            point (numpy.ndarray): Point coordinates\n            start (numpy.ndarray): Start point of the line\n            end (numpy.ndarray): End point of the line\n\n        Returns:\n            float: Distance from point to line\n        \"\"\"\n        if np.all(np.equal(start, end)):\n            return np.linalg.norm(point - start)\n\n        # Fix for NumPy 2.0 deprecation warning - handle 2D vectors properly\n        # Instead of using cross product directly, calculate the area of the\n        # parallelogram formed by the vectors and divide by the length of the line\n        line_vec = end - start\n        point_vec = point - start\n\n        # Area of parallelogram = |a|*|b|*sin(\u03b8)\n        # For 2D vectors: |a\u00d7b| = |a|*|b|*sin(\u03b8) = determinant([ax, ay], [bx, by])\n        area = abs(line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0])\n\n        # Distance = Area / |line_vec|\n        return area / np.linalg.norm(line_vec)\n\n    def rdp_rec(M, epsilon, dist=pldist):\n        \"\"\"\n        Recursive implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Simplified points\n        \"\"\"\n        dmax = 0.0\n        index = -1\n\n        for i in range(1, M.shape[0]):\n            d = dist(M[i], M[0], M[-1])\n\n            if d &gt; dmax:\n                index = i\n                dmax = d\n\n        if dmax &gt; epsilon:\n            r1 = rdp_rec(M[: index + 1], epsilon, dist)\n            r2 = rdp_rec(M[index:], epsilon, dist)\n\n            return np.vstack((r1[:-1], r2))\n        else:\n            return np.vstack((M[0], M[-1]))\n\n    def _rdp_iter(M, start_index, last_index, epsilon, dist=pldist):\n        \"\"\"\n        Internal iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            start_index (int): Start index\n            last_index (int): Last index\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Boolean mask of points to keep\n        \"\"\"\n        stk = []\n        stk.append([start_index, last_index])\n        global_start_index = start_index\n        indices = np.ones(last_index - start_index + 1, dtype=bool)\n\n        while stk:\n            start_index, last_index = stk.pop()\n\n            dmax = 0.0\n            index = start_index\n\n            for i in range(index + 1, last_index):\n                if indices[i - global_start_index]:\n                    d = dist(M[i], M[start_index], M[last_index])\n                    if d &gt; dmax:\n                        index = i\n                        dmax = d\n\n            if dmax &gt; epsilon:\n                stk.append([start_index, index])\n                stk.append([index, last_index])\n            else:\n                for i in range(start_index + 1, last_index):\n                    indices[i - global_start_index] = False\n\n        return indices\n\n    def rdp_iter(M, epsilon, dist=pldist, return_mask=False):\n        \"\"\"\n        Iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray: Simplified points or boolean mask\n        \"\"\"\n        mask = _rdp_iter(M, 0, len(M) - 1, epsilon, dist)\n\n        if return_mask:\n            return mask\n\n        return M[mask]\n\n    # Read the raster data\n    with rasterio.open(input_path) as src:\n        # Read the first band (assuming it contains the mask)\n        mask = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Extract shapes from the raster mask\n        shapes = list(features.shapes(mask, transform=transform))\n\n        # Initialize progress bar\n        print(f\"Processing {len(shapes)} features...\")\n\n        # Convert shapes to GeoJSON features\n        features_list = []\n        for shape, value in tqdm(shapes, desc=\"Converting features\", unit=\"shape\"):\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                # Convert GeoJSON geometry to Shapely polygon\n                polygon = Polygon(shape[\"coordinates\"][0])\n\n                # Skip tiny polygons\n                if polygon.area &lt; min_area:\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n                    continue\n\n                # Check if shape is triangular and if we want to avoid triangular shapes\n                if detect_triangles:\n                    # Create a simplified version to check number of vertices\n                    simple_polygon = polygon.simplify(epsilon)\n                    if (\n                        len(simple_polygon.exterior.coords) &lt;= 4\n                    ):  # 3 points + closing point\n                        # Likely a triangular shape - skip orthogonalization\n                        features_list.append(\n                            {\n                                \"type\": \"Feature\",\n                                \"properties\": {\"value\": int(value)},\n                                \"geometry\": shape,\n                            }\n                        )\n                        continue\n\n                # Process larger, non-triangular polygons\n                try:\n                    # Convert shapely polygon to a ring format for orthogonalization\n                    exterior_ring = list(polygon.exterior.coords)\n                    interior_rings = [\n                        list(interior.coords) for interior in polygon.interiors\n                    ]\n\n                    # Calculate bounding box aspect ratio to help with parameter tuning\n                    minx, miny, maxx, maxy = polygon.bounds\n                    width = maxx - minx\n                    height = maxy - miny\n                    aspect_ratio = max(width, height) / max(1.0, min(width, height))\n\n                    # Determine if this shape is likely to be a building/rectangular object\n                    # Long thin objects might require different treatment\n                    is_rectangular = aspect_ratio &lt; 3.0\n\n                    # Rectangular objects usually need more careful orthogonalization\n                    epsilon_adjusted = epsilon\n                    min_segments_adjusted = min_segments\n\n                    if is_rectangular:\n                        # For rectangular objects, use more conservative epsilon\n                        epsilon_adjusted = epsilon * 0.75\n                        # Ensure we get at least 4 points for a proper rectangle\n                        min_segments_adjusted = max(4, min_segments)\n\n                    # Orthogonalize the exterior and interior rings\n                    orthogonalized_exterior = orthogonalize_ring(\n                        exterior_ring,\n                        epsilon=epsilon_adjusted,\n                        min_segments=min_segments_adjusted,\n                    )\n\n                    orthogonalized_interiors = [\n                        orthogonalize_ring(\n                            ring,\n                            epsilon=epsilon_adjusted,\n                            min_segments=min_segments_adjusted,\n                        )\n                        for ring in interior_rings\n                    ]\n\n                    # Validate the result - calculate area change\n                    original_area = polygon.area\n                    orthogonalized_poly = Polygon(orthogonalized_exterior)\n\n                    if orthogonalized_poly.is_valid:\n                        area_ratio = (\n                            orthogonalized_poly.area / original_area\n                            if original_area &gt; 0\n                            else 0\n                        )\n\n                        # If area changed too much, revert to original\n                        if area_ratio &lt; area_tolerance or area_ratio &gt; (\n                            1.0 / area_tolerance\n                        ):\n                            # Use original polygon instead\n                            geometry = shape\n                        else:\n                            # Create a new geometry with orthogonalized rings\n                            geometry = {\n                                \"type\": \"Polygon\",\n                                \"coordinates\": [orthogonalized_exterior],\n                            }\n\n                            # Add interior rings if they exist\n                            if orthogonalized_interiors:\n                                geometry[\"coordinates\"].extend(\n                                    [ring for ring in orthogonalized_interiors]\n                                )\n                    else:\n                        # If resulting polygon is invalid, use original\n                        geometry = shape\n\n                    # Add the feature to the list\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": geometry,\n                        }\n                    )\n                except Exception as e:\n                    # Keep the original shape if orthogonalization fails\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n\n        # Create the final GeoJSON structure\n        geojson = {\n            \"type\": \"FeatureCollection\",\n            \"crs\": {\"type\": \"name\", \"properties\": {\"name\": str(crs)}},\n            \"features\": features_list,\n        }\n\n        # Convert to GeoDataFrame and set the CRS\n        gdf = gpd.GeoDataFrame.from_features(geojson[\"features\"], crs=crs)\n\n        # Save to file if output_path is provided\n        if output_path:\n            print(f\"Saving to {output_path}...\")\n            gdf.to_file(output_path)\n            print(\"Done!\")\n\n        return gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n\n    try:\n        from torchgeo.datasets import unbind_samples\n    except ImportError as e:\n        raise ImportError(\n            \"Your torchgeo version is too old. Please upgrade to the latest version using 'pip install -U torchgeo'.\"\n        )\n\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"geoai/#geoai.geoai.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.plot_performance_metrics","title":"<code>plot_performance_metrics(history_path, figsize=(15, 5), verbose=True, save_path=None, kwargs=None)</code>","text":"<p>Plot performance metrics from a history object.</p> <p>Parameters:</p> Name Type Description Default <code>history_path</code> <code>str</code> <p>The history object to plot.</p> required <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size.</p> <code>(15, 5)</code> <code>verbose</code> <code>bool</code> <p>Whether to print the best and final metrics.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def plot_performance_metrics(\n    history_path: str,\n    figsize: Tuple[int, int] = (15, 5),\n    verbose: bool = True,\n    save_path: Optional[str] = None,\n    kwargs: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"Plot performance metrics from a history object.\n\n    Args:\n        history_path: The history object to plot.\n        figsize: The figure size.\n        verbose: Whether to print the best and final metrics.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    history = torch.load(history_path)\n\n    # Handle different key naming conventions\n    train_loss_key = \"train_losses\" if \"train_losses\" in history else \"train_loss\"\n    val_loss_key = \"val_losses\" if \"val_losses\" in history else \"val_loss\"\n    val_iou_key = \"val_ious\" if \"val_ious\" in history else \"val_iou\"\n    val_dice_key = \"val_dices\" if \"val_dices\" in history else \"val_dice\"\n\n    # Determine number of subplots based on available metrics\n    has_dice = val_dice_key in history\n    n_plots = 3 if has_dice else 2\n    figsize = (15, 5) if has_dice else (10, 5)\n\n    plt.figure(figsize=figsize)\n\n    # Plot loss\n    plt.subplot(1, n_plots, 1)\n    if train_loss_key in history:\n        plt.plot(history[train_loss_key], label=\"Train Loss\")\n    if val_loss_key in history:\n        plt.plot(history[val_loss_key], label=\"Val Loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot IoU\n    plt.subplot(1, n_plots, 2)\n    if val_iou_key in history:\n        plt.plot(history[val_iou_key], label=\"Val IoU\")\n    plt.title(\"IoU Score\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"IoU\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Dice if available\n    if has_dice:\n        plt.subplot(1, n_plots, 3)\n        plt.plot(history[val_dice_key], label=\"Val Dice\")\n        plt.title(\"Dice Score\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dice\")\n        plt.legend()\n        plt.grid(True)\n\n    plt.tight_layout()\n\n    if save_path:\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 150\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n        plt.savefig(save_path, **kwargs)\n\n    plt.show()\n\n    if verbose:\n        if val_iou_key in history:\n            print(f\"Best IoU: {max(history[val_iou_key]):.4f}\")\n            print(f\"Final IoU: {history[val_iou_key][-1]:.4f}\")\n        if val_dice_key in history:\n            print(f\"Best Dice: {max(history[val_dice_key]):.4f}\")\n            print(f\"Final Dice: {history[val_dice_key][-1]:.4f}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.plot_prediction_comparison","title":"<code>plot_prediction_comparison(original_image, prediction_image, ground_truth_image=None, titles=None, figsize=(15, 5), save_path=None, show_plot=True, prediction_colormap='gray', ground_truth_colormap='gray', original_colormap=None, indexes=None, divider=None)</code>","text":"<p>Plot original image, prediction, and optional ground truth side by side.</p> <p>Supports input as file paths, NumPy arrays, or PIL Images. For multi-band images, selected channels can be specified via <code>indexes</code>. If the image data is not normalized (e.g., Sentinel-2 [0, 10000]), the <code>divider</code> can be used to scale values for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>original_image</code> <code>Union[str, ndarray, Image]</code> <p>Original input image as a file path, NumPy array, or PIL Image.</p> required <code>prediction_image</code> <code>Union[str, ndarray, Image]</code> <p>Predicted segmentation mask image.</p> required <code>ground_truth_image</code> <code>Optional[Union[str, ndarray, Image]]</code> <p>Ground truth mask image. Defaults to None.</p> <code>None</code> <code>titles</code> <code>Optional[List[str]]</code> <p>List of titles for the subplots. If not provided, default titles are used.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Size of the entire figure in inches. Defaults to (15, 5).</p> <code>(15, 5)</code> <code>save_path</code> <code>Optional[str]</code> <p>If specified, saves the figure to this path. Defaults to None.</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Whether to display the figure using plt.show(). Defaults to True.</p> <code>True</code> <code>prediction_colormap</code> <code>str</code> <p>Colormap to use for the prediction mask. Defaults to \"gray\".</p> <code>'gray'</code> <code>ground_truth_colormap</code> <code>str</code> <p>Colormap to use for the ground truth mask. Defaults to \"gray\".</p> <code>'gray'</code> <code>original_colormap</code> <code>Optional[str]</code> <p>Colormap to use for the original image if it's grayscale. Defaults to None.</p> <code>None</code> <code>indexes</code> <code>Optional[List[int]]</code> <p>List of band/channel indexes (0-based for NumPy, 1-based for rasterio) to extract from the original image. Useful for multi-band imagery like Sentinel-2. Defaults to None.</p> <code>None</code> <code>divider</code> <code>Optional[float]</code> <p>Value to divide the original image by for normalization (e.g., 10000 for reflectance). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>matplotlib.figure.Figure: The generated matplotlib figure object.</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_prediction_comparison(\n    original_image: Union[str, np.ndarray, Image.Image],\n    prediction_image: Union[str, np.ndarray, Image.Image],\n    ground_truth_image: Optional[Union[str, np.ndarray, Image.Image]] = None,\n    titles: Optional[List[str]] = None,\n    figsize: Tuple[int, int] = (15, 5),\n    save_path: Optional[str] = None,\n    show_plot: bool = True,\n    prediction_colormap: str = \"gray\",\n    ground_truth_colormap: str = \"gray\",\n    original_colormap: Optional[str] = None,\n    indexes: Optional[List[int]] = None,\n    divider: Optional[float] = None,\n) -&gt; None:\n    \"\"\"Plot original image, prediction, and optional ground truth side by side.\n\n    Supports input as file paths, NumPy arrays, or PIL Images. For multi-band\n    images, selected channels can be specified via `indexes`. If the image data\n    is not normalized (e.g., Sentinel-2 [0, 10000]), the `divider` can be used\n    to scale values for visualization.\n\n    Args:\n        original_image (Union[str, np.ndarray, Image.Image]):\n            Original input image as a file path, NumPy array, or PIL Image.\n        prediction_image (Union[str, np.ndarray, Image.Image]):\n            Predicted segmentation mask image.\n        ground_truth_image (Optional[Union[str, np.ndarray, Image.Image]], optional):\n            Ground truth mask image. Defaults to None.\n        titles (Optional[List[str]], optional):\n            List of titles for the subplots. If not provided, default titles are used.\n        figsize (Tuple[int, int], optional):\n            Size of the entire figure in inches. Defaults to (15, 5).\n        save_path (Optional[str], optional):\n            If specified, saves the figure to this path. Defaults to None.\n        show_plot (bool, optional):\n            Whether to display the figure using plt.show(). Defaults to True.\n        prediction_colormap (str, optional):\n            Colormap to use for the prediction mask. Defaults to \"gray\".\n        ground_truth_colormap (str, optional):\n            Colormap to use for the ground truth mask. Defaults to \"gray\".\n        original_colormap (Optional[str], optional):\n            Colormap to use for the original image if it's grayscale. Defaults to None.\n        indexes (Optional[List[int]], optional):\n            List of band/channel indexes (0-based for NumPy, 1-based for rasterio) to extract from the original image.\n            Useful for multi-band imagery like Sentinel-2. Defaults to None.\n        divider (Optional[float], optional):\n            Value to divide the original image by for normalization (e.g., 10000 for reflectance). Defaults to None.\n\n    Returns:\n        matplotlib.figure.Figure:\n            The generated matplotlib figure object.\n    \"\"\"\n\n    def _load_image(img_input, indexes=None):\n        \"\"\"Helper function to load image from various input types.\"\"\"\n        if isinstance(img_input, str):\n            if img_input.lower().endswith((\".tif\", \".tiff\")):\n                with rasterio.open(img_input) as src:\n                    if indexes:\n                        img = src.read(indexes)  # 1-based\n                        img = (\n                            np.transpose(img, (1, 2, 0)) if len(indexes) &gt; 1 else img[0]\n                        )\n                    else:\n                        img = src.read()\n                        if img.shape[0] == 1:\n                            img = img[0]\n                        else:\n                            img = np.transpose(img, (1, 2, 0))\n            else:\n                img = np.array(Image.open(img_input))\n        elif isinstance(img_input, Image.Image):\n            img = np.array(img_input)\n        elif isinstance(img_input, np.ndarray):\n            img = img_input\n            if indexes is not None and img.ndim == 3:\n                img = img[:, :, indexes]\n        else:\n            raise ValueError(f\"Unsupported image type: {type(img_input)}\")\n        return img\n\n    # Load images\n    original = _load_image(original_image, indexes=indexes)\n    prediction = _load_image(prediction_image)\n    ground_truth = (\n        _load_image(ground_truth_image) if ground_truth_image is not None else None\n    )\n\n    # Apply divider normalization if requested\n    if divider is not None and isinstance(original, np.ndarray) and original.ndim == 3:\n        original = np.clip(original.astype(np.float32) / divider, 0, 1)\n\n    # Determine layout\n    num_plots = 3 if ground_truth is not None else 2\n    fig, axes = plt.subplots(1, num_plots, figsize=figsize)\n    if num_plots == 2:\n        axes = [axes[0], axes[1]]\n\n    if titles is None:\n        titles = [\"Original Image\", \"Prediction\"]\n        if ground_truth is not None:\n            titles.append(\"Ground Truth\")\n\n    # Plot original\n    if original.ndim == 3 and original.shape[2] in [3, 4]:\n        axes[0].imshow(original)\n    else:\n        axes[0].imshow(original, cmap=original_colormap)\n    axes[0].set_title(titles[0])\n    axes[0].axis(\"off\")\n\n    # Prediction\n    axes[1].imshow(prediction, cmap=prediction_colormap)\n    axes[1].set_title(titles[1])\n    axes[1].axis(\"off\")\n\n    # Ground truth\n    if ground_truth is not None:\n        axes[2].imshow(ground_truth, cmap=ground_truth_colormap)\n        axes[2].set_title(titles[2])\n        axes[2].axis(\"off\")\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Plot saved to: {save_path}\")\n\n    if show_plot:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"geoai/#geoai.geoai.print_raster_info","title":"<code>print_raster_info(raster_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a raster dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the raster. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing raster information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_raster_info(\n    raster_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a raster dataset and optionally show a preview.\n\n    Args:\n        raster_path (str): Path to the raster file\n        show_preview (bool, optional): Whether to display a visual preview of the raster.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing raster information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_raster_info(raster_path)\n\n        # Print basic information\n        print(f\"===== RASTER INFORMATION: {raster_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Dimensions: {info['width']} x {info['height']} pixels\")\n        print(f\"Number of bands: {info['count']}\")\n        print(f\"Data type: {info['dtype']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Georeferenced Bounds: {info['bounds']}\")\n        print(f\"Pixel Resolution: {info['resolution'][0]}, {info['resolution'][1]}\")\n        print(f\"NoData Value: {info['nodata']}\")\n\n        # Print band statistics\n        print(\"\\n----- Band Statistics -----\")\n        for band_stat in info[\"band_stats\"]:\n            print(f\"Band {band_stat['band']}:\")\n            print(f\"  Min: {band_stat['min']:.2f}\")\n            print(f\"  Max: {band_stat['max']:.2f}\")\n            print(f\"  Mean: {band_stat['mean']:.2f}\")\n            print(f\"  Std Dev: {band_stat['std']:.2f}\")\n\n        # Show a preview if requested\n        if show_preview:\n            with rasterio.open(raster_path) as src:\n                # For multi-band images, show RGB composite or first band\n                if src.count &gt;= 3:\n                    # Try to show RGB composite\n                    rgb = np.dstack([src.read(i) for i in range(1, 4)])\n                    plt.figure(figsize=figsize)\n                    plt.imshow(rgb)\n                    plt.title(f\"RGB Preview: {raster_path}\")\n                else:\n                    # Show first band for single-band images\n                    plt.figure(figsize=figsize)\n                    show(\n                        src.read(1),\n                        cmap=\"viridis\",\n                        title=f\"Band 1 Preview: {raster_path}\",\n                    )\n                    plt.colorbar(label=\"Pixel Value\")\n                plt.show()\n\n    except Exception as e:\n        print(f\"Error reading raster: {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.print_vector_info","title":"<code>print_vector_info(vector_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a vector dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the vector data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing vector information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_vector_info(\n    vector_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a vector dataset and optionally show a preview.\n\n    Args:\n        vector_path (str): Path to the vector file\n        show_preview (bool, optional): Whether to display a visual preview of the vector data.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing vector information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_vector_info(vector_path)\n\n        # Print basic information\n        print(f\"===== VECTOR INFORMATION: {vector_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Feature count: {info['feature_count']}\")\n        print(f\"Geometry types: {info['geometry_type']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Bounds: {info['bounds']}\")\n        print(f\"Number of attributes: {info['attribute_count']}\")\n        print(f\"Attribute names: {', '.join(info['attribute_names'])}\")\n\n        # Print attribute statistics\n        if info[\"attribute_stats\"]:\n            print(\"\\n----- Attribute Statistics -----\")\n            for attr, stats in info[\"attribute_stats\"].items():\n                print(f\"Attribute: {attr}\")\n                for stat_name, stat_value in stats.items():\n                    print(\n                        f\"  {stat_name}: {stat_value:.4f}\"\n                        if isinstance(stat_value, float)\n                        else f\"  {stat_name}: {stat_value}\"\n                    )\n\n        # Show a preview if requested\n        if show_preview:\n            gdf = (\n                gpd.read_parquet(vector_path)\n                if vector_path.endswith(\".parquet\")\n                else gpd.read_file(vector_path)\n            )\n            fig, ax = plt.subplots(figsize=figsize)\n            gdf.plot(ax=ax, cmap=\"viridis\")\n            ax.set_title(f\"Preview: {vector_path}\")\n            plt.tight_layout()\n            plt.show()\n\n            # # Show a sample of the attribute table\n            # if not gdf.empty:\n            #     print(\"\\n----- Sample of attribute table (first 5 rows) -----\")\n            #     print(gdf.head().to_string())\n\n    except Exception as e:\n        print(f\"Error reading vector data: {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.raster_to_vector","title":"<code>raster_to_vector(raster_path, output_path=None, threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', unique_attribute_value=False, output_format='geojson', plot_result=False)</code>","text":"<p>Convert a raster label mask to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file (e.g., GeoTIFF).</p> required <code>output_path</code> <code>str</code> <p>Path to save the output vector file. If None, returns GeoDataFrame without saving.</p> <code>None</code> <code>threshold</code> <code>int / float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>unique_attribute_value</code> <code>bool</code> <p>Whether to generate unique values for each shape within a class.</p> <code>False</code> <code>output_format</code> <code>str</code> <p>Format for output file - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting polygons overlaid on the raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector(\n    raster_path: str,\n    output_path: Optional[str] = None,\n    threshold: float = 0,\n    min_area: float = 10,\n    simplify_tolerance: Optional[float] = None,\n    class_values: Optional[List[int]] = None,\n    attribute_name: str = \"class\",\n    unique_attribute_value: bool = False,\n    output_format: str = \"geojson\",\n    plot_result: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a raster label mask to vector polygons.\n\n    Args:\n        raster_path (str): Path to the input raster file (e.g., GeoTIFF).\n        output_path (str): Path to save the output vector file. If None, returns GeoDataFrame without saving.\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        unique_attribute_value (bool): Whether to generate unique values for each shape within a class.\n        output_format (str): Format for output file - 'geojson', 'shapefile', 'gpkg'.\n        plot_result (bool): Whether to plot the resulting polygons overlaid on the raster.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.\n    \"\"\"\n    # Open the raster file\n    with rasterio.open(raster_path) as src:\n        # Read the data\n        data = src.read(1)\n\n        # Get metadata\n        transform = src.transform\n        crs = src.crs\n\n        # Create mask based on threshold and class values\n        if class_values is not None:\n            # Create a mask for each specified class value\n            masks = {val: (data == val) for val in class_values}\n        else:\n            # Create a mask for values above threshold\n            masks = {1: (data &gt; threshold)}\n            class_values = [1]  # Default class\n\n        # Initialize list to store features\n        all_features = []\n\n        # Process each class value\n        for class_val in class_values:\n            mask = masks[class_val]\n            shape_count = 1\n            # Vectorize the mask\n            for geom, value in features.shapes(\n                mask.astype(np.uint8), mask=mask, transform=transform\n            ):\n                # Convert to shapely geometry\n                geom = shape(geom)\n\n                # Skip small polygons\n                if geom.area &lt; min_area:\n                    continue\n\n                # Simplify geometry if requested\n                if simplify_tolerance is not None:\n                    geom = geom.simplify(simplify_tolerance)\n\n                # Add to features list with class value\n                if unique_attribute_value:\n                    all_features.append(\n                        {\"geometry\": geom, attribute_name: class_val * shape_count}\n                    )\n                else:\n                    all_features.append({\"geometry\": geom, attribute_name: class_val})\n\n                shape_count += 1\n\n        # Create GeoDataFrame\n        if all_features:\n            gdf = gpd.GeoDataFrame(all_features, crs=crs)\n        else:\n            print(\"Warning: No features were extracted from the raster.\")\n            # Return empty GeoDataFrame with correct CRS\n            gdf = gpd.GeoDataFrame([], geometry=[], crs=crs)\n\n        # Save to file if requested\n        if output_path is not None:\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n            # Save to file based on format\n            if output_format.lower() == \"geojson\":\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            elif output_format.lower() == \"shapefile\":\n                gdf.to_file(output_path)\n            elif output_format.lower() == \"gpkg\":\n                gdf.to_file(output_path, driver=\"GPKG\")\n            else:\n                raise ValueError(f\"Unsupported output format: {output_format}\")\n\n            print(f\"Vectorized data saved to {output_path}\")\n\n        # Plot result if requested\n        if plot_result:\n            fig, ax = plt.subplots(figsize=(12, 12))\n\n            # Plot raster\n            raster_img = src.read()\n            if raster_img.shape[0] == 1:\n                plt.imshow(raster_img[0], cmap=\"viridis\", alpha=0.7)\n            else:\n                # Use first 3 bands for RGB display\n                rgb = raster_img[:3].transpose(1, 2, 0)\n                # Normalize for display\n                rgb = np.clip(rgb / rgb.max(), 0, 1)\n                plt.imshow(rgb)\n\n            # Plot vector boundaries\n            if not gdf.empty:\n                gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n\n            plt.title(\"Raster with Vectorized Boundaries\")\n            plt.axis(\"off\")\n            plt.tight_layout()\n            plt.show()\n\n        return gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.raster_to_vector_batch","title":"<code>raster_to_vector_batch(input_dir, output_dir, pattern='*.tif', threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', output_format='geojson', merge_output=False, merge_filename='merged_vectors')</code>","text":"<p>Batch convert multiple raster files to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input raster files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output vector files.</p> required <code>pattern</code> <code>str</code> <p>Pattern to match raster files (e.g., '*.tif').</p> <code>'*.tif'</code> <code>threshold</code> <code>int / float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>output_format</code> <code>str</code> <p>Format for output files - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>merge_output</code> <code>bool</code> <p>Whether to merge all output vectors into a single file.</p> <code>False</code> <code>merge_filename</code> <code>str</code> <p>Filename for the merged output (without extension).</p> <code>'merged_vectors'</code> <p>Returns:</p> Type Description <code>Optional[GeoDataFrame]</code> <p>geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector_batch(\n    input_dir: str,\n    output_dir: str,\n    pattern: str = \"*.tif\",\n    threshold: float = 0,\n    min_area: float = 10,\n    simplify_tolerance: Optional[float] = None,\n    class_values: Optional[List[int]] = None,\n    attribute_name: str = \"class\",\n    output_format: str = \"geojson\",\n    merge_output: bool = False,\n    merge_filename: str = \"merged_vectors\",\n) -&gt; Optional[gpd.GeoDataFrame]:\n    \"\"\"\n    Batch convert multiple raster files to vector polygons.\n\n    Args:\n        input_dir (str): Directory containing input raster files.\n        output_dir (str): Directory to save output vector files.\n        pattern (str): Pattern to match raster files (e.g., '*.tif').\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        output_format (str): Format for output files - 'geojson', 'shapefile', 'gpkg'.\n        merge_output (bool): Whether to merge all output vectors into a single file.\n        merge_filename (str): Filename for the merged output (without extension).\n\n    Returns:\n        geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.\n    \"\"\"\n    import glob\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get list of raster files\n    raster_files = glob.glob(os.path.join(input_dir, pattern))\n\n    if not raster_files:\n        print(f\"No files matching pattern '{pattern}' found in {input_dir}\")\n        return None\n\n    print(f\"Found {len(raster_files)} raster files to process\")\n\n    # Process each raster file\n    gdfs = []\n    for raster_file in tqdm(raster_files, desc=\"Processing rasters\"):\n        # Get output filename\n        base_name = os.path.splitext(os.path.basename(raster_file))[0]\n        if output_format.lower() == \"geojson\":\n            out_file = os.path.join(output_dir, f\"{base_name}.geojson\")\n        elif output_format.lower() == \"shapefile\":\n            out_file = os.path.join(output_dir, f\"{base_name}.shp\")\n        elif output_format.lower() == \"gpkg\":\n            out_file = os.path.join(output_dir, f\"{base_name}.gpkg\")\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        # Convert raster to vector\n        if merge_output:\n            # Don't save individual files if merging\n            gdf = raster_to_vector(\n                raster_file,\n                output_path=None,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n            )\n\n            # Add filename as attribute\n            if not gdf.empty:\n                gdf[\"source_file\"] = base_name\n                gdfs.append(gdf)\n        else:\n            # Save individual files\n            raster_to_vector(\n                raster_file,\n                output_path=out_file,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n                output_format=output_format,\n            )\n\n    # Merge output if requested\n    if merge_output and gdfs:\n        merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n\n        # Set CRS to the CRS of the first GeoDataFrame\n        if merged_gdf.crs is None and gdfs:\n            merged_gdf.crs = gdfs[0].crs\n\n        # Save merged output\n        if output_format.lower() == \"geojson\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.geojson\")\n            merged_gdf.to_file(merged_file, driver=\"GeoJSON\")\n        elif output_format.lower() == \"shapefile\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.shp\")\n            merged_gdf.to_file(merged_file)\n        elif output_format.lower() == \"gpkg\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.gpkg\")\n            merged_gdf.to_file(merged_file, driver=\"GPKG\")\n\n        print(f\"Merged vector data saved to {merged_file}\")\n        return merged_gdf\n\n    return None\n</code></pre>"},{"location":"geoai/#geoai.geoai.read_raster","title":"<code>read_raster(source, band=None, masked=True, **kwargs)</code>","text":"<p>Reads raster data from various formats using rioxarray.</p> <p>This function reads raster data from local files or URLs into a rioxarray data structure with preserved geospatial metadata.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>String path to the raster file or URL.</p> required <code>band</code> <code>Optional[Union[int, List[int]]]</code> <p>Integer or list of integers specifying which band(s) to read. Defaults to None (all bands).</p> <code>None</code> <code>masked</code> <code>bool</code> <p>Boolean indicating whether to mask nodata values. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rioxarray.open_rasterio.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xarray.DataArray: A DataArray containing the raster data with geospatial metadata preserved.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local GeoTIFF</p> <pre><code>&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n&gt;&gt;&gt;\nRead only band 1 from a remote GeoTIFF\n&gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n&gt;&gt;&gt;\nRead a raster without masking nodata values\n&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_raster(\n    source: str,\n    band: Optional[Union[int, List[int]]] = None,\n    masked: bool = True,\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"Reads raster data from various formats using rioxarray.\n\n    This function reads raster data from local files or URLs into a rioxarray\n    data structure with preserved geospatial metadata.\n\n    Args:\n        source: String path to the raster file or URL.\n        band: Integer or list of integers specifying which band(s) to read.\n            Defaults to None (all bands).\n        masked: Boolean indicating whether to mask nodata values.\n            Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rioxarray.open_rasterio.\n\n    Returns:\n        xarray.DataArray: A DataArray containing the raster data with geospatial\n            metadata preserved.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n        &gt;&gt;&gt;\n        Read only band 1 from a remote GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n        &gt;&gt;&gt;\n        Read a raster without masking nodata values\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n    \"\"\"\n    import urllib.parse\n\n    from rasterio.errors import RasterioIOError\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"Raster file does not exist: {source}\")\n\n    try:\n        # Open the raster with rioxarray\n        raster = rxr.open_rasterio(source, masked=masked, **kwargs)\n\n        # Handle band selection if specified\n        if band is not None:\n            if isinstance(band, (list, tuple)):\n                # Convert from 1-based indexing to 0-based indexing\n                band_indices = [b - 1 for b in band]\n                raster = raster.isel(band=band_indices)\n            else:\n                # Single band selection (convert from 1-based to 0-based indexing)\n                raster = raster.isel(band=band - 1)\n\n        return raster\n\n    except RasterioIOError as e:\n        raise ValueError(f\"Could not read raster from source '{source}': {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading raster data: {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.read_vector","title":"<code>read_vector(source, layer=None, **kwargs)</code>","text":"<p>Reads vector data from various formats including GeoParquet.</p> <p>This function dynamically determines the file type based on extension and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>String path to the vector file or URL.</p> required <code>layer</code> <code>Optional[str]</code> <p>String or integer specifying which layer to read from multi-layer files (only applicable for formats like GPKG, GeoJSON, etc.). Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the underlying reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local shapefile</p> <pre><code>&gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n&gt;&gt;&gt;\nRead a GeoParquet file from URL\n&gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n&gt;&gt;&gt;\nRead a specific layer from a GeoPackage\n&gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_vector(\n    source: str, layer: Optional[str] = None, **kwargs: Any\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Reads vector data from various formats including GeoParquet.\n\n    This function dynamically determines the file type based on extension\n    and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.\n\n    Args:\n        source: String path to the vector file or URL.\n        layer: String or integer specifying which layer to read from multi-layer\n            files (only applicable for formats like GPKG, GeoJSON, etc.).\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the underlying reader.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local shapefile\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n        &gt;&gt;&gt;\n        Read a GeoParquet file from URL\n        &gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n        &gt;&gt;&gt;\n        Read a specific layer from a GeoPackage\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n    \"\"\"\n\n    import urllib.parse\n\n    import fiona\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"File does not exist: {source}\")\n\n    # Get file extension\n    _, ext = os.path.splitext(source)\n    ext = ext.lower()\n\n    # Handle GeoParquet files\n    if ext in [\".parquet\", \".pq\", \".geoparquet\"]:\n        return gpd.read_parquet(source, **kwargs)\n\n    # Handle common vector formats\n    if ext in [\".shp\", \".geojson\", \".json\", \".gpkg\", \".gml\", \".kml\", \".gpx\"]:\n        # For formats that might have multiple layers\n        if ext in [\".gpkg\", \".gml\"] and layer is not None:\n            return gpd.read_file(source, layer=layer, **kwargs)\n        return gpd.read_file(source, **kwargs)\n\n    # Try to use fiona to identify valid layers for formats that might have them\n    # Only attempt this for local files as fiona.listlayers might not work with URLs\n    if layer is None and ext in [\".gpkg\", \".gml\"] and not is_url:\n        try:\n            layers = fiona.listlayers(source)\n            if layers:\n                return gpd.read_file(source, layer=layers[0], **kwargs)\n        except Exception:\n            # If listing layers fails, we'll fall through to the generic read attempt\n            pass\n\n    # For other formats or when layer listing fails, attempt to read using GeoPandas\n    try:\n        return gpd.read_file(source, **kwargs)\n    except Exception as e:\n        raise ValueError(f\"Could not read from source '{source}': {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    import scipy.ndimage as ndi\n    from skimage import measure\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"axis_major_length\",\n            \"axis_minor_length\",\n            \"eccentricity\",\n            \"diameter_areagth\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"axis_major_length\"] / df[\"axis_minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n\n        if out_vector is not None:\n            tmp_raster = None\n            tmp_vector = None\n            try:\n                if out_image is None:\n                    tmp_raster = temp_file_path(\".tif\")\n                    da.rio.to_raster(tmp_raster, dtype=dtype)\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        tmp_raster,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                else:\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        out_image,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n            finally:\n                try:\n                    if tmp_raster is not None and os.path.exists(tmp_raster):\n                        os.remove(tmp_raster)\n                    if tmp_vector is not None and os.path.exists(tmp_vector):\n                        os.remove(tmp_vector)\n                except Exception as e:\n                    print(f\"Warning: Failed to delete temporary files: {str(e)}\")\n\n        return da, df\n</code></pre>"},{"location":"geoai/#geoai.geoai.regularization","title":"<code>regularization(building_polygons, angle_tolerance=10, simplify_tolerance=0.5, orthogonalize=True, preserve_topology=True)</code>","text":"<p>Regularizes building footprint polygons with multiple techniques beyond minimum rotated rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <code>angle_tolerance</code> <code>float</code> <p>Degrees within which angles will be regularized to 90/180 degrees</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Distance tolerance for Douglas-Peucker simplification</p> <code>0.5</code> <code>orthogonalize</code> <code>bool</code> <p>Whether to enforce orthogonal angles in the final polygons</p> <code>True</code> <code>preserve_topology</code> <code>bool</code> <p>Whether to preserve topology during simplification</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n    angle_tolerance: float = 10,\n    simplify_tolerance: float = 0.5,\n    orthogonalize: bool = True,\n    preserve_topology: bool = True,\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    Regularizes building footprint polygons with multiple techniques beyond minimum\n    rotated rectangles.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n        angle_tolerance: Degrees within which angles will be regularized to 90/180 degrees\n        simplify_tolerance: Distance tolerance for Douglas-Peucker simplification\n        orthogonalize: Whether to enforce orthogonal angles in the final polygons\n        preserve_topology: Whether to preserve topology during simplification\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely import wkt\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import Polygon, shape\n\n    regularized_buildings = []\n\n    # Check if we're dealing with a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    for building in geom_objects:\n        # Handle potential string representations of geometries\n        if isinstance(building, str):\n            try:\n                # Try to parse as WKT\n                building = wkt.loads(building)\n            except Exception:\n                print(f\"Failed to parse geometry string: {building[:30]}...\")\n                continue\n\n        # Ensure we have a valid geometry\n        if not hasattr(building, \"simplify\"):\n            print(f\"Invalid geometry type: {type(building)}\")\n            continue\n\n        # Step 1: Simplify to remove noise and small vertices\n        simplified = building.simplify(\n            simplify_tolerance, preserve_topology=preserve_topology\n        )\n\n        if orthogonalize:\n            # Make sure we have a valid polygon with an exterior\n            if not hasattr(simplified, \"exterior\") or simplified.exterior is None:\n                print(f\"Simplified geometry has no exterior: {simplified}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            # Step 2: Get the dominant angle to rotate building\n            coords = np.array(simplified.exterior.coords)\n\n            # Make sure we have enough coordinates for angle calculation\n            if len(coords) &lt; 3:\n                print(f\"Not enough coordinates for angle calculation: {len(coords)}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            segments = np.diff(coords, axis=0)\n            angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n            # Find most common angle classes (0, 90, 180, 270 degrees)\n            binned_angles = np.round(angles / 90) * 90\n            dominant_angle = np.bincount(binned_angles.astype(int) % 180).argmax()\n\n            # Step 3: Rotate to align with axes, regularize, then rotate back\n            rotated = rotate(simplified, -dominant_angle, origin=\"centroid\")\n\n            # Step 4: Rectify coordinates to enforce right angles\n            ext_coords = np.array(rotated.exterior.coords)\n            rect_coords = []\n\n            # Regularize each vertex to create orthogonal corners\n            for i in range(len(ext_coords) - 1):\n                rect_coords.append(ext_coords[i])\n\n                # Check if we need to add a right-angle vertex\n                angle = (\n                    np.arctan2(\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 1]\n                        - ext_coords[i, 1],\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 0]\n                        - ext_coords[i, 0],\n                    )\n                    * 180\n                    / np.pi\n                )\n\n                if abs(angle % 90) &gt; angle_tolerance and abs(angle % 90) &lt; (\n                    90 - angle_tolerance\n                ):\n                    # Add intermediate point to create right angle\n                    rect_coords.append(\n                        [\n                            ext_coords[(i + 1) % (len(ext_coords) - 1), 0],\n                            ext_coords[i, 1],\n                        ]\n                    )\n\n            # Close the polygon by adding the first point again\n            rect_coords.append(rect_coords[0])\n\n            # Create regularized polygon and rotate back\n            regularized = Polygon(rect_coords)\n            final_building = rotate(regularized, dominant_angle, origin=\"centroid\")\n        else:\n            final_building = simplified\n\n        regularized_buildings.append(final_building)\n\n    # If input was a GeoDataFrame, return a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        return gpd.GeoDataFrame(\n            geometry=regularized_buildings, crs=building_polygons.crs\n        )\n    else:\n        return regularized_buildings\n</code></pre>"},{"location":"geoai/#geoai.geoai.regularize","title":"<code>regularize(data, parallel_threshold=1.0, target_crs=None, simplify=True, simplify_tolerance=0.5, allow_45_degree=True, diagonal_threshold_reduction=15, allow_circles=True, circle_threshold=0.9, num_cores=1, include_metadata=False, output_path=None, **kwargs)</code>","text":"<p>Regularizes polygon geometries in a GeoDataFrame by aligning edges.</p> <p>Aligns edges to be parallel or perpendicular (optionally also 45 degrees) to their main direction. Handles reprojection, initial simplification, regularization, geometry cleanup, and parallel processing.</p> <p>This function is a wrapper around the <code>regularize_geodataframe</code> function from the <code>buildingregulariser</code> package. Credits to the original author Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[GeoDataFrame, str]</code> <p>Input GeoDataFrame with polygon or multipolygon geometries, or a file path to the GeoDataFrame.</p> required <code>parallel_threshold</code> <code>float</code> <p>Distance threshold for merging nearly parallel adjacent edges during regularization. Defaults to 1.0.</p> <code>1.0</code> <code>target_crs</code> <code>Optional[Union[str, CRS]]</code> <p>Target Coordinate Reference System for processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a projected CRS. Defaults to None.</p> <code>None</code> <code>simplify</code> <code>bool</code> <p>If True, applies initial simplification to the geometry before regularization. Defaults to True.</p> <code>True</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for the initial simplification step (if <code>simplify</code> is True). Also used for geometry cleanup steps. Defaults to 0.5.</p> <code>0.5</code> <code>allow_45_degree</code> <code>bool</code> <p>If True, allows edges to be oriented at 45-degree angles relative to the main direction during regularization. Defaults to True.</p> <code>True</code> <code>diagonal_threshold_reduction</code> <code>float</code> <p>Reduction factor in degrees to reduce the likelihood of diagonal edges being created. Larger values reduce the likelihood of diagonal edges. Defaults to 15.</p> <code>15</code> <code>allow_circles</code> <code>bool</code> <p>If True, attempts to detect polygons that are nearly circular and replaces them with perfect circles. Defaults to True.</p> <code>True</code> <code>circle_threshold</code> <code>float</code> <p>Intersection over Union (IoU) threshold used for circle detection (if <code>allow_circles</code> is True). Value between 0 and 1. Defaults to 0.9.</p> <code>0.9</code> <code>num_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. If 1, processing is done sequentially. Defaults to 1.</p> <code>1</code> <code>include_metadata</code> <code>bool</code> <p>If True, includes metadata about the regularization process in the output GeoDataFrame. Defaults to False.</p> <code>False</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoDataFrame. If None, the output is not saved. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>to_file</code> method when saving the output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are</p> <code>Any</code> <p>preserved. Geometries that failed processing might be dropped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularize(\n    data: Union[gpd.GeoDataFrame, str],\n    parallel_threshold: float = 1.0,\n    target_crs: Optional[Union[str, \"pyproj.CRS\"]] = None,\n    simplify: bool = True,\n    simplify_tolerance: float = 0.5,\n    allow_45_degree: bool = True,\n    diagonal_threshold_reduction: float = 15,\n    allow_circles: bool = True,\n    circle_threshold: float = 0.9,\n    num_cores: int = 1,\n    include_metadata: bool = False,\n    output_path: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Regularizes polygon geometries in a GeoDataFrame by aligning edges.\n\n    Aligns edges to be parallel or perpendicular (optionally also 45 degrees)\n    to their main direction. Handles reprojection, initial simplification,\n    regularization, geometry cleanup, and parallel processing.\n\n    This function is a wrapper around the `regularize_geodataframe` function\n    from the `buildingregulariser` package. Credits to the original author\n    Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.\n\n    Args:\n        data (Union[gpd.GeoDataFrame, str]): Input GeoDataFrame with polygon or multipolygon geometries,\n            or a file path to the GeoDataFrame.\n        parallel_threshold (float, optional): Distance threshold for merging nearly parallel adjacent edges\n            during regularization. Defaults to 1.0.\n        target_crs (Optional[Union[str, \"pyproj.CRS\"]], optional): Target Coordinate Reference System for\n            processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a\n            projected CRS. Defaults to None.\n        simplify (bool, optional): If True, applies initial simplification to the geometry before\n            regularization. Defaults to True.\n        simplify_tolerance (float, optional): Tolerance for the initial simplification step (if `simplify`\n            is True). Also used for geometry cleanup steps. Defaults to 0.5.\n        allow_45_degree (bool, optional): If True, allows edges to be oriented at 45-degree angles relative\n            to the main direction during regularization. Defaults to True.\n        diagonal_threshold_reduction (float, optional): Reduction factor in degrees to reduce the likelihood\n            of diagonal edges being created. Larger values reduce the likelihood of diagonal edges.\n            Defaults to 15.\n        allow_circles (bool, optional): If True, attempts to detect polygons that are nearly circular and\n            replaces them with perfect circles. Defaults to True.\n        circle_threshold (float, optional): Intersection over Union (IoU) threshold used for circle detection\n            (if `allow_circles` is True). Value between 0 and 1. Defaults to 0.9.\n        num_cores (int, optional): Number of CPU cores to use for parallel processing. If 1, processing is\n            done sequentially. Defaults to 1.\n        include_metadata (bool, optional): If True, includes metadata about the regularization process in the\n            output GeoDataFrame. Defaults to False.\n        output_path (Optional[str], optional): Path to save the output GeoDataFrame. If None, the output is\n            not saved. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the `to_file` method when saving the output.\n\n    Returns:\n        gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are\n        preserved. Geometries that failed processing might be dropped.\n\n    Raises:\n        ValueError: If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.\n    \"\"\"\n    try:\n        from buildingregulariser import regularize_geodataframe\n    except ImportError:\n        install_package(\"buildingregulariser\")\n        from buildingregulariser import regularize_geodataframe\n\n    if isinstance(data, str):\n        data = gpd.read_file(data)\n    elif not isinstance(data, gpd.GeoDataFrame):\n        raise ValueError(\"Input data must be a GeoDataFrame or a file path.\")\n\n    # Check if the input data is empty\n    if data.empty:\n        raise ValueError(\"Input GeoDataFrame is empty.\")\n\n    gdf = regularize_geodataframe(\n        data,\n        parallel_threshold=parallel_threshold,\n        target_crs=target_crs,\n        simplify=simplify,\n        simplify_tolerance=simplify_tolerance,\n        allow_45_degree=allow_45_degree,\n        diagonal_threshold_reduction=diagonal_threshold_reduction,\n        allow_circles=allow_circles,\n        circle_threshold=circle_threshold,\n        num_cores=num_cores,\n        include_metadata=include_metadata,\n    )\n\n    if output_path:\n        gdf.to_file(output_path, **kwargs)\n\n    return gdf\n</code></pre>"},{"location":"geoai/#geoai.geoai.rowcol_to_xy","title":"<code>rowcol_to_xy(src_fp, rows=None, cols=None, boxes=None, zs=None, offset='center', output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Converts a list of (row, col) coordinates to (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>rows</code> <code>list</code> <p>A list of row coordinates. Defaults to None.</p> <code>None</code> <code>cols</code> <code>list</code> <p>A list of col coordinates. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list</code> <p>A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]</p> <code>None</code> <code>zs</code> <code>Optional[List[float]]</code> <p>zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.</p> <code>None</code> <code>offset</code> <code>str</code> <p>Determines if the returned coordinates are for the center of the pixel or for a corner.</p> <code>'center'</code> <code>output</code> <code>str</code> <p>The output vector file path. Defaults to None.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.xy.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>A list of (x, y) coordinates.</p> Source code in <code>geoai/utils.py</code> <pre><code>def rowcol_to_xy(\n    src_fp: str,\n    rows: Optional[List[int]] = None,\n    cols: Optional[List[int]] = None,\n    boxes: Optional[List[List[int]]] = None,\n    zs: Optional[List[float]] = None,\n    offset: str = \"center\",\n    output: Optional[str] = None,\n    dst_crs: str = \"EPSG:4326\",\n    **kwargs: Any,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Converts a list of (row, col) coordinates to (x, y) coordinates.\n\n    Args:\n        src_fp (str): The source raster file path.\n        rows (list, optional): A list of row coordinates. Defaults to None.\n        cols (list, optional): A list of col coordinates. Defaults to None.\n        boxes (list, optional): A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]\n        zs: zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.\n        offset (str, optional): Determines if the returned coordinates are for the center of the pixel or for a corner.\n        output (str, optional): The output vector file path. Defaults to None.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.xy.\n\n    Returns:\n        A list of (x, y) coordinates.\n    \"\"\"\n\n    if boxes is not None:\n        rows = []\n        cols = []\n\n        for box in boxes:\n            rows.append(box[1])\n            rows.append(box[3])\n            cols.append(box[0])\n            cols.append(box[2])\n\n    if rows is None or cols is None:\n        raise ValueError(\"rows and cols must be provided.\")\n\n    with rasterio.open(src_fp) as src:\n        xs, ys = rasterio.transform.xy(src.transform, rows, cols, zs, offset, **kwargs)\n        src_crs = src.crs\n\n    if boxes is None:\n        return [[x, y] for x, y in zip(xs, ys)]\n    else:\n        result = [[xs[i], ys[i + 1], xs[i + 1], ys[i]] for i in range(0, len(xs), 2)]\n\n        if output is not None:\n            boxes_to_vector(result, src_crs, dst_crs, output)\n        else:\n            return result\n</code></pre>"},{"location":"geoai/#geoai.geoai.stack_bands","title":"<code>stack_bands(input_files, output_file, resolution=None, dtype=None, temp_vrt='stack.vrt', overwrite=False, compress='DEFLATE', output_format='COG', extra_gdal_translate_args=None)</code>","text":"<p>Stack bands from multiple images into a single multi-band GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>input_files</code> <code>List[str]</code> <p>List of input image paths.</p> required <code>output_file</code> <code>str</code> <p>Path to the output stacked image.</p> required <code>resolution</code> <code>float</code> <p>Output resolution. If None, inferred from first image.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Output data type (e.g., \"UInt16\", \"Float32\").</p> <code>None</code> <code>temp_vrt</code> <code>str</code> <p>Temporary VRT filename.</p> <code>'stack.vrt'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file.</p> <code>False</code> <code>compress</code> <code>str</code> <p>Compression method.</p> <code>'DEFLATE'</code> <code>output_format</code> <code>str</code> <p>GDAL output format (default is \"COG\").</p> <code>'COG'</code> <code>extra_gdal_translate_args</code> <code>List[str]</code> <p>Extra arguments for gdal_translate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def stack_bands(\n    input_files: List[str],\n    output_file: str,\n    resolution: Optional[float] = None,\n    dtype: Optional[str] = None,  # e.g., \"UInt16\", \"Float32\"\n    temp_vrt: str = \"stack.vrt\",\n    overwrite: bool = False,\n    compress: str = \"DEFLATE\",\n    output_format: str = \"COG\",\n    extra_gdal_translate_args: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Stack bands from multiple images into a single multi-band GeoTIFF.\n\n    Args:\n        input_files (List[str]): List of input image paths.\n        output_file (str): Path to the output stacked image.\n        resolution (float, optional): Output resolution. If None, inferred from first image.\n        dtype (str, optional): Output data type (e.g., \"UInt16\", \"Float32\").\n        temp_vrt (str): Temporary VRT filename.\n        overwrite (bool): Whether to overwrite the output file.\n        compress (str): Compression method.\n        output_format (str): GDAL output format (default is \"COG\").\n        extra_gdal_translate_args (List[str], optional): Extra arguments for gdal_translate.\n\n    Returns:\n        str: Path to the output file.\n    \"\"\"\n    import leafmap\n\n    if not input_files:\n        raise ValueError(\"No input files provided.\")\n    elif isinstance(input_files, str):\n        input_files = leafmap.find_files(input_files, \".tif\")\n\n    if os.path.exists(output_file) and not overwrite:\n        print(f\"Output file already exists: {output_file}\")\n        return output_file\n\n    # Infer resolution if not provided\n    if resolution is None:\n        resolution_x, resolution_y = get_raster_resolution(input_files[0])\n    else:\n        resolution_x = resolution_y = resolution\n\n    # Step 1: Build VRT\n    vrt_cmd = [\"gdalbuildvrt\", \"-separate\", temp_vrt] + input_files\n    subprocess.run(vrt_cmd, check=True)\n\n    # Step 2: Translate VRT to output GeoTIFF\n    translate_cmd = [\n        \"gdal_translate\",\n        \"-tr\",\n        str(resolution_x),\n        str(resolution_y),\n        temp_vrt,\n        output_file,\n        \"-of\",\n        output_format,\n        \"-co\",\n        f\"COMPRESS={compress}\",\n    ]\n\n    if dtype:\n        translate_cmd.insert(1, \"-ot\")\n        translate_cmd.insert(2, dtype)\n\n    if extra_gdal_translate_args:\n        translate_cmd += extra_gdal_translate_args\n\n    subprocess.run(translate_cmd, check=True)\n\n    # Step 3: Clean up VRT\n    if os.path.exists(temp_vrt):\n        os.remove(temp_vrt)\n\n    return output_file\n</code></pre>"},{"location":"geoai/#geoai.geoai.temp_file_path","title":"<code>temp_file_path(ext)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The temporary file path.</p> Source code in <code>geoai/utils.py</code> <pre><code>def temp_file_path(ext: str) -&gt; str:\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        ext (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not ext.startswith(\".\"):\n        ext = \".\" + ext\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{ext}\")\n\n    return file_path\n</code></pre>"},{"location":"geoai/#geoai.geoai.train_segmentation_landcover","title":"<code>train_segmentation_landcover(images_dir, labels_dir, output_dir, input_format='directory', architecture='unet', encoder_name='resnet34', encoder_weights='imagenet', num_channels=3, num_classes=2, batch_size=8, num_epochs=50, learning_rate=0.001, weight_decay=0.0001, seed=42, val_split=0.2, print_freq=10, verbose=True, save_best_only=True, plot_curves=False, device=None, checkpoint_path=None, resume_training=False, target_size=None, resize_mode='resize', num_workers=None, loss_function='crossentropy', ignore_index=None, use_class_weights=False, focal_alpha=1.0, focal_gamma=2.0, custom_multipliers=None, max_class_weight=50.0, use_inverse_frequency=True, validation_iou_mode='standard', boundary_alpha=1.0, training_callback=None, **kwargs)</code>","text":"<p>Train a semantic segmentation model with landcover-specific enhancements.</p> <p>This is a standalone version that wraps geoai.train.train_segmentation_model with landcover-specific loss functions, class weights, and metrics.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing training images</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing training labels</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model checkpoints and training history</p> required <code>input_format</code> <code>str</code> <p>Data format (\"directory\", \"COCO\", \"YOLO\")</p> <code>'directory'</code> <code>architecture</code> <code>str</code> <p>Model architecture (default: \"unet\")</p> <code>'unet'</code> <code>encoder_name</code> <code>str</code> <p>Encoder backbone (default: \"resnet34\")</p> <code>'resnet34'</code> <code>encoder_weights</code> <code>Optional[str]</code> <p>Pretrained weights (\"imagenet\" or None)</p> <code>'imagenet'</code> <code>num_channels</code> <code>int</code> <p>Number of input channels (default: 3)</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of output classes (default: 2)</p> <code>2</code> <code>batch_size</code> <code>int</code> <p>Training batch size (default: 8)</p> <code>8</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs (default: 50)</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate (default: 0.001)</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer (default: 1e-4)</p> <code>0.0001</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility (default: 42)</p> <code>42</code> <code>val_split</code> <code>float</code> <p>Validation split ratio (default: 0.2)</p> <code>0.2</code> <code>print_freq</code> <code>int</code> <p>Frequency of training progress prints (default: 10)</p> <code>10</code> <code>verbose</code> <code>bool</code> <p>Enable verbose output (default: True)</p> <code>True</code> <code>save_best_only</code> <code>bool</code> <p>Only save best model checkpoint (default: True)</p> <code>True</code> <code>plot_curves</code> <code>bool</code> <p>Plot training curves at end (default: False)</p> <code>False</code> <code>device</code> <code>Optional[device]</code> <p>Torch device (auto-detected if None)</p> <code>None</code> <code>checkpoint_path</code> <code>Optional[str]</code> <p>Path to checkpoint for resuming training</p> <code>None</code> <code>resume_training</code> <code>bool</code> <p>Whether to resume from checkpoint (default: False)</p> <code>False</code> <code>target_size</code> <code>Optional[Tuple[int, int]]</code> <p>Target size for resizing images (H, W) or None</p> <code>None</code> <code>resize_mode</code> <code>str</code> <p>How to resize (\"resize\", \"crop\", or \"pad\")</p> <code>'resize'</code> <code>num_workers</code> <code>Optional[int]</code> <p>Number of dataloader workers (default: auto)</p> <code>None</code> <code>loss_function</code> <code>str</code> <p>Loss function name (\"crossentropy\", \"focal\")</p> <code>'crossentropy'</code> <code>ignore_index</code> <code>Optional[int]</code> <p>Class index to ignore (0 for background, None to include all)</p> <code>None</code> <code>use_class_weights</code> <code>bool</code> <p>Whether to compute and use class weights (default: False)</p> <code>False</code> <code>focal_alpha</code> <code>float</code> <p>Focal loss alpha parameter (default: 1.0)</p> <code>1.0</code> <code>focal_gamma</code> <code>float</code> <p>Focal loss gamma parameter (default: 2.0)</p> <code>2.0</code> <code>custom_multipliers</code> <code>Optional[Dict[int, float]]</code> <p>Custom class weight multipliers {class_id: multiplier}</p> <code>None</code> <code>max_class_weight</code> <code>float</code> <p>Maximum allowed class weight (default: 50.0)</p> <code>50.0</code> <code>use_inverse_frequency</code> <code>bool</code> <p>Use inverse frequency for weights (default: True)</p> <code>True</code> <code>validation_iou_mode</code> <code>str</code> <p>IoU calculation mode for validation (default: \"standard\") - \"standard\": Unweighted mean IoU (all classes equal importance) - \"perclass_frequency\": Frequency-weighted IoU (classes weighted by pixel count) - \"boundary_weighted\": Boundary-distance weighted IoU (wIoU, focus on edges)</p> <code>'standard'</code> <code>boundary_alpha</code> <code>float</code> <p>Boundary importance factor for wIoU mode (default: 1.0) Higher values = more focus on boundaries (0.01-100 range)</p> <code>1.0</code> <code>training_callback</code> <code>Optional[callable]</code> <p>Optional callback function for automatic metric tracking</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to base training function</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>Trained model</p> Example <p>from landcover_train import train_segmentation_landcover</p> <p>model = train_segmentation_landcover( ...     images_dir=\"tiles/images\", ...     labels_dir=\"tiles/labels\", ...     output_dir=\"models/landcover_001\", ...     num_classes=5, ...     loss_function=\"focal\", ...     ignore_index=0,  # Ignore background ...     use_class_weights=True, ...     custom_multipliers={1: 1.5, 4: 0.8},  # Boost class 1, reduce class 4 ...     max_class_weight=50.0, ...     use_inverse_frequency=True,  # Use inverse frequency weighting ...     validation_iou_mode=\"boundary_weighted\",  # Focus on boundaries ...     boundary_alpha=2.0,  # Moderate boundary emphasis ... )</p> Source code in <code>geoai/landcover_train.py</code> <pre><code>def train_segmentation_landcover(\n    images_dir: str,\n    labels_dir: str,\n    output_dir: str,\n    input_format: str = \"directory\",\n    architecture: str = \"unet\",\n    encoder_name: str = \"resnet34\",\n    encoder_weights: Optional[str] = \"imagenet\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    batch_size: int = 8,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001,\n    weight_decay: float = 1e-4,\n    seed: int = 42,\n    val_split: float = 0.2,\n    print_freq: int = 10,\n    verbose: bool = True,\n    save_best_only: bool = True,\n    plot_curves: bool = False,\n    device: Optional[torch.device] = None,\n    checkpoint_path: Optional[str] = None,\n    resume_training: bool = False,\n    target_size: Optional[Tuple[int, int]] = None,\n    resize_mode: str = \"resize\",\n    num_workers: Optional[int] = None,\n    loss_function: str = \"crossentropy\",\n    ignore_index: Optional[int] = None,\n    use_class_weights: bool = False,\n    focal_alpha: float = 1.0,\n    focal_gamma: float = 2.0,\n    custom_multipliers: Optional[Dict[int, float]] = None,\n    max_class_weight: float = 50.0,\n    use_inverse_frequency: bool = True,\n    validation_iou_mode: str = \"standard\",\n    boundary_alpha: float = 1.0,\n    training_callback: Optional[callable] = None,\n    **kwargs: Any,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Train a semantic segmentation model with landcover-specific enhancements.\n\n    This is a standalone version that wraps geoai.train.train_segmentation_model\n    with landcover-specific loss functions, class weights, and metrics.\n\n    Args:\n        images_dir: Directory containing training images\n        labels_dir: Directory containing training labels\n        output_dir: Directory to save model checkpoints and training history\n        input_format: Data format (\"directory\", \"COCO\", \"YOLO\")\n        architecture: Model architecture (default: \"unet\")\n        encoder_name: Encoder backbone (default: \"resnet34\")\n        encoder_weights: Pretrained weights (\"imagenet\" or None)\n        num_channels: Number of input channels (default: 3)\n        num_classes: Number of output classes (default: 2)\n        batch_size: Training batch size (default: 8)\n        num_epochs: Number of training epochs (default: 50)\n        learning_rate: Initial learning rate (default: 0.001)\n        weight_decay: Weight decay for optimizer (default: 1e-4)\n        seed: Random seed for reproducibility (default: 42)\n        val_split: Validation split ratio (default: 0.2)\n        print_freq: Frequency of training progress prints (default: 10)\n        verbose: Enable verbose output (default: True)\n        save_best_only: Only save best model checkpoint (default: True)\n        plot_curves: Plot training curves at end (default: False)\n        device: Torch device (auto-detected if None)\n        checkpoint_path: Path to checkpoint for resuming training\n        resume_training: Whether to resume from checkpoint (default: False)\n        target_size: Target size for resizing images (H, W) or None\n        resize_mode: How to resize (\"resize\", \"crop\", or \"pad\")\n        num_workers: Number of dataloader workers (default: auto)\n        loss_function: Loss function name (\"crossentropy\", \"focal\")\n        ignore_index: Class index to ignore (0 for background, None to include all)\n        use_class_weights: Whether to compute and use class weights (default: False)\n        focal_alpha: Focal loss alpha parameter (default: 1.0)\n        focal_gamma: Focal loss gamma parameter (default: 2.0)\n        custom_multipliers: Custom class weight multipliers {class_id: multiplier}\n        max_class_weight: Maximum allowed class weight (default: 50.0)\n        use_inverse_frequency: Use inverse frequency for weights (default: True)\n        validation_iou_mode: IoU calculation mode for validation (default: \"standard\")\n            - \"standard\": Unweighted mean IoU (all classes equal importance)\n            - \"perclass_frequency\": Frequency-weighted IoU (classes weighted by pixel count)\n            - \"boundary_weighted\": Boundary-distance weighted IoU (wIoU, focus on edges)\n        boundary_alpha: Boundary importance factor for wIoU mode (default: 1.0)\n            Higher values = more focus on boundaries (0.01-100 range)\n        training_callback: Optional callback function for automatic metric tracking\n        **kwargs: Additional arguments passed to base training function\n\n    Returns:\n        Trained model\n\n    Example:\n        &gt;&gt;&gt; from landcover_train import train_segmentation_landcover\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; model = train_segmentation_landcover(\n        ...     images_dir=\"tiles/images\",\n        ...     labels_dir=\"tiles/labels\",\n        ...     output_dir=\"models/landcover_001\",\n        ...     num_classes=5,\n        ...     loss_function=\"focal\",\n        ...     ignore_index=0,  # Ignore background\n        ...     use_class_weights=True,\n        ...     custom_multipliers={1: 1.5, 4: 0.8},  # Boost class 1, reduce class 4\n        ...     max_class_weight=50.0,\n        ...     use_inverse_frequency=True,  # Use inverse frequency weighting\n        ...     validation_iou_mode=\"boundary_weighted\",  # Focus on boundaries\n        ...     boundary_alpha=2.0,  # Moderate boundary emphasis\n        ... )\n    \"\"\"\n\n    # Import geoai training function\n    try:\n        from geoai.train import train_segmentation_model\n    except ImportError:\n        raise ImportError(\"geoai package not found. Install with: pip install geoai-py\")\n\n    # Convert ignore_index to format expected by base function\n    # Base function uses Union[int, bool], we use Optional[int]\n    ignore_idx_param = ignore_index if ignore_index is not None else False\n\n    # Compute class weights if requested\n    class_weights = None\n    if use_class_weights:\n        if verbose:\n            print(\"\\n\" + \"=\" * 60)\n            print(\"COMPUTING CLASS WEIGHTS\")\n            print(\"=\" * 60)\n\n        class_weights = compute_class_weights(\n            labels_dir=labels_dir,\n            num_classes=num_classes,\n            ignore_index=ignore_index if ignore_index is not None else -100,\n            custom_multipliers=custom_multipliers,\n            max_weight=max_class_weight,\n            use_inverse_frequency=use_inverse_frequency,\n        )\n\n        if verbose:\n            print(\"=\" * 60 + \"\\n\")\n\n    # Call base training function with enhanced parameters\n    model = train_segmentation_model(\n        images_dir=images_dir,\n        labels_dir=labels_dir,\n        output_dir=output_dir,\n        input_format=input_format,\n        architecture=architecture,\n        encoder_name=encoder_name,\n        encoder_weights=encoder_weights,\n        num_channels=num_channels,\n        num_classes=num_classes,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        seed=seed,\n        val_split=val_split,\n        print_freq=print_freq,\n        verbose=verbose,\n        save_best_only=save_best_only,\n        plot_curves=plot_curves,\n        device=device,\n        checkpoint_path=checkpoint_path,\n        resume_training=resume_training,\n        target_size=target_size,\n        resize_mode=resize_mode,\n        num_workers=num_workers,\n        loss_function=loss_function,\n        ignore_index=ignore_idx_param,\n        use_class_weights=use_class_weights,\n        focal_alpha=focal_alpha,\n        focal_gamma=focal_gamma,\n        custom_multipliers=custom_multipliers,\n        max_class_weight=max_class_weight,\n        use_inverse_frequency=use_inverse_frequency,\n        validation_iou_mode=validation_iou_mode,\n        boundary_alpha=boundary_alpha,\n        training_callback=training_callback,\n        **kwargs,\n    )\n\n    return model\n</code></pre>"},{"location":"geoai/#geoai.geoai.try_common_architectures","title":"<code>try_common_architectures(state_dict)</code>","text":"<p>Try to load the state_dict into common architectures to see which one fits.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>The model's state dictionary</p> required Source code in <code>geoai/utils.py</code> <pre><code>def try_common_architectures(state_dict: Dict[str, Any]) -&gt; Optional[str]:\n    \"\"\"\n    Try to load the state_dict into common architectures to see which one fits.\n\n    Args:\n        state_dict: The model's state dictionary\n    \"\"\"\n    import torchinfo\n\n    # Test models and their initializations\n    models_to_try = {\n        \"FCN-ResNet50\": lambda: fcn_resnet50(num_classes=9),\n        \"DeepLabV3-ResNet50\": lambda: deeplabv3_resnet50(num_classes=9),\n    }\n\n    print(\"\\nTrying to load state_dict into common architectures:\")\n\n    for name, model_fn in models_to_try.items():\n        try:\n            model = model_fn()\n            # Sometimes state_dict keys have 'model.' prefix\n            if all(k.startswith(\"model.\") for k in state_dict.keys()):\n                cleaned_state_dict = {k[6:]: v for k, v in state_dict.items()}\n                model.load_state_dict(cleaned_state_dict, strict=False)\n            else:\n                model.load_state_dict(state_dict, strict=False)\n\n            print(\n                f\"- {name}: Successfully loaded (may have missing or unexpected keys)\"\n            )\n\n            # Generate model summary\n            print(f\"\\nSummary of {name} architecture:\")\n            summary = torchinfo.summary(model, input_size=(1, 3, 224, 224), verbose=0)\n            print(summary)\n\n        except Exception as e:\n            print(f\"- {name}: Failed to load - {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.vector_to_geojson","title":"<code>vector_to_geojson(filename, output=None, **kwargs)</code>","text":"<p>Converts a vector file to a geojson file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <code>output</code> <code>str</code> <p>The output geojson file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>str</code> <p>The geojson dictionary.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_geojson(\n    filename: str, output: Optional[str] = None, **kwargs: Any\n) -&gt; str:\n    \"\"\"Converts a vector file to a geojson file.\n\n    Args:\n        filename (str): The vector file path.\n        output (str, optional): The output geojson file path. Defaults to None.\n\n    Returns:\n        dict: The geojson dictionary.\n    \"\"\"\n\n    if filename.startswith(\"http\"):\n        filename = download_file(filename)\n\n    gdf = gpd.read_file(filename, **kwargs)\n    if output is None:\n        return gdf.__geo_interface__\n    else:\n        gdf.to_file(output, driver=\"GeoJSON\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.vector_to_raster","title":"<code>vector_to_raster(vector_path, output_path=None, reference_raster=None, attribute_field=None, output_shape=None, transform=None, pixel_size=None, bounds=None, crs=None, all_touched=False, fill_value=0, dtype=np.uint8, nodata=None, plot_result=False)</code>","text":"<p>Convert vector data to a raster.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output raster file. If None, returns the array without saving.</p> <code>None</code> <code>reference_raster</code> <code>str</code> <p>Path to a reference raster for dimensions, transform and CRS.</p> <code>None</code> <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values. If None, all vector features will be burned with value 1.</p> <code>None</code> <code>output_shape</code> <code>tuple</code> <p>Shape of the output raster as (height, width). Required if reference_raster is not provided.</p> <code>None</code> <code>transform</code> <code>Affine</code> <p>Affine transformation matrix. Required if reference_raster is not provided.</p> <code>None</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size (resolution) as single value or (x_res, y_res). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>bounds</code> <code>tuple</code> <p>Bounds of the output raster as (left, bottom, right, top). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>crs</code> <code>str or CRS</code> <p>Coordinate reference system of the output raster. Required if reference_raster is not provided.</p> <code>None</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in. If False, only pixels whose center is within the geometry will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type of the output raster.</p> <code>uint8</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: The rasterized data array if output_path is None, else None.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_raster(\n    vector_path: Union[str, gpd.GeoDataFrame],\n    output_path: Optional[str] = None,\n    reference_raster: Optional[str] = None,\n    attribute_field: Optional[str] = None,\n    output_shape: Optional[Tuple[int, int]] = None,\n    transform: Optional[Any] = None,\n    pixel_size: Optional[float] = None,\n    bounds: Optional[List[float]] = None,\n    crs: Optional[str] = None,\n    all_touched: bool = False,\n    fill_value: Union[int, float] = 0,\n    dtype: Any = np.uint8,\n    nodata: Optional[Union[int, float]] = None,\n    plot_result: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert vector data to a raster.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_path (str): Path to save the output raster file. If None, returns the array without saving.\n        reference_raster (str): Path to a reference raster for dimensions, transform and CRS.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n            If None, all vector features will be burned with value 1.\n        output_shape (tuple): Shape of the output raster as (height, width).\n            Required if reference_raster is not provided.\n        transform (affine.Affine): Affine transformation matrix.\n            Required if reference_raster is not provided.\n        pixel_size (float or tuple): Pixel size (resolution) as single value or (x_res, y_res).\n            Used to calculate transform if transform is not provided.\n        bounds (tuple): Bounds of the output raster as (left, bottom, right, top).\n            Used to calculate transform if transform is not provided.\n        crs (str or CRS): Coordinate reference system of the output raster.\n            Required if reference_raster is not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n            If False, only pixels whose center is within the geometry will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n        plot_result (bool): Whether to plot the resulting raster.\n\n    Returns:\n        numpy.ndarray: The rasterized data array if output_path is None, else None.\n    \"\"\"\n    # Load vector data\n    if isinstance(vector_path, gpd.GeoDataFrame):\n        gdf = vector_path\n    else:\n        gdf = gpd.read_file(vector_path)\n\n    # Check if vector data is empty\n    if gdf.empty:\n        warnings.warn(\"The input vector data is empty. Creating an empty raster.\")\n\n    # Get CRS from vector data if not provided\n    if crs is None and reference_raster is None:\n        crs = gdf.crs\n\n    # Get transform and output shape from reference raster if provided\n    if reference_raster is not None:\n        with rasterio.open(reference_raster) as src:\n            transform = src.transform\n            output_shape = src.shape\n            crs = src.crs\n            if nodata is None:\n                nodata = src.nodata\n    else:\n        # Check if we have all required parameters\n        if transform is None:\n            if pixel_size is None or bounds is None:\n                raise ValueError(\n                    \"Either reference_raster, transform, or both pixel_size and bounds must be provided.\"\n                )\n\n            # Calculate transform from pixel size and bounds\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n                y_res = abs(y_res) * -1  # Convert to negative for north-up raster\n\n            left, bottom, right, top = bounds\n            transform = rasterio.transform.from_bounds(\n                left,\n                bottom,\n                right,\n                top,\n                int((right - left) / x_res),\n                int((top - bottom) / abs(y_res)),\n            )\n\n        if output_shape is None:\n            # Calculate output shape from bounds and pixel size\n            if bounds is None or pixel_size is None:\n                raise ValueError(\n                    \"output_shape must be provided if reference_raster is not provided and \"\n                    \"cannot be calculated from bounds and pixel_size.\"\n                )\n\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n\n            left, bottom, right, top = bounds\n            width = int((right - left) / x_res)\n            height = int((top - bottom) / abs(y_res))\n            output_shape = (height, width)\n\n    # Ensure CRS is set\n    if crs is None:\n        raise ValueError(\n            \"CRS must be provided either directly, from reference_raster, or from input vector data.\"\n        )\n\n    # Reproject vector data if its CRS doesn't match the output CRS\n    if gdf.crs != crs:\n        print(f\"Reprojecting vector data from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Create empty raster filled with fill_value\n    raster_data = np.full(output_shape, fill_value, dtype=dtype)\n\n    # Burn vector features into raster\n    if not gdf.empty:\n        # Prepare shapes for burning\n        if attribute_field is not None and attribute_field in gdf.columns:\n            # Use attribute field for values\n            shapes = [\n                (geom, value) for geom, value in zip(gdf.geometry, gdf[attribute_field])\n            ]\n        else:\n            # Burn with value 1\n            shapes = [(geom, 1) for geom in gdf.geometry]\n\n        # Burn shapes into raster\n        burned = features.rasterize(\n            shapes=shapes,\n            out_shape=output_shape,\n            transform=transform,\n            fill=fill_value,\n            all_touched=all_touched,\n            dtype=dtype,\n        )\n\n        # Update raster data\n        raster_data = burned\n\n    # Save raster if output path is provided\n    if output_path is not None:\n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        # Define metadata\n        metadata = {\n            \"driver\": \"GTiff\",\n            \"height\": output_shape[0],\n            \"width\": output_shape[1],\n            \"count\": 1,\n            \"dtype\": raster_data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n        }\n\n        # Add nodata value if provided\n        if nodata is not None:\n            metadata[\"nodata\"] = nodata\n\n        # Write raster\n        with rasterio.open(output_path, \"w\", **metadata) as dst:\n            dst.write(raster_data, 1)\n\n        print(f\"Rasterized data saved to {output_path}\")\n\n    # Plot result if requested\n    if plot_result:\n        fig, ax = plt.subplots(figsize=(10, 10))\n\n        # Plot raster\n        im = ax.imshow(raster_data, cmap=\"viridis\")\n        plt.colorbar(im, ax=ax, label=attribute_field if attribute_field else \"Value\")\n\n        # Plot vector boundaries for reference\n        if output_path is not None:\n            # Get the extent of the raster\n            with rasterio.open(output_path) as src:\n                bounds = src.bounds\n                raster_bbox = box(*bounds)\n        else:\n            # Calculate extent from transform and shape\n            height, width = output_shape\n            left, top = transform * (0, 0)\n            right, bottom = transform * (width, height)\n            raster_bbox = box(left, bottom, right, top)\n\n        # Clip vector to raster extent for clarity in plot\n        if not gdf.empty:\n            gdf_clipped = gpd.clip(gdf, raster_bbox)\n            if not gdf_clipped.empty:\n                gdf_clipped.boundary.plot(ax=ax, color=\"red\", linewidth=1)\n\n        plt.title(\"Rasterized Vector Data\")\n        plt.tight_layout()\n        plt.show()\n\n    return raster_data\n</code></pre>"},{"location":"geoai/#geoai.geoai.view_image","title":"<code>view_image(image, transpose=False, bdx=None, clip_percentiles=(2, 98), gamma=None, figsize=(10, 5), axis_off=True, title=None, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[ndarray, Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    clip_percentiles: Optional[Tuple[float, float]] = (2, 98),\n    gamma: Optional[float] = None,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        title (Optional[str], optional): The title of the plot. Defaults to None.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n    elif isinstance(image, str):\n        image = rasterio.open(image).read().transpose(1, 2, 0)\n\n    ax = plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if clip_percentiles is not None:\n        p_low, p_high = clip_percentiles\n        lower = np.percentile(image, p_low)\n        upper = np.percentile(image, p_high)\n        image = np.clip((image - lower) / (upper - lower), 0, 1)\n\n    if gamma is not None:\n        image = np.power(image, gamma)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n\n    return ax\n</code></pre>"},{"location":"geoai/#geoai.geoai.view_raster","title":"<code>view_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': False}, basemap='OpenStreetMap', basemap_args=None, backend='ipyleaflet', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>basemap_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the basemap. Defaults to None.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The backend to use. Defaults to \"ipyleaflet\".</p> <code>'ipyleaflet'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>leafmap.Map: The map object with the raster layer added.</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = None,\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    basemap_args: Optional[Dict] = None,\n    backend: Optional[str] = \"ipyleaflet\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        basemap_args (Optional[Dict], optional): Additional arguments for the basemap. Defaults to None.\n        backend (Optional[str], optional): The backend to use. Defaults to \"ipyleaflet\".\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n\n    if backend == \"folium\":\n        import leafmap.foliumap as leafmap\n    else:\n        import leafmap.leafmap as leafmap\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if array_args is None:\n        array_args = {}\n\n    m = leafmap.Map()\n\n    if isinstance(basemap, str):\n        if basemap.lower().endswith(\".tif\"):\n            if basemap.lower().startswith(\"http\"):\n                if \"name\" not in basemap_args:\n                    basemap_args[\"name\"] = \"Basemap\"\n                m.add_cog_layer(basemap, **basemap_args)\n            else:\n                if \"layer_name\" not in basemap_args:\n                    basemap_args[\"layer_name\"] = \"Basemap\"\n                m.add_raster(basemap, **basemap_args)\n    else:\n        m.add_basemap(basemap, **basemap_args)\n\n    if isinstance(source, dict):\n        source = dict_to_image(source)\n\n    if (\n        isinstance(source, str)\n        and source.lower().endswith(\".tif\")\n        and source.startswith(\"http\")\n    ):\n        if indexes is not None:\n            kwargs[\"bidx\"] = indexes\n        if colormap is not None:\n            kwargs[\"colormap_name\"] = colormap\n        if attribution is None:\n            attribution = \"TiTiler\"\n\n        m.add_cog_layer(\n            source,\n            name=layer_name,\n            opacity=opacity,\n            attribution=attribution,\n            zoom_to_layer=zoom_to_layer,\n            **kwargs,\n        )\n    else:\n        m.add_raster(\n            source=source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            layer_index=layer_index,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            **kwargs,\n        )\n    return m\n</code></pre>"},{"location":"geoai/#geoai.geoai.view_vector","title":"<code>view_vector(vector_data, column=None, cmap='viridis', figsize=(10, 10), title=None, legend=True, basemap=False, basemap_type='streets', alpha=0.7, edge_color='black', classification='quantiles', n_classes=5, highlight_index=None, highlight_color='red', scheme=None, save_path=None, dpi=300)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>column</code> <code>str</code> <p>Column to use for choropleth mapping. If None, a single color will be used. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str or Colormap</code> <p>Colormap to use for choropleth mapping. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height) in inches. Defaults to (10, 10).</p> <code>(10, 10)</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to None.</p> <code>None</code> <code>legend</code> <code>bool</code> <p>Whether to display a legend. Defaults to True.</p> <code>True</code> <code>basemap</code> <code>bool</code> <p>Whether to add a web basemap. Requires contextily. Defaults to False.</p> <code>False</code> <code>basemap_type</code> <code>str</code> <p>Type of basemap to use. Options: 'streets', 'satellite'. Defaults to 'streets'.</p> <code>'streets'</code> <code>alpha</code> <code>float</code> <p>Transparency of the vector features, between 0-1. Defaults to 0.7.</p> <code>0.7</code> <code>edge_color</code> <code>str</code> <p>Color for feature edges. Defaults to \"black\".</p> <code>'black'</code> <code>classification</code> <code>str</code> <p>Classification method for choropleth maps. Options: \"quantiles\", \"equal_interval\", \"natural_breaks\". Defaults to \"quantiles\".</p> <code>'quantiles'</code> <code>n_classes</code> <code>int</code> <p>Number of classes for choropleth maps. Defaults to 5.</p> <code>5</code> <code>highlight_index</code> <code>list</code> <p>List of indices to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color to use for highlighted features. Defaults to \"red\".</p> <code>'red'</code> <code>scheme</code> <code>str</code> <p>MapClassify classification scheme. Overrides classification parameter if provided. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, the figure is not saved. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>DPI for saved figure. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>Any</code> <p>matplotlib.axes.Axes: The Axes object containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    column: Optional[str] = None,\n    cmap: str = \"viridis\",\n    figsize: Tuple[int, int] = (10, 10),\n    title: Optional[str] = None,\n    legend: bool = True,\n    basemap: bool = False,\n    basemap_type: str = \"streets\",\n    alpha: float = 0.7,\n    edge_color: str = \"black\",\n    classification: str = \"quantiles\",\n    n_classes: int = 5,\n    highlight_index: Optional[int] = None,\n    highlight_color: str = \"red\",\n    scheme: Optional[str] = None,\n    save_path: Optional[str] = None,\n    dpi: int = 300,\n) -&gt; Any:\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        column (str, optional): Column to use for choropleth mapping. If None,\n            a single color will be used. Defaults to None.\n        cmap (str or matplotlib.colors.Colormap, optional): Colormap to use for\n            choropleth mapping. Defaults to \"viridis\".\n        figsize (tuple, optional): Figure size as (width, height) in inches.\n            Defaults to (10, 10).\n        title (str, optional): Title for the plot. Defaults to None.\n        legend (bool, optional): Whether to display a legend. Defaults to True.\n        basemap (bool, optional): Whether to add a web basemap. Requires contextily.\n            Defaults to False.\n        basemap_type (str, optional): Type of basemap to use. Options: 'streets', 'satellite'.\n            Defaults to 'streets'.\n        alpha (float, optional): Transparency of the vector features, between 0-1.\n            Defaults to 0.7.\n        edge_color (str, optional): Color for feature edges. Defaults to \"black\".\n        classification (str, optional): Classification method for choropleth maps.\n            Options: \"quantiles\", \"equal_interval\", \"natural_breaks\".\n            Defaults to \"quantiles\".\n        n_classes (int, optional): Number of classes for choropleth maps.\n            Defaults to 5.\n        highlight_index (list, optional): List of indices to highlight.\n            Defaults to None.\n        highlight_color (str, optional): Color to use for highlighted features.\n            Defaults to \"red\".\n        scheme (str, optional): MapClassify classification scheme. Overrides\n            classification parameter if provided. Defaults to None.\n        save_path (str, optional): Path to save the figure. If None, the figure\n            is not saved. Defaults to None.\n        dpi (int, optional): DPI for saved figure. Defaults to 300.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing the plot.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n    \"\"\"\n    import contextily as ctx\n\n    if isinstance(vector_data, str):\n        vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    # Make a copy to avoid changing the original data\n    gdf = vector_data.copy()\n\n    # Set up figure and axis\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Determine geometry type\n    geom_type = gdf.geometry.iloc[0].geom_type\n\n    # Plotting parameters\n    plot_kwargs = {\"alpha\": alpha, \"ax\": ax}\n\n    # Set up keyword arguments based on geometry type\n    if \"Point\" in geom_type:\n        plot_kwargs[\"markersize\"] = 50\n        plot_kwargs[\"edgecolor\"] = edge_color\n    elif \"Line\" in geom_type:\n        plot_kwargs[\"linewidth\"] = 1\n    elif \"Polygon\" in geom_type:\n        plot_kwargs[\"edgecolor\"] = edge_color\n\n    # Classification options\n    if column is not None:\n        if scheme is not None:\n            # Use mapclassify scheme if provided\n            plot_kwargs[\"scheme\"] = scheme\n        else:\n            # Use classification parameter\n            if classification == \"quantiles\":\n                plot_kwargs[\"scheme\"] = \"quantiles\"\n            elif classification == \"equal_interval\":\n                plot_kwargs[\"scheme\"] = \"equal_interval\"\n            elif classification == \"natural_breaks\":\n                plot_kwargs[\"scheme\"] = \"fisher_jenks\"\n\n        plot_kwargs[\"k\"] = n_classes\n        plot_kwargs[\"cmap\"] = cmap\n        plot_kwargs[\"column\"] = column\n        plot_kwargs[\"legend\"] = legend\n\n    # Plot the main data\n    gdf.plot(**plot_kwargs)\n\n    # Highlight specific features if requested\n    if highlight_index is not None:\n        gdf.iloc[highlight_index].plot(\n            ax=ax, color=highlight_color, edgecolor=\"black\", linewidth=2, zorder=5\n        )\n\n    if basemap:\n        try:\n            basemap_options = {\n                \"streets\": ctx.providers.OpenStreetMap.Mapnik,\n                \"satellite\": ctx.providers.Esri.WorldImagery,\n            }\n            ctx.add_basemap(ax, crs=gdf.crs, source=basemap_options[basemap_type])\n        except Exception as e:\n            print(f\"Could not add basemap: {e}\")\n\n    # Set title if provided\n    if title:\n        ax.set_title(title, fontsize=14)\n\n    # Remove axes if not needed\n    ax.set_axis_off()\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save figure if a path is provided\n    if save_path:\n        plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n\n    return ax\n</code></pre>"},{"location":"geoai/#geoai.geoai.view_vector_interactive","title":"<code>view_vector_interactive(vector_data, layer_name='Vector Layer', tiles_args=None, **kwargs)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer. Defaults to \"Vector Layer\".</p> <code>'Vector Layer'</code> <code>tiles_args</code> <code>dict</code> <p>Additional arguments for the localtileserver client. get_folium_tile_layer function. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to GeoDataFrame.explore() function. See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>folium.Map: The map object with the vector data added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector_interactive(cities)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector_interactive(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    layer_name: str = \"Vector Layer\",\n    tiles_args: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        layer_name (str, optional): The name of the layer. Defaults to \"Vector Layer\".\n        tiles_args (dict, optional): Additional arguments for the localtileserver client.\n            get_folium_tile_layer function. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to GeoDataFrame.explore() function.\n            See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html\n\n    Returns:\n        folium.Map: The map object with the vector data added.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector_interactive(cities)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n    \"\"\"\n    import folium\n    import folium.plugins as plugins\n    from leafmap import cog_tile\n    from localtileserver import TileClient, get_folium_tile_layer\n\n    google_tiles = {\n        \"Roadmap\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Maps\",\n        },\n        \"Satellite\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Satellite\",\n        },\n        \"Terrain\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Terrain\",\n        },\n        \"Hybrid\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Hybrid\",\n        },\n    }\n\n    basemap_layer_name = None\n    raster_layer = None\n\n    if \"tiles\" in kwargs and isinstance(kwargs[\"tiles\"], str):\n        if kwargs[\"tiles\"].title() in google_tiles:\n            basemap_layer_name = google_tiles[kwargs[\"tiles\"].title()][\"name\"]\n            kwargs[\"tiles\"] = google_tiles[kwargs[\"tiles\"].title()][\"url\"]\n            kwargs[\"attr\"] = \"Google\"\n        elif kwargs[\"tiles\"].lower().endswith(\".tif\"):\n            if tiles_args is None:\n                tiles_args = {}\n            if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                basemap_layer_name = \"Remote Raster\"\n                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n                kwargs[\"attr\"] = \"TiTiler\"\n            else:\n                basemap_layer_name = \"Local Raster\"\n                client = TileClient(kwargs[\"tiles\"])\n                raster_layer = get_folium_tile_layer(client, **tiles_args)\n                kwargs[\"tiles\"] = raster_layer.tiles\n                kwargs[\"attr\"] = \"localtileserver\"\n\n    if \"max_zoom\" not in kwargs:\n        kwargs[\"max_zoom\"] = 30\n\n    if isinstance(vector_data, str):\n        if vector_data.endswith(\".parquet\"):\n            vector_data = gpd.read_parquet(vector_data)\n        else:\n            vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    layer_control = kwargs.pop(\"layer_control\", True)\n    fullscreen_control = kwargs.pop(\"fullscreen_control\", True)\n\n    m = vector_data.explore(**kwargs)\n\n    # Change the layer name\n    for layer in m._children.values():\n        if isinstance(layer, folium.GeoJson):\n            layer.layer_name = layer_name\n        if isinstance(layer, folium.TileLayer) and basemap_layer_name:\n            layer.layer_name = basemap_layer_name\n\n    if layer_control:\n        m.add_child(folium.LayerControl())\n\n    if fullscreen_control:\n        plugins.Fullscreen().add_to(m)\n\n    return m\n</code></pre>"},{"location":"geoai/#geoai.geoai.visualize_vector_by_attribute","title":"<code>visualize_vector_by_attribute(vector_path, attribute_name, cmap='viridis', figsize=(10, 8))</code>","text":"<p>Create a thematic map visualization of vector data based on an attribute.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to visualize</p> required <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if visualization was successful, False otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def visualize_vector_by_attribute(\n    vector_path: str,\n    attribute_name: str,\n    cmap: str = \"viridis\",\n    figsize: Tuple[int, int] = (10, 8),\n) -&gt; bool:\n    \"\"\"Create a thematic map visualization of vector data based on an attribute.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to visualize\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        bool: True if visualization was successful, False otherwise\n    \"\"\"\n    try:\n        # Read the vector data\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return False\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Determine plot type based on data type\n        if pd.api.types.is_numeric_dtype(gdf[attribute_name]):\n            # Continuous data\n            gdf.plot(column=attribute_name, cmap=cmap, legend=True, ax=ax)\n        else:\n            # Categorical data\n            gdf.plot(column=attribute_name, categorical=True, legend=True, ax=ax)\n\n        # Add title and labels\n        ax.set_title(f\"{os.path.basename(vector_path)} - {attribute_name}\")\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Add basemap or additional elements if available\n        # Note: Additional options could be added here for more complex maps\n\n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"Error visualizing data: {str(e)}\")\n</code></pre>"},{"location":"geoai/#geoai.geoai.write_colormap","title":"<code>write_colormap(image, colormap, output=None)</code>","text":"<p>Write a colormap to an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray]</code> <p>The image to write the colormap to.</p> required <code>colormap</code> <code>Union[str, Dict]</code> <p>The colormap to write to the image.</p> required <code>output</code> <code>Optional[str]</code> <p>The output file path.</p> <code>None</code> Source code in <code>geoai/utils.py</code> <pre><code>def write_colormap(\n    image: Union[str, np.ndarray],\n    colormap: Union[str, Dict],\n    output: Optional[str] = None,\n) -&gt; Optional[str]:\n    \"\"\"Write a colormap to an image.\n\n    Args:\n        image: The image to write the colormap to.\n        colormap: The colormap to write to the image.\n        output: The output file path.\n    \"\"\"\n    if isinstance(colormap, str):\n        colormap = leafmap.get_image_colormap(colormap)\n    leafmap.write_image_colormap(image, colormap, output)\n</code></pre>"},{"location":"hf/","title":"hf module","text":"<p>This module contains utility functions for working with Hugging Face models.</p>"},{"location":"hf/#geoai.hf.get_model_config","title":"<code>get_model_config(model_id)</code>","text":"<p>Get the model configuration for a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The Hugging Face model ID.</p> required <p>Returns:</p> Type Description <code>PretrainedConfig</code> <p>transformers.configuration_utils.PretrainedConfig: The model configuration.</p> Source code in <code>geoai/hf.py</code> <pre><code>def get_model_config(\n    model_id: str,\n) -&gt; \"transformers.configuration_utils.PretrainedConfig\":\n    \"\"\"\n    Get the model configuration for a Hugging Face model.\n\n    Args:\n        model_id (str): The Hugging Face model ID.\n\n    Returns:\n        transformers.configuration_utils.PretrainedConfig: The model configuration.\n    \"\"\"\n    return AutoConfig.from_pretrained(model_id)\n</code></pre>"},{"location":"hf/#geoai.hf.get_model_input_channels","title":"<code>get_model_input_channels(model_id)</code>","text":"<p>Check the number of input channels supported by a Hugging Face model.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The Hugging Face model ID.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of input channels the model accepts.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unable to determine the number of input channels.</p> Source code in <code>geoai/hf.py</code> <pre><code>def get_model_input_channels(model_id: str) -&gt; int:\n    \"\"\"\n    Check the number of input channels supported by a Hugging Face model.\n\n    Args:\n        model_id (str): The Hugging Face model ID.\n\n    Returns:\n        int: The number of input channels the model accepts.\n\n    Raises:\n        ValueError: If unable to determine the number of input channels.\n    \"\"\"\n    # Load the model configuration\n    config = AutoConfig.from_pretrained(model_id)\n\n    # For Mask2Former models\n    if hasattr(config, \"backbone_config\"):\n        if hasattr(config.backbone_config, \"num_channels\"):\n            return config.backbone_config.num_channels\n\n    # Try to load the model and inspect its architecture\n    try:\n        model = AutoModelForMaskedImageModeling.from_pretrained(model_id)\n\n        # For Swin Transformer-based models like Mask2Former\n        if hasattr(model, \"backbone\") and hasattr(model.backbone, \"embeddings\"):\n            if hasattr(model.backbone.embeddings, \"patch_embeddings\"):\n                # Swin models typically have patch embeddings that indicate channel count\n                return model.backbone.embeddings.patch_embeddings.in_channels\n    except Exception as e:\n        print(f\"Couldn't inspect model architecture: {e}\")\n\n    # Default for most vision models\n    return 3\n</code></pre>"},{"location":"hf/#geoai.hf.image_segmentation","title":"<code>image_segmentation(tif_path, output_path, labels_to_extract=None, dtype='uint8', model_name=None, segmenter_args=None, **kwargs)</code>","text":"<p>Segments an image with a Hugging Face segmentation model and saves the results as a single georeferenced image where each class has a unique integer value.</p> <p>Parameters:</p> Name Type Description Default <code>tif_path</code> <code>str</code> <p>Path to the input georeferenced TIF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output georeferenced segmentation will be saved.</p> required <code>labels_to_extract</code> <code>list</code> <p>List of labels to extract. If None, extracts all labels.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Data type to use for the output mask. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>model_name</code> <code>str</code> <p>Name of the Hugging Face model to use for segmentation, such as \"facebook/mask2former-swin-large-cityscapes-semantic\". Defaults to None. See https://huggingface.co/models?pipeline_tag=image-segmentation&amp;sort=trending for options.</p> <code>None</code> <code>segmenter_args</code> <code>dict</code> <p>Additional arguments to pass to the segmenter. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the segmentation pipeline</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>str</code> <p>(Path to saved image, dictionary mapping label names to their assigned values, dictionary mapping label names to confidence scores)</p> Source code in <code>geoai/hf.py</code> <pre><code>def image_segmentation(\n    tif_path: str,\n    output_path: str,\n    labels_to_extract: Optional[List[str]] = None,\n    dtype: str = \"uint8\",\n    model_name: Optional[str] = None,\n    segmenter_args: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Segments an image with a Hugging Face segmentation model and saves the results\n    as a single georeferenced image where each class has a unique integer value.\n\n    Args:\n        tif_path (str): Path to the input georeferenced TIF file.\n        output_path (str): Path where the output georeferenced segmentation will be saved.\n        labels_to_extract (list, optional): List of labels to extract. If None, extracts all labels.\n        dtype (str, optional): Data type to use for the output mask. Defaults to \"uint8\".\n        model_name (str, optional): Name of the Hugging Face model to use for segmentation,\n            such as \"facebook/mask2former-swin-large-cityscapes-semantic\". Defaults to None.\n            See https://huggingface.co/models?pipeline_tag=image-segmentation&amp;sort=trending for options.\n        segmenter_args (dict, optional): Additional arguments to pass to the segmenter.\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the segmentation pipeline\n\n    Returns:\n        tuple: (Path to saved image, dictionary mapping label names to their assigned values,\n            dictionary mapping label names to confidence scores)\n    \"\"\"\n    # Load the original georeferenced image to extract metadata\n    with rasterio.open(tif_path) as src:\n        # Save the metadata for later use\n        meta = src.meta.copy()\n        # Get the dimensions\n        height = src.height\n        width = src.width\n        # Get the transform and CRS for georeferencing\n        # transform = src.transform\n        # crs = src.crs\n\n    # Initialize the segmentation pipeline\n    if model_name is None:\n        model_name = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n\n    kwargs[\"task\"] = \"image-segmentation\"\n\n    segmenter = pipeline(model=model_name, **kwargs)\n\n    # Run the segmentation on the GeoTIFF\n    if segmenter_args is None:\n        segmenter_args = {}\n\n    segments = segmenter(tif_path, **segmenter_args)\n\n    # If no specific labels are requested, extract all available ones\n    if labels_to_extract is None:\n        labels_to_extract = [segment[\"label\"] for segment in segments]\n\n    # Create an empty mask to hold all the labels\n    # Using uint8 for up to 255 classes, switch to uint16 for more\n    combined_mask = np.zeros((height, width), dtype=np.uint8)\n\n    # Create a dictionary to map labels to values and store scores\n    label_to_value = {}\n    label_to_score = {}\n\n    # Process each segment we want to keep\n    for i, segment in enumerate(\n        [s for s in segments if s[\"label\"] in labels_to_extract]\n    ):\n        # Assign a unique value to each label (starting from 1)\n        value = i + 1\n        label = segment[\"label\"]\n        score = segment[\"score\"]\n\n        label_to_value[label] = value\n        label_to_score[label] = score\n\n        # Convert PIL image to numpy array\n        mask = np.array(segment[\"mask\"])\n\n        # Apply a threshold if it's a probability mask (not binary)\n        if mask.dtype == float:\n            mask = (mask &gt; 0.5).astype(np.uint8)\n\n        # Resize if needed to match original dimensions\n        if mask.shape != (height, width):\n            mask_img = Image.fromarray(mask)\n            mask_img = mask_img.resize((width, height))\n            mask = np.array(mask_img)\n\n        # Add this class to the combined mask\n        # Only overwrite if the pixel isn't already assigned to another class\n        # This handles overlapping segments by giving priority to earlier segments\n        combined_mask = np.where(\n            (mask &gt; 0) &amp; (combined_mask == 0), value, combined_mask\n        )\n\n    # Update metadata for the output raster\n    meta.update(\n        {\n            \"count\": 1,  # One band for the mask\n            \"dtype\": dtype,  # Use uint8 for up to 255 classes\n            \"nodata\": 0,  # 0 represents no class\n        }\n    )\n\n    # Save the mask as a new georeferenced GeoTIFF\n    with rasterio.open(output_path, \"w\", **meta) as dst:\n        dst.write(combined_mask[np.newaxis, :, :])  # Add channel dimension\n\n    # Create a CSV colormap file with scores included\n    csv_path = os.path.splitext(output_path)[0] + \"_colormap.csv\"\n    with open(csv_path, \"w\", newline=\"\") as csvfile:\n        fieldnames = [\"ClassValue\", \"ClassName\", \"ConfidenceScore\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n\n        writer.writeheader()\n        for label, value in label_to_value.items():\n            writer.writerow(\n                {\n                    \"ClassValue\": value,\n                    \"ClassName\": label,\n                    \"ConfidenceScore\": f\"{label_to_score[label]:.4f}\",\n                }\n            )\n\n    return output_path, label_to_value, label_to_score\n</code></pre>"},{"location":"hf/#geoai.hf.mask_generation","title":"<code>mask_generation(input_path, output_mask_path, output_csv_path, model='facebook/sam-vit-base', confidence_threshold=0.5, points_per_side=32, crop_size=None, batch_size=1, band_indices=None, min_object_size=0, generator_kwargs=None, **kwargs)</code>","text":"<p>Process a GeoTIFF using SAM mask generation and save results as a GeoTIFF and CSV.</p> <p>The function reads a GeoTIFF image, applies the SAM mask generator from the Hugging Face transformers pipeline, rasterizes the resulting masks to create a labeled mask GeoTIFF, and saves mask scores and geometries to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF image.</p> required <code>output_mask_path</code> <code>str</code> <p>Path where the output mask GeoTIFF will be saved.</p> required <code>output_csv_path</code> <code>str</code> <p>Path where the mask scores CSV will be saved.</p> required <code>model</code> <code>str</code> <p>HuggingFace model checkpoint for the SAM model.</p> <code>'facebook/sam-vit-base'</code> <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence score for masks to be included.</p> <code>0.5</code> <code>points_per_side</code> <code>int</code> <p>Number of points to sample along each side of the image.</p> <code>32</code> <code>crop_size</code> <code>Optional[int]</code> <p>Size of image crops for processing. If None, process the full image.</p> <code>None</code> <code>band_indices</code> <code>Optional[List[int]]</code> <p>List of band indices to use. If None, use all bands.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>1</code> <code>min_object_size</code> <code>int</code> <p>Minimum size in pixels for objects to be included. Smaller masks will be filtered out.</p> <code>0</code> <code>generator_kwargs</code> <code>Optional[Dict]</code> <p>Additional keyword arguments to pass to the mask generator.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple containing the paths to the saved mask GeoTIFF and CSV file.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input file cannot be opened or processed.</p> <code>RuntimeError</code> <p>If mask generation fails.</p> Source code in <code>geoai/hf.py</code> <pre><code>def mask_generation(\n    input_path: str,\n    output_mask_path: str,\n    output_csv_path: str,\n    model: str = \"facebook/sam-vit-base\",\n    confidence_threshold: float = 0.5,\n    points_per_side: int = 32,\n    crop_size: Optional[int] = None,\n    batch_size: int = 1,\n    band_indices: Optional[List[int]] = None,\n    min_object_size: int = 0,\n    generator_kwargs: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Process a GeoTIFF using SAM mask generation and save results as a GeoTIFF and CSV.\n\n    The function reads a GeoTIFF image, applies the SAM mask generator from the\n    Hugging Face transformers pipeline, rasterizes the resulting masks to create\n    a labeled mask GeoTIFF, and saves mask scores and geometries to a CSV file.\n\n    Args:\n        input_path: Path to the input GeoTIFF image.\n        output_mask_path: Path where the output mask GeoTIFF will be saved.\n        output_csv_path: Path where the mask scores CSV will be saved.\n        model: HuggingFace model checkpoint for the SAM model.\n        confidence_threshold: Minimum confidence score for masks to be included.\n        points_per_side: Number of points to sample along each side of the image.\n        crop_size: Size of image crops for processing. If None, process the full image.\n        band_indices: List of band indices to use. If None, use all bands.\n        batch_size: Batch size for inference.\n        min_object_size: Minimum size in pixels for objects to be included. Smaller masks will be filtered out.\n        generator_kwargs: Additional keyword arguments to pass to the mask generator.\n\n    Returns:\n        Tuple containing the paths to the saved mask GeoTIFF and CSV file.\n\n    Raises:\n        ValueError: If the input file cannot be opened or processed.\n        RuntimeError: If mask generation fails.\n    \"\"\"\n    # Set up the mask generator\n    print(\"Setting up mask generator...\")\n    mask_generator = pipeline(model=model, task=\"mask-generation\", **kwargs)\n\n    # Open the GeoTIFF file\n    try:\n        print(f\"Reading input GeoTIFF: {input_path}\")\n        with rasterio.open(input_path) as src:\n            # Read metadata\n            profile = src.profile\n            # transform = src.transform\n            # crs = src.crs\n\n            # Read the image data\n            if band_indices is not None:\n                print(f\"Using specified bands: {band_indices}\")\n                image_data = np.stack([src.read(i + 1) for i in band_indices])\n            else:\n                print(\"Using all bands\")\n                image_data = src.read()\n\n            # Handle image with more than 3 bands (convert to RGB for visualization)\n            if image_data.shape[0] &gt; 3:\n                print(\n                    f\"Converting {image_data.shape[0]} bands to RGB (using first 3 bands)\"\n                )\n                # Select first three bands or perform other band combination\n                image_data = image_data[:3]\n            elif image_data.shape[0] == 1:\n                print(\"Duplicating single band to create 3-band image\")\n                # Duplicate single band to create a 3-band image\n                image_data = np.vstack([image_data] * 3)\n\n            # Transpose to HWC format for the model\n            image_data = np.transpose(image_data, (1, 2, 0))\n\n            # Normalize the image if needed\n            if image_data.dtype != np.uint8:\n                print(f\"Normalizing image from {image_data.dtype} to uint8\")\n                image_data = (image_data / image_data.max() * 255).astype(np.uint8)\n    except Exception as e:\n        raise ValueError(f\"Failed to open or process input GeoTIFF: {e}\")\n\n    # Process the image with the mask generator\n    try:\n        # Convert numpy array to PIL Image for the pipeline\n        # Ensure the array is in the right format (HWC and uint8)\n        if image_data.dtype != np.uint8:\n            image_data = (image_data / image_data.max() * 255).astype(np.uint8)\n\n        # Create a PIL Image from the numpy array\n        print(\"Converting to PIL Image for mask generation\")\n        pil_image = Image.fromarray(image_data)\n\n        # Use the SAM pipeline for mask generation\n        if generator_kwargs is None:\n            generator_kwargs = {}\n\n        print(\"Running mask generation...\")\n        mask_results = mask_generator(\n            pil_image,\n            points_per_side=points_per_side,\n            crop_n_points_downscale_factor=1 if crop_size is None else 2,\n            point_grids=None,\n            pred_iou_thresh=confidence_threshold,\n            stability_score_thresh=confidence_threshold,\n            crops_n_layers=0 if crop_size is None else 1,\n            crop_overlap_ratio=0.5,\n            batch_size=batch_size,\n            **generator_kwargs,\n        )\n\n        print(\n            f\"Number of initial masks: {len(mask_results['masks']) if isinstance(mask_results, dict) and 'masks' in mask_results else len(mask_results)}\"\n        )\n\n    except Exception as e:\n        raise RuntimeError(f\"Mask generation failed: {e}\")\n\n    # Create a mask raster with unique IDs for each mask\n    mask_raster = np.zeros((image_data.shape[0], image_data.shape[1]), dtype=np.uint32)\n    mask_records = []\n\n    # Process each mask based on the structure of mask_results\n    if (\n        isinstance(mask_results, dict)\n        and \"masks\" in mask_results\n        and \"scores\" in mask_results\n    ):\n        # Handle dictionary with 'masks' and 'scores' lists\n        print(\"Processing masks...\")\n        total_masks = len(mask_results[\"masks\"])\n\n        # Create progress bar\n        for i, (mask_data, score) in enumerate(\n            tqdm(\n                zip(mask_results[\"masks\"], mask_results[\"scores\"]),\n                total=total_masks,\n                desc=\"Processing masks\",\n            )\n        ):\n            mask_id = i + 1  # Start IDs at 1\n\n            # Convert to numpy if not already\n            if not isinstance(mask_data, np.ndarray):\n                # Try to convert from tensor or other format if needed\n                try:\n                    mask_data = np.array(mask_data)\n                except:\n                    print(f\"Could not convert mask at index {i} to numpy array\")\n                    continue\n\n            mask_binary = mask_data.astype(bool)\n            area_pixels = np.sum(mask_binary)\n\n            # Skip if mask is smaller than the minimum size\n            if area_pixels &lt; min_object_size:\n                continue\n\n            # Add the mask to the raster with a unique ID\n            mask_raster[mask_binary] = mask_id\n\n            # Create a record for the CSV - without geometry calculation\n            mask_records.append(\n                {\"mask_id\": mask_id, \"score\": float(score), \"area_pixels\": area_pixels}\n            )\n    elif isinstance(mask_results, list):\n        # Handle list of dictionaries format (SAM original format)\n        print(\"Processing masks...\")\n        total_masks = len(mask_results)\n\n        # Create progress bar\n        for i, mask_result in enumerate(tqdm(mask_results, desc=\"Processing masks\")):\n            mask_id = i + 1  # Start IDs at 1\n\n            # Try different possible key names for masks and scores\n            mask_data = None\n            score = None\n\n            if isinstance(mask_result, dict):\n                # Try to find mask data\n                if \"segmentation\" in mask_result:\n                    mask_data = mask_result[\"segmentation\"]\n                elif \"mask\" in mask_result:\n                    mask_data = mask_result[\"mask\"]\n\n                # Try to find score\n                if \"score\" in mask_result:\n                    score = mask_result[\"score\"]\n                elif \"predicted_iou\" in mask_result:\n                    score = mask_result[\"predicted_iou\"]\n                elif \"stability_score\" in mask_result:\n                    score = mask_result[\"stability_score\"]\n                else:\n                    score = 1.0  # Default score if none found\n            else:\n                # If mask_result is not a dict, it might be the mask directly\n                try:\n                    mask_data = np.array(mask_result)\n                    score = 1.0  # Default score\n                except:\n                    print(f\"Could not process mask at index {i}\")\n                    continue\n\n            if mask_data is not None:\n                # Convert to numpy if not already\n                if not isinstance(mask_data, np.ndarray):\n                    try:\n                        mask_data = np.array(mask_data)\n                    except:\n                        print(f\"Could not convert mask at index {i} to numpy array\")\n                        continue\n\n                mask_binary = mask_data.astype(bool)\n                area_pixels = np.sum(mask_binary)\n\n                # Skip if mask is smaller than the minimum size\n                if area_pixels &lt; min_object_size:\n                    continue\n\n                # Add the mask to the raster with a unique ID\n                mask_raster[mask_binary] = mask_id\n\n                # Create a record for the CSV - without geometry calculation\n                mask_records.append(\n                    {\n                        \"mask_id\": mask_id,\n                        \"score\": float(score),\n                        \"area_pixels\": area_pixels,\n                    }\n                )\n    else:\n        # If we couldn't figure out the format, raise an error\n        raise ValueError(f\"Unexpected format for mask_results: {type(mask_results)}\")\n\n    print(f\"Number of final masks (after size filtering): {len(mask_records)}\")\n\n    # Save the mask raster as a GeoTIFF\n    print(f\"Saving mask GeoTIFF to {output_mask_path}\")\n    output_profile = profile.copy()\n    output_profile.update(dtype=rasterio.uint32, count=1, compress=\"lzw\", nodata=0)\n\n    with rasterio.open(output_mask_path, \"w\", **output_profile) as dst:\n        dst.write(mask_raster.astype(rasterio.uint32), 1)\n\n    # Save the mask data as a CSV\n    print(f\"Saving mask metadata to {output_csv_path}\")\n    mask_df = pd.DataFrame(mask_records)\n    mask_df.to_csv(output_csv_path, index=False)\n\n    print(\"Processing complete!\")\n    return output_mask_path, output_csv_path\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers various methods for installing GeoAI on different platforms with different package managers.</p>"},{"location":"installation/#prerequisites","title":"\u2705 Prerequisites","text":"<p>GeoAI requires:</p> <ul> <li>Python 3.10 or above</li> <li>The required dependencies will be installed automatically</li> </ul>"},{"location":"installation/#recommended-installation-methods","title":"\ud83d\ude80 Recommended Installation Methods","text":""},{"location":"installation/#using-pip","title":"\ud83d\udc0d Using pip","text":"<p>The simplest way to install the latest stable release of GeoAI is via pip:</p> <pre><code>pip install geoai-py\n</code></pre> <p>To install GeoAI with all optional dependencies for additional features:</p> <pre><code>pip install \"geoai-py[all]\"\n</code></pre>"},{"location":"installation/#using-uv","title":"\ud83d\udc0d Using uv","text":"<p>To install the latest stable release of GeoAI with uv, a faster alternative to pip:</p> <pre><code>uv pip install geoai-py\n</code></pre>"},{"location":"installation/#using-conda","title":"\ud83d\udc3c Using conda","text":"<p>For Anaconda/Miniconda users, we recommend installing GeoAI via conda-forge, which handles dependencies like GDAL more elegantly:</p> <pre><code>conda install -c conda-forge geoai\n</code></pre>"},{"location":"installation/#using-mamba","title":"\ud83e\udda1 Using mamba","text":"<p>Mamba provides faster dependency resolution compared to conda. This is especially useful for large packages like GeoAI:</p> <pre><code>conda create -n geo python=3.12\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge geoai\n</code></pre>"},{"location":"installation/#advanced-installation-options","title":"\ud83d\udd27 Advanced Installation Options","text":""},{"location":"installation/#gpu-support","title":"\ud83d\udda5\ufe0f GPU Support","text":"<p>To enable GPU acceleration for deep learning models (requires NVIDIA GPU):</p> <pre><code>mamba install -c conda-forge geoai \"pytorch=*=cuda*\"\n</code></pre> <p>This will install the appropriate PyTorch version with CUDA support.</p> <p>If you run into issues with the ipympl package, you can install it using the following command:</p> <pre><code>mamba install -c conda-forge geoai \"pytorch=*=cuda*\" jupyterlab ipympl\n</code></pre> <p>If you encounter issues with the sqlite package, you can update it using the following command:</p> <pre><code>mamba update -c conda-forge sqlite\n</code></pre>"},{"location":"installation/#notes-for-windows-users","title":"Notes for Windows Users","text":"<p>If you use mamba to install geoai, you may not have the latest version of torchgeo, which may cause issues when importing geoai. To fix this, you can install the latest version of torchgeo using the following command:</p> <pre><code>pip install -U torchgeo\n</code></pre>"},{"location":"installation/#development-installation","title":"\ud83d\udc69\u200d\ud83d\udcbb Development Installation","text":"<p>For contributing to GeoAI development, install directly from the source repository:</p> <pre><code>git clone https://github.com/opengeos/geoai.git\ncd geoai\npip install -e .\n</code></pre> <p>The <code>-e</code> flag installs the package in development mode, allowing you to modify the code and immediately see the effects.</p>"},{"location":"installation/#installing-from-github","title":"\ud83d\udce6 Installing from GitHub","text":"<p>To install the latest development version directly from GitHub:</p> <pre><code>pip install git+https://github.com/opengeos/geoai.git\n</code></pre> <p>For a specific branch:</p> <pre><code>pip install git+https://github.com/opengeos/geoai.git@branch-name\n</code></pre>"},{"location":"installation/#verifying-installation","title":"\u2713 Verifying Installation","text":"<p>To verify your installation, run:</p> <pre><code>import geoai\nprint(geoai.__version__)\n</code></pre>"},{"location":"installation/#troubleshooting","title":"\u26a0\ufe0f Troubleshooting","text":"<p>If you encounter installation problems:</p> <ol> <li>Search for similar issues in our GitHub Issues</li> <li>Ask for help in our GitHub Discussions</li> </ol>"},{"location":"installation/#upgrading","title":"\ud83d\udd04 Upgrading","text":"<p>To upgrade GeoAI to the latest version:</p> <pre><code>pip install -U geoai-py\n</code></pre> <p>Or with conda:</p> <pre><code>conda update -c conda-forge geoai\n</code></pre>"},{"location":"map_tools/","title":"map_tools module","text":""},{"location":"map_tools/#geoai.agents.map_tools.MapSession","title":"<code>MapSession</code>","text":"<p>Manages a leafmap session with map instance.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>class MapSession:\n    \"\"\"Manages a leafmap session with map instance.\"\"\"\n\n    def __init__(self, m: Optional[leafmap.Map] = None) -&gt; None:\n        \"\"\"Initialize map session.\n\n        Args:\n            m: Optional existing map instance. If None, creates a default map.\n        \"\"\"\n        # allow user to pass a map, otherwise create a default\n        self.m: leafmap.Map = m or leafmap.Map(style=\"liberty\", projection=\"globe\")\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapSession.__init__","title":"<code>__init__(m=None)</code>","text":"<p>Initialize map session.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>Optional[Map]</code> <p>Optional existing map instance. If None, creates a default map.</p> <code>None</code> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>def __init__(self, m: Optional[leafmap.Map] = None) -&gt; None:\n    \"\"\"Initialize map session.\n\n    Args:\n        m: Optional existing map instance. If None, creates a default map.\n    \"\"\"\n    # allow user to pass a map, otherwise create a default\n    self.m: leafmap.Map = m or leafmap.Map(style=\"liberty\", projection=\"globe\")\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools","title":"<code>MapTools</code>","text":"<p>Collection of tools for interacting with leafmap instances.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>class MapTools:\n    \"\"\"Collection of tools for interacting with leafmap instances.\"\"\"\n\n    def __init__(self, session: Optional[MapSession] = None) -&gt; None:\n        \"\"\"Initialize map tools.\n\n        Args:\n            session: Optional MapSession instance. If None, creates a default session.\n        \"\"\"\n        self.session: MapSession = session or MapSession()\n\n    @tool(\n        description=\"Create or reset a Leafmap map with optional center/zoom and basemap.\"\n    )\n    def create_map(\n        self,\n        center_lat: float = 20.0,\n        center_lon: float = 0.0,\n        zoom: int = 2,\n        style: str = \"liberty\",\n        projection: str = \"globe\",\n        use_message_queue: bool = True,\n    ) -&gt; str:\n        \"\"\"Create or reset a Leafmap map with specified parameters.\n\n        Args:\n            center_lat: Latitude for map center (default: 20.0).\n            center_lon: Longitude for map center (default: 0.0).\n            zoom: Initial zoom level (default: 2).\n            style: Map style name (default: \"liberty\").\n            projection: Map projection (default: \"globe\").\n            use_message_queue: Whether to use message queue (default: True).\n\n        Returns:\n            Confirmation message.\n        \"\"\"\n        self.session.m = leafmap.Map(\n            center=[center_lon, center_lat],\n            zoom=zoom,\n            style=style,\n            projection=projection,\n            use_message_queue=use_message_queue,\n        )\n        self.session.m.create_container()\n        return \"Map created.\"\n\n    @tool(description=\"Add a basemap by name\")\n    def add_basemap(self, name: str) -&gt; str:\n        \"\"\"Add a basemap to the map by name.\n\n        Args:\n            name: Name of the basemap to add.\n\n        Returns:\n            Confirmation message with basemap name.\n        \"\"\"\n        self.session.m.add_basemap(name)\n        return f\"Basemap added: {name}\"\n\n    @tool(description=\"Add a vector dataset (GeoJSON, Shapefile, etc.)\")\n    def add_vector(self, data: str, name: Optional[str] = None) -&gt; str:\n        \"\"\"Add a vector dataset to the map.\n\n        Args:\n            data: Path or URL to the vector data file.\n            name: Optional name for the layer.\n\n        Returns:\n            Confirmation message with layer name.\n        \"\"\"\n        self.session.m.add_vector(data=data, name=name)\n        return f\"Vector added: {name}\"\n\n    @tool(description=\"Fly to a specific location\")\n    def fly_to(self, longitude: float, latitude: float, zoom: int = 12) -&gt; str:\n        \"\"\"Fly to a specific geographic location.\n\n        Args:\n            longitude: Target longitude coordinate.\n            latitude: Target latitude coordinate.\n            zoom: Zoom level for the target location (default: 12).\n\n        Returns:\n            Confirmation message with coordinates and zoom level.\n        \"\"\"\n        self.session.m.fly_to(longitude, latitude, zoom)\n        return f\"Flown to: {longitude}, {latitude}, zoom {zoom}\"\n\n    @tool(description=\"Add Cloud Optimized GeoTIFF (COG) to the map\")\n    def add_cog_layer(\n        self,\n        url: str,\n        name: Optional[str] = None,\n        attribution: str = \"TiTiler\",\n        opacity: float = 1.0,\n        visible: bool = True,\n        bands: Optional[List[int]] = None,\n        nodata: Optional[Union[int, float]] = 0,\n        titiler_endpoint: str = None,\n    ) -&gt; str:\n        \"\"\"Add a Cloud Optimized GeoTIFF (COG) layer to the map.\n\n        Args:\n            url: URL to the COG file.\n            name: Optional name for the layer.\n            attribution: Attribution text (default: \"TiTiler\").\n            opacity: Layer opacity from 0.0 to 1.0 (default: 1.0).\n            visible: Whether the layer is initially visible (default: True).\n            bands: Optional list of band indices to display.\n            nodata: No data value (default: 0).\n            titiler_endpoint: TiTiler endpoint URL (default: \"https://giswqs-titiler-endpoint.hf.space\").\n\n        Returns:\n            Confirmation message with COG URL.\n        \"\"\"\n        self.session.m.add_cog_layer(\n            url, name, attribution, opacity, visible, bands, nodata, titiler_endpoint\n        )\n        return f\"COG layer added: {url}\"\n\n    @tool(description=\"Remove a layer by name\")\n    def remove_layer(self, name: str) -&gt; str:\n        \"\"\"Remove a layer from the map by name.\n\n        Args:\n            name: Name of the layer to remove.\n\n        Returns:\n            Confirmation message with removed layer name.\n        \"\"\"\n        layer_names = self.session.m.get_layer_names()\n        if name in layer_names:\n            self.session.m.remove_layer(name)\n            return f\"Removed: {name}\"\n        else:\n            for layer_name in layer_names:\n                if name.lower() in layer_name.lower():\n                    self.session.m.remove_layer(layer_name)\n                    return f\"Removed: {layer_name}\"\n            return f\"Layer {name} not found\"\n\n    @tool(description=\"Add 3D buildings from Overture Maps to the map\")\n    def add_overture_3d_buildings(\n        self,\n        release: Optional[str] = None,\n        style: Optional[Dict[str, Any]] = None,\n        values: Optional[List[int]] = None,\n        colors: Optional[List[str]] = None,\n        visible: bool = True,\n        opacity: float = 1.0,\n        tooltip: bool = True,\n        template: str = \"simple\",\n        fit_bounds: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Add 3D buildings from Overture Maps to the map.\n\n        Args:\n            release (Optional[str], optional): The release date of the Overture Maps data.\n                Defaults to the latest release. For more info, see\n                https://github.com/OvertureMaps/overture-tiles.\n            style (Optional[Dict[str, Any]], optional): The style dictionary for\n                the buildings. Defaults to None.\n            values (Optional[List[int]], optional): List of height values for\n                color interpolation. Defaults to None.\n            colors (Optional[List[str]], optional): List of colors corresponding\n                to the height values. Defaults to None.\n            visible (bool, optional): Whether the buildings layer is visible.\n                Defaults to True.\n            opacity (float, optional): The opacity of the buildings layer.\n                Defaults to 1.0.\n            tooltip (bool, optional): Whether to show tooltips on the buildings.\n                Defaults to True.\n            template (str, optional): The template for the tooltip. It can be\n                \"simple\" or \"all\". Defaults to \"simple\".\n            fit_bounds (bool, optional): Whether to fit the map bounds to the\n                buildings layer. Defaults to False.\n\n        Raises:\n            ValueError: If the length of values and colors lists are not the same.\n        \"\"\"\n        self.session.m.add_overture_3d_buildings(\n            release=release,\n            style=style,\n            values=values,\n            colors=colors,\n            visible=visible,\n            opacity=opacity,\n            tooltip=tooltip,\n            template=template,\n            fit_bounds=fit_bounds,\n            **kwargs,\n        )\n        return f\"Overture 3D buildings added: {release}\"\n\n    @tool(description=\"Set the pitch of the map\")\n    def set_pitch(self, pitch: float) -&gt; None:\n        \"\"\"\n        Sets the pitch of the map.\n\n        This function sets the pitch of the map to the specified value. The pitch is the\n        angle of the camera measured in degrees where 0 is looking straight down, and 60 is\n        looking towards the horizon. Additional keyword arguments can be provided to control\n        the pitch. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#setpitch\n\n        Args:\n            pitch (float): The pitch value to set.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.set_pitch(pitch=pitch)\n        return f\"Map pitched to: {pitch}\"\n\n    @tool\n    def add_draw_control(\n        self,\n        options: Optional[Dict[str, Any]] = None,\n        controls: Optional[Dict[str, Any]] = None,\n        position: str = \"top-right\",\n        geojson: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds a drawing control to the map.\n\n        This method enables users to add interactive drawing controls to the map,\n        allowing for the creation, editing, and deletion of geometric shapes on\n        the map. The options, position, and initial GeoJSON can be customized.\n\n        Args:\n            options (Optional[Dict[str, Any]]): Configuration options for the\n                drawing control. Defaults to None.\n            controls (Optional[Dict[str, Any]]): The drawing controls to enable.\n                Can be one or more of the following: 'polygon', 'line_string',\n                'point', 'trash', 'combine_features', 'uncombine_features'.\n                Defaults to None.\n            position (str): The position of the control on the map. Defaults\n                to \"top-right\".\n            geojson (Optional[Dict[str, Any]]): Initial GeoJSON data to load\n                into the drawing control. Defaults to None.\n            **kwargs (Any): Additional keyword arguments to be passed to the\n                drawing control.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_draw_control(\n            options=options,\n            controls=controls,\n            position=position,\n            geojson=geojson,\n            **kwargs,\n        )\n        return f\"Draw control added: {position}\"\n\n    @tool\n    def add_vector_tile(\n        self,\n        url: str,\n        layer_id: str,\n        layer_type: str = \"fill\",\n        source_layer: Optional[str] = None,\n        name: Optional[str] = None,\n        paint: Optional[Dict] = None,\n        layout: Optional[Dict] = None,\n        filter: Optional[Dict] = None,\n        minzoom: Optional[int] = None,\n        maxzoom: Optional[int] = None,\n        visible: bool = True,\n        opacity: float = 1.0,\n        add_popup: bool = True,\n        before_id: Optional[str] = None,\n        source_args: Dict = None,\n        overwrite: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds a vector tile layer to the map.\n\n        This method adds a vector tile layer to the map using a vector tile source.\n        Vector tiles are a data format for efficiently storing and transmitting\n        vector map data.\n\n        Args:\n            url (str): The URL template for the vector tiles. Should contain {z}, {x},\n                and {y} placeholders for tile coordinates.\n            layer_id (str): The ID of the layer within the vector tile source.\n            layer_type (str, optional): The type of layer to create. Can be 'fill',\n                'line', 'symbol', 'circle', etc. Defaults to 'fill'.\n            source_layer (str, optional): The name of the source layer within the\n                vector tiles. If None, uses layer_id.\n            name (str, optional): The name to use for the layer. If None, uses layer_id.\n            paint (dict, optional): Paint properties for the layer. If None, uses\n                default styling based on layer_type.\n            layout (dict, optional): Layout properties for the layer.\n            filter (dict, optional): Filter expression for the layer.\n            minzoom (int, optional): Minimum zoom level for the layer.\n            maxzoom (int, optional): Maximum zoom level for the layer.\n            visible (bool, optional): Whether the layer should be visible by default.\n                Defaults to True.\n            opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n            add_popup (bool, optional): Whether to add a popup to the layer. Defaults to True.\n            before_id (str, optional): The ID of an existing layer before which the\n                new layer should be inserted.\n            source_args (dict, optional): Additional keyword arguments passed to the\n                vector tile source.\n            overwrite (bool, optional): Whether to overwrite an existing layer with\n                the same name. Defaults to False.\n            **kwargs: Additional keyword arguments passed to the Layer class.\n\n        Returns:\n            None\n\n        Example:\n            &gt;&gt;&gt; m = Map()\n            &gt;&gt;&gt; m.add_vector_tile(\n            ...     url=\"https://api.maptiler.com/tiles/contours/tiles.json?key={api_key}\",\n            ...     layer_id=\"contour-lines\",\n            ...     layer_type=\"line\",\n            ...     source_layer=\"contour\",\n            ...     paint={\"line-color\": \"#ff69b4\", \"line-width\": 1}\n            ... )\n        \"\"\"\n        self.session.m.add_vector_tile(\n            url=url,\n            layer_id=layer_id,\n            layer_type=layer_type,\n            source_layer=source_layer,\n            name=name,\n            paint=paint,\n            layout=layout,\n            filter=filter,\n            minzoom=minzoom,\n            maxzoom=maxzoom,\n            visible=visible,\n            opacity=opacity,\n            add_popup=add_popup,\n            before_id=before_id,\n            source_args=source_args,\n            overwrite=overwrite,\n            **kwargs,\n        )\n        return f\"Vector tile layer added: {url}\"\n\n    @tool\n    def add_wms_layer(\n        self,\n        url: str,\n        layers: str,\n        format: str = \"image/png\",\n        name: str = \"WMS Layer\",\n        attribution: str = \"\",\n        opacity: float = 1.0,\n        visible: bool = True,\n        tile_size: int = 256,\n        before_id: Optional[str] = None,\n        source_args: Dict = None,\n        overwrite: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds a WMS layer to the map.\n\n        This method adds a WMS layer to the map. The WMS  is created from\n            the specified URL, and it is added to the map with the specified\n            name, attribution, visibility, and tile size.\n\n        Args:\n            url (str): The URL of the tile layer.\n            layers (str): The layers to include in the WMS request.\n            format (str, optional): The format of the tiles in the layer.\n            name (str, optional): The name to use for the layer. Defaults to\n                'WMS Layer'.\n            attribution (str, optional): The attribution to use for the layer.\n                Defaults to ''.\n            visible (bool, optional): Whether the layer should be visible by\n                default. Defaults to True.\n            tile_size (int, optional): The size of the tiles in the layer.\n                Defaults to 256.\n            before_id (str, optional): The ID of an existing layer before which\n                the new layer should be inserted.\n            source_args (dict, optional): Additional keyword arguments that are\n                passed to the RasterTileSource class.\n            overwrite (bool, optional): Whether to overwrite an existing layer with the same name.\n                Defaults to False.\n            **kwargs: Additional keyword arguments that are passed to the Layer class.\n                See https://eodagmbh.github.io/py-maplibregl/api/layer/ for more information.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_wms_layer(\n            url=url,\n            layers=layers,\n            format=format,\n            name=name,\n            attribution=attribution,\n            opacity=opacity,\n            visible=visible,\n            tile_size=tile_size,\n            before_id=before_id,\n            source_args=source_args,\n            overwrite=overwrite,\n            **kwargs,\n        )\n        return f\"WMS layer added: {url}\"\n\n    @tool\n    def add_nwi_basemap(\n        self,\n        name: str = \"NWI Wetlands\",\n        format: str = \"image/png\",\n        attribution: str = \"USFWS\",\n        opacity: float = 1.0,\n        visible: bool = True,\n        tile_size: int = 256,\n        before_id: Optional[str] = None,\n        overwrite: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds a NWI Wetlands basemap to the map.\n\n        This method adds a NWI Wetlands basemap to the map. The NWI Wetlands basemap is created from\n            the specified URL, and it is added to the map with the specified\n            name, attribution, visibility, and tile size.\n\n        Args:\n            name (str, optional): The name to use for the layer. Defaults to\n                'NWI Wetlands'.\n            format (str, optional): The format of the tiles in the layer.\n            attribution (str, optional): The attribution to use for the layer.\n                Defaults to ''.\n            visible (bool, optional): Whether the layer should be visible by\n                default. Defaults to True.\n            tile_size (int, optional): The size of the tiles in the layer.\n                Defaults to 256.\n            before_id (str, optional): The ID of an existing layer before which\n                the new layer should be inserted.\n            overwrite (bool, optional): Whether to overwrite an existing layer with the same name.\n                Defaults to False.\n            **kwargs: Additional keyword arguments that are passed to the Layer class.\n                See https://eodagmbh.github.io/py-maplibregl/api/layer/ for more information.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_nwi_basemap(\n            name=name,\n            format=format,\n            attribution=attribution,\n            opacity=opacity,\n            visible=visible,\n            tile_size=tile_size,\n            before_id=before_id,\n            overwrite=overwrite,\n            **kwargs,\n        )\n        return f\"NWI Wetlands basemap added: {name}\"\n\n    @tool\n    def add_raster(\n        self,\n        source,\n        indexes=None,\n        colormap=None,\n        vmin=None,\n        vmax=None,\n        nodata=None,\n        name=\"Raster\",\n        before_id=None,\n        fit_bounds=True,\n        visible=True,\n        opacity=1.0,\n        array_args=None,\n        client_args={\"cors_all\": True},\n        overwrite: bool = True,\n        **kwargs: Any,\n    ):\n        \"\"\"Add a local raster dataset to the map.\n            If you are using this function in JupyterHub on a remote server\n            (e.g., Binder, Microsoft Planetary Computer) and if the raster\n            does not render properly, try installing jupyter-server-proxy using\n            `pip install jupyter-server-proxy`, then running the following code\n            before calling this function. For more info, see https://bit.ly/3JbmF93.\n\n            import os\n            os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n        Args:\n            source (str): The path to the GeoTIFF file or the URL of the Cloud\n                Optimized GeoTIFF.\n            indexes (int, optional): The band(s) to use. Band indexing starts\n                at 1. Defaults to None.\n            colormap (str, optional): The name of the colormap from `matplotlib`\n                to use when plotting a single band.\n                See https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n                Default is greyscale.\n            vmin (float, optional): The minimum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            vmax (float, optional): The maximum value to use when colormapping\n                the palette when plotting a single band. Defaults to None.\n            nodata (float, optional): The value from the band to use to interpret\n                as not valid data. Defaults to None.\n            visible (bool, optional): Whether the layer is visible. Defaults to True.\n            opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n            array_args (dict, optional): Additional arguments to pass to\n                `array_to_memory_file` when reading the raster. Defaults to {}.\n            client_args (dict, optional): Additional arguments to pass to\n                localtileserver.TileClient. Defaults to { \"cors_all\": False }.\n            overwrite (bool, optional): Whether to overwrite an existing layer with the same name.\n                Defaults to True.\n            **kwargs: Additional keyword arguments to be passed to the underlying\n                `add_tile_layer` method.\n        \"\"\"\n        self.session.m.add_raster(\n            source=source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            name=name,\n            before_id=before_id,\n            fit_bounds=fit_bounds,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            overwrite=overwrite,\n            **kwargs,\n        )\n        return f\"Raster added: {source}\"\n\n    @tool\n    def save_map(\n        self,\n        output: str = \"map.html\",\n        title: str = \"My Awesome Map\",\n        width: str = \"100%\",\n        height: str = \"100%\",\n        replace_key: bool = False,\n        remove_port: bool = True,\n        preview: bool = False,\n        overwrite: bool = False,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Render the map to an HTML page.\n\n        Args:\n            output (str, optional): The output HTML file. If None, the HTML content\n                is returned as a string. Defaults to 'map.html'.\n            title (str, optional): The title of the HTML page. Defaults to 'My Awesome Map'.\n            width (str, optional): The width of the map. Defaults to '100%'.\n            height (str, optional): The height of the map. Defaults to '100%'.\n            replace_key (bool, optional): Whether to replace the API key in the HTML.\n                If True, the API key is replaced with the public API key.\n                The API key is read from the environment variable `MAPTILER_KEY`.\n                The public API key is read from the environment variable `MAPTILER_KEY_PUBLIC`.\n                Defaults to False.\n            remove_port (bool, optional): Whether to remove the port number from the HTML.\n            preview (bool, optional): Whether to preview the HTML file in a web browser.\n                Defaults to False.\n            overwrite (bool, optional): Whether to overwrite the output file if it already exists.\n            **kwargs: Additional keyword arguments that are passed to the\n                `maplibre.ipywidget.MapWidget.to_html()` method.\n\n        Returns:\n            str: The HTML content of the map.\n        \"\"\"\n        self.session.m.to_html(\n            output=output,\n            title=title,\n            width=width,\n            height=height,\n            replace_key=replace_key,\n            remove_port=remove_port,\n            preview=preview,\n            overwrite=overwrite,\n            **kwargs,\n        )\n        return f\"HTML file created: {output}\"\n\n    @tool\n    def set_paint_property(self, name: str, prop: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the paint property of a layer.\n\n        This method sets the opacity of the specified layer to the specified value.\n\n        Args:\n            name (str): The name of the layer.\n            prop (str): The paint property to set.\n            value (Any): The value to set.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.set_paint_property(name=name, prop=prop, value=value)\n        return f\"Paint property set: {name}, {prop}, {value}\"\n\n    @tool\n    def set_layout_property(self, name: str, prop: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the layout property of a layer.\n\n        This method sets the layout property of the specified layer to the specified value.\n\n        Args:\n            name (str): The name of the layer.\n            prop (str): The layout property to set.\n            value (Any): The value to set.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.set_layout_property(name=name, prop=prop, value=value)\n        return f\"Layout property set: {name}, {prop}, {value}\"\n\n    @tool\n    def set_color(self, name: str, color: str) -&gt; None:\n        \"\"\"\n        Set the color of a layer.\n\n        This method sets the color of the specified layer to the specified value.\n\n        Args:\n            name (str): The name of the layer.\n            color (str): The color value to set.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.set_color(name=name, color=color)\n        return f\"Color set: {name}, {color}\"\n\n    @tool\n    def set_opacity(self, name: str, opacity: float) -&gt; None:\n        \"\"\"\n        Set the opacity of a layer.\n\n        This method sets the opacity of the specified layer to the specified value.\n\n        Args:\n            name (str): The name of the layer.\n            opacity (float): The opacity value to set.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.set_opacity(name=name, opacity=opacity)\n        return f\"Opacity set: {name}, {opacity}\"\n\n    @tool\n    def set_visibility(self, name: str, visible: bool) -&gt; None:\n        \"\"\"\n        Set the visibility of a layer.\n\n        This method sets the visibility of the specified layer to the specified value.\n\n        Args:\n            name (str): The name of the layer.\n            visible (bool): The visibility value to set.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.set_visibility(name=name, visible=visible)\n        return f\"Visibility set: {name}, {visible}\"\n\n    @tool\n    def add_pmtiles(\n        self,\n        url: str,\n        style: Optional[Dict] = None,\n        visible: bool = True,\n        opacity: float = 1.0,\n        exclude_mask: bool = False,\n        tooltip: bool = True,\n        properties: Optional[Dict] = None,\n        template: Optional[str] = None,\n        attribution: str = \"PMTiles\",\n        fit_bounds: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds a PMTiles layer to the map.\n\n        Args:\n            url (str): The URL of the PMTiles file.\n            style (dict, optional): The CSS style to apply to the layer. Defaults to None.\n                See https://docs.mapbox.com/style-spec/reference/layers/ for more info.\n            visible (bool, optional): Whether the layer should be shown initially. Defaults to True.\n            opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n            exclude_mask (bool, optional): Whether to exclude the mask layer. Defaults to False.\n            tooltip (bool, optional): Whether to show tooltips on the layer. Defaults to True.\n            properties (dict, optional): The properties to use for the tooltips. Defaults to None.\n            template (str, optional): The template to use for the tooltips. Defaults to None.\n            attribution (str, optional): The attribution to use for the layer. Defaults to 'PMTiles'.\n            fit_bounds (bool, optional): Whether to zoom to the layer extent. Defaults to True.\n            **kwargs: Additional keyword arguments to pass to the PMTilesLayer constructor.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_pmtiles(\n            url=url,\n            style=style,\n            visible=visible,\n            opacity=opacity,\n            exclude_mask=exclude_mask,\n            tooltip=tooltip,\n            properties=properties,\n            template=template,\n            attribution=attribution,\n            fit_bounds=fit_bounds,\n            **kwargs,\n        )\n        return f\"PMTiles layer added: {url}\"\n\n    @tool\n    def add_marker(\n        self,\n        lng_lat: List[Union[float, float]],\n        popup: Optional[Dict] = None,\n        options: Optional[Dict] = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a marker to the map.\n\n        Args:\n            lng_lat (List[Union[float, float]]): A list of two floats\n                representing the longitude and latitude of the marker.\n            popup (Optional[str], optional): The text to display in a popup when\n                the marker is clicked. Defaults to None.\n            options (Optional[Dict], optional): A dictionary of options to\n                customize the marker. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_marker(lng_lat=lng_lat, popup=popup, options=options)\n        return f\"Marker added: {lng_lat}\"\n\n    @tool\n    def add_image(\n        self,\n        id: str = None,\n        image: Union[str, Dict] = None,\n        width: int = None,\n        height: int = None,\n        coordinates: List[float] = None,\n        position: str = None,\n        icon_size: float = 1.0,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Add an image to the map.\n\n        Args:\n            id (str): The layer ID of the image.\n            image (Union[str, Dict, np.ndarray]): The URL or local file path to\n                the image, or a dictionary containing image data, or a numpy\n                array representing the image.\n            width (int, optional): The width of the image. Defaults to None.\n            height (int, optional): The height of the image. Defaults to None.\n            coordinates (List[float], optional): The longitude and latitude\n                coordinates to place the image.\n            position (str, optional): The position of the image. Defaults to None.\n                Can be one of 'top-right', 'top-left', 'bottom-right', 'bottom-left'.\n            icon_size (float, optional): The size of the icon. Defaults to 1.0.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_image(\n            id=id,\n            image=image,\n            width=width,\n            height=height,\n            coordinates=coordinates,\n            position=position,\n            icon_size=icon_size,\n            **kwargs,\n        )\n        return f\"Image added: {id}\"\n\n    @tool\n    def rotate_to(\n        self, bearing: float, options: Dict[str, Any] = {}, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Rotate the map to a specified bearing.\n\n        This function rotates the map to a specified bearing. The bearing is specified in degrees\n        counter-clockwise from true north. If the bearing is not specified, the map will rotate to\n        true north. Additional options and keyword arguments can be provided to control the rotation.\n        For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#rotateto\n\n        Args:\n            bearing (float): The bearing to rotate to, in degrees counter-clockwise from true north.\n            options (Dict[str, Any], optional): Additional options to control the rotation. Defaults to {}.\n            **kwargs (Any): Additional keyword arguments to control the rotation.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.rotate_to(bearing=bearing, options=options, **kwargs)\n        return f\"Map rotated to: {bearing}\"\n\n    @tool\n    def pan_to(\n        self,\n        lnglat: List[float],\n        options: Dict[str, Any] = {},\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Pans the map to a specified location.\n\n        This function pans the map to the specified longitude and latitude coordinates.\n        Additional options and keyword arguments can be provided to control the panning.\n        For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#panto\n\n        Args:\n            lnglat (List[float, float]): The longitude and latitude coordinates to pan to.\n            options (Dict[str, Any], optional): Additional options to control the panning. Defaults to {}.\n            **kwargs (Any): Additional keyword arguments to control the panning.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.pan_to(lnglat=lnglat, options=options, **kwargs)\n        return f\"Map panned to: {lnglat}\"\n\n    @tool\n    def jump_to(self, options: Dict[str, Any] = {}, **kwargs: Any) -&gt; None:\n        \"\"\"\n        Jumps the map to a specified location.\n\n        This function jumps the map to the specified location with the specified options.\n        Additional keyword arguments can be provided to control the jump. For more information,\n        see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#jumpto\n\n        Args:\n            options (Dict[str, Any], optional): Additional options to control the jump. Defaults to {}.\n            **kwargs (Any): Additional keyword arguments to control the jump.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.jump_to(options=options, **kwargs)\n        return f\"Map jumped to: {options}\"\n\n    @tool\n    def zoom_to(self, zoom: float, options: Dict[str, Any] = {}) -&gt; None:\n        \"\"\"\n        Zooms the map to a specified zoom level.\n\n        This function zooms the map to the specified zoom level. Additional options and keyword\n        arguments can be provided to control the zoom. For more information, see\n        https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#zoomto\n\n        Args:\n            zoom (float): The zoom level to zoom to.\n            options (Dict[str, Any], optional): Additional options to control the zoom. Defaults to {}.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.zoom_to(zoom=zoom, options=options)\n        return f\"Map zoomed to: {zoom}\"\n\n    @tool\n    def first_symbol_layer_id(self) -&gt; Optional[str]:\n        \"\"\"\n        Get the ID of the first symbol layer in the map's current style.\n        \"\"\"\n        return self.session.m.first_symbol_layer_id\n\n    @tool\n    def add_text(\n        self,\n        text: str,\n        fontsize: int = 20,\n        fontcolor: str = \"black\",\n        bold: bool = False,\n        padding: str = \"5px\",\n        bg_color: str = \"white\",\n        border_radius: str = \"5px\",\n        position: str = \"bottom-right\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds text to the map with customizable styling.\n\n        This method allows adding a text widget to the map with various styling options such as font size, color,\n        background color, and more. The text's appearance can be further customized using additional CSS properties\n        passed through kwargs.\n\n        Args:\n            text (str): The text to add to the map.\n            fontsize (int, optional): The font size of the text. Defaults to 20.\n            fontcolor (str, optional): The color of the text. Defaults to \"black\".\n            bold (bool, optional): If True, the text will be bold. Defaults to False.\n            padding (str, optional): The padding around the text. Defaults to \"5px\".\n            bg_color (str, optional): The background color of the text widget. Defaults to \"white\".\n                To make the background transparent, set this to \"transparent\".\n                To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".\n            border_radius (str, optional): The border radius of the text widget. Defaults to \"5px\".\n            position (str, optional): The position of the text widget on the map. Defaults to \"bottom-right\".\n            **kwargs (Any): Additional CSS properties to apply to the text widget.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_text(\n            text=text,\n            fontsize=fontsize,\n            fontcolor=fontcolor,\n            bold=bold,\n            padding=padding,\n            bg_color=bg_color,\n            border_radius=border_radius,\n            position=position,\n            **kwargs,\n        )\n        return f\"Text added: {text}\"\n\n    @tool\n    def add_html(\n        self,\n        html: str,\n        bg_color: str = \"white\",\n        position: str = \"bottom-right\",\n        **kwargs: Union[str, int, float],\n    ) -&gt; None:\n        \"\"\"\n        Add HTML content to the map.\n\n        This method allows for the addition of arbitrary HTML content to the map, which can be used to display\n        custom information or controls. The background color and position of the HTML content can be customized.\n\n        Args:\n            html (str): The HTML content to add.\n            bg_color (str, optional): The background color of the HTML content. Defaults to \"white\".\n                To make the background transparent, set this to \"transparent\".\n                To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".\n            position (str, optional): The position of the HTML content on the map. Can be one of \"top-left\",\n                \"top-right\", \"bottom-left\", \"bottom-right\". Defaults to \"bottom-right\".\n            **kwargs: Additional keyword arguments for future use.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_html(\n            html=html, bg_color=bg_color, position=position, **kwargs\n        )\n        return f\"HTML added: {html}\"\n\n    @tool\n    def add_legend(\n        self,\n        title: str = \"Legend\",\n        legend_dict: Optional[Dict[str, str]] = None,\n        labels: Optional[List[str]] = None,\n        colors: Optional[List[str]] = None,\n        fontsize: int = 15,\n        bg_color: str = \"white\",\n        position: str = \"bottom-right\",\n        builtin_legend: Optional[str] = None,\n        shape_type: str = \"rectangle\",\n        **kwargs: Union[str, int, float],\n    ) -&gt; None:\n        \"\"\"\n        Adds a legend to the map.\n\n        This method allows for the addition of a legend to the map. The legend can be customized with a title,\n        labels, colors, and more. A built-in legend can also be specified.\n\n        Args:\n            title (str, optional): The title of the legend. Defaults to \"Legend\".\n            legend_dict (Optional[Dict[str, str]], optional): A dictionary with legend items as keys and colors as values.\n                If provided, `labels` and `colors` will be ignored. Defaults to None.\n            labels (Optional[List[str]], optional): A list of legend labels. Defaults to None.\n            colors (Optional[List[str]], optional): A list of colors corresponding to the labels. Defaults to None.\n            fontsize (int, optional): The font size of the legend text. Defaults to 15.\n            bg_color (str, optional): The background color of the legend. Defaults to \"white\".\n                To make the background transparent, set this to \"transparent\".\n                To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".\n            position (str, optional): The position of the legend on the map. Can be one of \"top-left\",\n                \"top-right\", \"bottom-left\", \"bottom-right\". Defaults to \"bottom-right\".\n            builtin_legend (Optional[str], optional): The name of a built-in legend to use. Defaults to None.\n            shape_type (str, optional): The shape type of the legend items. Can be one of \"rectangle\", \"circle\", or \"line\".\n            **kwargs: Additional keyword arguments for future use.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_legend(\n            title=title,\n            legend_dict=legend_dict,\n            labels=labels,\n            colors=colors,\n            fontsize=fontsize,\n            bg_color=bg_color,\n            position=position,\n            builtin_legend=builtin_legend,\n            shape_type=shape_type,\n            **kwargs,\n        )\n        return f\"Legend added: {title}\"\n\n    @tool\n    def add_colorbar(\n        self,\n        width: Optional[float] = 3.0,\n        height: Optional[float] = 0.2,\n        vmin: Optional[float] = 0,\n        vmax: Optional[float] = 1.0,\n        palette: Optional[List[str]] = None,\n        vis_params: Optional[Dict[str, Union[str, float, int]]] = None,\n        cmap: Optional[str] = \"gray\",\n        discrete: Optional[bool] = False,\n        label: Optional[str] = None,\n        label_size: Optional[int] = 10,\n        label_weight: Optional[str] = \"normal\",\n        tick_size: Optional[int] = 8,\n        bg_color: Optional[str] = \"white\",\n        orientation: Optional[str] = \"horizontal\",\n        dpi: Optional[Union[str, float]] = \"figure\",\n        transparent: Optional[bool] = False,\n        position: str = \"bottom-right\",\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"\n        Add a colorbar to the map.\n\n        This function uses matplotlib to generate a colorbar, saves it as a PNG file, and adds it to the map using\n        the Map.add_html() method. The colorbar can be customized in various ways including its size, color palette,\n        label, and orientation.\n\n        Args:\n            width (Optional[float]): Width of the colorbar in inches. Defaults to 3.0.\n            height (Optional[float]): Height of the colorbar in inches. Defaults to 0.2.\n            vmin (Optional[float]): Minimum value of the colorbar. Defaults to 0.\n            vmax (Optional[float]): Maximum value of the colorbar. Defaults to 1.0.\n            palette (Optional[List[str]]): List of colors or a colormap name for the colorbar. Defaults to None.\n            vis_params (Optional[Dict[str, Union[str, float, int]]]): Visualization parameters as a dictionary.\n            cmap (Optional[str]): Matplotlib colormap name. Defaults to \"gray\".\n            discrete (Optional[bool]): Whether to create a discrete colorbar. Defaults to False.\n            label (Optional[str]): Label for the colorbar. Defaults to None.\n            label_size (Optional[int]): Font size for the colorbar label. Defaults to 10.\n            label_weight (Optional[str]): Font weight for the colorbar label. Defaults to \"normal\".\n            tick_size (Optional[int]): Font size for the colorbar tick labels. Defaults to 8.\n            bg_color (Optional[str]): Background color for the colorbar. Defaults to \"white\".\n            orientation (Optional[str]): Orientation of the colorbar (\"vertical\" or \"horizontal\"). Defaults to \"horizontal\".\n            dpi (Optional[Union[str, float]]): Resolution in dots per inch. If 'figure', uses the figure's dpi value. Defaults to \"figure\".\n            transparent (Optional[bool]): Whether the background is transparent. Defaults to False.\n            position (str): Position of the colorbar on the map. Defaults to \"bottom-right\".\n            **kwargs: Additional keyword arguments passed to matplotlib.pyplot.savefig().\n\n        Returns:\n            str: Path to the generated colorbar image.\n        \"\"\"\n        self.session.m.add_colorbar(\n            width=width,\n            height=height,\n            vmin=vmin,\n            vmax=vmax,\n            palette=palette,\n            vis_params=vis_params,\n            cmap=cmap,\n            discrete=discrete,\n            label=label,\n            label_size=label_size,\n            label_weight=label_weight,\n            tick_size=tick_size,\n            bg_color=bg_color,\n            orientation=orientation,\n            dpi=dpi,\n            transparent=transparent,\n            position=position,\n            **kwargs,\n        )\n        return f\"Colorbar added: {position}\"\n\n    @tool\n    def add_layer_control(\n        self,\n        layer_ids: Optional[List[str]] = None,\n        theme: str = \"default\",\n        css_text: Optional[str] = None,\n        position: str = \"top-left\",\n        bg_layers: Optional[Union[bool, List[str]]] = False,\n    ) -&gt; None:\n        \"\"\"\n        Adds a layer control to the map.\n\n        This function creates and adds a layer switcher control to the map, allowing users to toggle the visibility\n        of specified layers. The appearance and functionality of the layer control can be customized with parameters\n        such as theme, CSS styling, and position on the map.\n\n        Args:\n            layer_ids (Optional[List[str]]): A list of layer IDs to include in the control. If None, all layers\n                in the map will be included. Defaults to None.\n            theme (str): The theme for the layer switcher control. Can be \"default\" or other custom themes. Defaults to \"default\".\n            css_text (Optional[str]): Custom CSS text for styling the layer control. If None, a default style will be applied.\n                Defaults to None.\n            position (str): The position of the layer control on the map. Can be \"top-left\", \"top-right\", \"bottom-left\",\n                or \"bottom-right\". Defaults to \"top-left\".\n            bg_layers (bool): If True, background layers will be included in the control. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_layer_control(\n            layer_ids=layer_ids,\n            theme=theme,\n            css_text=css_text,\n            position=position,\n            bg_layers=bg_layers,\n        )\n        return f\"Layer control added: {position}\"\n\n    @tool\n    def add_video(\n        self,\n        urls: Union[str, List[str]],\n        coordinates: List[List[float]],\n        layer_id: str = \"video\",\n        before_id: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Adds a video layer to the map.\n\n        This method allows embedding a video into the map by specifying the video URLs and the geographical coordinates\n        that the video should cover. The video will be stretched and fitted into the specified coordinates.\n\n        Args:\n            urls (Union[str, List[str]]): A single video URL or a list of video URLs. These URLs must be accessible\n                from the client's location.\n            coordinates (List[List[float]]): A list of four coordinates in [longitude, latitude] format, specifying\n                the corners of the video. The coordinates order should be top-left, top-right, bottom-right, bottom-left.\n            layer_id (str): The ID for the video layer. Defaults to \"video\".\n            before_id (Optional[str]): The ID of an existing layer to insert the new layer before. If None, the layer\n                will be added on top. Defaults to None.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_video(\n            urls=urls,\n            coordinates=coordinates,\n            layer_id=layer_id,\n            before_id=before_id,\n        )\n        return f\"Video added: {layer_id}\"\n\n    @tool\n    def add_nlcd(\n        self, years: list = [2023], add_legend: bool = True, **kwargs: Any\n    ) -&gt; None:\n        \"\"\"\n        Adds National Land Cover Database (NLCD) data to the map.\n\n        Args:\n            years (list): A list of years to add. It can be any of 1985-2023. Defaults to [2023].\n            add_legend (bool): Whether to add a legend to the map. Defaults to True.\n            **kwargs: Additional keyword arguments to pass to the add_cog_layer method.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_nlcd(\n            years=years,\n            add_legend=add_legend,\n            **kwargs,\n        )\n        return f\"NLCD added: {years}\"\n\n    @tool\n    def add_data(\n        self,\n        data: Union[str],\n        column: str,\n        cmap: Optional[str] = None,\n        colors: Optional[str] = None,\n        labels: Optional[str] = None,\n        scheme: Optional[str] = \"Quantiles\",\n        k: int = 5,\n        add_legend: Optional[bool] = True,\n        legend_title: Optional[bool] = None,\n        legend_position: Optional[str] = \"bottom-right\",\n        legend_kwds: Optional[dict] = None,\n        classification_kwds: Optional[dict] = None,\n        legend_args: Optional[dict] = None,\n        layer_type: Optional[str] = None,\n        extrude: Optional[bool] = False,\n        scale_factor: Optional[float] = 1.0,\n        filter: Optional[Dict] = None,\n        paint: Optional[Dict] = None,\n        name: Optional[str] = None,\n        fit_bounds: bool = True,\n        visible: bool = True,\n        opacity: float = 1.0,\n        before_id: Optional[str] = None,\n        source_args: Dict = {},\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Add vector data to the map with a variety of classification schemes.\n\n        Args:\n            data (str | pd.DataFrame | gpd.GeoDataFrame): The data to classify.\n                It can be a filepath to a vector dataset, a pandas dataframe, or\n                a geopandas geodataframe.\n            column (str): The column to classify.\n            cmap (str, optional): The name of a colormap recognized by matplotlib. Defaults to None.\n            colors (list, optional): A list of colors to use for the classification. Defaults to None.\n            labels (list, optional): A list of labels to use for the legend. Defaults to None.\n            scheme (str, optional): Name of a choropleth classification scheme (requires mapclassify).\n                Name of a choropleth classification scheme (requires mapclassify).\n                A mapclassify.MapClassifier object will be used\n                under the hood. Supported are all schemes provided by mapclassify (e.g.\n                'BoxPlot', 'EqualInterval', 'FisherJenks', 'FisherJenksSampled',\n                'HeadTailBreaks', 'JenksCaspall', 'JenksCaspallForced',\n                'JenksCaspallSampled', 'MaxP', 'MaximumBreaks',\n                'NaturalBreaks', 'Quantiles', 'Percentiles', 'StdMean',\n                'UserDefined'). Arguments can be passed in classification_kwds.\n            k (int, optional): Number of classes (ignored if scheme is None or if\n                column is categorical). Default to 5.\n            add_legend (bool, optional): Whether to add a legend to the map. Defaults to True.\n            legend_title (str, optional): The title of the legend. Defaults to None.\n            legend_position (str, optional): The position of the legend. Can be 'top-left',\n                'top-right', 'bottom-left', or 'bottom-right'. Defaults to 'bottom-right'.\n            legend_kwds (dict, optional): Keyword arguments to pass to :func:`matplotlib.pyplot.legend`\n                or `matplotlib.pyplot.colorbar`. Defaults to None.\n                Keyword arguments to pass to :func:`matplotlib.pyplot.legend` or\n                Additional accepted keywords when `scheme` is specified:\n                fmt : string\n                    A formatting specification for the bin edges of the classes in the\n                    legend. For example, to have no decimals: ``{\"fmt\": \"{:.0f}\"}``.\n                labels : list-like\n                    A list of legend labels to override the auto-generated labblels.\n                    Needs to have the same number of elements as the number of\n                    classes (`k`).\n                interval : boolean (default False)\n                    An option to control brackets from mapclassify legend.\n                    If True, open/closed interval brackets are shown in the legend.\n            classification_kwds (dict, optional): Keyword arguments to pass to mapclassify.\n                Defaults to None.\n            legend_args (dict, optional): Additional keyword arguments for the add_legend method. Defaults to None.\n            layer_type (str, optional): The type of layer to add. Can be 'circle', 'line', or 'fill'. Defaults to None.\n            filter (dict, optional): The filter to apply to the layer. If None,\n                no filter is applied.\n            paint (dict, optional): The paint properties to apply to the layer.\n                If None, no paint properties are applied.\n            name (str, optional): The name of the layer. If None, a random name\n                is generated.\n            fit_bounds (bool, optional): Whether to adjust the viewport of the\n                map to fit the bounds of the GeoJSON data. Defaults to True.\n            visible (bool, optional): Whether the layer is visible or not.\n                Defaults to True.\n            before_id (str, optional): The ID of an existing layer before which\n                the new layer should be inserted.\n            source_args (dict, optional): Additional keyword arguments that are\n                passed to the GeoJSONSource class.\n            **kwargs: Additional keyword arguments to pass to the GeoJSON class, such as\n                fields, which can be a list of column names to be included in the popup.\n\n        \"\"\"\n        self.session.m.add_data(\n            data=data,\n            column=column,\n            cmap=cmap,\n            colors=colors,\n            labels=labels,\n            scheme=scheme,\n            k=k,\n            add_legend=add_legend,\n            legend_title=legend_title,\n            legend_position=legend_position,\n            legend_kwds=legend_kwds,\n            classification_kwds=classification_kwds,\n            legend_args=legend_args,\n            layer_type=layer_type,\n            extrude=extrude,\n            scale_factor=scale_factor,\n            filter=filter,\n            paint=paint,\n            name=name,\n            fit_bounds=fit_bounds,\n            visible=visible,\n            opacity=opacity,\n            before_id=before_id,\n            source_args=source_args,\n            **kwargs,\n        )\n        return f\"Data added: {name}\"\n\n    @tool\n    def add_mapillary(\n        self,\n        minzoom: int = 6,\n        maxzoom: int = 14,\n        sequence_lyr_name: str = \"sequence\",\n        image_lyr_name: str = \"image\",\n        before_id: str = None,\n        sequence_paint: dict = None,\n        image_paint: dict = None,\n        image_minzoom: int = 17,\n        add_popup: bool = True,\n        access_token: str = None,\n        opacity: float = 1.0,\n        visible: bool = True,\n        add_to_sidebar: bool = False,\n        style: str = \"photo\",\n        radius: float = 0.00005,\n        height: int = 420,\n        frame_border: int = 0,\n        default_message: str = \"No Mapillary image found\",\n        widget_icon: str = \"mdi-image\",\n        widget_label: str = \"Mapillary StreetView\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds Mapillary layers to the map.\n\n        Args:\n            minzoom (int): Minimum zoom level for the Mapillary tiles. Defaults to 6.\n            maxzoom (int): Maximum zoom level for the Mapillary tiles. Defaults to 14.\n            sequence_lyr_name (str): Name of the sequence layer. Defaults to \"sequence\".\n            image_lyr_name (str): Name of the image layer. Defaults to \"image\".\n            before_id (str): The ID of an existing layer to insert the new layer before. Defaults to None.\n            sequence_paint (dict, optional): Paint properties for the sequence layer. Defaults to None.\n            image_paint (dict, optional): Paint properties for the image layer. Defaults to None.\n            image_minzoom (int): Minimum zoom level for the image layer. Defaults to 17.\n            add_popup (bool): Whether to add popups to the layers. Defaults to True.\n            access_token (str, optional): Access token for Mapillary API. Defaults to None.\n            opacity (float): Opacity of the Mapillary layers. Defaults to 1.0.\n            visible (bool): Whether the Mapillary layers are visible. Defaults to True.\n\n        Raises:\n            ValueError: If no access token is provided.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_mapillary(\n            minzoom=minzoom,\n            maxzoom=maxzoom,\n            sequence_lyr_name=sequence_lyr_name,\n            image_lyr_name=image_lyr_name,\n            before_id=before_id,\n            sequence_paint=sequence_paint,\n            image_paint=image_paint,\n            image_minzoom=image_minzoom,\n            add_popup=add_popup,\n            access_token=access_token,\n            opacity=opacity,\n            visible=visible,\n            add_to_sidebar=add_to_sidebar,\n            style=style,\n            radius=radius,\n            height=height,\n            frame_border=frame_border,\n            default_message=default_message,\n            widget_icon=widget_icon,\n            widget_label=widget_label,\n            **kwargs,\n        )\n        return f\"Mapillary added: {sequence_lyr_name}\"\n\n    @tool\n    def add_labels(\n        self,\n        source: Union[str, Dict[str, Any]],\n        column: str,\n        name: Optional[str] = None,\n        text_size: int = 14,\n        text_anchor: str = \"center\",\n        text_color: str = \"black\",\n        min_zoom: Optional[float] = None,\n        max_zoom: Optional[float] = None,\n        layout: Optional[Dict[str, Any]] = None,\n        paint: Optional[Dict[str, Any]] = None,\n        before_id: Optional[str] = None,\n        opacity: float = 1.0,\n        visible: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Adds a label layer to the map.\n\n        This method adds a label layer to the map using the specified source and column for text values.\n\n        Args:\n            source (Union[str, Dict[str, Any]]): The data source for the labels. It can be a GeoJSON file path\n                or a dictionary containing GeoJSON data.\n            column (str): The column name in the source data to use for the label text.\n            name (Optional[str]): The name of the label layer. If None, a random name is generated. Defaults to None.\n            text_size (int): The size of the label text. Defaults to 14.\n            text_anchor (str): The anchor position of the text. Can be \"center\", \"left\", \"right\", etc. Defaults to \"center\".\n            text_color (str): The color of the label text. Defaults to \"black\".\n            min_zoom (Optional[float]): The minimum zoom level at which the labels are visible. Defaults to None.\n            max_zoom (Optional[float]): The maximum zoom level at which the labels are visible. Defaults to None.\n            layout (Optional[Dict[str, Any]]): Additional layout properties for the label layer. Defaults to None.\n                For more information, refer to https://maplibre.org/maplibre-style-spec/layers/#symbol.\n            paint (Optional[Dict[str, Any]]): Additional paint properties for the label layer. Defaults to None.\n            before_id (Optional[str]): The ID of an existing layer before which the new layer should be inserted. Defaults to None.\n            opacity (float): The opacity of the label layer. Defaults to 1.0.\n            visible (bool): Whether the label layer is visible by default. Defaults to True.\n            **kwargs (Any): Additional keyword arguments to customize the label layer.\n\n        Returns:\n            None\n        \"\"\"\n        self.session.m.add_labels(\n            source=source,\n            column=column,\n            name=name,\n            text_size=text_size,\n            text_anchor=text_anchor,\n            text_color=text_color,\n            min_zoom=min_zoom,\n            max_zoom=max_zoom,\n            layout=layout,\n            paint=paint,\n            before_id=before_id,\n            opacity=opacity,\n            visible=visible,\n            **kwargs,\n        )\n        return f\"Labels added: {name}\"\n\n    @tool\n    def get_layer_names(self) -&gt; list:\n        \"\"\"Gets layer names as a list.\n\n        Returns:\n            list: A list of layer names.\n        \"\"\"\n        return self.session.m.get_layer_names()\n\n    @tool\n    def set_terrain(\n        self,\n        source: str = \"https://elevation-tiles-prod.s3.amazonaws.com/terrarium/{z}/{x}/{y}.png\",\n        exaggeration: float = 1.0,\n        tile_size: int = 256,\n        encoding: str = \"terrarium\",\n        source_id: str = \"terrain-dem\",\n    ) -&gt; None:\n        \"\"\"Add terrain visualization to the map.\n\n        Args:\n            source: URL template for terrain tiles. Defaults to AWS elevation tiles.\n            exaggeration: Terrain exaggeration factor. Defaults to 1.0.\n            tile_size: Tile size in pixels. Defaults to 256.\n            encoding: Encoding for the terrain tiles. Defaults to \"terrarium\".\n            source_id: Unique identifier for the terrain source. Defaults to \"terrain-dem\".\n        \"\"\"\n        self.session.m.set_terrain(\n            source=source,\n            exaggeration=exaggeration,\n            tile_size=tile_size,\n            encoding=encoding,\n            source_id=source_id,\n        )\n        return f\"Terrain added: {source}\"\n\n    @tool\n    def remove_terrain(self) -&gt; None:\n        \"\"\"Remove terrain visualization from the map.\"\"\"\n        self.session.m.remove_terrain()\n        return \"Terrain removed.\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.__init__","title":"<code>__init__(session=None)</code>","text":"<p>Initialize map tools.</p> <p>Parameters:</p> Name Type Description Default <code>session</code> <code>Optional[MapSession]</code> <p>Optional MapSession instance. If None, creates a default session.</p> <code>None</code> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>def __init__(self, session: Optional[MapSession] = None) -&gt; None:\n    \"\"\"Initialize map tools.\n\n    Args:\n        session: Optional MapSession instance. If None, creates a default session.\n    \"\"\"\n    self.session: MapSession = session or MapSession()\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_basemap","title":"<code>add_basemap(name)</code>","text":"<p>Add a basemap to the map by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the basemap to add.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Confirmation message with basemap name.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Add a basemap by name\")\ndef add_basemap(self, name: str) -&gt; str:\n    \"\"\"Add a basemap to the map by name.\n\n    Args:\n        name: Name of the basemap to add.\n\n    Returns:\n        Confirmation message with basemap name.\n    \"\"\"\n    self.session.m.add_basemap(name)\n    return f\"Basemap added: {name}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_cog_layer","title":"<code>add_cog_layer(url, name=None, attribution='TiTiler', opacity=1.0, visible=True, bands=None, nodata=0, titiler_endpoint=None)</code>","text":"<p>Add a Cloud Optimized GeoTIFF (COG) layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL to the COG file.</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the layer.</p> <code>None</code> <code>attribution</code> <code>str</code> <p>Attribution text (default: \"TiTiler\").</p> <code>'TiTiler'</code> <code>opacity</code> <code>float</code> <p>Layer opacity from 0.0 to 1.0 (default: 1.0).</p> <code>1.0</code> <code>visible</code> <code>bool</code> <p>Whether the layer is initially visible (default: True).</p> <code>True</code> <code>bands</code> <code>Optional[List[int]]</code> <p>Optional list of band indices to display.</p> <code>None</code> <code>nodata</code> <code>Optional[Union[int, float]]</code> <p>No data value (default: 0).</p> <code>0</code> <code>titiler_endpoint</code> <code>str</code> <p>TiTiler endpoint URL (default: \"https://giswqs-titiler-endpoint.hf.space\").</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Confirmation message with COG URL.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Add Cloud Optimized GeoTIFF (COG) to the map\")\ndef add_cog_layer(\n    self,\n    url: str,\n    name: Optional[str] = None,\n    attribution: str = \"TiTiler\",\n    opacity: float = 1.0,\n    visible: bool = True,\n    bands: Optional[List[int]] = None,\n    nodata: Optional[Union[int, float]] = 0,\n    titiler_endpoint: str = None,\n) -&gt; str:\n    \"\"\"Add a Cloud Optimized GeoTIFF (COG) layer to the map.\n\n    Args:\n        url: URL to the COG file.\n        name: Optional name for the layer.\n        attribution: Attribution text (default: \"TiTiler\").\n        opacity: Layer opacity from 0.0 to 1.0 (default: 1.0).\n        visible: Whether the layer is initially visible (default: True).\n        bands: Optional list of band indices to display.\n        nodata: No data value (default: 0).\n        titiler_endpoint: TiTiler endpoint URL (default: \"https://giswqs-titiler-endpoint.hf.space\").\n\n    Returns:\n        Confirmation message with COG URL.\n    \"\"\"\n    self.session.m.add_cog_layer(\n        url, name, attribution, opacity, visible, bands, nodata, titiler_endpoint\n    )\n    return f\"COG layer added: {url}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_colorbar","title":"<code>add_colorbar(width=3.0, height=0.2, vmin=0, vmax=1.0, palette=None, vis_params=None, cmap='gray', discrete=False, label=None, label_size=10, label_weight='normal', tick_size=8, bg_color='white', orientation='horizontal', dpi='figure', transparent=False, position='bottom-right', **kwargs)</code>","text":"<p>Add a colorbar to the map.</p> <p>This function uses matplotlib to generate a colorbar, saves it as a PNG file, and adds it to the map using the Map.add_html() method. The colorbar can be customized in various ways including its size, color palette, label, and orientation.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[float]</code> <p>Width of the colorbar in inches. Defaults to 3.0.</p> <code>3.0</code> <code>height</code> <code>Optional[float]</code> <p>Height of the colorbar in inches. Defaults to 0.2.</p> <code>0.2</code> <code>vmin</code> <code>Optional[float]</code> <p>Minimum value of the colorbar. Defaults to 0.</p> <code>0</code> <code>vmax</code> <code>Optional[float]</code> <p>Maximum value of the colorbar. Defaults to 1.0.</p> <code>1.0</code> <code>palette</code> <code>Optional[List[str]]</code> <p>List of colors or a colormap name for the colorbar. Defaults to None.</p> <code>None</code> <code>vis_params</code> <code>Optional[Dict[str, Union[str, float, int]]]</code> <p>Visualization parameters as a dictionary.</p> <code>None</code> <code>cmap</code> <code>Optional[str]</code> <p>Matplotlib colormap name. Defaults to \"gray\".</p> <code>'gray'</code> <code>discrete</code> <code>Optional[bool]</code> <p>Whether to create a discrete colorbar. Defaults to False.</p> <code>False</code> <code>label</code> <code>Optional[str]</code> <p>Label for the colorbar. Defaults to None.</p> <code>None</code> <code>label_size</code> <code>Optional[int]</code> <p>Font size for the colorbar label. Defaults to 10.</p> <code>10</code> <code>label_weight</code> <code>Optional[str]</code> <p>Font weight for the colorbar label. Defaults to \"normal\".</p> <code>'normal'</code> <code>tick_size</code> <code>Optional[int]</code> <p>Font size for the colorbar tick labels. Defaults to 8.</p> <code>8</code> <code>bg_color</code> <code>Optional[str]</code> <p>Background color for the colorbar. Defaults to \"white\".</p> <code>'white'</code> <code>orientation</code> <code>Optional[str]</code> <p>Orientation of the colorbar (\"vertical\" or \"horizontal\"). Defaults to \"horizontal\".</p> <code>'horizontal'</code> <code>dpi</code> <code>Optional[Union[str, float]]</code> <p>Resolution in dots per inch. If 'figure', uses the figure's dpi value. Defaults to \"figure\".</p> <code>'figure'</code> <code>transparent</code> <code>Optional[bool]</code> <p>Whether the background is transparent. Defaults to False.</p> <code>False</code> <code>position</code> <code>str</code> <p>Position of the colorbar on the map. Defaults to \"bottom-right\".</p> <code>'bottom-right'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to matplotlib.pyplot.savefig().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the generated colorbar image.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_colorbar(\n    self,\n    width: Optional[float] = 3.0,\n    height: Optional[float] = 0.2,\n    vmin: Optional[float] = 0,\n    vmax: Optional[float] = 1.0,\n    palette: Optional[List[str]] = None,\n    vis_params: Optional[Dict[str, Union[str, float, int]]] = None,\n    cmap: Optional[str] = \"gray\",\n    discrete: Optional[bool] = False,\n    label: Optional[str] = None,\n    label_size: Optional[int] = 10,\n    label_weight: Optional[str] = \"normal\",\n    tick_size: Optional[int] = 8,\n    bg_color: Optional[str] = \"white\",\n    orientation: Optional[str] = \"horizontal\",\n    dpi: Optional[Union[str, float]] = \"figure\",\n    transparent: Optional[bool] = False,\n    position: str = \"bottom-right\",\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Add a colorbar to the map.\n\n    This function uses matplotlib to generate a colorbar, saves it as a PNG file, and adds it to the map using\n    the Map.add_html() method. The colorbar can be customized in various ways including its size, color palette,\n    label, and orientation.\n\n    Args:\n        width (Optional[float]): Width of the colorbar in inches. Defaults to 3.0.\n        height (Optional[float]): Height of the colorbar in inches. Defaults to 0.2.\n        vmin (Optional[float]): Minimum value of the colorbar. Defaults to 0.\n        vmax (Optional[float]): Maximum value of the colorbar. Defaults to 1.0.\n        palette (Optional[List[str]]): List of colors or a colormap name for the colorbar. Defaults to None.\n        vis_params (Optional[Dict[str, Union[str, float, int]]]): Visualization parameters as a dictionary.\n        cmap (Optional[str]): Matplotlib colormap name. Defaults to \"gray\".\n        discrete (Optional[bool]): Whether to create a discrete colorbar. Defaults to False.\n        label (Optional[str]): Label for the colorbar. Defaults to None.\n        label_size (Optional[int]): Font size for the colorbar label. Defaults to 10.\n        label_weight (Optional[str]): Font weight for the colorbar label. Defaults to \"normal\".\n        tick_size (Optional[int]): Font size for the colorbar tick labels. Defaults to 8.\n        bg_color (Optional[str]): Background color for the colorbar. Defaults to \"white\".\n        orientation (Optional[str]): Orientation of the colorbar (\"vertical\" or \"horizontal\"). Defaults to \"horizontal\".\n        dpi (Optional[Union[str, float]]): Resolution in dots per inch. If 'figure', uses the figure's dpi value. Defaults to \"figure\".\n        transparent (Optional[bool]): Whether the background is transparent. Defaults to False.\n        position (str): Position of the colorbar on the map. Defaults to \"bottom-right\".\n        **kwargs: Additional keyword arguments passed to matplotlib.pyplot.savefig().\n\n    Returns:\n        str: Path to the generated colorbar image.\n    \"\"\"\n    self.session.m.add_colorbar(\n        width=width,\n        height=height,\n        vmin=vmin,\n        vmax=vmax,\n        palette=palette,\n        vis_params=vis_params,\n        cmap=cmap,\n        discrete=discrete,\n        label=label,\n        label_size=label_size,\n        label_weight=label_weight,\n        tick_size=tick_size,\n        bg_color=bg_color,\n        orientation=orientation,\n        dpi=dpi,\n        transparent=transparent,\n        position=position,\n        **kwargs,\n    )\n    return f\"Colorbar added: {position}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_data","title":"<code>add_data(data, column, cmap=None, colors=None, labels=None, scheme='Quantiles', k=5, add_legend=True, legend_title=None, legend_position='bottom-right', legend_kwds=None, classification_kwds=None, legend_args=None, layer_type=None, extrude=False, scale_factor=1.0, filter=None, paint=None, name=None, fit_bounds=True, visible=True, opacity=1.0, before_id=None, source_args={}, **kwargs)</code>","text":"<p>Add vector data to the map with a variety of classification schemes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | DataFrame | GeoDataFrame</code> <p>The data to classify. It can be a filepath to a vector dataset, a pandas dataframe, or a geopandas geodataframe.</p> required <code>column</code> <code>str</code> <p>The column to classify.</p> required <code>cmap</code> <code>str</code> <p>The name of a colormap recognized by matplotlib. Defaults to None.</p> <code>None</code> <code>colors</code> <code>list</code> <p>A list of colors to use for the classification. Defaults to None.</p> <code>None</code> <code>labels</code> <code>list</code> <p>A list of labels to use for the legend. Defaults to None.</p> <code>None</code> <code>scheme</code> <code>str</code> <p>Name of a choropleth classification scheme (requires mapclassify). Name of a choropleth classification scheme (requires mapclassify). A mapclassify.MapClassifier object will be used under the hood. Supported are all schemes provided by mapclassify (e.g. 'BoxPlot', 'EqualInterval', 'FisherJenks', 'FisherJenksSampled', 'HeadTailBreaks', 'JenksCaspall', 'JenksCaspallForced', 'JenksCaspallSampled', 'MaxP', 'MaximumBreaks', 'NaturalBreaks', 'Quantiles', 'Percentiles', 'StdMean', 'UserDefined'). Arguments can be passed in classification_kwds.</p> <code>'Quantiles'</code> <code>k</code> <code>int</code> <p>Number of classes (ignored if scheme is None or if column is categorical). Default to 5.</p> <code>5</code> <code>add_legend</code> <code>bool</code> <p>Whether to add a legend to the map. Defaults to True.</p> <code>True</code> <code>legend_title</code> <code>str</code> <p>The title of the legend. Defaults to None.</p> <code>None</code> <code>legend_position</code> <code>str</code> <p>The position of the legend. Can be 'top-left', 'top-right', 'bottom-left', or 'bottom-right'. Defaults to 'bottom-right'.</p> <code>'bottom-right'</code> <code>legend_kwds</code> <code>dict</code> <p>Keyword arguments to pass to :func:<code>matplotlib.pyplot.legend</code> or <code>matplotlib.pyplot.colorbar</code>. Defaults to None. Keyword arguments to pass to :func:<code>matplotlib.pyplot.legend</code> or Additional accepted keywords when <code>scheme</code> is specified: fmt : string     A formatting specification for the bin edges of the classes in the     legend. For example, to have no decimals: <code>{\"fmt\": \"{:.0f}\"}</code>. labels : list-like     A list of legend labels to override the auto-generated labblels.     Needs to have the same number of elements as the number of     classes (<code>k</code>). interval : boolean (default False)     An option to control brackets from mapclassify legend.     If True, open/closed interval brackets are shown in the legend.</p> <code>None</code> <code>classification_kwds</code> <code>dict</code> <p>Keyword arguments to pass to mapclassify. Defaults to None.</p> <code>None</code> <code>legend_args</code> <code>dict</code> <p>Additional keyword arguments for the add_legend method. Defaults to None.</p> <code>None</code> <code>layer_type</code> <code>str</code> <p>The type of layer to add. Can be 'circle', 'line', or 'fill'. Defaults to None.</p> <code>None</code> <code>filter</code> <code>dict</code> <p>The filter to apply to the layer. If None, no filter is applied.</p> <code>None</code> <code>paint</code> <code>dict</code> <p>The paint properties to apply to the layer. If None, no paint properties are applied.</p> <code>None</code> <code>name</code> <code>str</code> <p>The name of the layer. If None, a random name is generated.</p> <code>None</code> <code>fit_bounds</code> <code>bool</code> <p>Whether to adjust the viewport of the map to fit the bounds of the GeoJSON data. Defaults to True.</p> <code>True</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible or not. Defaults to True.</p> <code>True</code> <code>before_id</code> <code>str</code> <p>The ID of an existing layer before which the new layer should be inserted.</p> <code>None</code> <code>source_args</code> <code>dict</code> <p>Additional keyword arguments that are passed to the GeoJSONSource class.</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the GeoJSON class, such as fields, which can be a list of column names to be included in the popup.</p> <code>{}</code> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_data(\n    self,\n    data: Union[str],\n    column: str,\n    cmap: Optional[str] = None,\n    colors: Optional[str] = None,\n    labels: Optional[str] = None,\n    scheme: Optional[str] = \"Quantiles\",\n    k: int = 5,\n    add_legend: Optional[bool] = True,\n    legend_title: Optional[bool] = None,\n    legend_position: Optional[str] = \"bottom-right\",\n    legend_kwds: Optional[dict] = None,\n    classification_kwds: Optional[dict] = None,\n    legend_args: Optional[dict] = None,\n    layer_type: Optional[str] = None,\n    extrude: Optional[bool] = False,\n    scale_factor: Optional[float] = 1.0,\n    filter: Optional[Dict] = None,\n    paint: Optional[Dict] = None,\n    name: Optional[str] = None,\n    fit_bounds: bool = True,\n    visible: bool = True,\n    opacity: float = 1.0,\n    before_id: Optional[str] = None,\n    source_args: Dict = {},\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Add vector data to the map with a variety of classification schemes.\n\n    Args:\n        data (str | pd.DataFrame | gpd.GeoDataFrame): The data to classify.\n            It can be a filepath to a vector dataset, a pandas dataframe, or\n            a geopandas geodataframe.\n        column (str): The column to classify.\n        cmap (str, optional): The name of a colormap recognized by matplotlib. Defaults to None.\n        colors (list, optional): A list of colors to use for the classification. Defaults to None.\n        labels (list, optional): A list of labels to use for the legend. Defaults to None.\n        scheme (str, optional): Name of a choropleth classification scheme (requires mapclassify).\n            Name of a choropleth classification scheme (requires mapclassify).\n            A mapclassify.MapClassifier object will be used\n            under the hood. Supported are all schemes provided by mapclassify (e.g.\n            'BoxPlot', 'EqualInterval', 'FisherJenks', 'FisherJenksSampled',\n            'HeadTailBreaks', 'JenksCaspall', 'JenksCaspallForced',\n            'JenksCaspallSampled', 'MaxP', 'MaximumBreaks',\n            'NaturalBreaks', 'Quantiles', 'Percentiles', 'StdMean',\n            'UserDefined'). Arguments can be passed in classification_kwds.\n        k (int, optional): Number of classes (ignored if scheme is None or if\n            column is categorical). Default to 5.\n        add_legend (bool, optional): Whether to add a legend to the map. Defaults to True.\n        legend_title (str, optional): The title of the legend. Defaults to None.\n        legend_position (str, optional): The position of the legend. Can be 'top-left',\n            'top-right', 'bottom-left', or 'bottom-right'. Defaults to 'bottom-right'.\n        legend_kwds (dict, optional): Keyword arguments to pass to :func:`matplotlib.pyplot.legend`\n            or `matplotlib.pyplot.colorbar`. Defaults to None.\n            Keyword arguments to pass to :func:`matplotlib.pyplot.legend` or\n            Additional accepted keywords when `scheme` is specified:\n            fmt : string\n                A formatting specification for the bin edges of the classes in the\n                legend. For example, to have no decimals: ``{\"fmt\": \"{:.0f}\"}``.\n            labels : list-like\n                A list of legend labels to override the auto-generated labblels.\n                Needs to have the same number of elements as the number of\n                classes (`k`).\n            interval : boolean (default False)\n                An option to control brackets from mapclassify legend.\n                If True, open/closed interval brackets are shown in the legend.\n        classification_kwds (dict, optional): Keyword arguments to pass to mapclassify.\n            Defaults to None.\n        legend_args (dict, optional): Additional keyword arguments for the add_legend method. Defaults to None.\n        layer_type (str, optional): The type of layer to add. Can be 'circle', 'line', or 'fill'. Defaults to None.\n        filter (dict, optional): The filter to apply to the layer. If None,\n            no filter is applied.\n        paint (dict, optional): The paint properties to apply to the layer.\n            If None, no paint properties are applied.\n        name (str, optional): The name of the layer. If None, a random name\n            is generated.\n        fit_bounds (bool, optional): Whether to adjust the viewport of the\n            map to fit the bounds of the GeoJSON data. Defaults to True.\n        visible (bool, optional): Whether the layer is visible or not.\n            Defaults to True.\n        before_id (str, optional): The ID of an existing layer before which\n            the new layer should be inserted.\n        source_args (dict, optional): Additional keyword arguments that are\n            passed to the GeoJSONSource class.\n        **kwargs: Additional keyword arguments to pass to the GeoJSON class, such as\n            fields, which can be a list of column names to be included in the popup.\n\n    \"\"\"\n    self.session.m.add_data(\n        data=data,\n        column=column,\n        cmap=cmap,\n        colors=colors,\n        labels=labels,\n        scheme=scheme,\n        k=k,\n        add_legend=add_legend,\n        legend_title=legend_title,\n        legend_position=legend_position,\n        legend_kwds=legend_kwds,\n        classification_kwds=classification_kwds,\n        legend_args=legend_args,\n        layer_type=layer_type,\n        extrude=extrude,\n        scale_factor=scale_factor,\n        filter=filter,\n        paint=paint,\n        name=name,\n        fit_bounds=fit_bounds,\n        visible=visible,\n        opacity=opacity,\n        before_id=before_id,\n        source_args=source_args,\n        **kwargs,\n    )\n    return f\"Data added: {name}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_draw_control","title":"<code>add_draw_control(options=None, controls=None, position='top-right', geojson=None, **kwargs)</code>","text":"<p>Adds a drawing control to the map.</p> <p>This method enables users to add interactive drawing controls to the map, allowing for the creation, editing, and deletion of geometric shapes on the map. The options, position, and initial GeoJSON can be customized.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration options for the drawing control. Defaults to None.</p> <code>None</code> <code>controls</code> <code>Optional[Dict[str, Any]]</code> <p>The drawing controls to enable. Can be one or more of the following: 'polygon', 'line_string', 'point', 'trash', 'combine_features', 'uncombine_features'. Defaults to None.</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the control on the map. Defaults to \"top-right\".</p> <code>'top-right'</code> <code>geojson</code> <code>Optional[Dict[str, Any]]</code> <p>Initial GeoJSON data to load into the drawing control. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the drawing control.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_draw_control(\n    self,\n    options: Optional[Dict[str, Any]] = None,\n    controls: Optional[Dict[str, Any]] = None,\n    position: str = \"top-right\",\n    geojson: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds a drawing control to the map.\n\n    This method enables users to add interactive drawing controls to the map,\n    allowing for the creation, editing, and deletion of geometric shapes on\n    the map. The options, position, and initial GeoJSON can be customized.\n\n    Args:\n        options (Optional[Dict[str, Any]]): Configuration options for the\n            drawing control. Defaults to None.\n        controls (Optional[Dict[str, Any]]): The drawing controls to enable.\n            Can be one or more of the following: 'polygon', 'line_string',\n            'point', 'trash', 'combine_features', 'uncombine_features'.\n            Defaults to None.\n        position (str): The position of the control on the map. Defaults\n            to \"top-right\".\n        geojson (Optional[Dict[str, Any]]): Initial GeoJSON data to load\n            into the drawing control. Defaults to None.\n        **kwargs (Any): Additional keyword arguments to be passed to the\n            drawing control.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_draw_control(\n        options=options,\n        controls=controls,\n        position=position,\n        geojson=geojson,\n        **kwargs,\n    )\n    return f\"Draw control added: {position}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_html","title":"<code>add_html(html, bg_color='white', position='bottom-right', **kwargs)</code>","text":"<p>Add HTML content to the map.</p> <p>This method allows for the addition of arbitrary HTML content to the map, which can be used to display custom information or controls. The background color and position of the HTML content can be customized.</p> <p>Parameters:</p> Name Type Description Default <code>html</code> <code>str</code> <p>The HTML content to add.</p> required <code>bg_color</code> <code>str</code> <p>The background color of the HTML content. Defaults to \"white\". To make the background transparent, set this to \"transparent\". To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".</p> <code>'white'</code> <code>position</code> <code>str</code> <p>The position of the HTML content on the map. Can be one of \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\". Defaults to \"bottom-right\".</p> <code>'bottom-right'</code> <code>**kwargs</code> <code>Union[str, int, float]</code> <p>Additional keyword arguments for future use.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_html(\n    self,\n    html: str,\n    bg_color: str = \"white\",\n    position: str = \"bottom-right\",\n    **kwargs: Union[str, int, float],\n) -&gt; None:\n    \"\"\"\n    Add HTML content to the map.\n\n    This method allows for the addition of arbitrary HTML content to the map, which can be used to display\n    custom information or controls. The background color and position of the HTML content can be customized.\n\n    Args:\n        html (str): The HTML content to add.\n        bg_color (str, optional): The background color of the HTML content. Defaults to \"white\".\n            To make the background transparent, set this to \"transparent\".\n            To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".\n        position (str, optional): The position of the HTML content on the map. Can be one of \"top-left\",\n            \"top-right\", \"bottom-left\", \"bottom-right\". Defaults to \"bottom-right\".\n        **kwargs: Additional keyword arguments for future use.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_html(\n        html=html, bg_color=bg_color, position=position, **kwargs\n    )\n    return f\"HTML added: {html}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_image","title":"<code>add_image(id=None, image=None, width=None, height=None, coordinates=None, position=None, icon_size=1.0, **kwargs)</code>","text":"<p>Add an image to the map.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The layer ID of the image.</p> <code>None</code> <code>image</code> <code>Union[str, Dict, ndarray]</code> <p>The URL or local file path to the image, or a dictionary containing image data, or a numpy array representing the image.</p> <code>None</code> <code>width</code> <code>int</code> <p>The width of the image. Defaults to None.</p> <code>None</code> <code>height</code> <code>int</code> <p>The height of the image. Defaults to None.</p> <code>None</code> <code>coordinates</code> <code>List[float]</code> <p>The longitude and latitude coordinates to place the image.</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the image. Defaults to None. Can be one of 'top-right', 'top-left', 'bottom-right', 'bottom-left'.</p> <code>None</code> <code>icon_size</code> <code>float</code> <p>The size of the icon. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_image(\n    self,\n    id: str = None,\n    image: Union[str, Dict] = None,\n    width: int = None,\n    height: int = None,\n    coordinates: List[float] = None,\n    position: str = None,\n    icon_size: float = 1.0,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Add an image to the map.\n\n    Args:\n        id (str): The layer ID of the image.\n        image (Union[str, Dict, np.ndarray]): The URL or local file path to\n            the image, or a dictionary containing image data, or a numpy\n            array representing the image.\n        width (int, optional): The width of the image. Defaults to None.\n        height (int, optional): The height of the image. Defaults to None.\n        coordinates (List[float], optional): The longitude and latitude\n            coordinates to place the image.\n        position (str, optional): The position of the image. Defaults to None.\n            Can be one of 'top-right', 'top-left', 'bottom-right', 'bottom-left'.\n        icon_size (float, optional): The size of the icon. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_image(\n        id=id,\n        image=image,\n        width=width,\n        height=height,\n        coordinates=coordinates,\n        position=position,\n        icon_size=icon_size,\n        **kwargs,\n    )\n    return f\"Image added: {id}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_labels","title":"<code>add_labels(source, column, name=None, text_size=14, text_anchor='center', text_color='black', min_zoom=None, max_zoom=None, layout=None, paint=None, before_id=None, opacity=1.0, visible=True, **kwargs)</code>","text":"<p>Adds a label layer to the map.</p> <p>This method adds a label layer to the map using the specified source and column for text values.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Dict[str, Any]]</code> <p>The data source for the labels. It can be a GeoJSON file path or a dictionary containing GeoJSON data.</p> required <code>column</code> <code>str</code> <p>The column name in the source data to use for the label text.</p> required <code>name</code> <code>Optional[str]</code> <p>The name of the label layer. If None, a random name is generated. Defaults to None.</p> <code>None</code> <code>text_size</code> <code>int</code> <p>The size of the label text. Defaults to 14.</p> <code>14</code> <code>text_anchor</code> <code>str</code> <p>The anchor position of the text. Can be \"center\", \"left\", \"right\", etc. Defaults to \"center\".</p> <code>'center'</code> <code>text_color</code> <code>str</code> <p>The color of the label text. Defaults to \"black\".</p> <code>'black'</code> <code>min_zoom</code> <code>Optional[float]</code> <p>The minimum zoom level at which the labels are visible. Defaults to None.</p> <code>None</code> <code>max_zoom</code> <code>Optional[float]</code> <p>The maximum zoom level at which the labels are visible. Defaults to None.</p> <code>None</code> <code>layout</code> <code>Optional[Dict[str, Any]]</code> <p>Additional layout properties for the label layer. Defaults to None. For more information, refer to https://maplibre.org/maplibre-style-spec/layers/#symbol.</p> <code>None</code> <code>paint</code> <code>Optional[Dict[str, Any]]</code> <p>Additional paint properties for the label layer. Defaults to None.</p> <code>None</code> <code>before_id</code> <code>Optional[str]</code> <p>The ID of an existing layer before which the new layer should be inserted. Defaults to None.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>The opacity of the label layer. Defaults to 1.0.</p> <code>1.0</code> <code>visible</code> <code>bool</code> <p>Whether the label layer is visible by default. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to customize the label layer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_labels(\n    self,\n    source: Union[str, Dict[str, Any]],\n    column: str,\n    name: Optional[str] = None,\n    text_size: int = 14,\n    text_anchor: str = \"center\",\n    text_color: str = \"black\",\n    min_zoom: Optional[float] = None,\n    max_zoom: Optional[float] = None,\n    layout: Optional[Dict[str, Any]] = None,\n    paint: Optional[Dict[str, Any]] = None,\n    before_id: Optional[str] = None,\n    opacity: float = 1.0,\n    visible: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds a label layer to the map.\n\n    This method adds a label layer to the map using the specified source and column for text values.\n\n    Args:\n        source (Union[str, Dict[str, Any]]): The data source for the labels. It can be a GeoJSON file path\n            or a dictionary containing GeoJSON data.\n        column (str): The column name in the source data to use for the label text.\n        name (Optional[str]): The name of the label layer. If None, a random name is generated. Defaults to None.\n        text_size (int): The size of the label text. Defaults to 14.\n        text_anchor (str): The anchor position of the text. Can be \"center\", \"left\", \"right\", etc. Defaults to \"center\".\n        text_color (str): The color of the label text. Defaults to \"black\".\n        min_zoom (Optional[float]): The minimum zoom level at which the labels are visible. Defaults to None.\n        max_zoom (Optional[float]): The maximum zoom level at which the labels are visible. Defaults to None.\n        layout (Optional[Dict[str, Any]]): Additional layout properties for the label layer. Defaults to None.\n            For more information, refer to https://maplibre.org/maplibre-style-spec/layers/#symbol.\n        paint (Optional[Dict[str, Any]]): Additional paint properties for the label layer. Defaults to None.\n        before_id (Optional[str]): The ID of an existing layer before which the new layer should be inserted. Defaults to None.\n        opacity (float): The opacity of the label layer. Defaults to 1.0.\n        visible (bool): Whether the label layer is visible by default. Defaults to True.\n        **kwargs (Any): Additional keyword arguments to customize the label layer.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_labels(\n        source=source,\n        column=column,\n        name=name,\n        text_size=text_size,\n        text_anchor=text_anchor,\n        text_color=text_color,\n        min_zoom=min_zoom,\n        max_zoom=max_zoom,\n        layout=layout,\n        paint=paint,\n        before_id=before_id,\n        opacity=opacity,\n        visible=visible,\n        **kwargs,\n    )\n    return f\"Labels added: {name}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_layer_control","title":"<code>add_layer_control(layer_ids=None, theme='default', css_text=None, position='top-left', bg_layers=False)</code>","text":"<p>Adds a layer control to the map.</p> <p>This function creates and adds a layer switcher control to the map, allowing users to toggle the visibility of specified layers. The appearance and functionality of the layer control can be customized with parameters such as theme, CSS styling, and position on the map.</p> <p>Parameters:</p> Name Type Description Default <code>layer_ids</code> <code>Optional[List[str]]</code> <p>A list of layer IDs to include in the control. If None, all layers in the map will be included. Defaults to None.</p> <code>None</code> <code>theme</code> <code>str</code> <p>The theme for the layer switcher control. Can be \"default\" or other custom themes. Defaults to \"default\".</p> <code>'default'</code> <code>css_text</code> <code>Optional[str]</code> <p>Custom CSS text for styling the layer control. If None, a default style will be applied. Defaults to None.</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the layer control on the map. Can be \"top-left\", \"top-right\", \"bottom-left\", or \"bottom-right\". Defaults to \"top-left\".</p> <code>'top-left'</code> <code>bg_layers</code> <code>bool</code> <p>If True, background layers will be included in the control. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_layer_control(\n    self,\n    layer_ids: Optional[List[str]] = None,\n    theme: str = \"default\",\n    css_text: Optional[str] = None,\n    position: str = \"top-left\",\n    bg_layers: Optional[Union[bool, List[str]]] = False,\n) -&gt; None:\n    \"\"\"\n    Adds a layer control to the map.\n\n    This function creates and adds a layer switcher control to the map, allowing users to toggle the visibility\n    of specified layers. The appearance and functionality of the layer control can be customized with parameters\n    such as theme, CSS styling, and position on the map.\n\n    Args:\n        layer_ids (Optional[List[str]]): A list of layer IDs to include in the control. If None, all layers\n            in the map will be included. Defaults to None.\n        theme (str): The theme for the layer switcher control. Can be \"default\" or other custom themes. Defaults to \"default\".\n        css_text (Optional[str]): Custom CSS text for styling the layer control. If None, a default style will be applied.\n            Defaults to None.\n        position (str): The position of the layer control on the map. Can be \"top-left\", \"top-right\", \"bottom-left\",\n            or \"bottom-right\". Defaults to \"top-left\".\n        bg_layers (bool): If True, background layers will be included in the control. Defaults to False.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_layer_control(\n        layer_ids=layer_ids,\n        theme=theme,\n        css_text=css_text,\n        position=position,\n        bg_layers=bg_layers,\n    )\n    return f\"Layer control added: {position}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_legend","title":"<code>add_legend(title='Legend', legend_dict=None, labels=None, colors=None, fontsize=15, bg_color='white', position='bottom-right', builtin_legend=None, shape_type='rectangle', **kwargs)</code>","text":"<p>Adds a legend to the map.</p> <p>This method allows for the addition of a legend to the map. The legend can be customized with a title, labels, colors, and more. A built-in legend can also be specified.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>The title of the legend. Defaults to \"Legend\".</p> <code>'Legend'</code> <code>legend_dict</code> <code>Optional[Dict[str, str]]</code> <p>A dictionary with legend items as keys and colors as values. If provided, <code>labels</code> and <code>colors</code> will be ignored. Defaults to None.</p> <code>None</code> <code>labels</code> <code>Optional[List[str]]</code> <p>A list of legend labels. Defaults to None.</p> <code>None</code> <code>colors</code> <code>Optional[List[str]]</code> <p>A list of colors corresponding to the labels. Defaults to None.</p> <code>None</code> <code>fontsize</code> <code>int</code> <p>The font size of the legend text. Defaults to 15.</p> <code>15</code> <code>bg_color</code> <code>str</code> <p>The background color of the legend. Defaults to \"white\". To make the background transparent, set this to \"transparent\". To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".</p> <code>'white'</code> <code>position</code> <code>str</code> <p>The position of the legend on the map. Can be one of \"top-left\", \"top-right\", \"bottom-left\", \"bottom-right\". Defaults to \"bottom-right\".</p> <code>'bottom-right'</code> <code>builtin_legend</code> <code>Optional[str]</code> <p>The name of a built-in legend to use. Defaults to None.</p> <code>None</code> <code>shape_type</code> <code>str</code> <p>The shape type of the legend items. Can be one of \"rectangle\", \"circle\", or \"line\".</p> <code>'rectangle'</code> <code>**kwargs</code> <code>Union[str, int, float]</code> <p>Additional keyword arguments for future use.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_legend(\n    self,\n    title: str = \"Legend\",\n    legend_dict: Optional[Dict[str, str]] = None,\n    labels: Optional[List[str]] = None,\n    colors: Optional[List[str]] = None,\n    fontsize: int = 15,\n    bg_color: str = \"white\",\n    position: str = \"bottom-right\",\n    builtin_legend: Optional[str] = None,\n    shape_type: str = \"rectangle\",\n    **kwargs: Union[str, int, float],\n) -&gt; None:\n    \"\"\"\n    Adds a legend to the map.\n\n    This method allows for the addition of a legend to the map. The legend can be customized with a title,\n    labels, colors, and more. A built-in legend can also be specified.\n\n    Args:\n        title (str, optional): The title of the legend. Defaults to \"Legend\".\n        legend_dict (Optional[Dict[str, str]], optional): A dictionary with legend items as keys and colors as values.\n            If provided, `labels` and `colors` will be ignored. Defaults to None.\n        labels (Optional[List[str]], optional): A list of legend labels. Defaults to None.\n        colors (Optional[List[str]], optional): A list of colors corresponding to the labels. Defaults to None.\n        fontsize (int, optional): The font size of the legend text. Defaults to 15.\n        bg_color (str, optional): The background color of the legend. Defaults to \"white\".\n            To make the background transparent, set this to \"transparent\".\n            To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".\n        position (str, optional): The position of the legend on the map. Can be one of \"top-left\",\n            \"top-right\", \"bottom-left\", \"bottom-right\". Defaults to \"bottom-right\".\n        builtin_legend (Optional[str], optional): The name of a built-in legend to use. Defaults to None.\n        shape_type (str, optional): The shape type of the legend items. Can be one of \"rectangle\", \"circle\", or \"line\".\n        **kwargs: Additional keyword arguments for future use.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_legend(\n        title=title,\n        legend_dict=legend_dict,\n        labels=labels,\n        colors=colors,\n        fontsize=fontsize,\n        bg_color=bg_color,\n        position=position,\n        builtin_legend=builtin_legend,\n        shape_type=shape_type,\n        **kwargs,\n    )\n    return f\"Legend added: {title}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_mapillary","title":"<code>add_mapillary(minzoom=6, maxzoom=14, sequence_lyr_name='sequence', image_lyr_name='image', before_id=None, sequence_paint=None, image_paint=None, image_minzoom=17, add_popup=True, access_token=None, opacity=1.0, visible=True, add_to_sidebar=False, style='photo', radius=5e-05, height=420, frame_border=0, default_message='No Mapillary image found', widget_icon='mdi-image', widget_label='Mapillary StreetView', **kwargs)</code>","text":"<p>Adds Mapillary layers to the map.</p> <p>Parameters:</p> Name Type Description Default <code>minzoom</code> <code>int</code> <p>Minimum zoom level for the Mapillary tiles. Defaults to 6.</p> <code>6</code> <code>maxzoom</code> <code>int</code> <p>Maximum zoom level for the Mapillary tiles. Defaults to 14.</p> <code>14</code> <code>sequence_lyr_name</code> <code>str</code> <p>Name of the sequence layer. Defaults to \"sequence\".</p> <code>'sequence'</code> <code>image_lyr_name</code> <code>str</code> <p>Name of the image layer. Defaults to \"image\".</p> <code>'image'</code> <code>before_id</code> <code>str</code> <p>The ID of an existing layer to insert the new layer before. Defaults to None.</p> <code>None</code> <code>sequence_paint</code> <code>dict</code> <p>Paint properties for the sequence layer. Defaults to None.</p> <code>None</code> <code>image_paint</code> <code>dict</code> <p>Paint properties for the image layer. Defaults to None.</p> <code>None</code> <code>image_minzoom</code> <code>int</code> <p>Minimum zoom level for the image layer. Defaults to 17.</p> <code>17</code> <code>add_popup</code> <code>bool</code> <p>Whether to add popups to the layers. Defaults to True.</p> <code>True</code> <code>access_token</code> <code>str</code> <p>Access token for Mapillary API. Defaults to None.</p> <code>None</code> <code>opacity</code> <code>float</code> <p>Opacity of the Mapillary layers. Defaults to 1.0.</p> <code>1.0</code> <code>visible</code> <code>bool</code> <p>Whether the Mapillary layers are visible. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no access token is provided.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_mapillary(\n    self,\n    minzoom: int = 6,\n    maxzoom: int = 14,\n    sequence_lyr_name: str = \"sequence\",\n    image_lyr_name: str = \"image\",\n    before_id: str = None,\n    sequence_paint: dict = None,\n    image_paint: dict = None,\n    image_minzoom: int = 17,\n    add_popup: bool = True,\n    access_token: str = None,\n    opacity: float = 1.0,\n    visible: bool = True,\n    add_to_sidebar: bool = False,\n    style: str = \"photo\",\n    radius: float = 0.00005,\n    height: int = 420,\n    frame_border: int = 0,\n    default_message: str = \"No Mapillary image found\",\n    widget_icon: str = \"mdi-image\",\n    widget_label: str = \"Mapillary StreetView\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds Mapillary layers to the map.\n\n    Args:\n        minzoom (int): Minimum zoom level for the Mapillary tiles. Defaults to 6.\n        maxzoom (int): Maximum zoom level for the Mapillary tiles. Defaults to 14.\n        sequence_lyr_name (str): Name of the sequence layer. Defaults to \"sequence\".\n        image_lyr_name (str): Name of the image layer. Defaults to \"image\".\n        before_id (str): The ID of an existing layer to insert the new layer before. Defaults to None.\n        sequence_paint (dict, optional): Paint properties for the sequence layer. Defaults to None.\n        image_paint (dict, optional): Paint properties for the image layer. Defaults to None.\n        image_minzoom (int): Minimum zoom level for the image layer. Defaults to 17.\n        add_popup (bool): Whether to add popups to the layers. Defaults to True.\n        access_token (str, optional): Access token for Mapillary API. Defaults to None.\n        opacity (float): Opacity of the Mapillary layers. Defaults to 1.0.\n        visible (bool): Whether the Mapillary layers are visible. Defaults to True.\n\n    Raises:\n        ValueError: If no access token is provided.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_mapillary(\n        minzoom=minzoom,\n        maxzoom=maxzoom,\n        sequence_lyr_name=sequence_lyr_name,\n        image_lyr_name=image_lyr_name,\n        before_id=before_id,\n        sequence_paint=sequence_paint,\n        image_paint=image_paint,\n        image_minzoom=image_minzoom,\n        add_popup=add_popup,\n        access_token=access_token,\n        opacity=opacity,\n        visible=visible,\n        add_to_sidebar=add_to_sidebar,\n        style=style,\n        radius=radius,\n        height=height,\n        frame_border=frame_border,\n        default_message=default_message,\n        widget_icon=widget_icon,\n        widget_label=widget_label,\n        **kwargs,\n    )\n    return f\"Mapillary added: {sequence_lyr_name}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_marker","title":"<code>add_marker(lng_lat, popup=None, options=None)</code>","text":"<p>Adds a marker to the map.</p> <p>Parameters:</p> Name Type Description Default <code>lng_lat</code> <code>List[Union[float, float]]</code> <p>A list of two floats representing the longitude and latitude of the marker.</p> required <code>popup</code> <code>Optional[str]</code> <p>The text to display in a popup when the marker is clicked. Defaults to None.</p> <code>None</code> <code>options</code> <code>Optional[Dict]</code> <p>A dictionary of options to customize the marker. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_marker(\n    self,\n    lng_lat: List[Union[float, float]],\n    popup: Optional[Dict] = None,\n    options: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"\n    Adds a marker to the map.\n\n    Args:\n        lng_lat (List[Union[float, float]]): A list of two floats\n            representing the longitude and latitude of the marker.\n        popup (Optional[str], optional): The text to display in a popup when\n            the marker is clicked. Defaults to None.\n        options (Optional[Dict], optional): A dictionary of options to\n            customize the marker. Defaults to None.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_marker(lng_lat=lng_lat, popup=popup, options=options)\n    return f\"Marker added: {lng_lat}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_nlcd","title":"<code>add_nlcd(years=[2023], add_legend=True, **kwargs)</code>","text":"<p>Adds National Land Cover Database (NLCD) data to the map.</p> <p>Parameters:</p> Name Type Description Default <code>years</code> <code>list</code> <p>A list of years to add. It can be any of 1985-2023. Defaults to [2023].</p> <code>[2023]</code> <code>add_legend</code> <code>bool</code> <p>Whether to add a legend to the map. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the add_cog_layer method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_nlcd(\n    self, years: list = [2023], add_legend: bool = True, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Adds National Land Cover Database (NLCD) data to the map.\n\n    Args:\n        years (list): A list of years to add. It can be any of 1985-2023. Defaults to [2023].\n        add_legend (bool): Whether to add a legend to the map. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to the add_cog_layer method.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_nlcd(\n        years=years,\n        add_legend=add_legend,\n        **kwargs,\n    )\n    return f\"NLCD added: {years}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_nwi_basemap","title":"<code>add_nwi_basemap(name='NWI Wetlands', format='image/png', attribution='USFWS', opacity=1.0, visible=True, tile_size=256, before_id=None, overwrite=False, **kwargs)</code>","text":"<p>Adds a NWI Wetlands basemap to the map.</p> <p>This method adds a NWI Wetlands basemap to the map. The NWI Wetlands basemap is created from     the specified URL, and it is added to the map with the specified     name, attribution, visibility, and tile size.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to use for the layer. Defaults to 'NWI Wetlands'.</p> <code>'NWI Wetlands'</code> <code>format</code> <code>str</code> <p>The format of the tiles in the layer.</p> <code>'image/png'</code> <code>attribution</code> <code>str</code> <p>The attribution to use for the layer. Defaults to ''.</p> <code>'USFWS'</code> <code>visible</code> <code>bool</code> <p>Whether the layer should be visible by default. Defaults to True.</p> <code>True</code> <code>tile_size</code> <code>int</code> <p>The size of the tiles in the layer. Defaults to 256.</p> <code>256</code> <code>before_id</code> <code>str</code> <p>The ID of an existing layer before which the new layer should be inserted.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing layer with the same name. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that are passed to the Layer class. See https://eodagmbh.github.io/py-maplibregl/api/layer/ for more information.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_nwi_basemap(\n    self,\n    name: str = \"NWI Wetlands\",\n    format: str = \"image/png\",\n    attribution: str = \"USFWS\",\n    opacity: float = 1.0,\n    visible: bool = True,\n    tile_size: int = 256,\n    before_id: Optional[str] = None,\n    overwrite: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds a NWI Wetlands basemap to the map.\n\n    This method adds a NWI Wetlands basemap to the map. The NWI Wetlands basemap is created from\n        the specified URL, and it is added to the map with the specified\n        name, attribution, visibility, and tile size.\n\n    Args:\n        name (str, optional): The name to use for the layer. Defaults to\n            'NWI Wetlands'.\n        format (str, optional): The format of the tiles in the layer.\n        attribution (str, optional): The attribution to use for the layer.\n            Defaults to ''.\n        visible (bool, optional): Whether the layer should be visible by\n            default. Defaults to True.\n        tile_size (int, optional): The size of the tiles in the layer.\n            Defaults to 256.\n        before_id (str, optional): The ID of an existing layer before which\n            the new layer should be inserted.\n        overwrite (bool, optional): Whether to overwrite an existing layer with the same name.\n            Defaults to False.\n        **kwargs: Additional keyword arguments that are passed to the Layer class.\n            See https://eodagmbh.github.io/py-maplibregl/api/layer/ for more information.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_nwi_basemap(\n        name=name,\n        format=format,\n        attribution=attribution,\n        opacity=opacity,\n        visible=visible,\n        tile_size=tile_size,\n        before_id=before_id,\n        overwrite=overwrite,\n        **kwargs,\n    )\n    return f\"NWI Wetlands basemap added: {name}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_overture_3d_buildings","title":"<code>add_overture_3d_buildings(release=None, style=None, values=None, colors=None, visible=True, opacity=1.0, tooltip=True, template='simple', fit_bounds=False, **kwargs)</code>","text":"<p>Add 3D buildings from Overture Maps to the map.</p> <p>Parameters:</p> Name Type Description Default <code>release</code> <code>Optional[str]</code> <p>The release date of the Overture Maps data. Defaults to the latest release. For more info, see https://github.com/OvertureMaps/overture-tiles.</p> <code>None</code> <code>style</code> <code>Optional[Dict[str, Any]]</code> <p>The style dictionary for the buildings. Defaults to None.</p> <code>None</code> <code>values</code> <code>Optional[List[int]]</code> <p>List of height values for color interpolation. Defaults to None.</p> <code>None</code> <code>colors</code> <code>Optional[List[str]]</code> <p>List of colors corresponding to the height values. Defaults to None.</p> <code>None</code> <code>visible</code> <code>bool</code> <p>Whether the buildings layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>float</code> <p>The opacity of the buildings layer. Defaults to 1.0.</p> <code>1.0</code> <code>tooltip</code> <code>bool</code> <p>Whether to show tooltips on the buildings. Defaults to True.</p> <code>True</code> <code>template</code> <code>str</code> <p>The template for the tooltip. It can be \"simple\" or \"all\". Defaults to \"simple\".</p> <code>'simple'</code> <code>fit_bounds</code> <code>bool</code> <p>Whether to fit the map bounds to the buildings layer. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of values and colors lists are not the same.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Add 3D buildings from Overture Maps to the map\")\ndef add_overture_3d_buildings(\n    self,\n    release: Optional[str] = None,\n    style: Optional[Dict[str, Any]] = None,\n    values: Optional[List[int]] = None,\n    colors: Optional[List[str]] = None,\n    visible: bool = True,\n    opacity: float = 1.0,\n    tooltip: bool = True,\n    template: str = \"simple\",\n    fit_bounds: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Add 3D buildings from Overture Maps to the map.\n\n    Args:\n        release (Optional[str], optional): The release date of the Overture Maps data.\n            Defaults to the latest release. For more info, see\n            https://github.com/OvertureMaps/overture-tiles.\n        style (Optional[Dict[str, Any]], optional): The style dictionary for\n            the buildings. Defaults to None.\n        values (Optional[List[int]], optional): List of height values for\n            color interpolation. Defaults to None.\n        colors (Optional[List[str]], optional): List of colors corresponding\n            to the height values. Defaults to None.\n        visible (bool, optional): Whether the buildings layer is visible.\n            Defaults to True.\n        opacity (float, optional): The opacity of the buildings layer.\n            Defaults to 1.0.\n        tooltip (bool, optional): Whether to show tooltips on the buildings.\n            Defaults to True.\n        template (str, optional): The template for the tooltip. It can be\n            \"simple\" or \"all\". Defaults to \"simple\".\n        fit_bounds (bool, optional): Whether to fit the map bounds to the\n            buildings layer. Defaults to False.\n\n    Raises:\n        ValueError: If the length of values and colors lists are not the same.\n    \"\"\"\n    self.session.m.add_overture_3d_buildings(\n        release=release,\n        style=style,\n        values=values,\n        colors=colors,\n        visible=visible,\n        opacity=opacity,\n        tooltip=tooltip,\n        template=template,\n        fit_bounds=fit_bounds,\n        **kwargs,\n    )\n    return f\"Overture 3D buildings added: {release}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_pmtiles","title":"<code>add_pmtiles(url, style=None, visible=True, opacity=1.0, exclude_mask=False, tooltip=True, properties=None, template=None, attribution='PMTiles', fit_bounds=True, **kwargs)</code>","text":"<p>Adds a PMTiles layer to the map.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the PMTiles file.</p> required <code>style</code> <code>dict</code> <p>The CSS style to apply to the layer. Defaults to None. See https://docs.mapbox.com/style-spec/reference/layers/ for more info.</p> <code>None</code> <code>visible</code> <code>bool</code> <p>Whether the layer should be shown initially. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>float</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>exclude_mask</code> <code>bool</code> <p>Whether to exclude the mask layer. Defaults to False.</p> <code>False</code> <code>tooltip</code> <code>bool</code> <p>Whether to show tooltips on the layer. Defaults to True.</p> <code>True</code> <code>properties</code> <code>dict</code> <p>The properties to use for the tooltips. Defaults to None.</p> <code>None</code> <code>template</code> <code>str</code> <p>The template to use for the tooltips. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>str</code> <p>The attribution to use for the layer. Defaults to 'PMTiles'.</p> <code>'PMTiles'</code> <code>fit_bounds</code> <code>bool</code> <p>Whether to zoom to the layer extent. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the PMTilesLayer constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_pmtiles(\n    self,\n    url: str,\n    style: Optional[Dict] = None,\n    visible: bool = True,\n    opacity: float = 1.0,\n    exclude_mask: bool = False,\n    tooltip: bool = True,\n    properties: Optional[Dict] = None,\n    template: Optional[str] = None,\n    attribution: str = \"PMTiles\",\n    fit_bounds: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds a PMTiles layer to the map.\n\n    Args:\n        url (str): The URL of the PMTiles file.\n        style (dict, optional): The CSS style to apply to the layer. Defaults to None.\n            See https://docs.mapbox.com/style-spec/reference/layers/ for more info.\n        visible (bool, optional): Whether the layer should be shown initially. Defaults to True.\n        opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n        exclude_mask (bool, optional): Whether to exclude the mask layer. Defaults to False.\n        tooltip (bool, optional): Whether to show tooltips on the layer. Defaults to True.\n        properties (dict, optional): The properties to use for the tooltips. Defaults to None.\n        template (str, optional): The template to use for the tooltips. Defaults to None.\n        attribution (str, optional): The attribution to use for the layer. Defaults to 'PMTiles'.\n        fit_bounds (bool, optional): Whether to zoom to the layer extent. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to the PMTilesLayer constructor.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_pmtiles(\n        url=url,\n        style=style,\n        visible=visible,\n        opacity=opacity,\n        exclude_mask=exclude_mask,\n        tooltip=tooltip,\n        properties=properties,\n        template=template,\n        attribution=attribution,\n        fit_bounds=fit_bounds,\n        **kwargs,\n    )\n    return f\"PMTiles layer added: {url}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_raster","title":"<code>add_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, name='Raster', before_id=None, fit_bounds=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': True}, overwrite=True, **kwargs)</code>","text":"<p>Add a local raster dataset to the map.     If you are using this function in JupyterHub on a remote server     (e.g., Binder, Microsoft Planetary Computer) and if the raster     does not render properly, try installing jupyter-server-proxy using     <code>pip install jupyter-server-proxy</code>, then running the following code     before calling this function. For more info, see https://bit.ly/3JbmF93.</p> <pre><code>import os\nos.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the GeoTIFF file or the URL of the Cloud Optimized GeoTIFF.</p> required <code>indexes</code> <code>int</code> <p>The band(s) to use. Band indexing starts at 1. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>str</code> <p>The name of the colormap from <code>matplotlib</code> to use when plotting a single band. See https://matplotlib.org/stable/gallery/color/colormap_reference.html. Default is greyscale.</p> <code>None</code> <code>vmin</code> <code>float</code> <p>The minimum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>float</code> <p>The maximum value to use when colormapping the palette when plotting a single band. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>float</code> <p>The value from the band to use to interpret as not valid data. Defaults to None.</p> <code>None</code> <code>visible</code> <code>bool</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>float</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>dict</code> <p>Additional arguments to pass to <code>array_to_memory_file</code> when reading the raster. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>dict</code> <p>Additional arguments to pass to localtileserver.TileClient. Defaults to { \"cors_all\": False }.</p> <code>{'cors_all': True}</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing layer with the same name. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to be passed to the underlying <code>add_tile_layer</code> method.</p> <code>{}</code> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_raster(\n    self,\n    source,\n    indexes=None,\n    colormap=None,\n    vmin=None,\n    vmax=None,\n    nodata=None,\n    name=\"Raster\",\n    before_id=None,\n    fit_bounds=True,\n    visible=True,\n    opacity=1.0,\n    array_args=None,\n    client_args={\"cors_all\": True},\n    overwrite: bool = True,\n    **kwargs: Any,\n):\n    \"\"\"Add a local raster dataset to the map.\n        If you are using this function in JupyterHub on a remote server\n        (e.g., Binder, Microsoft Planetary Computer) and if the raster\n        does not render properly, try installing jupyter-server-proxy using\n        `pip install jupyter-server-proxy`, then running the following code\n        before calling this function. For more info, see https://bit.ly/3JbmF93.\n\n        import os\n        os.environ['LOCALTILESERVER_CLIENT_PREFIX'] = 'proxy/{port}'\n\n    Args:\n        source (str): The path to the GeoTIFF file or the URL of the Cloud\n            Optimized GeoTIFF.\n        indexes (int, optional): The band(s) to use. Band indexing starts\n            at 1. Defaults to None.\n        colormap (str, optional): The name of the colormap from `matplotlib`\n            to use when plotting a single band.\n            See https://matplotlib.org/stable/gallery/color/colormap_reference.html.\n            Default is greyscale.\n        vmin (float, optional): The minimum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        vmax (float, optional): The maximum value to use when colormapping\n            the palette when plotting a single band. Defaults to None.\n        nodata (float, optional): The value from the band to use to interpret\n            as not valid data. Defaults to None.\n        visible (bool, optional): Whether the layer is visible. Defaults to True.\n        opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n        array_args (dict, optional): Additional arguments to pass to\n            `array_to_memory_file` when reading the raster. Defaults to {}.\n        client_args (dict, optional): Additional arguments to pass to\n            localtileserver.TileClient. Defaults to { \"cors_all\": False }.\n        overwrite (bool, optional): Whether to overwrite an existing layer with the same name.\n            Defaults to True.\n        **kwargs: Additional keyword arguments to be passed to the underlying\n            `add_tile_layer` method.\n    \"\"\"\n    self.session.m.add_raster(\n        source=source,\n        indexes=indexes,\n        colormap=colormap,\n        vmin=vmin,\n        vmax=vmax,\n        nodata=nodata,\n        name=name,\n        before_id=before_id,\n        fit_bounds=fit_bounds,\n        visible=visible,\n        opacity=opacity,\n        array_args=array_args,\n        client_args=client_args,\n        overwrite=overwrite,\n        **kwargs,\n    )\n    return f\"Raster added: {source}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_text","title":"<code>add_text(text, fontsize=20, fontcolor='black', bold=False, padding='5px', bg_color='white', border_radius='5px', position='bottom-right', **kwargs)</code>","text":"<p>Adds text to the map with customizable styling.</p> <p>This method allows adding a text widget to the map with various styling options such as font size, color, background color, and more. The text's appearance can be further customized using additional CSS properties passed through kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to add to the map.</p> required <code>fontsize</code> <code>int</code> <p>The font size of the text. Defaults to 20.</p> <code>20</code> <code>fontcolor</code> <code>str</code> <p>The color of the text. Defaults to \"black\".</p> <code>'black'</code> <code>bold</code> <code>bool</code> <p>If True, the text will be bold. Defaults to False.</p> <code>False</code> <code>padding</code> <code>str</code> <p>The padding around the text. Defaults to \"5px\".</p> <code>'5px'</code> <code>bg_color</code> <code>str</code> <p>The background color of the text widget. Defaults to \"white\". To make the background transparent, set this to \"transparent\". To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".</p> <code>'white'</code> <code>border_radius</code> <code>str</code> <p>The border radius of the text widget. Defaults to \"5px\".</p> <code>'5px'</code> <code>position</code> <code>str</code> <p>The position of the text widget on the map. Defaults to \"bottom-right\".</p> <code>'bottom-right'</code> <code>**kwargs</code> <code>Any</code> <p>Additional CSS properties to apply to the text widget.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_text(\n    self,\n    text: str,\n    fontsize: int = 20,\n    fontcolor: str = \"black\",\n    bold: bool = False,\n    padding: str = \"5px\",\n    bg_color: str = \"white\",\n    border_radius: str = \"5px\",\n    position: str = \"bottom-right\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds text to the map with customizable styling.\n\n    This method allows adding a text widget to the map with various styling options such as font size, color,\n    background color, and more. The text's appearance can be further customized using additional CSS properties\n    passed through kwargs.\n\n    Args:\n        text (str): The text to add to the map.\n        fontsize (int, optional): The font size of the text. Defaults to 20.\n        fontcolor (str, optional): The color of the text. Defaults to \"black\".\n        bold (bool, optional): If True, the text will be bold. Defaults to False.\n        padding (str, optional): The padding around the text. Defaults to \"5px\".\n        bg_color (str, optional): The background color of the text widget. Defaults to \"white\".\n            To make the background transparent, set this to \"transparent\".\n            To make the background half transparent, set this to \"rgba(255, 255, 255, 0.5)\".\n        border_radius (str, optional): The border radius of the text widget. Defaults to \"5px\".\n        position (str, optional): The position of the text widget on the map. Defaults to \"bottom-right\".\n        **kwargs (Any): Additional CSS properties to apply to the text widget.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_text(\n        text=text,\n        fontsize=fontsize,\n        fontcolor=fontcolor,\n        bold=bold,\n        padding=padding,\n        bg_color=bg_color,\n        border_radius=border_radius,\n        position=position,\n        **kwargs,\n    )\n    return f\"Text added: {text}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_vector","title":"<code>add_vector(data, name=None)</code>","text":"<p>Add a vector dataset to the map.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str</code> <p>Path or URL to the vector data file.</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the layer.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Confirmation message with layer name.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Add a vector dataset (GeoJSON, Shapefile, etc.)\")\ndef add_vector(self, data: str, name: Optional[str] = None) -&gt; str:\n    \"\"\"Add a vector dataset to the map.\n\n    Args:\n        data: Path or URL to the vector data file.\n        name: Optional name for the layer.\n\n    Returns:\n        Confirmation message with layer name.\n    \"\"\"\n    self.session.m.add_vector(data=data, name=name)\n    return f\"Vector added: {name}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_vector_tile","title":"<code>add_vector_tile(url, layer_id, layer_type='fill', source_layer=None, name=None, paint=None, layout=None, filter=None, minzoom=None, maxzoom=None, visible=True, opacity=1.0, add_popup=True, before_id=None, source_args=None, overwrite=False, **kwargs)</code>","text":"<p>Adds a vector tile layer to the map.</p> <p>This method adds a vector tile layer to the map using a vector tile source. Vector tiles are a data format for efficiently storing and transmitting vector map data.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL template for the vector tiles. Should contain {z}, {x}, and {y} placeholders for tile coordinates.</p> required <code>layer_id</code> <code>str</code> <p>The ID of the layer within the vector tile source.</p> required <code>layer_type</code> <code>str</code> <p>The type of layer to create. Can be 'fill', 'line', 'symbol', 'circle', etc. Defaults to 'fill'.</p> <code>'fill'</code> <code>source_layer</code> <code>str</code> <p>The name of the source layer within the vector tiles. If None, uses layer_id.</p> <code>None</code> <code>name</code> <code>str</code> <p>The name to use for the layer. If None, uses layer_id.</p> <code>None</code> <code>paint</code> <code>dict</code> <p>Paint properties for the layer. If None, uses default styling based on layer_type.</p> <code>None</code> <code>layout</code> <code>dict</code> <p>Layout properties for the layer.</p> <code>None</code> <code>filter</code> <code>dict</code> <p>Filter expression for the layer.</p> <code>None</code> <code>minzoom</code> <code>int</code> <p>Minimum zoom level for the layer.</p> <code>None</code> <code>maxzoom</code> <code>int</code> <p>Maximum zoom level for the layer.</p> <code>None</code> <code>visible</code> <code>bool</code> <p>Whether the layer should be visible by default. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>float</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>add_popup</code> <code>bool</code> <p>Whether to add a popup to the layer. Defaults to True.</p> <code>True</code> <code>before_id</code> <code>str</code> <p>The ID of an existing layer before which the new layer should be inserted.</p> <code>None</code> <code>source_args</code> <code>dict</code> <p>Additional keyword arguments passed to the vector tile source.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing layer with the same name. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the Layer class.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <p>m = Map() m.add_vector_tile( ...     url=\"https://api.maptiler.com/tiles/contours/tiles.json?key={api_key}\", ...     layer_id=\"contour-lines\", ...     layer_type=\"line\", ...     source_layer=\"contour\", ...     paint={\"line-color\": \"#ff69b4\", \"line-width\": 1} ... )</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_vector_tile(\n    self,\n    url: str,\n    layer_id: str,\n    layer_type: str = \"fill\",\n    source_layer: Optional[str] = None,\n    name: Optional[str] = None,\n    paint: Optional[Dict] = None,\n    layout: Optional[Dict] = None,\n    filter: Optional[Dict] = None,\n    minzoom: Optional[int] = None,\n    maxzoom: Optional[int] = None,\n    visible: bool = True,\n    opacity: float = 1.0,\n    add_popup: bool = True,\n    before_id: Optional[str] = None,\n    source_args: Dict = None,\n    overwrite: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds a vector tile layer to the map.\n\n    This method adds a vector tile layer to the map using a vector tile source.\n    Vector tiles are a data format for efficiently storing and transmitting\n    vector map data.\n\n    Args:\n        url (str): The URL template for the vector tiles. Should contain {z}, {x},\n            and {y} placeholders for tile coordinates.\n        layer_id (str): The ID of the layer within the vector tile source.\n        layer_type (str, optional): The type of layer to create. Can be 'fill',\n            'line', 'symbol', 'circle', etc. Defaults to 'fill'.\n        source_layer (str, optional): The name of the source layer within the\n            vector tiles. If None, uses layer_id.\n        name (str, optional): The name to use for the layer. If None, uses layer_id.\n        paint (dict, optional): Paint properties for the layer. If None, uses\n            default styling based on layer_type.\n        layout (dict, optional): Layout properties for the layer.\n        filter (dict, optional): Filter expression for the layer.\n        minzoom (int, optional): Minimum zoom level for the layer.\n        maxzoom (int, optional): Maximum zoom level for the layer.\n        visible (bool, optional): Whether the layer should be visible by default.\n            Defaults to True.\n        opacity (float, optional): The opacity of the layer. Defaults to 1.0.\n        add_popup (bool, optional): Whether to add a popup to the layer. Defaults to True.\n        before_id (str, optional): The ID of an existing layer before which the\n            new layer should be inserted.\n        source_args (dict, optional): Additional keyword arguments passed to the\n            vector tile source.\n        overwrite (bool, optional): Whether to overwrite an existing layer with\n            the same name. Defaults to False.\n        **kwargs: Additional keyword arguments passed to the Layer class.\n\n    Returns:\n        None\n\n    Example:\n        &gt;&gt;&gt; m = Map()\n        &gt;&gt;&gt; m.add_vector_tile(\n        ...     url=\"https://api.maptiler.com/tiles/contours/tiles.json?key={api_key}\",\n        ...     layer_id=\"contour-lines\",\n        ...     layer_type=\"line\",\n        ...     source_layer=\"contour\",\n        ...     paint={\"line-color\": \"#ff69b4\", \"line-width\": 1}\n        ... )\n    \"\"\"\n    self.session.m.add_vector_tile(\n        url=url,\n        layer_id=layer_id,\n        layer_type=layer_type,\n        source_layer=source_layer,\n        name=name,\n        paint=paint,\n        layout=layout,\n        filter=filter,\n        minzoom=minzoom,\n        maxzoom=maxzoom,\n        visible=visible,\n        opacity=opacity,\n        add_popup=add_popup,\n        before_id=before_id,\n        source_args=source_args,\n        overwrite=overwrite,\n        **kwargs,\n    )\n    return f\"Vector tile layer added: {url}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_video","title":"<code>add_video(urls, coordinates, layer_id='video', before_id=None)</code>","text":"<p>Adds a video layer to the map.</p> <p>This method allows embedding a video into the map by specifying the video URLs and the geographical coordinates that the video should cover. The video will be stretched and fitted into the specified coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>Union[str, List[str]]</code> <p>A single video URL or a list of video URLs. These URLs must be accessible from the client's location.</p> required <code>coordinates</code> <code>List[List[float]]</code> <p>A list of four coordinates in [longitude, latitude] format, specifying the corners of the video. The coordinates order should be top-left, top-right, bottom-right, bottom-left.</p> required <code>layer_id</code> <code>str</code> <p>The ID for the video layer. Defaults to \"video\".</p> <code>'video'</code> <code>before_id</code> <code>Optional[str]</code> <p>The ID of an existing layer to insert the new layer before. If None, the layer will be added on top. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_video(\n    self,\n    urls: Union[str, List[str]],\n    coordinates: List[List[float]],\n    layer_id: str = \"video\",\n    before_id: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Adds a video layer to the map.\n\n    This method allows embedding a video into the map by specifying the video URLs and the geographical coordinates\n    that the video should cover. The video will be stretched and fitted into the specified coordinates.\n\n    Args:\n        urls (Union[str, List[str]]): A single video URL or a list of video URLs. These URLs must be accessible\n            from the client's location.\n        coordinates (List[List[float]]): A list of four coordinates in [longitude, latitude] format, specifying\n            the corners of the video. The coordinates order should be top-left, top-right, bottom-right, bottom-left.\n        layer_id (str): The ID for the video layer. Defaults to \"video\".\n        before_id (Optional[str]): The ID of an existing layer to insert the new layer before. If None, the layer\n            will be added on top. Defaults to None.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_video(\n        urls=urls,\n        coordinates=coordinates,\n        layer_id=layer_id,\n        before_id=before_id,\n    )\n    return f\"Video added: {layer_id}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.add_wms_layer","title":"<code>add_wms_layer(url, layers, format='image/png', name='WMS Layer', attribution='', opacity=1.0, visible=True, tile_size=256, before_id=None, source_args=None, overwrite=False, **kwargs)</code>","text":"<p>Adds a WMS layer to the map.</p> <p>This method adds a WMS layer to the map. The WMS  is created from     the specified URL, and it is added to the map with the specified     name, attribution, visibility, and tile size.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the tile layer.</p> required <code>layers</code> <code>str</code> <p>The layers to include in the WMS request.</p> required <code>format</code> <code>str</code> <p>The format of the tiles in the layer.</p> <code>'image/png'</code> <code>name</code> <code>str</code> <p>The name to use for the layer. Defaults to 'WMS Layer'.</p> <code>'WMS Layer'</code> <code>attribution</code> <code>str</code> <p>The attribution to use for the layer. Defaults to ''.</p> <code>''</code> <code>visible</code> <code>bool</code> <p>Whether the layer should be visible by default. Defaults to True.</p> <code>True</code> <code>tile_size</code> <code>int</code> <p>The size of the tiles in the layer. Defaults to 256.</p> <code>256</code> <code>before_id</code> <code>str</code> <p>The ID of an existing layer before which the new layer should be inserted.</p> <code>None</code> <code>source_args</code> <code>dict</code> <p>Additional keyword arguments that are passed to the RasterTileSource class.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an existing layer with the same name. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that are passed to the Layer class. See https://eodagmbh.github.io/py-maplibregl/api/layer/ for more information.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef add_wms_layer(\n    self,\n    url: str,\n    layers: str,\n    format: str = \"image/png\",\n    name: str = \"WMS Layer\",\n    attribution: str = \"\",\n    opacity: float = 1.0,\n    visible: bool = True,\n    tile_size: int = 256,\n    before_id: Optional[str] = None,\n    source_args: Dict = None,\n    overwrite: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Adds a WMS layer to the map.\n\n    This method adds a WMS layer to the map. The WMS  is created from\n        the specified URL, and it is added to the map with the specified\n        name, attribution, visibility, and tile size.\n\n    Args:\n        url (str): The URL of the tile layer.\n        layers (str): The layers to include in the WMS request.\n        format (str, optional): The format of the tiles in the layer.\n        name (str, optional): The name to use for the layer. Defaults to\n            'WMS Layer'.\n        attribution (str, optional): The attribution to use for the layer.\n            Defaults to ''.\n        visible (bool, optional): Whether the layer should be visible by\n            default. Defaults to True.\n        tile_size (int, optional): The size of the tiles in the layer.\n            Defaults to 256.\n        before_id (str, optional): The ID of an existing layer before which\n            the new layer should be inserted.\n        source_args (dict, optional): Additional keyword arguments that are\n            passed to the RasterTileSource class.\n        overwrite (bool, optional): Whether to overwrite an existing layer with the same name.\n            Defaults to False.\n        **kwargs: Additional keyword arguments that are passed to the Layer class.\n            See https://eodagmbh.github.io/py-maplibregl/api/layer/ for more information.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.add_wms_layer(\n        url=url,\n        layers=layers,\n        format=format,\n        name=name,\n        attribution=attribution,\n        opacity=opacity,\n        visible=visible,\n        tile_size=tile_size,\n        before_id=before_id,\n        source_args=source_args,\n        overwrite=overwrite,\n        **kwargs,\n    )\n    return f\"WMS layer added: {url}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.create_map","title":"<code>create_map(center_lat=20.0, center_lon=0.0, zoom=2, style='liberty', projection='globe', use_message_queue=True)</code>","text":"<p>Create or reset a Leafmap map with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>center_lat</code> <code>float</code> <p>Latitude for map center (default: 20.0).</p> <code>20.0</code> <code>center_lon</code> <code>float</code> <p>Longitude for map center (default: 0.0).</p> <code>0.0</code> <code>zoom</code> <code>int</code> <p>Initial zoom level (default: 2).</p> <code>2</code> <code>style</code> <code>str</code> <p>Map style name (default: \"liberty\").</p> <code>'liberty'</code> <code>projection</code> <code>str</code> <p>Map projection (default: \"globe\").</p> <code>'globe'</code> <code>use_message_queue</code> <code>bool</code> <p>Whether to use message queue (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>Confirmation message.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(\n    description=\"Create or reset a Leafmap map with optional center/zoom and basemap.\"\n)\ndef create_map(\n    self,\n    center_lat: float = 20.0,\n    center_lon: float = 0.0,\n    zoom: int = 2,\n    style: str = \"liberty\",\n    projection: str = \"globe\",\n    use_message_queue: bool = True,\n) -&gt; str:\n    \"\"\"Create or reset a Leafmap map with specified parameters.\n\n    Args:\n        center_lat: Latitude for map center (default: 20.0).\n        center_lon: Longitude for map center (default: 0.0).\n        zoom: Initial zoom level (default: 2).\n        style: Map style name (default: \"liberty\").\n        projection: Map projection (default: \"globe\").\n        use_message_queue: Whether to use message queue (default: True).\n\n    Returns:\n        Confirmation message.\n    \"\"\"\n    self.session.m = leafmap.Map(\n        center=[center_lon, center_lat],\n        zoom=zoom,\n        style=style,\n        projection=projection,\n        use_message_queue=use_message_queue,\n    )\n    self.session.m.create_container()\n    return \"Map created.\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.first_symbol_layer_id","title":"<code>first_symbol_layer_id()</code>","text":"<p>Get the ID of the first symbol layer in the map's current style.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef first_symbol_layer_id(self) -&gt; Optional[str]:\n    \"\"\"\n    Get the ID of the first symbol layer in the map's current style.\n    \"\"\"\n    return self.session.m.first_symbol_layer_id\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.fly_to","title":"<code>fly_to(longitude, latitude, zoom=12)</code>","text":"<p>Fly to a specific geographic location.</p> <p>Parameters:</p> Name Type Description Default <code>longitude</code> <code>float</code> <p>Target longitude coordinate.</p> required <code>latitude</code> <code>float</code> <p>Target latitude coordinate.</p> required <code>zoom</code> <code>int</code> <p>Zoom level for the target location (default: 12).</p> <code>12</code> <p>Returns:</p> Type Description <code>str</code> <p>Confirmation message with coordinates and zoom level.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Fly to a specific location\")\ndef fly_to(self, longitude: float, latitude: float, zoom: int = 12) -&gt; str:\n    \"\"\"Fly to a specific geographic location.\n\n    Args:\n        longitude: Target longitude coordinate.\n        latitude: Target latitude coordinate.\n        zoom: Zoom level for the target location (default: 12).\n\n    Returns:\n        Confirmation message with coordinates and zoom level.\n    \"\"\"\n    self.session.m.fly_to(longitude, latitude, zoom)\n    return f\"Flown to: {longitude}, {latitude}, zoom {zoom}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.get_layer_names","title":"<code>get_layer_names()</code>","text":"<p>Gets layer names as a list.</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of layer names.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef get_layer_names(self) -&gt; list:\n    \"\"\"Gets layer names as a list.\n\n    Returns:\n        list: A list of layer names.\n    \"\"\"\n    return self.session.m.get_layer_names()\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.jump_to","title":"<code>jump_to(options={}, **kwargs)</code>","text":"<p>Jumps the map to a specified location.</p> <p>This function jumps the map to the specified location with the specified options. Additional keyword arguments can be provided to control the jump. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#jumpto</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Dict[str, Any]</code> <p>Additional options to control the jump. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to control the jump.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef jump_to(self, options: Dict[str, Any] = {}, **kwargs: Any) -&gt; None:\n    \"\"\"\n    Jumps the map to a specified location.\n\n    This function jumps the map to the specified location with the specified options.\n    Additional keyword arguments can be provided to control the jump. For more information,\n    see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#jumpto\n\n    Args:\n        options (Dict[str, Any], optional): Additional options to control the jump. Defaults to {}.\n        **kwargs (Any): Additional keyword arguments to control the jump.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.jump_to(options=options, **kwargs)\n    return f\"Map jumped to: {options}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.pan_to","title":"<code>pan_to(lnglat, options={}, **kwargs)</code>","text":"<p>Pans the map to a specified location.</p> <p>This function pans the map to the specified longitude and latitude coordinates. Additional options and keyword arguments can be provided to control the panning. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#panto</p> <p>Parameters:</p> Name Type Description Default <code>lnglat</code> <code>List[float, float]</code> <p>The longitude and latitude coordinates to pan to.</p> required <code>options</code> <code>Dict[str, Any]</code> <p>Additional options to control the panning. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to control the panning.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef pan_to(\n    self,\n    lnglat: List[float],\n    options: Dict[str, Any] = {},\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Pans the map to a specified location.\n\n    This function pans the map to the specified longitude and latitude coordinates.\n    Additional options and keyword arguments can be provided to control the panning.\n    For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#panto\n\n    Args:\n        lnglat (List[float, float]): The longitude and latitude coordinates to pan to.\n        options (Dict[str, Any], optional): Additional options to control the panning. Defaults to {}.\n        **kwargs (Any): Additional keyword arguments to control the panning.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.pan_to(lnglat=lnglat, options=options, **kwargs)\n    return f\"Map panned to: {lnglat}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.remove_layer","title":"<code>remove_layer(name)</code>","text":"<p>Remove a layer from the map by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the layer to remove.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Confirmation message with removed layer name.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Remove a layer by name\")\ndef remove_layer(self, name: str) -&gt; str:\n    \"\"\"Remove a layer from the map by name.\n\n    Args:\n        name: Name of the layer to remove.\n\n    Returns:\n        Confirmation message with removed layer name.\n    \"\"\"\n    layer_names = self.session.m.get_layer_names()\n    if name in layer_names:\n        self.session.m.remove_layer(name)\n        return f\"Removed: {name}\"\n    else:\n        for layer_name in layer_names:\n            if name.lower() in layer_name.lower():\n                self.session.m.remove_layer(layer_name)\n                return f\"Removed: {layer_name}\"\n        return f\"Layer {name} not found\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.remove_terrain","title":"<code>remove_terrain()</code>","text":"<p>Remove terrain visualization from the map.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef remove_terrain(self) -&gt; None:\n    \"\"\"Remove terrain visualization from the map.\"\"\"\n    self.session.m.remove_terrain()\n    return \"Terrain removed.\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.rotate_to","title":"<code>rotate_to(bearing, options={}, **kwargs)</code>","text":"<p>Rotate the map to a specified bearing.</p> <p>This function rotates the map to a specified bearing. The bearing is specified in degrees counter-clockwise from true north. If the bearing is not specified, the map will rotate to true north. Additional options and keyword arguments can be provided to control the rotation. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#rotateto</p> <p>Parameters:</p> Name Type Description Default <code>bearing</code> <code>float</code> <p>The bearing to rotate to, in degrees counter-clockwise from true north.</p> required <code>options</code> <code>Dict[str, Any]</code> <p>Additional options to control the rotation. Defaults to {}.</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to control the rotation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef rotate_to(\n    self, bearing: float, options: Dict[str, Any] = {}, **kwargs: Any\n) -&gt; None:\n    \"\"\"\n    Rotate the map to a specified bearing.\n\n    This function rotates the map to a specified bearing. The bearing is specified in degrees\n    counter-clockwise from true north. If the bearing is not specified, the map will rotate to\n    true north. Additional options and keyword arguments can be provided to control the rotation.\n    For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#rotateto\n\n    Args:\n        bearing (float): The bearing to rotate to, in degrees counter-clockwise from true north.\n        options (Dict[str, Any], optional): Additional options to control the rotation. Defaults to {}.\n        **kwargs (Any): Additional keyword arguments to control the rotation.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.rotate_to(bearing=bearing, options=options, **kwargs)\n    return f\"Map rotated to: {bearing}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.save_map","title":"<code>save_map(output='map.html', title='My Awesome Map', width='100%', height='100%', replace_key=False, remove_port=True, preview=False, overwrite=False, **kwargs)</code>","text":"<p>Render the map to an HTML page.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output HTML file. If None, the HTML content is returned as a string. Defaults to 'map.html'.</p> <code>'map.html'</code> <code>title</code> <code>str</code> <p>The title of the HTML page. Defaults to 'My Awesome Map'.</p> <code>'My Awesome Map'</code> <code>width</code> <code>str</code> <p>The width of the map. Defaults to '100%'.</p> <code>'100%'</code> <code>height</code> <code>str</code> <p>The height of the map. Defaults to '100%'.</p> <code>'100%'</code> <code>replace_key</code> <code>bool</code> <p>Whether to replace the API key in the HTML. If True, the API key is replaced with the public API key. The API key is read from the environment variable <code>MAPTILER_KEY</code>. The public API key is read from the environment variable <code>MAPTILER_KEY_PUBLIC</code>. Defaults to False.</p> <code>False</code> <code>remove_port</code> <code>bool</code> <p>Whether to remove the port number from the HTML.</p> <code>True</code> <code>preview</code> <code>bool</code> <p>Whether to preview the HTML file in a web browser. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file if it already exists.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that are passed to the <code>maplibre.ipywidget.MapWidget.to_html()</code> method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The HTML content of the map.</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef save_map(\n    self,\n    output: str = \"map.html\",\n    title: str = \"My Awesome Map\",\n    width: str = \"100%\",\n    height: str = \"100%\",\n    replace_key: bool = False,\n    remove_port: bool = True,\n    preview: bool = False,\n    overwrite: bool = False,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Render the map to an HTML page.\n\n    Args:\n        output (str, optional): The output HTML file. If None, the HTML content\n            is returned as a string. Defaults to 'map.html'.\n        title (str, optional): The title of the HTML page. Defaults to 'My Awesome Map'.\n        width (str, optional): The width of the map. Defaults to '100%'.\n        height (str, optional): The height of the map. Defaults to '100%'.\n        replace_key (bool, optional): Whether to replace the API key in the HTML.\n            If True, the API key is replaced with the public API key.\n            The API key is read from the environment variable `MAPTILER_KEY`.\n            The public API key is read from the environment variable `MAPTILER_KEY_PUBLIC`.\n            Defaults to False.\n        remove_port (bool, optional): Whether to remove the port number from the HTML.\n        preview (bool, optional): Whether to preview the HTML file in a web browser.\n            Defaults to False.\n        overwrite (bool, optional): Whether to overwrite the output file if it already exists.\n        **kwargs: Additional keyword arguments that are passed to the\n            `maplibre.ipywidget.MapWidget.to_html()` method.\n\n    Returns:\n        str: The HTML content of the map.\n    \"\"\"\n    self.session.m.to_html(\n        output=output,\n        title=title,\n        width=width,\n        height=height,\n        replace_key=replace_key,\n        remove_port=remove_port,\n        preview=preview,\n        overwrite=overwrite,\n        **kwargs,\n    )\n    return f\"HTML file created: {output}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_color","title":"<code>set_color(name, color)</code>","text":"<p>Set the color of a layer.</p> <p>This method sets the color of the specified layer to the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> required <code>color</code> <code>str</code> <p>The color value to set.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef set_color(self, name: str, color: str) -&gt; None:\n    \"\"\"\n    Set the color of a layer.\n\n    This method sets the color of the specified layer to the specified value.\n\n    Args:\n        name (str): The name of the layer.\n        color (str): The color value to set.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.set_color(name=name, color=color)\n    return f\"Color set: {name}, {color}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_layout_property","title":"<code>set_layout_property(name, prop, value)</code>","text":"<p>Set the layout property of a layer.</p> <p>This method sets the layout property of the specified layer to the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> required <code>prop</code> <code>str</code> <p>The layout property to set.</p> required <code>value</code> <code>Any</code> <p>The value to set.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef set_layout_property(self, name: str, prop: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the layout property of a layer.\n\n    This method sets the layout property of the specified layer to the specified value.\n\n    Args:\n        name (str): The name of the layer.\n        prop (str): The layout property to set.\n        value (Any): The value to set.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.set_layout_property(name=name, prop=prop, value=value)\n    return f\"Layout property set: {name}, {prop}, {value}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_opacity","title":"<code>set_opacity(name, opacity)</code>","text":"<p>Set the opacity of a layer.</p> <p>This method sets the opacity of the specified layer to the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> required <code>opacity</code> <code>float</code> <p>The opacity value to set.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef set_opacity(self, name: str, opacity: float) -&gt; None:\n    \"\"\"\n    Set the opacity of a layer.\n\n    This method sets the opacity of the specified layer to the specified value.\n\n    Args:\n        name (str): The name of the layer.\n        opacity (float): The opacity value to set.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.set_opacity(name=name, opacity=opacity)\n    return f\"Opacity set: {name}, {opacity}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_paint_property","title":"<code>set_paint_property(name, prop, value)</code>","text":"<p>Set the paint property of a layer.</p> <p>This method sets the opacity of the specified layer to the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> required <code>prop</code> <code>str</code> <p>The paint property to set.</p> required <code>value</code> <code>Any</code> <p>The value to set.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef set_paint_property(self, name: str, prop: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the paint property of a layer.\n\n    This method sets the opacity of the specified layer to the specified value.\n\n    Args:\n        name (str): The name of the layer.\n        prop (str): The paint property to set.\n        value (Any): The value to set.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.set_paint_property(name=name, prop=prop, value=value)\n    return f\"Paint property set: {name}, {prop}, {value}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_pitch","title":"<code>set_pitch(pitch)</code>","text":"<p>Sets the pitch of the map.</p> <p>This function sets the pitch of the map to the specified value. The pitch is the angle of the camera measured in degrees where 0 is looking straight down, and 60 is looking towards the horizon. Additional keyword arguments can be provided to control the pitch. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#setpitch</p> <p>Parameters:</p> Name Type Description Default <code>pitch</code> <code>float</code> <p>The pitch value to set.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool(description=\"Set the pitch of the map\")\ndef set_pitch(self, pitch: float) -&gt; None:\n    \"\"\"\n    Sets the pitch of the map.\n\n    This function sets the pitch of the map to the specified value. The pitch is the\n    angle of the camera measured in degrees where 0 is looking straight down, and 60 is\n    looking towards the horizon. Additional keyword arguments can be provided to control\n    the pitch. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#setpitch\n\n    Args:\n        pitch (float): The pitch value to set.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.set_pitch(pitch=pitch)\n    return f\"Map pitched to: {pitch}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_terrain","title":"<code>set_terrain(source='https://elevation-tiles-prod.s3.amazonaws.com/terrarium/{z}/{x}/{y}.png', exaggeration=1.0, tile_size=256, encoding='terrarium', source_id='terrain-dem')</code>","text":"<p>Add terrain visualization to the map.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>URL template for terrain tiles. Defaults to AWS elevation tiles.</p> <code>'https://elevation-tiles-prod.s3.amazonaws.com/terrarium/{z}/{x}/{y}.png'</code> <code>exaggeration</code> <code>float</code> <p>Terrain exaggeration factor. Defaults to 1.0.</p> <code>1.0</code> <code>tile_size</code> <code>int</code> <p>Tile size in pixels. Defaults to 256.</p> <code>256</code> <code>encoding</code> <code>str</code> <p>Encoding for the terrain tiles. Defaults to \"terrarium\".</p> <code>'terrarium'</code> <code>source_id</code> <code>str</code> <p>Unique identifier for the terrain source. Defaults to \"terrain-dem\".</p> <code>'terrain-dem'</code> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef set_terrain(\n    self,\n    source: str = \"https://elevation-tiles-prod.s3.amazonaws.com/terrarium/{z}/{x}/{y}.png\",\n    exaggeration: float = 1.0,\n    tile_size: int = 256,\n    encoding: str = \"terrarium\",\n    source_id: str = \"terrain-dem\",\n) -&gt; None:\n    \"\"\"Add terrain visualization to the map.\n\n    Args:\n        source: URL template for terrain tiles. Defaults to AWS elevation tiles.\n        exaggeration: Terrain exaggeration factor. Defaults to 1.0.\n        tile_size: Tile size in pixels. Defaults to 256.\n        encoding: Encoding for the terrain tiles. Defaults to \"terrarium\".\n        source_id: Unique identifier for the terrain source. Defaults to \"terrain-dem\".\n    \"\"\"\n    self.session.m.set_terrain(\n        source=source,\n        exaggeration=exaggeration,\n        tile_size=tile_size,\n        encoding=encoding,\n        source_id=source_id,\n    )\n    return f\"Terrain added: {source}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.set_visibility","title":"<code>set_visibility(name, visible)</code>","text":"<p>Set the visibility of a layer.</p> <p>This method sets the visibility of the specified layer to the specified value.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the layer.</p> required <code>visible</code> <code>bool</code> <p>The visibility value to set.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef set_visibility(self, name: str, visible: bool) -&gt; None:\n    \"\"\"\n    Set the visibility of a layer.\n\n    This method sets the visibility of the specified layer to the specified value.\n\n    Args:\n        name (str): The name of the layer.\n        visible (bool): The visibility value to set.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.set_visibility(name=name, visible=visible)\n    return f\"Visibility set: {name}, {visible}\"\n</code></pre>"},{"location":"map_tools/#geoai.agents.map_tools.MapTools.zoom_to","title":"<code>zoom_to(zoom, options={})</code>","text":"<p>Zooms the map to a specified zoom level.</p> <p>This function zooms the map to the specified zoom level. Additional options and keyword arguments can be provided to control the zoom. For more information, see https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#zoomto</p> <p>Parameters:</p> Name Type Description Default <code>zoom</code> <code>float</code> <p>The zoom level to zoom to.</p> required <code>options</code> <code>Dict[str, Any]</code> <p>Additional options to control the zoom. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/agents/map_tools.py</code> <pre><code>@tool\ndef zoom_to(self, zoom: float, options: Dict[str, Any] = {}) -&gt; None:\n    \"\"\"\n    Zooms the map to a specified zoom level.\n\n    This function zooms the map to the specified zoom level. Additional options and keyword\n    arguments can be provided to control the zoom. For more information, see\n    https://maplibre.org/maplibre-gl-js/docs/API/classes/Map/#zoomto\n\n    Args:\n        zoom (float): The zoom level to zoom to.\n        options (Dict[str, Any], optional): Additional options to control the zoom. Defaults to {}.\n\n    Returns:\n        None\n    \"\"\"\n    self.session.m.zoom_to(zoom=zoom, options=options)\n    return f\"Map zoomed to: {zoom}\"\n</code></pre>"},{"location":"map_widgets/","title":"map_widgets module","text":"<p>Interactive widget for GeoAI.</p>"},{"location":"map_widgets/#geoai.map_widgets.DINOv3GUI","title":"<code>DINOv3GUI</code>","text":"<p>               Bases: <code>VBox</code></p> <p>Interactive widget for DINOv3.</p> Source code in <code>geoai/map_widgets.py</code> <pre><code>class DINOv3GUI(widgets.VBox):\n    \"\"\"Interactive widget for DINOv3.\"\"\"\n\n    def __init__(\n        self,\n        raster: str,\n        processor=None,\n        features=None,\n        host_map=None,\n        position=\"topright\",\n        colormap_options=None,\n        raster_args=None,\n    ):\n        \"\"\"Initialize the DINOv3 GUI.\n\n        Args:\n            raster (str): The path to the raster image.\n            processor (DINOv3GeoProcessor): The DINOv3 processor.\n            features (torch.Tensor): The features of the raster image.\n            host_map (Map): The host map.\n            position (str): The position of the widget.\n            colormap_options (list): The colormap options.\n            raster_args (dict): The raster arguments.\n\n        Example:\n            &gt;&gt;&gt; processor = DINOv3GeoProcessor()\n            &gt;&gt;&gt; features, h_patches, w_patches = processor.extract_features(raster)\n            &gt;&gt;&gt; gui = DINOv3GUI(raster, processor, features, host_map=m)\n        \"\"\"\n        super().__init__()\n\n        if raster_args is None:\n            raster_args = {}\n\n        if \"layer_name\" not in raster_args:\n            raster_args[\"layer_name\"] = \"Raster\"\n\n        if colormap_options is None:\n            colormap_options = [\n                \"jet\",\n                \"viridis\",\n                \"plasma\",\n                \"inferno\",\n                \"magma\",\n                \"cividis\",\n            ]\n\n        main_widget = widgets.VBox(layout=widgets.Layout(width=\"230px\"))\n        style = {\"description_width\": \"initial\"}\n        layout = widgets.Layout(width=\"95%\", padding=\"0px 5px 0px 5px\")\n\n        interpolation_checkbox = widgets.Checkbox(\n            value=True,\n            description=\"Use interpolation\",\n            style=style,\n            layout=layout,\n        )\n\n        threshold_slider = widgets.FloatSlider(\n            value=0.7,\n            min=0,\n            max=1,\n            step=0.01,\n            description=\"Threshold\",\n            style=style,\n            layout=layout,\n        )\n\n        opacity_slider = widgets.FloatSlider(\n            value=0.5,\n            min=0,\n            max=1,\n            step=0.01,\n            description=\"Opacity\",\n            style=style,\n            layout=layout,\n        )\n        colormap_dropdown = widgets.Dropdown(\n            options=colormap_options,\n            value=\"jet\",\n            description=\"Colormap\",\n            style=style,\n            layout=layout,\n        )\n        layer_name_input = widgets.Text(\n            value=\"Similarity\",\n            description=\"Layer name\",\n            style=style,\n            layout=layout,\n        )\n\n        save_button = widgets.Button(\n            description=\"Save\",\n        )\n\n        reset_button = widgets.Button(\n            description=\"Reset\",\n        )\n\n        output = widgets.Output()\n\n        main_widget.children = [\n            interpolation_checkbox,\n            threshold_slider,\n            opacity_slider,\n            colormap_dropdown,\n            layer_name_input,\n            widgets.HBox([save_button, reset_button]),\n            output,\n        ]\n\n        if host_map is not None:\n\n            host_map.add_widget(main_widget, add_header=True, position=position)\n\n            if raster is not None:\n                host_map.add_raster(raster, **raster_args)\n\n            def handle_map_interaction(**kwargs):\n                try:\n                    if kwargs.get(\"type\") == \"click\":\n                        latlon = kwargs.get(\"coordinates\")\n                        with output:\n                            output.clear_output()\n\n                            results = processor.compute_similarity(\n                                source=raster,\n                                features=features,\n                                query_coords=latlon[::-1],\n                                output_dir=\"dinov3_results\",\n                                use_interpolation=interpolation_checkbox.value,\n                                coord_crs=\"EPSG:4326\",\n                            )\n                            array = results[\"image_dict\"][\"image\"]\n                            binary_array = array &gt; threshold_slider.value\n                            image = dict_to_image(results[\"image_dict\"])\n                            binary_image = dict_to_image(\n                                {\n                                    \"image\": binary_array,\n                                    \"crs\": results[\"image_dict\"][\"crs\"],\n                                    \"bounds\": results[\"image_dict\"][\"bounds\"],\n                                }\n                            )\n                            host_map.add_raster(\n                                image,\n                                colormap=colormap_dropdown.value,\n                                opacity=opacity_slider.value,\n                                layer_name=layer_name_input.value,\n                                zoom_to_layer=False,\n                                overwrite=True,\n                            )\n                            host_map.add_raster(\n                                binary_image,\n                                colormap=\"jet\",\n                                nodata=0,\n                                opacity=opacity_slider.value,\n                                layer_name=\"Foreground\",\n                                zoom_to_layer=False,\n                                overwrite=True,\n                                visible=False,\n                            )\n                except Exception as e:\n                    with output:\n                        print(e)\n\n            host_map.on_interaction(handle_map_interaction)\n            host_map.default_style = {\"cursor\": \"crosshair\"}\n</code></pre>"},{"location":"map_widgets/#geoai.map_widgets.DINOv3GUI.__init__","title":"<code>__init__(raster, processor=None, features=None, host_map=None, position='topright', colormap_options=None, raster_args=None)</code>","text":"<p>Initialize the DINOv3 GUI.</p> <p>Parameters:</p> Name Type Description Default <code>raster</code> <code>str</code> <p>The path to the raster image.</p> required <code>processor</code> <code>DINOv3GeoProcessor</code> <p>The DINOv3 processor.</p> <code>None</code> <code>features</code> <code>Tensor</code> <p>The features of the raster image.</p> <code>None</code> <code>host_map</code> <code>Map</code> <p>The host map.</p> <code>None</code> <code>position</code> <code>str</code> <p>The position of the widget.</p> <code>'topright'</code> <code>colormap_options</code> <code>list</code> <p>The colormap options.</p> <code>None</code> <code>raster_args</code> <code>dict</code> <p>The raster arguments.</p> <code>None</code> Example <p>processor = DINOv3GeoProcessor() features, h_patches, w_patches = processor.extract_features(raster) gui = DINOv3GUI(raster, processor, features, host_map=m)</p> Source code in <code>geoai/map_widgets.py</code> <pre><code>def __init__(\n    self,\n    raster: str,\n    processor=None,\n    features=None,\n    host_map=None,\n    position=\"topright\",\n    colormap_options=None,\n    raster_args=None,\n):\n    \"\"\"Initialize the DINOv3 GUI.\n\n    Args:\n        raster (str): The path to the raster image.\n        processor (DINOv3GeoProcessor): The DINOv3 processor.\n        features (torch.Tensor): The features of the raster image.\n        host_map (Map): The host map.\n        position (str): The position of the widget.\n        colormap_options (list): The colormap options.\n        raster_args (dict): The raster arguments.\n\n    Example:\n        &gt;&gt;&gt; processor = DINOv3GeoProcessor()\n        &gt;&gt;&gt; features, h_patches, w_patches = processor.extract_features(raster)\n        &gt;&gt;&gt; gui = DINOv3GUI(raster, processor, features, host_map=m)\n    \"\"\"\n    super().__init__()\n\n    if raster_args is None:\n        raster_args = {}\n\n    if \"layer_name\" not in raster_args:\n        raster_args[\"layer_name\"] = \"Raster\"\n\n    if colormap_options is None:\n        colormap_options = [\n            \"jet\",\n            \"viridis\",\n            \"plasma\",\n            \"inferno\",\n            \"magma\",\n            \"cividis\",\n        ]\n\n    main_widget = widgets.VBox(layout=widgets.Layout(width=\"230px\"))\n    style = {\"description_width\": \"initial\"}\n    layout = widgets.Layout(width=\"95%\", padding=\"0px 5px 0px 5px\")\n\n    interpolation_checkbox = widgets.Checkbox(\n        value=True,\n        description=\"Use interpolation\",\n        style=style,\n        layout=layout,\n    )\n\n    threshold_slider = widgets.FloatSlider(\n        value=0.7,\n        min=0,\n        max=1,\n        step=0.01,\n        description=\"Threshold\",\n        style=style,\n        layout=layout,\n    )\n\n    opacity_slider = widgets.FloatSlider(\n        value=0.5,\n        min=0,\n        max=1,\n        step=0.01,\n        description=\"Opacity\",\n        style=style,\n        layout=layout,\n    )\n    colormap_dropdown = widgets.Dropdown(\n        options=colormap_options,\n        value=\"jet\",\n        description=\"Colormap\",\n        style=style,\n        layout=layout,\n    )\n    layer_name_input = widgets.Text(\n        value=\"Similarity\",\n        description=\"Layer name\",\n        style=style,\n        layout=layout,\n    )\n\n    save_button = widgets.Button(\n        description=\"Save\",\n    )\n\n    reset_button = widgets.Button(\n        description=\"Reset\",\n    )\n\n    output = widgets.Output()\n\n    main_widget.children = [\n        interpolation_checkbox,\n        threshold_slider,\n        opacity_slider,\n        colormap_dropdown,\n        layer_name_input,\n        widgets.HBox([save_button, reset_button]),\n        output,\n    ]\n\n    if host_map is not None:\n\n        host_map.add_widget(main_widget, add_header=True, position=position)\n\n        if raster is not None:\n            host_map.add_raster(raster, **raster_args)\n\n        def handle_map_interaction(**kwargs):\n            try:\n                if kwargs.get(\"type\") == \"click\":\n                    latlon = kwargs.get(\"coordinates\")\n                    with output:\n                        output.clear_output()\n\n                        results = processor.compute_similarity(\n                            source=raster,\n                            features=features,\n                            query_coords=latlon[::-1],\n                            output_dir=\"dinov3_results\",\n                            use_interpolation=interpolation_checkbox.value,\n                            coord_crs=\"EPSG:4326\",\n                        )\n                        array = results[\"image_dict\"][\"image\"]\n                        binary_array = array &gt; threshold_slider.value\n                        image = dict_to_image(results[\"image_dict\"])\n                        binary_image = dict_to_image(\n                            {\n                                \"image\": binary_array,\n                                \"crs\": results[\"image_dict\"][\"crs\"],\n                                \"bounds\": results[\"image_dict\"][\"bounds\"],\n                            }\n                        )\n                        host_map.add_raster(\n                            image,\n                            colormap=colormap_dropdown.value,\n                            opacity=opacity_slider.value,\n                            layer_name=layer_name_input.value,\n                            zoom_to_layer=False,\n                            overwrite=True,\n                        )\n                        host_map.add_raster(\n                            binary_image,\n                            colormap=\"jet\",\n                            nodata=0,\n                            opacity=opacity_slider.value,\n                            layer_name=\"Foreground\",\n                            zoom_to_layer=False,\n                            overwrite=True,\n                            visible=False,\n                        )\n            except Exception as e:\n                with output:\n                    print(e)\n\n        host_map.on_interaction(handle_map_interaction)\n        host_map.default_style = {\"cursor\": \"crosshair\"}\n</code></pre>"},{"location":"sam/","title":"sam module","text":"<p>The SamGeo class provides an interface for segmenting geospatial data using the Segment Anything Model (SAM).</p>"},{"location":"sam/#geoai.sam.SamGeo","title":"<code>SamGeo</code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://huggingface.co/docs/transformers/main/en/model_doc/sam for details.</p> Source code in <code>geoai/sam.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://huggingface.co/docs/transformers/main/en/model_doc/sam for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"facebook/sam-vit-huge\",\n        automatic: bool = True,\n        device: Optional[Union[str, int]] = None,\n        sam_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the class.\n\n        Args:\n            model (str, optional): The model type, such as \"facebook/sam-vit-huge\", \"facebook/sam-vit-large\", or \"facebook/sam-vit-base\".\n                Defaults to 'facebook/sam-vit-huge'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (Union[str, int], optional): The device to use. It can be one of the following: 'cpu', 'cuda', or an integer\n                representing the CUDA device index. Defaults to None, which will use 'cuda' if available.\n            sam_kwargs (Dict[str, Any], optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            kwargs (Any): Other arguments for the automatic mask generator.\n        \"\"\"\n\n        self.model = model\n        self.model_version = \"sam\"\n\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        self.embeddings = None  # Store the image embeddings\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Use cuda if available\n            if device is None:\n                device = 0 if torch.cuda.is_available() else -1\n            if device &gt;= 0:\n                torch.cuda.empty_cache()\n            self.device = device\n\n            self.mask_generator = pipeline(\n                task=\"mask-generation\",\n                model=model,\n                device=device,\n                **kwargs,\n            )\n\n        else:\n            if device is None:\n                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n                if device.type == \"cuda\":\n                    torch.cuda.empty_cache()\n                self.device = device\n\n                self.predictor = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(\n                    device\n                )\n                self.processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n\n    def generate(\n        self,\n        source: Union[str, np.ndarray],\n        output: Optional[str] = None,\n        foreground: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        output_args: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Generate masks for the input image.\n\n        Args:\n            source (Union[str, np.ndarray]): The path to the input image or the input image as a numpy array.\n            output (Optional[str], optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (Optional[Tuple[int, int]], optional): The erosion kernel for filtering object masks and extracting borders.\n                For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (Optional[int], optional): The maximum size of the objects. Defaults to None.\n            output_args (Optional[Dict[str, Any]], optional): Additional arguments for saving the output. Defaults to None.\n            **kwargs (Any): Other arguments for the mask generator.\n\n        Raises:\n            ValueError: If the input source is not a valid path or numpy array.\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        if output_args is None:\n            output_args = {}\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        # masks = mask_generator.generate(image)  # Segment the input image\n        result = mask_generator(source, **kwargs)\n        masks = result[\"masks\"] if \"masks\" in result else result  # Get the masks\n        scores = result[\"scores\"] if \"scores\" in result else None  # Get the scores\n\n        # format the masks as a list of dictionaries, similar to the output of SAM.\n        formatted_masks = []\n        for mask, score in zip(masks, scores):\n            area = int(np.sum(mask))  # number of True pixels\n            formatted_masks.append(\n                {\n                    \"segmentation\": mask,\n                    \"area\": area,\n                    \"score\": float(score),  # ensure it's a native Python float\n                }\n            )\n\n        self.output = result  # Store the result\n        self.masks = formatted_masks  # Store the masks as a list of dictionaries\n        self.batch = False\n        # self.scores = scores  # Store the scores\n        self._min_size = min_size\n        self._max_size = max_size\n\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output,\n            foreground,\n            unique,\n            erosion_kernel,\n            mask_multiplier,\n            min_size,\n            max_size,\n            **output_args,\n        )\n\n    def generate_batch(\n        self,\n        inputs: List[Union[str, np.ndarray]],\n        output_dir: Optional[str] = None,\n        suffix: str = \"_masks\",\n        foreground: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        unique: bool = True,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        output_args: Optional[Dict[str, Any]] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Generate masks for a batch of input images.\n\n        Args:\n            inputs (List[Union[str, np.ndarray]]): A list of paths to input images or numpy arrays representing the images.\n            output_dir (Optional[str], optional): The directory to save the output masks. Defaults to the current working directory.\n            suffix (str, optional): The suffix to append to the output filenames. Defaults to \"_masks\".\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (Optional[Tuple[int, int]], optional): The erosion kernel for filtering object masks and extracting borders.\n                For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (Optional[int], optional): The maximum size of the objects. Defaults to None.\n            output_args (Optional[Dict[str, Any]], optional): Additional arguments for saving the output. Defaults to None.\n            **kwargs (Any): Other arguments for the mask generator.\n\n        Raises:\n            ValueError: If the input list is empty or contains invalid paths.\n        \"\"\"\n\n        mask_generator = self.mask_generator  # The automatic mask generator\n        outputs = mask_generator(inputs, **kwargs)\n\n        if output_args is None:\n            output_args = {}\n\n        if output_dir is None:\n            output_dir = os.getcwd()\n\n        for index, result in enumerate(outputs):\n\n            basename = os.path.basename(inputs[index])\n            file_ext = os.path.splitext(basename)[1]\n            filename = f\"{os.path.splitext(basename)[0]}{suffix}{file_ext}\"\n            filepath = os.path.join(output_dir, filename)\n\n            masks = result[\"masks\"] if \"masks\" in result else result  # Get the masks\n            scores = result[\"scores\"] if \"scores\" in result else None  # Get the scores\n\n            # format the masks as a list of dictionaries, similar to the output of SAM.\n            formatted_masks = []\n            for mask, score in zip(masks, scores):\n                area = int(np.sum(mask))  # number of True pixels\n                formatted_masks.append(\n                    {\n                        \"segmentation\": mask,\n                        \"area\": area,\n                        \"score\": float(score),  # ensure it's a native Python float\n                    }\n                )\n\n            self.source = inputs[index]  # Store the input image path\n            self.output = result  # Store the result\n            self.masks = formatted_masks  # Store the masks as a list of dictionaries\n            # self.scores = scores  # Store the scores\n            self._min_size = min_size\n            self._max_size = max_size\n\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                filepath,\n                foreground,\n                unique,\n                erosion_kernel,\n                mask_multiplier,\n                min_size,\n                max_size,\n                **output_args,\n            )\n\n    def save_masks(\n        self,\n        output: Optional[str] = None,\n        foreground: bool = True,\n        unique: bool = True,\n        erosion_kernel: Optional[Tuple[int, int]] = None,\n        mask_multiplier: int = 255,\n        min_size: int = 0,\n        max_size: Optional[int] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (Optional[str], optional): The path to the output image. Defaults to None, saving the masks to `SamGeo.objects`.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (Optional[Tuple[int, int]], optional): The erosion kernel for filtering object masks and extracting borders.\n                For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            min_size (int, optional): The minimum size of the objects. Defaults to 0.\n            max_size (Optional[int], optional): The maximum size of the objects. Defaults to None.\n            **kwargs (Any): Other arguments for `array_to_image()`.\n\n        Raises:\n            ValueError: If no masks are found or if `generate()` has not been run.\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        if self.image is None:\n            (\n                h,\n                w,\n            ) = self.masks[\n                0\n            ][\"segmentation\"].shape\n        else:\n            h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in descending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            count = len(sorted_masks)\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and ann[\"area\"] &gt; max_size:\n                    continue\n                objects[m] = count - index\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                    continue\n                if max_size is not None and m[\"area\"] &gt; max_size:\n                    continue\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        cmap: str = \"binary_r\",\n        axis: str = \"off\",\n        foreground: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Display the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (Tuple[int, int], optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap to use for displaying the mask. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs (Any): Additional arguments for the `save_masks()` method.\n\n        Raises:\n            ValueError: If no masks are available and `save_masks()` cannot generate them.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize: Tuple[int, int] = (12, 10),\n        axis: str = \"off\",\n        alpha: float = 0.35,\n        output: Optional[str] = None,\n        blend: bool = True,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (Tuple[int, int], optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (Optional[str], optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image blended with annotations. Defaults to True.\n            **kwargs (Any): Additional arguments for saving the output image.\n\n        Raises:\n            ValueError: If the input image or annotations are not available.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n                continue\n            if (\n                hasattr(self, \"_max_size\")\n                and isinstance(self._max_size, int)\n                and ann[\"area\"] &gt; self._max_size\n            ):\n                continue\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        # if \"dpi\" not in kwargs:\n        #     kwargs[\"dpi\"] = 100\n\n        # if \"bbox_inches\" not in kwargs:\n        #     kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source, **kwargs)\n\n    def set_image(self, image: Union[str, np.ndarray], **kwargs: Any) -&gt; None:\n        \"\"\"\n        Set the input image as a numpy array.\n\n        Args:\n            image (Union[str, np.ndarray]): The input image, either as a file path (string) or a numpy array.\n            **kwargs (Any): Additional arguments for the image processor.\n\n        Raises:\n            ValueError: If the input image path does not exist.\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = Image.open(image).convert(\"RGB\")\n            self.image = image\n\n        inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n        self.embeddings = self.predictor.get_image_embeddings(\n            inputs[\"pixel_values\"], **kwargs\n        )\n\n    def save_prediction(\n        self,\n        output: str,\n        index: Optional[int] = None,\n        mask_multiplier: int = 255,\n        dtype: np.dtype = np.float32,\n        vector: Optional[str] = None,\n        simplify_tolerance: Optional[float] = None,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (Optional[int], optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                Defaults to 255.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            vector (Optional[str], optional): The path to the output vector file. Defaults to None.\n            simplify_tolerance (Optional[float], optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry. Defaults to None.\n            **kwargs (Any): Additional arguments for saving the output image.\n\n        Raises:\n            ValueError: If no predictions are found.\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n        out_of_bounds = []\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords, out_of_bounds = coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                elif len(out_of_bounds) &gt; 0:\n                    print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                    point_labels_new = []\n                    for i, p in enumerate(point_labels):\n                        if i not in out_of_bounds:\n                            point_labels_new.append(p)\n                    point_labels = point_labels_new\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n        if input_boxes is not None:\n            input_boxes = [input_boxes]\n\n        if point_coords is not None:\n            point_coords = [[point_coords]]\n            point_labels = [[point_labels]]\n\n        inputs = self.processor(\n            self.image,\n            input_points=point_coords,\n            # input_labels=point_labels,\n            input_boxes=input_boxes,\n            return_tensors=\"pt\",\n            **kwargs,\n        ).to(self.device)\n\n        inputs.pop(\"pixel_values\", None)\n        inputs.update({\"image_embeddings\": self.embeddings})\n\n        with torch.no_grad():\n            outputs = self.predictor(**inputs)\n\n        # https://huggingface.co/docs/transformers/en/model_doc/sam#transformers.SamImageProcessor.post_process_masks\n        self.masks = self.processor.image_processor.post_process_masks(\n            outputs.pred_masks.cpu(),\n            inputs[\"original_sizes\"].cpu(),\n            inputs[\"reshaped_input_sizes\"].cpu(),\n        )\n        self.scores = outputs.iou_scores\n\n        # if (\n        #     boxes is None\n        #     or (len(boxes) == 1)\n        #     or (len(boxes) == 4 and isinstance(boxes[0], float))\n        # ):\n        #     if isinstance(boxes, list) and isinstance(boxes[0], list):\n        #         boxes = boxes[0]\n        #     masks, scores, logits = predictor.predict(\n        #         point_coords,\n        #         point_labels,\n        #         input_boxes,\n        #         mask_input,\n        #         multimask_output,\n        #         return_logits,\n        #     )\n        # else:\n        #     masks, scores, logits = predictor.predict_torch(\n        #         point_coords=point_coords,\n        #         point_labels=point_coords,\n        #         boxes=input_boxes,\n        #         multimask_output=True,\n        #     )\n\n        # self.masks = masks\n        # self.scores = scores\n        # self.logits = logits\n\n        # if output is not None:\n        #     if boxes is None or (not isinstance(boxes[0], list)):\n        #         self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        #     else:\n        #         self.tensor_to_numpy(\n        #             index, output, mask_multiplier, dtype, save_args=kwargs\n        #         )\n\n        # if return_results:\n        #     return masks, scores, logits\n\n    def tensor_to_numpy(\n        self,\n        index: Optional[int] = None,\n        output: Optional[str] = None,\n        mask_multiplier: int = 255,\n        dtype: Union[str, np.dtype] = \"uint8\",\n        save_args: Optional[Dict[str, Any]] = None,\n    ) -&gt; Optional[np.ndarray]:\n        \"\"\"\n        Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (Optional[int], optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (Optional[str], optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                Defaults to 255.\n            dtype (Union[str, np.dtype], optional): The data type of the output image. Defaults to \"uint8\".\n            save_args (Optional[Dict[str, Any]], optional): Optional arguments for saving the output image. Defaults to None.\n\n        Returns:\n            Optional[np.ndarray]: The predicted mask as a numpy array if `output` is None. Otherwise, saves the mask to the specified path.\n\n        Raises:\n            ValueError: If no objects are found in the image or if the masks are not available.\n        \"\"\"\n\n        if save_args is None:\n            save_args = {}\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run the prediction method first.\")\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.__init__","title":"<code>__init__(model='facebook/sam-vit-huge', automatic=True, device=None, sam_kwargs=None, **kwargs)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model type, such as \"facebook/sam-vit-huge\", \"facebook/sam-vit-large\", or \"facebook/sam-vit-base\". Defaults to 'facebook/sam-vit-huge'. See https://bit.ly/3VrpxUh for more details.</p> <code>'facebook/sam-vit-huge'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>Union[str, int]</code> <p>The device to use. It can be one of the following: 'cpu', 'cuda', or an integer representing the CUDA device index. Defaults to None, which will use 'cuda' if available.</p> <code>None</code> <code>sam_kwargs</code> <code>Dict[str, Any]</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Other arguments for the automatic mask generator.</p> <code>{}</code> Source code in <code>geoai/sam.py</code> <pre><code>def __init__(\n    self,\n    model: str = \"facebook/sam-vit-huge\",\n    automatic: bool = True,\n    device: Optional[Union[str, int]] = None,\n    sam_kwargs: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Initialize the class.\n\n    Args:\n        model (str, optional): The model type, such as \"facebook/sam-vit-huge\", \"facebook/sam-vit-large\", or \"facebook/sam-vit-base\".\n            Defaults to 'facebook/sam-vit-huge'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (Union[str, int], optional): The device to use. It can be one of the following: 'cpu', 'cuda', or an integer\n            representing the CUDA device index. Defaults to None, which will use 'cuda' if available.\n        sam_kwargs (Dict[str, Any], optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n        kwargs (Any): Other arguments for the automatic mask generator.\n    \"\"\"\n\n    self.model = model\n    self.model_version = \"sam\"\n\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    self.embeddings = None  # Store the image embeddings\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Use cuda if available\n        if device is None:\n            device = 0 if torch.cuda.is_available() else -1\n        if device &gt;= 0:\n            torch.cuda.empty_cache()\n        self.device = device\n\n        self.mask_generator = pipeline(\n            task=\"mask-generation\",\n            model=model,\n            device=device,\n            **kwargs,\n        )\n\n    else:\n        if device is None:\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            if device.type == \"cuda\":\n                torch.cuda.empty_cache()\n            self.device = device\n\n            self.predictor = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(\n                device\n            )\n            self.processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.generate","title":"<code>generate(source, output=None, foreground=True, erosion_kernel=None, mask_multiplier=255, unique=True, min_size=0, max_size=None, output_args=None, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, ndarray]</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>Optional[Tuple[int, int]]</code> <p>The erosion kernel for filtering object masks and extracting borders. For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>Optional[int]</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>output_args</code> <code>Optional[Dict[str, Any]]</code> <p>Additional arguments for saving the output. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other arguments for the mask generator.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input source is not a valid path or numpy array.</p> Source code in <code>geoai/sam.py</code> <pre><code>def generate(\n    self,\n    source: Union[str, np.ndarray],\n    output: Optional[str] = None,\n    foreground: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    output_args: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Generate masks for the input image.\n\n    Args:\n        source (Union[str, np.ndarray]): The path to the input image or the input image as a numpy array.\n        output (Optional[str], optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (Optional[Tuple[int, int]], optional): The erosion kernel for filtering object masks and extracting borders.\n            For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (Optional[int], optional): The maximum size of the objects. Defaults to None.\n        output_args (Optional[Dict[str, Any]], optional): Additional arguments for saving the output. Defaults to None.\n        **kwargs (Any): Other arguments for the mask generator.\n\n    Raises:\n        ValueError: If the input source is not a valid path or numpy array.\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    if output_args is None:\n        output_args = {}\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    # masks = mask_generator.generate(image)  # Segment the input image\n    result = mask_generator(source, **kwargs)\n    masks = result[\"masks\"] if \"masks\" in result else result  # Get the masks\n    scores = result[\"scores\"] if \"scores\" in result else None  # Get the scores\n\n    # format the masks as a list of dictionaries, similar to the output of SAM.\n    formatted_masks = []\n    for mask, score in zip(masks, scores):\n        area = int(np.sum(mask))  # number of True pixels\n        formatted_masks.append(\n            {\n                \"segmentation\": mask,\n                \"area\": area,\n                \"score\": float(score),  # ensure it's a native Python float\n            }\n        )\n\n    self.output = result  # Store the result\n    self.masks = formatted_masks  # Store the masks as a list of dictionaries\n    self.batch = False\n    # self.scores = scores  # Store the scores\n    self._min_size = min_size\n    self._max_size = max_size\n\n    # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n    self.save_masks(\n        output,\n        foreground,\n        unique,\n        erosion_kernel,\n        mask_multiplier,\n        min_size,\n        max_size,\n        **output_args,\n    )\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.generate_batch","title":"<code>generate_batch(inputs, output_dir=None, suffix='_masks', foreground=True, erosion_kernel=None, mask_multiplier=255, unique=True, min_size=0, max_size=None, output_args=None, **kwargs)</code>","text":"<p>Generate masks for a batch of input images.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Union[str, ndarray]]</code> <p>A list of paths to input images or numpy arrays representing the images.</p> required <code>output_dir</code> <code>Optional[str]</code> <p>The directory to save the output masks. Defaults to the current working directory.</p> <code>None</code> <code>suffix</code> <code>str</code> <p>The suffix to append to the output filenames. Defaults to \"_masks\".</p> <code>'_masks'</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>Optional[Tuple[int, int]]</code> <p>The erosion kernel for filtering object masks and extracting borders. For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>Optional[int]</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>output_args</code> <code>Optional[Dict[str, Any]]</code> <p>Additional arguments for saving the output. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other arguments for the mask generator.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input list is empty or contains invalid paths.</p> Source code in <code>geoai/sam.py</code> <pre><code>def generate_batch(\n    self,\n    inputs: List[Union[str, np.ndarray]],\n    output_dir: Optional[str] = None,\n    suffix: str = \"_masks\",\n    foreground: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    unique: bool = True,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    output_args: Optional[Dict[str, Any]] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Generate masks for a batch of input images.\n\n    Args:\n        inputs (List[Union[str, np.ndarray]]): A list of paths to input images or numpy arrays representing the images.\n        output_dir (Optional[str], optional): The directory to save the output masks. Defaults to the current working directory.\n        suffix (str, optional): The suffix to append to the output filenames. Defaults to \"_masks\".\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (Optional[Tuple[int, int]], optional): The erosion kernel for filtering object masks and extracting borders.\n            For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (Optional[int], optional): The maximum size of the objects. Defaults to None.\n        output_args (Optional[Dict[str, Any]], optional): Additional arguments for saving the output. Defaults to None.\n        **kwargs (Any): Other arguments for the mask generator.\n\n    Raises:\n        ValueError: If the input list is empty or contains invalid paths.\n    \"\"\"\n\n    mask_generator = self.mask_generator  # The automatic mask generator\n    outputs = mask_generator(inputs, **kwargs)\n\n    if output_args is None:\n        output_args = {}\n\n    if output_dir is None:\n        output_dir = os.getcwd()\n\n    for index, result in enumerate(outputs):\n\n        basename = os.path.basename(inputs[index])\n        file_ext = os.path.splitext(basename)[1]\n        filename = f\"{os.path.splitext(basename)[0]}{suffix}{file_ext}\"\n        filepath = os.path.join(output_dir, filename)\n\n        masks = result[\"masks\"] if \"masks\" in result else result  # Get the masks\n        scores = result[\"scores\"] if \"scores\" in result else None  # Get the scores\n\n        # format the masks as a list of dictionaries, similar to the output of SAM.\n        formatted_masks = []\n        for mask, score in zip(masks, scores):\n            area = int(np.sum(mask))  # number of True pixels\n            formatted_masks.append(\n                {\n                    \"segmentation\": mask,\n                    \"area\": area,\n                    \"score\": float(score),  # ensure it's a native Python float\n                }\n            )\n\n        self.source = inputs[index]  # Store the input image path\n        self.output = result  # Store the result\n        self.masks = formatted_masks  # Store the masks as a list of dictionaries\n        # self.scores = scores  # Store the scores\n        self._min_size = min_size\n        self._max_size = max_size\n\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            filepath,\n            foreground,\n            unique,\n            erosion_kernel,\n            mask_multiplier,\n            min_size,\n            max_size,\n            **output_args,\n        )\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.predict","title":"<code>predict(point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>geoai/sam.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n    out_of_bounds = []\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords, out_of_bounds = coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            elif len(out_of_bounds) &gt; 0:\n                print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                point_labels_new = []\n                for i, p in enumerate(point_labels):\n                    if i not in out_of_bounds:\n                        point_labels_new.append(p)\n                point_labels = point_labels_new\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n\n    if input_boxes is not None:\n        input_boxes = [input_boxes]\n\n    if point_coords is not None:\n        point_coords = [[point_coords]]\n        point_labels = [[point_labels]]\n\n    inputs = self.processor(\n        self.image,\n        input_points=point_coords,\n        # input_labels=point_labels,\n        input_boxes=input_boxes,\n        return_tensors=\"pt\",\n        **kwargs,\n    ).to(self.device)\n\n    inputs.pop(\"pixel_values\", None)\n    inputs.update({\"image_embeddings\": self.embeddings})\n\n    with torch.no_grad():\n        outputs = self.predictor(**inputs)\n\n    # https://huggingface.co/docs/transformers/en/model_doc/sam#transformers.SamImageProcessor.post_process_masks\n    self.masks = self.processor.image_processor.post_process_masks(\n        outputs.pred_masks.cpu(),\n        inputs[\"original_sizes\"].cpu(),\n        inputs[\"reshaped_input_sizes\"].cpu(),\n    )\n    self.scores = outputs.iou_scores\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.save_masks","title":"<code>save_masks(output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, min_size=0, max_size=None, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None, saving the masks to <code>SamGeo.objects</code>.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>Optional[Tuple[int, int]]</code> <p>The erosion kernel for filtering object masks and extracting borders. For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> <code>min_size</code> <code>int</code> <p>The minimum size of the objects. Defaults to 0.</p> <code>0</code> <code>max_size</code> <code>Optional[int]</code> <p>The maximum size of the objects. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Other arguments for <code>array_to_image()</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no masks are found or if <code>generate()</code> has not been run.</p> Source code in <code>geoai/sam.py</code> <pre><code>def save_masks(\n    self,\n    output: Optional[str] = None,\n    foreground: bool = True,\n    unique: bool = True,\n    erosion_kernel: Optional[Tuple[int, int]] = None,\n    mask_multiplier: int = 255,\n    min_size: int = 0,\n    max_size: Optional[int] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (Optional[str], optional): The path to the output image. Defaults to None, saving the masks to `SamGeo.objects`.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (Optional[Tuple[int, int]], optional): The erosion kernel for filtering object masks and extracting borders.\n            For example, (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        min_size (int, optional): The minimum size of the objects. Defaults to 0.\n        max_size (Optional[int], optional): The maximum size of the objects. Defaults to None.\n        **kwargs (Any): Other arguments for `array_to_image()`.\n\n    Raises:\n        ValueError: If no masks are found or if `generate()` has not been run.\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    if self.image is None:\n        (\n            h,\n            w,\n        ) = self.masks[\n            0\n        ][\"segmentation\"].shape\n    else:\n        h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in descending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=True)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        count = len(sorted_masks)\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            if min_size &gt; 0 and ann[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and ann[\"area\"] &gt; max_size:\n                continue\n            objects[m] = count - index\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            if min_size &gt; 0 and m[\"area\"] &lt; min_size:\n                continue\n            if max_size is not None and m[\"area\"] &gt; max_size:\n                continue\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.save_prediction","title":"<code>save_prediction(output, index=None, mask_multiplier=255, dtype=np.float32, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>float32</code> <code>vector</code> <code>Optional[str]</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>simplify_tolerance</code> <code>Optional[float]</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for saving the output image.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no predictions are found.</p> Source code in <code>geoai/sam.py</code> <pre><code>def save_prediction(\n    self,\n    output: str,\n    index: Optional[int] = None,\n    mask_multiplier: int = 255,\n    dtype: np.dtype = np.float32,\n    vector: Optional[str] = None,\n    simplify_tolerance: Optional[float] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (Optional[int], optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            Defaults to 255.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        vector (Optional[str], optional): The path to the output vector file. Defaults to None.\n        simplify_tolerance (Optional[float], optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry. Defaults to None.\n        **kwargs (Any): Additional arguments for saving the output image.\n\n    Raises:\n        ValueError: If no predictions are found.\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.set_image","title":"<code>set_image(image, **kwargs)</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray]</code> <p>The input image, either as a file path (string) or a numpy array.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the image processor.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input image path does not exist.</p> Source code in <code>geoai/sam.py</code> <pre><code>def set_image(self, image: Union[str, np.ndarray], **kwargs: Any) -&gt; None:\n    \"\"\"\n    Set the input image as a numpy array.\n\n    Args:\n        image (Union[str, np.ndarray]): The input image, either as a file path (string) or a numpy array.\n        **kwargs (Any): Additional arguments for the image processor.\n\n    Raises:\n        ValueError: If the input image path does not exist.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = Image.open(image).convert(\"RGB\")\n        self.image = image\n\n    inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n    self.embeddings = self.predictor.get_image_embeddings(\n        inputs[\"pixel_values\"], **kwargs\n    )\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.show_anns","title":"<code>show_anns(figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image blended with annotations. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for saving the output image.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input image or annotations are not available.</p> Source code in <code>geoai/sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    axis: str = \"off\",\n    alpha: float = 0.35,\n    output: Optional[str] = None,\n    blend: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (Tuple[int, int], optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (Optional[str], optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image blended with annotations. Defaults to True.\n        **kwargs (Any): Additional arguments for saving the output image.\n\n    Raises:\n        ValueError: If the input image or annotations are not available.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        if hasattr(self, \"_min_size\") and (ann[\"area\"] &lt; self._min_size):\n            continue\n        if (\n            hasattr(self, \"_max_size\")\n            and isinstance(self._max_size, int)\n            and ann[\"area\"] &gt; self._max_size\n        ):\n            continue\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    # if \"dpi\" not in kwargs:\n    #     kwargs[\"dpi\"] = 100\n\n    # if \"bbox_inches\" not in kwargs:\n    #     kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source, **kwargs)\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.show_masks","title":"<code>show_masks(figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Display the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap to use for displaying the mask. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for the <code>save_masks()</code> method.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no masks are available and <code>save_masks()</code> cannot generate them.</p> Source code in <code>geoai/sam.py</code> <pre><code>def show_masks(\n    self,\n    figsize: Tuple[int, int] = (12, 10),\n    cmap: str = \"binary_r\",\n    axis: str = \"off\",\n    foreground: bool = True,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Display the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (Tuple[int, int], optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap to use for displaying the mask. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs (Any): Additional arguments for the `save_masks()` method.\n\n    Raises:\n        ValueError: If no masks are available and `save_masks()` cannot generate them.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"sam/#geoai.sam.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(index=None, output=None, mask_multiplier=255, dtype='uint8', save_args=None)</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Optional[int]</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>Optional[str]</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>Union[str, dtype]</code> <p>The data type of the output image. Defaults to \"uint8\".</p> <code>'uint8'</code> <code>save_args</code> <code>Optional[Dict[str, Any]]</code> <p>Optional arguments for saving the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Optional[np.ndarray]: The predicted mask as a numpy array if <code>output</code> is None. Otherwise, saves the mask to the specified path.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no objects are found in the image or if the masks are not available.</p> Source code in <code>geoai/sam.py</code> <pre><code>def tensor_to_numpy(\n    self,\n    index: Optional[int] = None,\n    output: Optional[str] = None,\n    mask_multiplier: int = 255,\n    dtype: Union[str, np.dtype] = \"uint8\",\n    save_args: Optional[Dict[str, Any]] = None,\n) -&gt; Optional[np.ndarray]:\n    \"\"\"\n    Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (Optional[int], optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (Optional[str], optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            Defaults to 255.\n        dtype (Union[str, np.dtype], optional): The data type of the output image. Defaults to \"uint8\".\n        save_args (Optional[Dict[str, Any]], optional): Optional arguments for saving the output image. Defaults to None.\n\n    Returns:\n        Optional[np.ndarray]: The predicted mask as a numpy array if `output` is None. Otherwise, saves the mask to the specified path.\n\n    Raises:\n        ValueError: If no objects are found in the image or if the masks are not available.\n    \"\"\"\n\n    if save_args is None:\n        save_args = {}\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run the prediction method first.\")\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"sam/#geoai.sam.adaptive_regularization","title":"<code>adaptive_regularization(building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True)</code>","text":"<p>Adaptively regularizes building footprints based on their characteristics.</p> <p>This approach determines the best regularization method for each building.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons</p> required <code>simplify_tolerance</code> <code>float</code> <p>Distance tolerance for simplification</p> <code>0.5</code> <code>area_threshold</code> <code>float</code> <p>Minimum acceptable area ratio</p> <code>0.9</code> <code>preserve_shape</code> <code>bool</code> <p>Whether to preserve overall shape for complex buildings</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def adaptive_regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n    simplify_tolerance: float = 0.5,\n    area_threshold: float = 0.9,\n    preserve_shape: bool = True,\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    Adaptively regularizes building footprints based on their characteristics.\n\n    This approach determines the best regularization method for each building.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons\n        simplify_tolerance: Distance tolerance for simplification\n        area_threshold: Minimum acceptable area ratio\n        preserve_shape: Whether to preserve overall shape for complex buildings\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Analyze the overall dataset to set appropriate parameters\n    if is_gdf := isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # Skip invalid geometries\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Measure building complexity\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Determine if the building has a clear principal direction\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Normalize angles to 0-180 range and get histogram\n        norm_angles = angles % 180\n        hist, bins = np.histogram(\n            norm_angles, bins=18, range=(0, 180), weights=segment_lengths\n        )\n\n        # Calculate direction clarity (ratio of longest direction to total)\n        direction_clarity = np.max(hist) / np.sum(hist) if np.sum(hist) &gt; 0 else 0\n\n        # Choose regularization method based on building characteristics\n        if complexity &lt; 1.2 and direction_clarity &gt; 0.5:\n            # Simple building with clear direction: use rotated rectangle\n            bin_max = np.argmax(hist)\n            bin_centers = (bins[:-1] + bins[1:]) / 2\n            dominant_angle = bin_centers[bin_max]\n\n            # Rotate to align with coordinate system\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create bounding box in rotated space\n            bounds = rotated.bounds\n            rect = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            # Rotate back\n            result = rotate(rect, dominant_angle, origin=\"centroid\")\n\n            # Quality check\n            if (\n                result.area / building.area &lt; area_threshold\n                or result.area / building.area &gt; (1.0 / area_threshold)\n            ):\n                # Too much area change, use simplified original\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n\n        else:\n            # Complex building or no clear direction: preserve shape\n            if preserve_shape:\n                # Simplify with topology preservation\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n            else:\n                # Fall back to convex hull for very complex shapes\n                result = building.convex_hull\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"sam/#geoai.sam.add_geometric_properties","title":"<code>add_geometric_properties(data, properties=None, area_unit='m2', length_unit='m')</code>","text":"<p>Calculates geometric properties and adds them to the GeoDataFrame.</p> <p>This function calculates various geometric properties of features in a GeoDataFrame and adds them as new columns without modifying existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing vector features.</p> required <code>properties</code> <code>Optional[List[str]]</code> <p>List of geometric properties to calculate. Options include: 'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds', 'convex_hull_area', 'orientation', 'complexity', 'area_bbox', 'area_convex', 'area_filled', 'major_length', 'minor_length', 'eccentricity', 'diameter_areagth', 'extent', 'solidity', 'elongation'. Defaults to ['area', 'length'] if None.</p> <code>None</code> <code>area_unit</code> <code>str</code> <p>String specifying the unit for area calculation ('m2', 'km2', 'ha'). Defaults to 'm2'.</p> <code>'m2'</code> <code>length_unit</code> <code>str</code> <p>String specifying the unit for length calculation ('m', 'km'). Defaults to 'm'.</p> <code>'m'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added</p> <code>GeoDataFrame</code> <p>geometric property columns.</p> Source code in <code>geoai/utils.py</code> <pre><code>def add_geometric_properties(\n    data: gpd.GeoDataFrame,\n    properties: Optional[List[str]] = None,\n    area_unit: str = \"m2\",\n    length_unit: str = \"m\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Calculates geometric properties and adds them to the GeoDataFrame.\n\n    This function calculates various geometric properties of features in a\n    GeoDataFrame and adds them as new columns without modifying existing attributes.\n\n    Args:\n        data: GeoDataFrame containing vector features.\n        properties: List of geometric properties to calculate. Options include:\n            'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds',\n            'convex_hull_area', 'orientation', 'complexity', 'area_bbox',\n            'area_convex', 'area_filled', 'major_length', 'minor_length',\n            'eccentricity', 'diameter_areagth', 'extent', 'solidity',\n            'elongation'.\n            Defaults to ['area', 'length'] if None.\n        area_unit: String specifying the unit for area calculation ('m2', 'km2',\n            'ha'). Defaults to 'm2'.\n        length_unit: String specifying the unit for length calculation ('m', 'km').\n            Defaults to 'm'.\n\n    Returns:\n        geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added\n        geometric property columns.\n    \"\"\"\n    from shapely.ops import unary_union\n\n    if isinstance(data, str):\n        data = read_vector(data)\n\n    # Make a copy to avoid modifying the original\n    result = data.copy()\n\n    # Default properties to calculate\n    if properties is None:\n        properties = [\n            \"area\",\n            \"length\",\n            \"perimeter\",\n            \"convex_hull_area\",\n            \"orientation\",\n            \"complexity\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"diameter_area\",\n            \"extent\",\n            \"solidity\",\n            \"elongation\",\n        ]\n\n    # Make sure we're working with a GeoDataFrame with a valid CRS\n\n    if not isinstance(result, gpd.GeoDataFrame):\n        raise ValueError(\"Input must be a GeoDataFrame\")\n\n    if result.crs is None:\n        raise ValueError(\n            \"GeoDataFrame must have a defined coordinate reference system (CRS)\"\n        )\n\n    # Ensure we're working with a projected CRS for accurate measurements\n    if result.crs.is_geographic:\n        # Reproject to a suitable projected CRS for accurate measurements\n        result = result.to_crs(result.estimate_utm_crs())\n\n    # Basic area calculation with unit conversion\n    if \"area\" in properties:\n        # Calculate area (only for polygons)\n        result[\"area\"] = result.geometry.apply(\n            lambda geom: geom.area if isinstance(geom, (Polygon, MultiPolygon)) else 0\n        )\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area\"] = result[\"area\"] / 1_000_000  # m\u00b2 to km\u00b2\n            result.rename(columns={\"area\": \"area_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area\"] = result[\"area\"] / 10_000  # m\u00b2 to hectares\n            result.rename(columns={\"area\": \"area_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area\": \"area_m2\"}, inplace=True)\n\n    # Length calculation with unit conversion\n    if \"length\" in properties:\n        # Calculate length (works for lines and polygon boundaries)\n        result[\"length\"] = result.geometry.length\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"length\"] = result[\"length\"] / 1_000  # m to km\n            result.rename(columns={\"length\": \"length_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"length\": \"length_m\"}, inplace=True)\n\n    # Perimeter calculation (for polygons)\n    if \"perimeter\" in properties:\n        result[\"perimeter\"] = result.geometry.apply(\n            lambda geom: (\n                geom.boundary.length if isinstance(geom, (Polygon, MultiPolygon)) else 0\n            )\n        )\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"perimeter\"] = result[\"perimeter\"] / 1_000  # m to km\n            result.rename(columns={\"perimeter\": \"perimeter_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"perimeter\": \"perimeter_m\"}, inplace=True)\n\n    # Centroid coordinates\n    if \"centroid_x\" in properties or \"centroid_y\" in properties:\n        centroids = result.geometry.centroid\n\n        if \"centroid_x\" in properties:\n            result[\"centroid_x\"] = centroids.x\n\n        if \"centroid_y\" in properties:\n            result[\"centroid_y\"] = centroids.y\n\n    # Bounding box properties\n    if \"bounds\" in properties:\n        bounds = result.geometry.bounds\n        result[\"minx\"] = bounds.minx\n        result[\"miny\"] = bounds.miny\n        result[\"maxx\"] = bounds.maxx\n        result[\"maxy\"] = bounds.maxy\n\n    # Area of bounding box\n    if \"area_bbox\" in properties:\n        bounds = result.geometry.bounds\n        result[\"area_bbox\"] = (bounds.maxx - bounds.minx) * (bounds.maxy - bounds.miny)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 1_000_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 10_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_bbox\": \"area_bbox_m2\"}, inplace=True)\n\n    # Area of convex hull\n    if \"area_convex\" in properties or \"convex_hull_area\" in properties:\n        result[\"area_convex\"] = result.geometry.convex_hull.area\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 1_000_000\n            result.rename(columns={\"area_convex\": \"area_convex_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 10_000\n            result.rename(columns={\"area_convex\": \"area_convex_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_convex\": \"area_convex_m2\"}, inplace=True)\n\n        # For backward compatibility\n        if \"convex_hull_area\" in properties and \"area_convex\" not in properties:\n            result[\"convex_hull_area\"] = result[\"area_convex\"]\n            if area_unit == \"km2\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_km2\"}, inplace=True\n                )\n            elif area_unit == \"ha\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_ha\"}, inplace=True\n                )\n            else:\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_m2\"}, inplace=True\n                )\n\n    # Area of filled geometry (no holes)\n    if \"area_filled\" in properties:\n\n        def get_filled_area(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return 0\n\n            if isinstance(geom, MultiPolygon):\n                # For MultiPolygon, fill all constituent polygons\n                filled_polys = [Polygon(p.exterior) for p in geom.geoms]\n                return unary_union(filled_polys).area\n            else:\n                # For single Polygon, create a new one with just the exterior ring\n                return Polygon(geom.exterior).area\n\n        result[\"area_filled\"] = result.geometry.apply(get_filled_area)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 1_000_000\n            result.rename(columns={\"area_filled\": \"area_filled_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 10_000\n            result.rename(columns={\"area_filled\": \"area_filled_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_filled\": \"area_filled_m2\"}, inplace=True)\n\n    # Axes lengths, eccentricity, orientation, and elongation\n    if any(\n        p in properties\n        for p in [\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"orientation\",\n            \"elongation\",\n        ]\n    ):\n\n        def get_axes_properties(geom):\n            # Skip non-polygons\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return None, None, None, None, None\n\n            # Handle multipolygons by using the largest polygon\n            if isinstance(geom, MultiPolygon):\n                # Get the polygon with the largest area\n                geom = sorted(list(geom.geoms), key=lambda p: p.area, reverse=True)[0]\n\n            try:\n                # Get the minimum rotated rectangle\n                rect = geom.minimum_rotated_rectangle\n\n                # Extract coordinates\n                coords = list(rect.exterior.coords)[\n                    :-1\n                ]  # Remove the duplicated last point\n\n                if len(coords) &lt; 4:\n                    return None, None, None, None, None\n\n                # Calculate lengths of all four sides\n                sides = []\n                for i in range(len(coords)):\n                    p1 = coords[i]\n                    p2 = coords[(i + 1) % len(coords)]\n                    dx = p2[0] - p1[0]\n                    dy = p2[1] - p1[1]\n                    length = np.sqrt(dx**2 + dy**2)\n                    angle = np.degrees(np.arctan2(dy, dx)) % 180\n                    sides.append((length, angle, p1, p2))\n\n                # Group sides by length (allowing for small differences due to floating point precision)\n                # This ensures we correctly identify the rectangle's dimensions\n                sides_grouped = {}\n                tolerance = 1e-6  # Tolerance for length comparison\n\n                for s in sides:\n                    length, angle = s[0], s[1]\n                    matched = False\n\n                    for key in sides_grouped:\n                        if abs(length - key) &lt; tolerance:\n                            sides_grouped[key].append(s)\n                            matched = True\n                            break\n\n                    if not matched:\n                        sides_grouped[length] = [s]\n\n                # Get unique lengths (should be 2 for a rectangle, parallel sides have equal length)\n                unique_lengths = sorted(sides_grouped.keys(), reverse=True)\n\n                if len(unique_lengths) != 2:\n                    # If we don't get exactly 2 unique lengths, something is wrong with the rectangle\n                    # Fall back to simpler method using bounds\n                    bounds = rect.bounds\n                    width = bounds[2] - bounds[0]\n                    height = bounds[3] - bounds[1]\n                    major_length = max(width, height)\n                    minor_length = min(width, height)\n                    orientation = 0 if width &gt; height else 90\n                else:\n                    major_length = unique_lengths[0]\n                    minor_length = unique_lengths[1]\n                    # Get orientation from the major axis\n                    orientation = sides_grouped[major_length][0][1]\n\n                # Calculate eccentricity\n                if major_length &gt; 0:\n                    # Eccentricity for an ellipse: e = sqrt(1 - (b\u00b2/a\u00b2))\n                    # where a is the semi-major axis and b is the semi-minor axis\n                    eccentricity = np.sqrt(\n                        1 - ((minor_length / 2) ** 2 / (major_length / 2) ** 2)\n                    )\n                else:\n                    eccentricity = 0\n\n                # Calculate elongation (ratio of minor to major axis)\n                elongation = major_length / minor_length if major_length &gt; 0 else 1\n\n                return major_length, minor_length, eccentricity, orientation, elongation\n\n            except Exception as e:\n                # For debugging\n                # print(f\"Error calculating axes: {e}\")\n                return None, None, None, None, None\n\n        # Apply the function and split the results\n        axes_data = result.geometry.apply(get_axes_properties)\n\n        if \"major_length\" in properties:\n            result[\"major_length\"] = axes_data.apply(lambda x: x[0] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"major_length\"] = result[\"major_length\"] / 1_000\n                result.rename(columns={\"major_length\": \"major_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"major_length\": \"major_length_m\"}, inplace=True)\n\n        if \"minor_length\" in properties:\n            result[\"minor_length\"] = axes_data.apply(lambda x: x[1] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"minor_length\"] = result[\"minor_length\"] / 1_000\n                result.rename(columns={\"minor_length\": \"minor_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"minor_length\": \"minor_length_m\"}, inplace=True)\n\n        if \"eccentricity\" in properties:\n            result[\"eccentricity\"] = axes_data.apply(lambda x: x[2] if x else None)\n\n        if \"orientation\" in properties:\n            result[\"orientation\"] = axes_data.apply(lambda x: x[3] if x else None)\n\n        if \"elongation\" in properties:\n            result[\"elongation\"] = axes_data.apply(lambda x: x[4] if x else None)\n\n    # Equivalent diameter based on area\n    if \"diameter_areagth\" in properties:\n\n        def get_equivalent_diameter(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n            # Diameter of a circle with the same area: d = 2 * sqrt(A / \u03c0)\n            return 2 * np.sqrt(geom.area / np.pi)\n\n        result[\"diameter_areagth\"] = result.geometry.apply(get_equivalent_diameter)\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"diameter_areagth\"] = result[\"diameter_areagth\"] / 1_000\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_km\"},\n                inplace=True,\n            )\n        else:\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_m\"},\n                inplace=True,\n            )\n\n    # Extent (ratio of shape area to bounding box area)\n    if \"extent\" in properties:\n\n        def get_extent(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            bounds = geom.bounds\n            bbox_area = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])\n\n            if bbox_area &gt; 0:\n                return geom.area / bbox_area\n            return None\n\n        result[\"extent\"] = result.geometry.apply(get_extent)\n\n    # Solidity (ratio of shape area to convex hull area)\n    if \"solidity\" in properties:\n\n        def get_solidity(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            convex_hull_area = geom.convex_hull.area\n\n            if convex_hull_area &gt; 0:\n                return geom.area / convex_hull_area\n            return None\n\n        result[\"solidity\"] = result.geometry.apply(get_solidity)\n\n    # Complexity (ratio of perimeter to area)\n    if \"complexity\" in properties:\n\n        def calc_complexity(geom):\n            if isinstance(geom, (Polygon, MultiPolygon)) and geom.area &gt; 0:\n                # Shape index: P / (2 * sqrt(\u03c0 * A))\n                # Normalized to 1 for a circle, higher for more complex shapes\n                return geom.boundary.length / (2 * np.sqrt(np.pi * geom.area))\n            return None\n\n        result[\"complexity\"] = result.geometry.apply(calc_complexity)\n\n    return result\n</code></pre>"},{"location":"sam/#geoai.sam.analyze_vector_attributes","title":"<code>analyze_vector_attributes(vector_path, attribute_name)</code>","text":"<p>Analyze a specific attribute in a vector dataset and create a histogram.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to analyze</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing analysis results for the attribute</p> Source code in <code>geoai/utils.py</code> <pre><code>def analyze_vector_attributes(\n    vector_path: str, attribute_name: str\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Analyze a specific attribute in a vector dataset and create a histogram.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to analyze\n\n    Returns:\n        dict: Dictionary containing analysis results for the attribute\n    \"\"\"\n    try:\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return None\n\n        # Get the attribute series\n        attr = gdf[attribute_name]\n\n        # Perform different analyses based on data type\n        if pd.api.types.is_numeric_dtype(attr):\n            # Numeric attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"numeric\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"min\": attr.min(),\n                \"max\": attr.max(),\n                \"mean\": attr.mean(),\n                \"median\": attr.median(),\n                \"std\": attr.std(),\n                \"unique_values\": attr.nunique(),\n            }\n\n            # Create histogram\n            plt.figure(figsize=(10, 6))\n            plt.hist(attr.dropna(), bins=20, alpha=0.7, color=\"blue\")\n            plt.title(f\"Histogram of {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Frequency\")\n            plt.grid(True, alpha=0.3)\n            plt.show()\n\n        else:\n            # Categorical attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"categorical\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"unique_values\": attr.nunique(),\n                \"value_counts\": attr.value_counts().to_dict(),\n            }\n\n            # Create bar plot for top categories\n            top_n = min(10, attr.nunique())\n            plt.figure(figsize=(10, 6))\n            attr.value_counts().head(top_n).plot(kind=\"bar\", color=\"skyblue\")\n            plt.title(f\"Top {top_n} values for {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Count\")\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n\n        return analysis\n\n    except Exception as e:\n        print(f\"Error analyzing attribute: {str(e)}\")\n        return None\n</code></pre>"},{"location":"sam/#geoai.sam.batch_vector_to_raster","title":"<code>batch_vector_to_raster(vector_path, output_dir, attribute_field=None, reference_rasters=None, bounds_list=None, output_filename_pattern='{vector_name}_{index}', pixel_size=1.0, all_touched=False, fill_value=0, dtype=np.uint8, nodata=None)</code>","text":"<p>Batch convert vector data to multiple rasters based on different extents or reference rasters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output raster files.</p> required <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values.</p> <code>None</code> <code>reference_rasters</code> <code>list</code> <p>List of paths to reference rasters for dimensions, transform and CRS.</p> <code>None</code> <code>bounds_list</code> <code>list</code> <p>List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.</p> <code>None</code> <code>output_filename_pattern</code> <code>str</code> <p>Pattern for output filenames. Can include {vector_name} and {index} placeholders.</p> <code>'{vector_name}_{index}'</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size to use if reference_rasters not provided.</p> <code>1.0</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type of the output raster.</p> <code>uint8</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to the created raster files.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_vector_to_raster(\n    vector_path,\n    output_dir,\n    attribute_field=None,\n    reference_rasters=None,\n    bounds_list=None,\n    output_filename_pattern=\"{vector_name}_{index}\",\n    pixel_size=1.0,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n) -&gt; List[str]:\n    \"\"\"\n    Batch convert vector data to multiple rasters based on different extents or reference rasters.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_dir (str): Directory to save output raster files.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n        reference_rasters (list): List of paths to reference rasters for dimensions, transform and CRS.\n        bounds_list (list): List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.\n        output_filename_pattern (str): Pattern for output filenames.\n            Can include {vector_name} and {index} placeholders.\n        pixel_size (float or tuple): Pixel size to use if reference_rasters not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n\n    Returns:\n        List[str]: List of paths to the created raster files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load vector data if it's a path\n    if isinstance(vector_path, str):\n        gdf = gpd.read_file(vector_path)\n        vector_name = os.path.splitext(os.path.basename(vector_path))[0]\n    else:\n        gdf = vector_path\n        vector_name = \"vector\"\n\n    # Check input parameters\n    if reference_rasters is None and bounds_list is None:\n        raise ValueError(\"Either reference_rasters or bounds_list must be provided.\")\n\n    # Use reference_rasters if provided, otherwise use bounds_list\n    if reference_rasters is not None:\n        sources = reference_rasters\n        is_raster_reference = True\n    else:\n        sources = bounds_list\n        is_raster_reference = False\n\n    # Create output filenames\n    output_files = []\n\n    # Process each source (reference raster or bounds)\n    for i, source in enumerate(tqdm(sources, desc=\"Processing\")):\n        # Generate output filename\n        output_filename = output_filename_pattern.format(\n            vector_name=vector_name, index=i\n        )\n        if not output_filename.endswith(\".tif\"):\n            output_filename += \".tif\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        if is_raster_reference:\n            # Use reference raster\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                reference_raster=source,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n        else:\n            # Use bounds\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                bounds=source,\n                pixel_size=pixel_size,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n\n        output_files.append(output_path)\n\n    return output_files\n</code></pre>"},{"location":"sam/#geoai.sam.bbox_to_xy","title":"<code>bbox_to_xy(src_fp, coords, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.     Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright     While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[float]</code> <p>A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.</p> Source code in <code>geoai/utils.py</code> <pre><code>def bbox_to_xy(\n    src_fp: str, coords: List[float], coord_crs: str = \"epsg:4326\", **kwargs: Any\n) -&gt; List[float]:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n        Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n        While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n\n    Args:\n        src_fp (str): The source raster file path.\n        coords (list): A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]\n        coord_crs (str, optional): The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.\n    \"\"\"\n\n    if isinstance(coords, str):\n        gdf = gpd.read_file(coords)\n        coords = gdf.geometry.bounds.values.tolist()\n        if gdf.crs is not None:\n            coord_crs = f\"epsg:{gdf.crs.to_epsg()}\"\n    elif isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n    if isinstance(coords, dict):\n        import json\n\n        geojson = json.dumps(coords)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        coords = gdf.geometry.bounds.values.tolist()\n\n    elif not isinstance(coords, list):\n        raise ValueError(\"coords must be a list of coordinates.\")\n\n    if not isinstance(coords[0], list):\n        coords = [coords]\n\n    new_coords = []\n\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n\n        for coord in coords:\n            minx, miny, maxx, maxy = coord\n\n            if coord_crs != src.crs:\n                minx, miny = transform_coords(minx, miny, coord_crs, src.crs, **kwargs)\n                maxx, maxy = transform_coords(maxx, maxy, coord_crs, src.crs, **kwargs)\n\n                rows1, cols1 = rasterio.transform.rowcol(\n                    src.transform, minx, miny, **kwargs\n                )\n                rows2, cols2 = rasterio.transform.rowcol(\n                    src.transform, maxx, maxy, **kwargs\n                )\n\n                new_coords.append([cols1, rows1, cols2, rows2])\n\n            else:\n                new_coords.append([minx, miny, maxx, maxy])\n\n    result = []\n\n    for coord in new_coords:\n        minx, miny, maxx, maxy = coord\n\n        if (\n            minx &gt;= 0\n            and miny &gt;= 0\n            and maxx &gt;= 0\n            and maxy &gt;= 0\n            and minx &lt; width\n            and miny &lt; height\n            and maxx &lt; width\n            and maxy &lt; height\n        ):\n            # Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n            # While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n            result.append([minx, maxy, maxx, miny])\n\n    if len(result) == 0:\n        print(\"No valid pixel coordinates found.\")\n        return None\n    elif len(result) == 1:\n        return result[0]\n    elif len(result) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    return result\n</code></pre>"},{"location":"sam/#geoai.sam.boxes_to_vector","title":"<code>boxes_to_vector(coords, src_crs, dst_crs='EPSG:4326', output=None, **kwargs)</code>","text":"<p>Convert a list of bounding box coordinates to vector data.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].</p> required <code>src_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.</p> required <code>dst_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").</p> <code>'EPSG:4326'</code> <code>output</code> <code>str or None</code> <p>The full file path (including the directory and filename without the extension) where the vector data should be saved.                            If None (default), the function returns the GeoDataFrame without saving it to a file.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def boxes_to_vector(\n    coords: Union[List[List[float]], np.ndarray],\n    src_crs: str,\n    dst_crs: str = \"EPSG:4326\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a list of bounding box coordinates to vector data.\n\n    Args:\n        coords (list): A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].\n        src_crs (int or str): The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.\n        dst_crs (int or str, optional): The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").\n        output (str or None, optional): The full file path (including the directory and filename without the extension) where the vector data should be saved.\n                                       If None (default), the function returns the GeoDataFrame without saving it to a file.\n        **kwargs: Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.\n\n    Returns:\n        geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.\n    \"\"\"\n\n    from shapely.geometry import box\n\n    # Create a list of Shapely Polygon objects based on the provided coordinates\n    polygons = [box(*coord) for coord in coords]\n\n    # Create a GeoDataFrame with the Shapely Polygon objects\n    gdf = gpd.GeoDataFrame({\"geometry\": polygons}, crs=src_crs)\n\n    # Reproject the GeoDataFrame to the specified EPSG code\n    gdf_reprojected = gdf.to_crs(dst_crs)\n\n    if output is not None:\n        gdf_reprojected.to_file(output, **kwargs)\n    else:\n        return gdf_reprojected\n</code></pre>"},{"location":"sam/#geoai.sam.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.</p> Source code in <code>geoai/utils.py</code> <pre><code>def calc_stats(dataset, divide_by: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rasterio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"sam/#geoai.sam.clip_raster_by_bbox","title":"<code>clip_raster_by_bbox(input_raster, output_raster, bbox, bands=None, bbox_type='geo', bbox_crs=None)</code>","text":"<p>Clip a raster dataset using a bounding box and optionally select specific bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_raster</code> <code>str</code> <p>Path to the input raster file.</p> required <code>output_raster</code> <code>str</code> <p>Path where the clipped raster will be saved.</p> required <code>bbox</code> <code>tuple</code> <p>Bounding box coordinates either as:          - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"          - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"</p> required <code>bands</code> <code>list</code> <p>List of band indices to keep (1-based indexing).                    If None, all bands will be kept.</p> <code>None</code> <code>bbox_type</code> <code>str</code> <p>Type of bounding box coordinates. Either \"geo\" for                       geographic coordinates or \"pixel\" for row/column indices.                       Default is \"geo\".</p> <code>'geo'</code> <code>bbox_crs</code> <code>str or dict</code> <p>CRS of the bbox if different from the raster CRS.                              Can be provided as EPSG code (e.g., \"EPSG:4326\") or                              as a proj4 string. Only applies when bbox_type=\"geo\".                              If None, assumes bbox is in the same CRS as the raster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the clipped output raster.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>ValueError</code> <p>If the bbox is invalid, bands are out of range, or bbox_type is invalid.</p> <code>RuntimeError</code> <p>If the clipping operation fails.</p> <p>Examples:</p> <p>Clip using geographic coordinates in the same CRS as the raster</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n'clipped_geo.tif'\n</code></pre> <p>Clip using WGS84 coordinates when the raster is in a different CRS</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n...                     bbox_crs=\"EPSG:4326\")\n'clipped_wgs84.tif'\n</code></pre> <p>Clip using row/column indices</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n...                     bbox_type=\"pixel\")\n'clipped_pixel.tif'\n</code></pre> <p>Clip with band selection</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n...                     bands=[1, 3])\n'clipped_bands.tif'\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def clip_raster_by_bbox(\n    input_raster: str,\n    output_raster: str,\n    bbox: List[float],\n    bands: Optional[List[int]] = None,\n    bbox_type: str = \"geo\",\n    bbox_crs: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Clip a raster dataset using a bounding box and optionally select specific bands.\n\n    Args:\n        input_raster (str): Path to the input raster file.\n        output_raster (str): Path where the clipped raster will be saved.\n        bbox (tuple): Bounding box coordinates either as:\n                     - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"\n                     - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"\n        bands (list, optional): List of band indices to keep (1-based indexing).\n                               If None, all bands will be kept.\n        bbox_type (str, optional): Type of bounding box coordinates. Either \"geo\" for\n                                  geographic coordinates or \"pixel\" for row/column indices.\n                                  Default is \"geo\".\n        bbox_crs (str or dict, optional): CRS of the bbox if different from the raster CRS.\n                                         Can be provided as EPSG code (e.g., \"EPSG:4326\") or\n                                         as a proj4 string. Only applies when bbox_type=\"geo\".\n                                         If None, assumes bbox is in the same CRS as the raster.\n\n    Returns:\n        str: Path to the clipped output raster.\n\n    Raises:\n        ImportError: If required dependencies are not installed.\n        ValueError: If the bbox is invalid, bands are out of range, or bbox_type is invalid.\n        RuntimeError: If the clipping operation fails.\n\n    Examples:\n        Clip using geographic coordinates in the same CRS as the raster\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n        'clipped_geo.tif'\n\n        Clip using WGS84 coordinates when the raster is in a different CRS\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n        ...                     bbox_crs=\"EPSG:4326\")\n        'clipped_wgs84.tif'\n\n        Clip using row/column indices\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n        ...                     bbox_type=\"pixel\")\n        'clipped_pixel.tif'\n\n        Clip with band selection\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n        ...                     bands=[1, 3])\n        'clipped_bands.tif'\n    \"\"\"\n    from rasterio.transform import from_bounds\n    from rasterio.warp import transform_bounds\n\n    # Validate bbox_type\n    if bbox_type not in [\"geo\", \"pixel\"]:\n        raise ValueError(\"bbox_type must be either 'geo' or 'pixel'\")\n\n    # Validate bbox\n    if len(bbox) != 4:\n        raise ValueError(\"bbox must contain exactly 4 values\")\n\n    # Open the source raster\n    with rasterio.open(input_raster) as src:\n        # Get the source CRS\n        src_crs = src.crs\n\n        # Handle different bbox types\n        if bbox_type == \"geo\":\n            minx, miny, maxx, maxy = bbox\n\n            # Validate geographic bbox\n            if minx &gt;= maxx or miny &gt;= maxy:\n                raise ValueError(\n                    \"Invalid geographic bbox. Expected (minx, miny, maxx, maxy) where minx &lt; maxx and miny &lt; maxy\"\n                )\n\n            # If bbox_crs is provided and different from the source CRS, transform the bbox\n            if bbox_crs is not None and bbox_crs != src_crs:\n                try:\n                    # Transform bbox coordinates from bbox_crs to src_crs\n                    minx, miny, maxx, maxy = transform_bounds(\n                        bbox_crs, src_crs, minx, miny, maxx, maxy\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Failed to transform bbox from {bbox_crs} to {src_crs}: {str(e)}\"\n                    )\n\n            # Calculate the pixel window from geographic coordinates\n            window = src.window(minx, miny, maxx, maxy)\n\n            # Use the same bounds for the output transform\n            output_bounds = (minx, miny, maxx, maxy)\n\n        else:  # bbox_type == \"pixel\"\n            min_row, min_col, max_row, max_col = bbox\n\n            # Validate pixel bbox\n            if min_row &gt;= max_row or min_col &gt;= max_col:\n                raise ValueError(\n                    \"Invalid pixel bbox. Expected (min_row, min_col, max_row, max_col) where min_row &lt; max_row and min_col &lt; max_col\"\n                )\n\n            if (\n                min_row &lt; 0\n                or min_col &lt; 0\n                or max_row &gt; src.height\n                or max_col &gt; src.width\n            ):\n                raise ValueError(\n                    f\"Pixel indices out of bounds. Raster dimensions are {src.height} rows x {src.width} columns\"\n                )\n\n            # Create a window from pixel coordinates\n            window = Window(min_col, min_row, max_col - min_col, max_row - min_row)\n\n            # Calculate the geographic bounds for this window\n            window_transform = src.window_transform(window)\n            output_bounds = rasterio.transform.array_bounds(\n                window.height, window.width, window_transform\n            )\n            # Reorder to (minx, miny, maxx, maxy)\n            output_bounds = (\n                output_bounds[0],\n                output_bounds[1],\n                output_bounds[2],\n                output_bounds[3],\n            )\n\n        # Get window dimensions\n        window_width = int(window.width)\n        window_height = int(window.height)\n\n        # Check if the window is valid\n        if window_width &lt;= 0 or window_height &lt;= 0:\n            raise ValueError(\"Bounding box results in an empty window\")\n\n        # Handle band selection\n        if bands is None:\n            # Use all bands\n            bands_to_read = list(range(1, src.count + 1))\n        else:\n            # Validate band indices\n            if not all(1 &lt;= b &lt;= src.count for b in bands):\n                raise ValueError(f\"Band indices must be between 1 and {src.count}\")\n            bands_to_read = bands\n\n        # Calculate new transform for the clipped raster\n        new_transform = from_bounds(\n            output_bounds[0],\n            output_bounds[1],\n            output_bounds[2],\n            output_bounds[3],\n            window_width,\n            window_height,\n        )\n\n        # Create a metadata dictionary for the output\n        out_meta = src.meta.copy()\n        out_meta.update(\n            {\n                \"height\": window_height,\n                \"width\": window_width,\n                \"transform\": new_transform,\n                \"count\": len(bands_to_read),\n            }\n        )\n\n        # Read the data for the selected bands\n        data = []\n        for band_idx in bands_to_read:\n            band_data = src.read(band_idx, window=window)\n            data.append(band_data)\n\n        # Stack the bands into a single array\n        if len(data) &gt; 1:\n            clipped_data = np.stack(data)\n        else:\n            clipped_data = data[0][np.newaxis, :, :]\n\n        # Write the output raster\n        with rasterio.open(output_raster, \"w\", **out_meta) as dst:\n            dst.write(clipped_data)\n\n    return output_raster\n</code></pre>"},{"location":"sam/#geoai.sam.coords_to_xy","title":"<code>coords_to_xy(src_fp, coords, coord_crs='epsg:4326', return_out_of_bounds=False, **kwargs)</code>","text":"<p>Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>ndarray</code> <p>A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]     or [[[x1, y1]], [[x2, y2]], ...].</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>return_out_of_bounds</code> <code>bool</code> <p>Whether to return out-of-bounds coordinates. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D or 3D array of pixel coordinates in the same format as the input.</p> Source code in <code>geoai/utils.py</code> <pre><code>def coords_to_xy(\n    src_fp: str,\n    coords: np.ndarray,\n    coord_crs: str = \"epsg:4326\",\n    return_out_of_bounds: bool = False,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    \"\"\"Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        coords: A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]\n                or [[[x1, y1]], [[x2, y2]], ...].\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        return_out_of_bounds: Whether to return out-of-bounds coordinates. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A 2D or 3D array of pixel coordinates in the same format as the input.\n    \"\"\"\n    from rasterio.warp import transform as transform_coords\n\n    out_of_bounds = []\n    if isinstance(coords, np.ndarray):\n        input_is_3d = coords.ndim == 3  # Check if the input is a 3D array\n    else:\n        input_is_3d = False\n\n    # Flatten the 3D array to 2D if necessary\n    if input_is_3d:\n        original_shape = coords.shape  # Store the original shape\n        coords = coords.reshape(-1, 2)  # Flatten to 2D\n\n    # Convert ndarray to a list if necessary\n    if isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n\n    xs, ys = zip(*coords)\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n        if coord_crs != src.crs:\n            xs, ys = transform_coords(coord_crs, src.crs, xs, ys, **kwargs)\n        rows, cols = rasterio.transform.rowcol(src.transform, xs, ys, **kwargs)\n\n    result = [[col, row] for col, row in zip(cols, rows)]\n\n    output = []\n\n    for i, (x, y) in enumerate(result):\n        if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height:\n            output.append([x, y])\n        else:\n            out_of_bounds.append(i)\n\n    # Convert the output back to the original shape if input was 3D\n    output = np.array(output)\n    if input_is_3d:\n        output = output.reshape(original_shape)\n\n    # Handle cases where no valid pixel coordinates are found\n    if len(output) == 0:\n        print(\"No valid pixel coordinates found.\")\n    elif len(output) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    if return_out_of_bounds:\n        return output, out_of_bounds\n    else:\n        return output\n</code></pre>"},{"location":"sam/#geoai.sam.create_overview_image","title":"<code>create_overview_image(src, tile_coordinates, output_path, tile_size, stride, geojson_path=None)</code>","text":"<p>Create an overview image showing all tiles and their status, with optional GeoJSON export.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>DatasetReader</code> <p>The source raster dataset.</p> required <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>output_path</code> <code>str</code> <p>The path where the overview image will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels.</p> required <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Controls overlap between adjacent tiles.</p> required <code>geojson_path</code> <code>str</code> <p>If provided, exports the tile rectangles as GeoJSON to this path.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved overview image.</p> Source code in <code>geoai/utils.py</code> <pre><code>def create_overview_image(\n    src, tile_coordinates, output_path, tile_size, stride, geojson_path=None\n) -&gt; str:\n    \"\"\"Create an overview image showing all tiles and their status, with optional GeoJSON export.\n\n    Args:\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        output_path (str): The path where the overview image will be saved.\n        tile_size (int): The size of each tile in pixels.\n        stride (int): The stride between tiles in pixels. Controls overlap between adjacent tiles.\n        geojson_path (str, optional): If provided, exports the tile rectangles as GeoJSON to this path.\n\n    Returns:\n        str: Path to the saved overview image.\n    \"\"\"\n    # Read a reduced version of the source image\n    overview_scale = max(\n        1, int(max(src.width, src.height) / 2000)\n    )  # Scale to max ~2000px\n    overview_width = src.width // overview_scale\n    overview_height = src.height // overview_scale\n\n    # Read downsampled image\n    overview_data = src.read(\n        out_shape=(src.count, overview_height, overview_width),\n        resampling=rasterio.enums.Resampling.average,\n    )\n\n    # Create RGB image for display\n    if overview_data.shape[0] &gt;= 3:\n        rgb = np.moveaxis(overview_data[:3], 0, -1)\n    else:\n        # For single band, create grayscale RGB\n        rgb = np.stack([overview_data[0], overview_data[0], overview_data[0]], axis=-1)\n\n    # Normalize for display\n    for i in range(rgb.shape[-1]):\n        band = rgb[..., i]\n        non_zero = band[band &gt; 0]\n        if len(non_zero) &gt; 0:\n            p2, p98 = np.percentile(non_zero, (2, 98))\n            rgb[..., i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n\n    # Create figure\n    plt.figure(figsize=(12, 12))\n    plt.imshow(rgb)\n\n    # If GeoJSON export is requested, prepare GeoJSON structures\n    if geojson_path:\n        features = []\n\n    # Draw tile boundaries\n    for tile in tile_coordinates:\n        # Convert bounds to pixel coordinates in overview\n        bounds = tile[\"bounds\"]\n        # Calculate scaled pixel coordinates\n        x_min = int((tile[\"x\"]) / overview_scale)\n        y_min = int((tile[\"y\"]) / overview_scale)\n        width = int(tile_size / overview_scale)\n        height = int(tile_size / overview_scale)\n\n        # Draw rectangle\n        color = \"lime\" if tile[\"has_features\"] else \"red\"\n        rect = plt.Rectangle(\n            (x_min, y_min), width, height, fill=False, edgecolor=color, linewidth=0.5\n        )\n        plt.gca().add_patch(rect)\n\n        # Add tile number if not too crowded\n        if width &gt; 20 and height &gt; 20:\n            plt.text(\n                x_min + width / 2,\n                y_min + height / 2,\n                str(tile[\"index\"]),\n                color=\"white\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n            )\n\n        # Add to GeoJSON features if exporting\n        if geojson_path:\n            # Create a polygon from the bounds (already in geo-coordinates)\n            minx, miny, maxx, maxy = bounds\n            polygon = box(minx, miny, maxx, maxy)\n\n            # Calculate overlap with neighboring tiles\n            overlap = 0\n            if stride &lt; tile_size:\n                overlap = tile_size - stride\n\n            # Create a GeoJSON feature\n            feature = {\n                \"type\": \"Feature\",\n                \"geometry\": mapping(polygon),\n                \"properties\": {\n                    \"index\": tile[\"index\"],\n                    \"has_features\": tile[\"has_features\"],\n                    \"bounds_pixel\": [\n                        tile[\"x\"],\n                        tile[\"y\"],\n                        tile[\"x\"] + tile_size,\n                        tile[\"y\"] + tile_size,\n                    ],\n                    \"tile_size_px\": tile_size,\n                    \"stride_px\": stride,\n                    \"overlap_px\": overlap,\n                },\n            }\n\n            # Add any additional properties from the tile\n            for key, value in tile.items():\n                if key not in [\"x\", \"y\", \"index\", \"has_features\", \"bounds\"]:\n                    feature[\"properties\"][key] = value\n\n            features.append(feature)\n\n    plt.title(\"Tile Overview (Green = Contains Features, Red = Empty)\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"Overview image saved to {output_path}\")\n\n    # Export GeoJSON if requested\n    if geojson_path:\n        geojson_collection = {\n            \"type\": \"FeatureCollection\",\n            \"features\": features,\n            \"properties\": {\n                \"crs\": (\n                    src.crs.to_string()\n                    if hasattr(src.crs, \"to_string\")\n                    else str(src.crs)\n                ),\n                \"total_tiles\": len(features),\n                \"source_raster_dimensions\": [src.width, src.height],\n            },\n        }\n\n        # Save to file\n        with open(geojson_path, \"w\") as f:\n            json.dump(geojson_collection, f)\n\n        print(f\"GeoJSON saved to {geojson_path}\")\n\n    return output_path\n</code></pre>"},{"location":"sam/#geoai.sam.create_split_map","title":"<code>create_split_map(left_layer='TERRAIN', right_layer='OpenTopoMap', left_args=None, right_args=None, left_array_args=None, right_array_args=None, zoom_control=True, fullscreen_control=True, layer_control=True, add_close_button=False, left_label=None, right_label=None, left_position='bottomleft', right_position='bottomright', widget_layout=None, draggable=True, center=[20, 0], zoom=2, height='600px', basemap=None, basemap_args=None, m=None, **kwargs)</code>","text":"<p>Adds split map.</p> <p>Parameters:</p> Name Type Description Default <code>left_layer</code> <code>str</code> <p>The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.</p> <code>'TERRAIN'</code> <code>right_layer</code> <code>str</code> <p>The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.</p> <code>'OpenTopoMap'</code> <code>left_args</code> <code>dict</code> <p>The arguments for the left tile layer. Defaults to {}.</p> <code>None</code> <code>right_args</code> <code>dict</code> <p>The arguments for the right tile layer. Defaults to {}.</p> <code>None</code> <code>left_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the left layer. Defaults to {}.</p> <code>None</code> <code>right_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the right layer. Defaults to {}.</p> <code>None</code> <code>zoom_control</code> <code>bool</code> <p>Whether to add zoom control. Defaults to True.</p> <code>True</code> <code>fullscreen_control</code> <code>bool</code> <p>Whether to add fullscreen control. Defaults to True.</p> <code>True</code> <code>layer_control</code> <code>bool</code> <p>Whether to add layer control. Defaults to True.</p> <code>True</code> <code>add_close_button</code> <code>bool</code> <p>Whether to add a close button. Defaults to False.</p> <code>False</code> <code>left_label</code> <code>str</code> <p>The label for the left layer. Defaults to None.</p> <code>None</code> <code>right_label</code> <code>str</code> <p>The label for the right layer. Defaults to None.</p> <code>None</code> <code>left_position</code> <code>str</code> <p>The position for the left label. Defaults to \"bottomleft\".</p> <code>'bottomleft'</code> <code>right_position</code> <code>str</code> <p>The position for the right label. Defaults to \"bottomright\".</p> <code>'bottomright'</code> <code>widget_layout</code> <code>dict</code> <p>The layout for the widget. Defaults to None.</p> <code>None</code> <code>draggable</code> <code>bool</code> <p>Whether the split map is draggable. Defaults to True.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def create_split_map(\n    left_layer: Optional[str] = \"TERRAIN\",\n    right_layer: Optional[str] = \"OpenTopoMap\",\n    left_args: Optional[dict] = None,\n    right_args: Optional[dict] = None,\n    left_array_args: Optional[dict] = None,\n    right_array_args: Optional[dict] = None,\n    zoom_control: Optional[bool] = True,\n    fullscreen_control: Optional[bool] = True,\n    layer_control: Optional[bool] = True,\n    add_close_button: Optional[bool] = False,\n    left_label: Optional[str] = None,\n    right_label: Optional[str] = None,\n    left_position: Optional[str] = \"bottomleft\",\n    right_position: Optional[str] = \"bottomright\",\n    widget_layout: Optional[dict] = None,\n    draggable: Optional[bool] = True,\n    center: Optional[List[float]] = [20, 0],\n    zoom: Optional[int] = 2,\n    height: Optional[int] = \"600px\",\n    basemap: Optional[str] = None,\n    basemap_args: Optional[Dict] = None,\n    m: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Adds split map.\n\n    Args:\n        left_layer (str, optional): The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.\n        right_layer (str, optional): The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.\n        left_args (dict, optional): The arguments for the left tile layer. Defaults to {}.\n        right_args (dict, optional): The arguments for the right tile layer. Defaults to {}.\n        left_array_args (dict, optional): The arguments for array_to_image for the left layer. Defaults to {}.\n        right_array_args (dict, optional): The arguments for array_to_image for the right layer. Defaults to {}.\n        zoom_control (bool, optional): Whether to add zoom control. Defaults to True.\n        fullscreen_control (bool, optional): Whether to add fullscreen control. Defaults to True.\n        layer_control (bool, optional): Whether to add layer control. Defaults to True.\n        add_close_button (bool, optional): Whether to add a close button. Defaults to False.\n        left_label (str, optional): The label for the left layer. Defaults to None.\n        right_label (str, optional): The label for the right layer. Defaults to None.\n        left_position (str, optional): The position for the left label. Defaults to \"bottomleft\".\n        right_position (str, optional): The position for the right label. Defaults to \"bottomright\".\n        widget_layout (dict, optional): The layout for the widget. Defaults to None.\n        draggable (bool, optional): Whether the split map is draggable. Defaults to True.\n    \"\"\"\n\n    if left_args is None:\n        left_args = {}\n\n    if right_args is None:\n        right_args = {}\n\n    if left_array_args is None:\n        left_array_args = {}\n\n    if right_array_args is None:\n        right_array_args = {}\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if m is None:\n        m = leafmap.Map(center=center, zoom=zoom, height=height, **kwargs)\n        m.clear_layers()\n    if isinstance(basemap, str):\n        if basemap.endswith(\".tif\"):\n            if basemap.startswith(\"http\"):\n                m.add_cog_layer(basemap, name=\"Basemap\", **basemap_args)\n            else:\n                m.add_raster(basemap, layer_name=\"Basemap\", **basemap_args)\n        else:\n            m.add_basemap(basemap)\n    m.split_map(\n        left_layer=left_layer,\n        right_layer=right_layer,\n        left_args=left_args,\n        right_args=right_args,\n        left_array_args=left_array_args,\n        right_array_args=right_array_args,\n        zoom_control=zoom_control,\n        fullscreen_control=fullscreen_control,\n        layer_control=layer_control,\n        add_close_button=add_close_button,\n        left_label=left_label,\n        right_label=right_label,\n        left_position=left_position,\n        right_position=right_position,\n        widget_layout=widget_layout,\n        draggable=draggable,\n    )\n\n    return m\n</code></pre>"},{"location":"sam/#geoai.sam.dict_to_image","title":"<code>dict_to_image(data_dict, output=None, **kwargs)</code>","text":"<p>Convert a dictionary containing spatial data to a rasterio dataset or save it to a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>This function transforms a dictionary with CRS, bounding box, and image data into a rasterio DatasetReader using leafmap's array_to_image utility after first converting to a rioxarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing: - 'crs': A pyproj CRS object - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes   and optionally mint, maxt for temporal bounds - 'image': A tensor or array-like object with image data</p> required <code>output</code> <code>Optional[str]</code> <p>Optional path to save the image to a file. If not provided, the image will be returned as a rasterio DatasetReader object.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to leafmap.array_to_image. Common options include: - colormap: str, name of the colormap (e.g., 'viridis', 'terrain') - vmin: float, minimum value for colormap scaling - vmax: float, maximum value for colormap scaling</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Any]</code> <p>A rasterio DatasetReader object that can be used for visualization or</p> <code>Union[str, Any]</code> <p>further processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = dict_to_image(\n...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n...     colormap='terrain'\n... )\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n&gt;&gt;&gt; show(image, ax=ax)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_image(\n    data_dict: Dict[str, Any], output: Optional[str] = None, **kwargs: Any\n) -&gt; Union[str, Any]:\n    \"\"\"Convert a dictionary containing spatial data to a rasterio dataset or save it to\n    a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\".\n    It can be generated from a TorchGeo dataset sampler.\n\n    This function transforms a dictionary with CRS, bounding box, and image data\n    into a rasterio DatasetReader using leafmap's array_to_image utility after\n    first converting to a rioxarray DataArray.\n\n    Args:\n        data_dict: A dictionary containing:\n            - 'crs': A pyproj CRS object\n            - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes\n              and optionally mint, maxt for temporal bounds\n            - 'image': A tensor or array-like object with image data\n        output: Optional path to save the image to a file. If not provided, the image\n            will be returned as a rasterio DatasetReader object.\n        **kwargs: Additional keyword arguments to pass to leafmap.array_to_image.\n            Common options include:\n            - colormap: str, name of the colormap (e.g., 'viridis', 'terrain')\n            - vmin: float, minimum value for colormap scaling\n            - vmax: float, maximum value for colormap scaling\n\n    Returns:\n        A rasterio DatasetReader object that can be used for visualization or\n        further processing.\n\n    Examples:\n        &gt;&gt;&gt; image = dict_to_image(\n        ...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n        ...     colormap='terrain'\n        ... )\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n        &gt;&gt;&gt; show(image, ax=ax)\n    \"\"\"\n    da = dict_to_rioxarray(data_dict)\n\n    if output is not None:\n        out_dir = os.path.abspath(os.path.dirname(output))\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        da.rio.to_raster(output)\n        return output\n    else:\n        image = leafmap.array_to_image(da, **kwargs)\n        return image\n</code></pre>"},{"location":"sam/#geoai.sam.dict_to_rioxarray","title":"<code>dict_to_rioxarray(data_dict)</code>","text":"<p>Convert a dictionary to a xarray DataArray. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>The dictionary containing the data.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: The xarray DataArray.</p> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_rioxarray(data_dict: Dict) -&gt; xr.DataArray:\n    \"\"\"Convert a dictionary to a xarray DataArray. The dictionary should contain the\n    following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo\n    dataset sampler.\n\n    Args:\n        data_dict (Dict): The dictionary containing the data.\n\n    Returns:\n        xr.DataArray: The xarray DataArray.\n    \"\"\"\n\n    from collections import namedtuple\n\n    from affine import Affine\n\n    BoundingBox = namedtuple(\"BoundingBox\", [\"minx\", \"maxx\", \"miny\", \"maxy\"])\n\n    # Extract components from the dictionary\n    crs = data_dict[\"crs\"]\n    bounds = data_dict[\"bounds\"]\n    image_tensor = data_dict[\"image\"]\n\n    if hasattr(bounds, \"left\"):\n        bounds = BoundingBox(bounds.left, bounds.right, bounds.bottom, bounds.top)\n\n    # Convert tensor to numpy array if needed\n    if hasattr(image_tensor, \"numpy\"):\n        # For PyTorch tensors\n        image_array = image_tensor.numpy()\n    else:\n        # If it's already a numpy array or similar\n        image_array = np.array(image_tensor)\n\n    # Calculate pixel resolution\n    width = image_array.shape[2]  # Width is the size of the last dimension\n    height = image_array.shape[1]  # Height is the size of the middle dimension\n\n    res_x = (bounds.maxx - bounds.minx) / width\n    res_y = (bounds.maxy - bounds.miny) / height\n\n    # Create the transform matrix\n    transform = Affine(res_x, 0.0, bounds.minx, 0.0, -res_y, bounds.maxy)\n\n    # Create dimensions\n    x_coords = np.linspace(bounds.minx + res_x / 2, bounds.maxx - res_x / 2, width)\n    y_coords = np.linspace(bounds.maxy - res_y / 2, bounds.miny + res_y / 2, height)\n\n    # If time dimension exists in the bounds\n    if hasattr(bounds, \"mint\") and hasattr(bounds, \"maxt\"):\n        # Create a single time value or range if needed\n        t_coords = [\n            bounds.mint\n        ]  # Or np.linspace(bounds.mint, bounds.maxt, num_time_steps)\n\n        # Create DataArray with time dimension\n        dims = (\n            (\"band\", \"y\", \"x\")\n            if image_array.shape[0] &lt;= 10\n            else (\"time\", \"band\", \"y\", \"x\")\n        )\n\n        if dims[0] == \"band\":\n            # For multi-band single time\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"band\": np.arange(1, image_array.shape[0] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n        else:\n            # For multi-time multi-band\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"time\": t_coords,\n                    \"band\": np.arange(1, image_array.shape[1] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n    else:\n        # Create DataArray without time dimension\n        da = xr.DataArray(\n            image_array,\n            dims=(\"band\", \"y\", \"x\"),\n            coords={\n                \"band\": np.arange(1, image_array.shape[0] + 1),\n                \"y\": y_coords,\n                \"x\": x_coords,\n            },\n        )\n\n    # Set spatial attributes\n    da.rio.write_crs(crs, inplace=True)\n    da.rio.write_transform(transform, inplace=True)\n\n    return da\n</code></pre>"},{"location":"sam/#geoai.sam.download_file","title":"<code>download_file(url, output_path=None, overwrite=False, unzip=True)</code>","text":"<p>Download a file from a given URL with a progress bar. Optionally unzip the file if it's a ZIP archive.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>The path where the downloaded file will be saved. If not provided, the filename from the URL will be used.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Whether to unzip the file if it is a ZIP archive.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the downloaded file or the extracted directory.</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_file(\n    url: str,\n    output_path: Optional[str] = None,\n    overwrite: bool = False,\n    unzip: bool = True,\n) -&gt; str:\n    \"\"\"\n    Download a file from a given URL with a progress bar.\n    Optionally unzip the file if it's a ZIP archive.\n\n    Args:\n        url (str): The URL of the file to download.\n        output_path (str, optional): The path where the downloaded file will be saved.\n            If not provided, the filename from the URL will be used.\n        overwrite (bool, optional): Whether to overwrite the file if it already exists.\n        unzip (bool, optional): Whether to unzip the file if it is a ZIP archive.\n\n    Returns:\n        str: The path to the downloaded file or the extracted directory.\n    \"\"\"\n\n    import zipfile\n\n    from tqdm import tqdm\n\n    if output_path is None:\n        output_path = os.path.basename(url)\n\n    if os.path.exists(output_path) and not overwrite:\n        print(f\"File already exists: {output_path}\")\n    else:\n        # Download the file with a progress bar\n        response = requests.get(url, stream=True, timeout=50)\n        response.raise_for_status()\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        with (\n            open(output_path, \"wb\") as file,\n            tqdm(\n                desc=f\"Downloading {os.path.basename(output_path)}\",\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as progress_bar,\n        ):\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    file.write(chunk)\n                    progress_bar.update(len(chunk))\n\n    # If the file is a ZIP archive and unzip is True\n    if unzip and zipfile.is_zipfile(output_path):\n        extract_dir = os.path.splitext(output_path)[0]\n        if not os.path.exists(extract_dir) or overwrite:\n            with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n                zip_ref.extractall(extract_dir)\n            print(f\"Extracted to: {extract_dir}\")\n        return extract_dir\n\n    return output_path\n</code></pre>"},{"location":"sam/#geoai.sam.download_model_from_hf","title":"<code>download_model_from_hf(model_path, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model file.</p> required <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded model file</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_model_from_hf(model_path: str, repo_id: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    from huggingface_hub import hf_hub_download\n\n    try:\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            print(\n                \"Repo is not specified, using default Hugging Face repo_id: giswqs/geoai\"\n            )\n            repo_id = \"giswqs/geoai\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"sam/#geoai.sam.empty_cache","title":"<code>empty_cache()</code>","text":"<p>Empty the cache of the current device.</p> Source code in <code>geoai/utils.py</code> <pre><code>def empty_cache() -&gt; None:\n    \"\"\"Empty the cache of the current device.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n</code></pre>"},{"location":"sam/#geoai.sam.export_geotiff_tiles","title":"<code>export_geotiff_tiles(in_raster, out_folder, in_class_data, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False)</code>","text":"<p>Export georeferenced GeoTIFF tiles and labels from raster and classification data.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image</p> required <code>out_folder</code> <code>str</code> <p>Path to output folder</p> required <code>in_class_data</code> <code>str</code> <p>Path to classification data - can be vector file or raster</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles(\n    in_raster,\n    out_folder,\n    in_class_data,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n):\n    \"\"\"\n    Export georeferenced GeoTIFF tiles and labels from raster and classification data.\n\n    Args:\n        in_raster (str): Path to input raster image\n        out_folder (str): Path to output folder\n        in_class_data (str): Path to classification data - can be vector file or raster\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n    \"\"\"\n\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Create output directories\n    os.makedirs(out_folder, exist_ok=True)\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n    ann_dir = os.path.join(out_folder, \"annotations\")\n    os.makedirs(ann_dir, exist_ok=True)\n\n    # Determine if class data is raster or vector\n    is_class_data_raster = False\n    if isinstance(in_class_data, str):\n        file_ext = Path(in_class_data).suffix.lower()\n        # Common raster extensions\n        if file_ext in [\".tif\", \".tiff\", \".img\", \".jp2\", \".png\", \".bmp\", \".gif\"]:\n            try:\n                with rasterio.open(in_class_data) as src:\n                    is_class_data_raster = True\n                    if not quiet:\n                        print(f\"Detected in_class_data as raster: {in_class_data}\")\n                        print(f\"Raster CRS: {src.crs}\")\n                        print(f\"Raster dimensions: {src.width} x {src.height}\")\n            except Exception:\n                is_class_data_raster = False\n                if not quiet:\n                    print(f\"Unable to open {in_class_data} as raster, trying as vector\")\n\n    # Open the input raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Resolution: {src.res}\")\n            print(f\"  Bands: {src.count}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Calculate number of tiles\n        num_tiles_x = math.ceil((src.width - tile_size) / stride) + 1\n        num_tiles_y = math.ceil((src.height - tile_size) / stride) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        if max_tiles is None:\n            max_tiles = total_tiles\n\n        # Process classification data\n        class_to_id = {}\n\n        if is_class_data_raster:\n            # Load raster class data\n            with rasterio.open(in_class_data) as class_src:\n                # Check if raster CRS matches\n                if class_src.crs != src.crs:\n                    warnings.warn(\n                        f\"CRS mismatch: Class raster ({class_src.crs}) doesn't match input raster ({src.crs}). \"\n                        f\"Results may be misaligned.\"\n                    )\n\n                # Get unique values from raster\n                # Sample to avoid loading huge rasters\n                sample_data = class_src.read(\n                    1,\n                    out_shape=(\n                        1,\n                        min(class_src.height, 1000),\n                        min(class_src.width, 1000),\n                    ),\n                )\n\n                unique_classes = np.unique(sample_data)\n                unique_classes = unique_classes[\n                    unique_classes &gt; 0\n                ]  # Remove 0 as it's typically background\n\n                if not quiet:\n                    print(\n                        f\"Found {len(unique_classes)} unique classes in raster: {unique_classes}\"\n                    )\n\n                # Create class mapping\n                class_to_id = {int(cls): i + 1 for i, cls in enumerate(unique_classes)}\n        else:\n            # Load vector class data\n            try:\n                gdf = gpd.read_file(in_class_data)\n                if not quiet:\n                    print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n                    print(f\"Vector CRS: {gdf.crs}\")\n\n                # Always reproject to match raster CRS\n                if gdf.crs != src.crs:\n                    if not quiet:\n                        print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n                    gdf = gdf.to_crs(src.crs)\n\n                # Apply buffer if specified\n                if buffer_radius &gt; 0:\n                    gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n                    if not quiet:\n                        print(f\"Applied buffer of {buffer_radius} units\")\n\n                # Check if class_value_field exists\n                if class_value_field in gdf.columns:\n                    unique_classes = gdf[class_value_field].unique()\n                    if not quiet:\n                        print(\n                            f\"Found {len(unique_classes)} unique classes: {unique_classes}\"\n                        )\n                    # Create class mapping\n                    class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n                else:\n                    if not quiet:\n                        print(\n                            f\"WARNING: '{class_value_field}' not found in vector data. Using default class ID 1.\"\n                        )\n                    class_to_id = {1: 1}  # Default mapping\n            except Exception as e:\n                raise ValueError(f\"Error processing vector data: {e}\")\n\n        # Create progress bar\n        pbar = tqdm(\n            total=min(total_tiles, max_tiles),\n            desc=\"Generating tiles\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Track statistics for summary\n        stats = {\n            \"total_tiles\": 0,\n            \"tiles_with_features\": 0,\n            \"feature_pixels\": 0,\n            \"errors\": 0,\n            \"tile_coordinates\": [],  # For overview image\n        }\n\n        # Process tiles\n        tile_index = 0\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                if tile_index &gt;= max_tiles:\n                    break\n\n                # Calculate window coordinates\n                window_x = x * stride\n                window_y = y * stride\n\n                # Adjust for edge cases\n                if window_x + tile_size &gt; src.width:\n                    window_x = src.width - tile_size\n                if window_y + tile_size &gt; src.height:\n                    window_y = src.height - tile_size\n\n                # Define window\n                window = Window(window_x, window_y, tile_size, tile_size)\n\n                # Get window transform and bounds\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size * window_transform[0]  # Add width\n                miny = maxy + tile_size * window_transform[4]  # Add height\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Store tile coordinates for overview\n                if create_overview:\n                    stats[\"tile_coordinates\"].append(\n                        {\n                            \"index\": tile_index,\n                            \"x\": window_x,\n                            \"y\": window_y,\n                            \"bounds\": [minx, miny, maxx, maxy],\n                            \"has_features\": False,\n                        }\n                    )\n\n                # Create label mask\n                label_mask = np.zeros((tile_size, tile_size), dtype=np.uint8)\n                has_features = False\n\n                # Process classification data to create labels\n                if is_class_data_raster:\n                    # For raster class data\n                    with rasterio.open(in_class_data) as class_src:\n                        # Calculate window in class raster\n                        src_bounds = src.bounds\n                        class_bounds = class_src.bounds\n\n                        # Check if windows overlap\n                        if (\n                            src_bounds.left &gt; class_bounds.right\n                            or src_bounds.right &lt; class_bounds.left\n                            or src_bounds.bottom &gt; class_bounds.top\n                            or src_bounds.top &lt; class_bounds.bottom\n                        ):\n                            warnings.warn(\n                                \"Class raster and input raster do not overlap.\"\n                            )\n                        else:\n                            # Get corresponding window in class raster\n                            window_class = rasterio.windows.from_bounds(\n                                minx, miny, maxx, maxy, class_src.transform\n                            )\n\n                            # Read label data\n                            try:\n                                label_data = class_src.read(\n                                    1,\n                                    window=window_class,\n                                    boundless=True,\n                                    out_shape=(tile_size, tile_size),\n                                )\n\n                                # Remap class values if needed\n                                if class_to_id:\n                                    remapped_data = np.zeros_like(label_data)\n                                    for orig_val, new_val in class_to_id.items():\n                                        remapped_data[label_data == orig_val] = new_val\n                                    label_mask = remapped_data\n                                else:\n                                    label_mask = label_data\n\n                                # Check if we have any features\n                                if np.any(label_mask &gt; 0):\n                                    has_features = True\n                                    stats[\"feature_pixels\"] += np.count_nonzero(\n                                        label_mask\n                                    )\n                            except Exception as e:\n                                pbar.write(f\"Error reading class raster window: {e}\")\n                                stats[\"errors\"] += 1\n                else:\n                    # For vector class data\n                    # Find features that intersect with window\n                    window_features = gdf[gdf.intersects(window_bounds)]\n\n                    if len(window_features) &gt; 0:\n                        for idx, feature in window_features.iterrows():\n                            # Get class value\n                            if class_value_field in feature:\n                                class_val = feature[class_value_field]\n                                class_id = class_to_id.get(class_val, 1)\n                            else:\n                                class_id = 1\n\n                            # Get geometry in window coordinates\n                            geom = feature.geometry.intersection(window_bounds)\n                            if not geom.is_empty:\n                                try:\n                                    # Rasterize feature\n                                    feature_mask = features.rasterize(\n                                        [(geom, class_id)],\n                                        out_shape=(tile_size, tile_size),\n                                        transform=window_transform,\n                                        fill=0,\n                                        all_touched=all_touched,\n                                    )\n\n                                    # Add to label mask\n                                    label_mask = np.maximum(label_mask, feature_mask)\n\n                                    # Check if the feature was actually rasterized\n                                    if np.any(feature_mask):\n                                        has_features = True\n                                        if create_overview and tile_index &lt; len(\n                                            stats[\"tile_coordinates\"]\n                                        ):\n                                            stats[\"tile_coordinates\"][tile_index][\n                                                \"has_features\"\n                                            ] = True\n                                except Exception as e:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                    stats[\"errors\"] += 1\n\n                # Skip tile if no features and skip_empty_tiles is True\n                if skip_empty_tiles and not has_features:\n                    pbar.update(1)\n                    tile_index += 1\n                    continue\n\n                # Read image data\n                image_data = src.read(window=window)\n\n                # Export image as GeoTIFF\n                image_path = os.path.join(image_dir, f\"tile_{tile_index:06d}.tif\")\n\n                # Create profile for image GeoTIFF\n                image_profile = src.profile.copy()\n                image_profile.update(\n                    {\n                        \"height\": tile_size,\n                        \"width\": tile_size,\n                        \"count\": image_data.shape[0],\n                        \"transform\": window_transform,\n                    }\n                )\n\n                # Save image as GeoTIFF\n                try:\n                    with rasterio.open(image_path, \"w\", **image_profile) as dst:\n                        dst.write(image_data)\n                    stats[\"total_tiles\"] += 1\n                except Exception as e:\n                    pbar.write(f\"ERROR saving image GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size,\n                    \"width\": tile_size,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Export label as GeoTIFF\n                label_path = os.path.join(label_dir, f\"tile_{tile_index:06d}.tif\")\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask.astype(np.uint8), 1)\n\n                    if has_features:\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += np.count_nonzero(label_mask)\n                except Exception as e:\n                    pbar.write(f\"ERROR saving label GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create XML annotation for object detection if using vector class data\n                if (\n                    not is_class_data_raster\n                    and \"gdf\" in locals()\n                    and len(window_features) &gt; 0\n                ):\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = f\"tile_{tile_index:06d}.tif\"\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size)\n                    ET.SubElement(size, \"height\").text = str(tile_size)\n                    ET.SubElement(size, \"depth\").text = str(image_data.shape[0])\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    # Add objects\n                    for idx, feature in window_features.iterrows():\n                        # Get feature class\n                        if class_value_field in feature:\n                            class_val = feature[class_value_field]\n                        else:\n                            class_val = \"object\"\n\n                        # Get geometry bounds in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            # Get bounds in world coordinates\n                            minx_f, miny_f, maxx_f, maxy_f = geom.bounds\n\n                            # Convert to pixel coordinates\n                            col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                            col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                            # Ensure coordinates are within tile bounds\n                            xmin = max(0, min(tile_size, int(col_min)))\n                            ymin = max(0, min(tile_size, int(row_min)))\n                            xmax = max(0, min(tile_size, int(col_max)))\n                            ymax = max(0, min(tile_size, int(row_max)))\n\n                            # Only add if the box has non-zero area\n                            if xmax &gt; xmin and ymax &gt; ymin:\n                                obj = ET.SubElement(root, \"object\")\n                                ET.SubElement(obj, \"name\").text = str(class_val)\n                                ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                                bbox = ET.SubElement(obj, \"bndbox\")\n                                ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                                ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                                ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                                ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    tree = ET.ElementTree(root)\n                    xml_path = os.path.join(ann_dir, f\"tile_{tile_index:06d}.xml\")\n                    tree.write(xml_path)\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                tile_index += 1\n                if tile_index &gt;= max_tiles:\n                    break\n\n            if tile_index &gt;= max_tiles:\n                break\n\n        # Close progress bar\n        pbar.close()\n\n        # Create overview image if requested\n        if create_overview and stats[\"tile_coordinates\"]:\n            try:\n                create_overview_image(\n                    src,\n                    stats[\"tile_coordinates\"],\n                    os.path.join(out_folder, \"overview.png\"),\n                    tile_size,\n                    stride,\n                )\n            except Exception as e:\n                print(f\"Failed to create overview image: {e}\")\n\n        # Report results\n        if not quiet:\n            print(\"\\n------- Export Summary -------\")\n            print(f\"Total tiles exported: {stats['total_tiles']}\")\n            print(\n                f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n            )\n            if stats[\"tiles_with_features\"] &gt; 0:\n                print(\n                    f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n                )\n            if stats[\"errors\"] &gt; 0:\n                print(f\"Errors encountered: {stats['errors']}\")\n            print(f\"Output saved to: {out_folder}\")\n\n            # Verify georeference in a sample image and label\n            if stats[\"total_tiles\"] &gt; 0:\n                print(\"\\n------- Georeference Verification -------\")\n                sample_image = os.path.join(image_dir, f\"tile_0.tif\")\n                sample_label = os.path.join(label_dir, f\"tile_0.tif\")\n\n                if os.path.exists(sample_image):\n                    try:\n                        with rasterio.open(sample_image) as img:\n                            print(f\"Image CRS: {img.crs}\")\n                            print(f\"Image transform: {img.transform}\")\n                            print(\n                                f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                            )\n                            print(\n                                f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying image georeference: {e}\")\n\n                if os.path.exists(sample_label):\n                    try:\n                        with rasterio.open(sample_label) as lbl:\n                            print(f\"Label CRS: {lbl.crs}\")\n                            print(f\"Label transform: {lbl.transform}\")\n                            print(\n                                f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                            )\n                            print(\n                                f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying label georeference: {e}\")\n\n        # Return statistics dictionary for further processing if needed\n        return stats\n</code></pre>"},{"location":"sam/#geoai.sam.export_geotiff_tiles_batch","title":"<code>export_geotiff_tiles_batch(images_folder, masks_folder, output_folder, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False, image_extensions=None, mask_extensions=None)</code>","text":"<p>Export georeferenced GeoTIFF tiles from folders of images and masks.</p> <p>This function processes multiple image-mask pairs from input folders, generating tiles for each pair. All image tiles are saved to a single 'images' folder and all mask tiles to a single 'masks' folder.</p> <p>Images and masks are paired by their sorted order (alphabetically), not by filename matching. The number of images and masks must be equal.</p> <p>Parameters:</p> Name Type Description Default <code>images_folder</code> <code>str</code> <p>Path to folder containing raster images</p> required <code>masks_folder</code> <code>str</code> <p>Path to folder containing classification masks/vectors</p> required <code>output_folder</code> <code>str</code> <p>Path to output folder</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process per image (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> <code>image_extensions</code> <code>list</code> <p>List of image file extensions to process (default: common raster formats)</p> <code>None</code> <code>mask_extensions</code> <code>list</code> <p>List of mask file extensions to process (default: common raster/vector formats)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing batch processing statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images or masks found, or if counts don't match</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles_batch(\n    images_folder,\n    masks_folder,\n    output_folder,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n    image_extensions=None,\n    mask_extensions=None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Export georeferenced GeoTIFF tiles from folders of images and masks.\n\n    This function processes multiple image-mask pairs from input folders,\n    generating tiles for each pair. All image tiles are saved to a single\n    'images' folder and all mask tiles to a single 'masks' folder.\n\n    Images and masks are paired by their sorted order (alphabetically), not by\n    filename matching. The number of images and masks must be equal.\n\n    Args:\n        images_folder (str): Path to folder containing raster images\n        masks_folder (str): Path to folder containing classification masks/vectors\n        output_folder (str): Path to output folder\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process per image (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n        image_extensions (list): List of image file extensions to process (default: common raster formats)\n        mask_extensions (list): List of mask file extensions to process (default: common raster/vector formats)\n\n    Returns:\n        Dict[str, Any]: Dictionary containing batch processing statistics\n\n    Raises:\n        ValueError: If no images or masks found, or if counts don't match\n    \"\"\"\n\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Default extensions if not provided\n    if image_extensions is None:\n        image_extensions = [\".tif\", \".tiff\", \".jpg\", \".jpeg\", \".png\", \".jp2\", \".img\"]\n    if mask_extensions is None:\n        mask_extensions = [\n            \".tif\",\n            \".tiff\",\n            \".jpg\",\n            \".jpeg\",\n            \".png\",\n            \".jp2\",\n            \".img\",\n            \".shp\",\n            \".geojson\",\n            \".gpkg\",\n            \".geoparquet\",\n            \".json\",\n        ]\n\n    # Convert extensions to lowercase for comparison\n    image_extensions = [ext.lower() for ext in image_extensions]\n    mask_extensions = [ext.lower() for ext in mask_extensions]\n\n    # Create output folder structure\n    os.makedirs(output_folder, exist_ok=True)\n    output_images_dir = os.path.join(output_folder, \"images\")\n    output_masks_dir = os.path.join(output_folder, \"masks\")\n    os.makedirs(output_images_dir, exist_ok=True)\n    os.makedirs(output_masks_dir, exist_ok=True)\n\n    # Get list of image files\n    image_files = []\n    for ext in image_extensions:\n        pattern = os.path.join(images_folder, f\"*{ext}\")\n        image_files.extend(glob.glob(pattern))\n\n    # Get list of mask files\n    mask_files = []\n    for ext in mask_extensions:\n        pattern = os.path.join(masks_folder, f\"*{ext}\")\n        mask_files.extend(glob.glob(pattern))\n\n    # Sort files for consistent processing\n    image_files.sort()\n    mask_files.sort()\n\n    if not image_files:\n        raise ValueError(\n            f\"No image files found in {images_folder} with extensions {image_extensions}\"\n        )\n\n    if not mask_files:\n        raise ValueError(\n            f\"No mask files found in {masks_folder} with extensions {mask_extensions}\"\n        )\n\n    if len(image_files) != len(mask_files):\n        raise ValueError(\n            f\"Number of image files ({len(image_files)}) does not match number of mask files ({len(mask_files)})\"\n        )\n\n    # Initialize batch statistics\n    batch_stats = {\n        \"total_image_pairs\": 0,\n        \"processed_pairs\": 0,\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"errors\": 0,\n        \"processed_files\": [],\n        \"failed_files\": [],\n    }\n\n    if not quiet:\n        print(\n            f\"Found {len(image_files)} image files and {len(mask_files)} mask files to process\"\n        )\n        print(f\"Processing batch from {images_folder} and {masks_folder}\")\n        print(f\"Output folder: {output_folder}\")\n        print(\"-\" * 60)\n\n    # Global tile counter for unique naming\n    global_tile_counter = 0\n\n    # Process each image-mask pair by sorted order\n    for idx, (image_file, mask_file) in enumerate(\n        tqdm(\n            zip(image_files, mask_files),\n            desc=\"Processing image pairs\",\n            disable=quiet,\n            total=len(image_files),\n        )\n    ):\n        batch_stats[\"total_image_pairs\"] += 1\n\n        # Get base filename without extension for naming (use image filename)\n        base_name = os.path.splitext(os.path.basename(image_file))[0]\n\n        try:\n            if not quiet:\n                print(f\"\\nProcessing: {base_name}\")\n                print(f\"  Image: {os.path.basename(image_file)}\")\n                print(f\"  Mask: {os.path.basename(mask_file)}\")\n\n            # Process the image-mask pair manually to get direct control over tile saving\n            tiles_generated = _process_image_mask_pair(\n                image_file=image_file,\n                mask_file=mask_file,\n                base_name=base_name,\n                output_images_dir=output_images_dir,\n                output_masks_dir=output_masks_dir,\n                global_tile_counter=global_tile_counter,\n                tile_size=tile_size,\n                stride=stride,\n                class_value_field=class_value_field,\n                buffer_radius=buffer_radius,\n                max_tiles=max_tiles,\n                all_touched=all_touched,\n                skip_empty_tiles=skip_empty_tiles,\n                quiet=quiet,\n            )\n\n            # Update counters\n            global_tile_counter += tiles_generated[\"total_tiles\"]\n\n            # Update batch statistics\n            batch_stats[\"processed_pairs\"] += 1\n            batch_stats[\"total_tiles\"] += tiles_generated[\"total_tiles\"]\n            batch_stats[\"tiles_with_features\"] += tiles_generated[\"tiles_with_features\"]\n            batch_stats[\"errors\"] += tiles_generated[\"errors\"]\n\n            batch_stats[\"processed_files\"].append(\n                {\n                    \"image\": image_file,\n                    \"mask\": mask_file,\n                    \"base_name\": base_name,\n                    \"tiles_generated\": tiles_generated[\"total_tiles\"],\n                    \"tiles_with_features\": tiles_generated[\"tiles_with_features\"],\n                }\n            )\n\n        except Exception as e:\n            if not quiet:\n                print(f\"ERROR processing {base_name}: {e}\")\n            batch_stats[\"failed_files\"].append(\n                {\"image\": image_file, \"mask\": mask_file, \"error\": str(e)}\n            )\n            batch_stats[\"errors\"] += 1\n\n    # Print batch summary\n    if not quiet:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"BATCH PROCESSING SUMMARY\")\n        print(\"=\" * 60)\n        print(f\"Total image pairs found: {batch_stats['total_image_pairs']}\")\n        print(f\"Successfully processed: {batch_stats['processed_pairs']}\")\n        print(f\"Failed to process: {len(batch_stats['failed_files'])}\")\n        print(f\"Total tiles generated: {batch_stats['total_tiles']}\")\n        print(f\"Tiles with features: {batch_stats['tiles_with_features']}\")\n\n        if batch_stats[\"total_tiles\"] &gt; 0:\n            feature_percentage = (\n                batch_stats[\"tiles_with_features\"] / batch_stats[\"total_tiles\"]\n            ) * 100\n            print(f\"Feature percentage: {feature_percentage:.1f}%\")\n\n        if batch_stats[\"errors\"] &gt; 0:\n            print(f\"Total errors: {batch_stats['errors']}\")\n\n        print(f\"Output saved to: {output_folder}\")\n        print(f\"  Images: {output_images_dir}\")\n        print(f\"  Masks: {output_masks_dir}\")\n\n        # List failed files if any\n        if batch_stats[\"failed_files\"]:\n            print(f\"\\nFailed files:\")\n            for failed in batch_stats[\"failed_files\"]:\n                print(f\"  - {os.path.basename(failed['image'])}: {failed['error']}\")\n\n    return batch_stats\n</code></pre>"},{"location":"sam/#geoai.sam.export_tiles_to_geojson","title":"<code>export_tiles_to_geojson(tile_coordinates, src, output_path, tile_size=None, stride=None)</code>","text":"<p>Export tile rectangles directly to GeoJSON without creating an overview image.</p> <p>Parameters:</p> Name Type Description Default <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>src</code> <code>DatasetReader</code> <p>The source raster dataset.</p> required <code>output_path</code> <code>str</code> <p>The path where the GeoJSON will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels. Only needed if not in tile_coordinates.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Used to calculate overlaps between tiles.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved GeoJSON file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_tiles_to_geojson(\n    tile_coordinates, src, output_path, tile_size=None, stride=None\n) -&gt; str:\n    \"\"\"\n    Export tile rectangles directly to GeoJSON without creating an overview image.\n\n    Args:\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        output_path (str): The path where the GeoJSON will be saved.\n        tile_size (int, optional): The size of each tile in pixels. Only needed if not in tile_coordinates.\n        stride (int, optional): The stride between tiles in pixels. Used to calculate overlaps between tiles.\n\n    Returns:\n        str: Path to the saved GeoJSON file.\n    \"\"\"\n    features = []\n\n    for tile in tile_coordinates:\n        # Get the size from the tile or use the provided parameter\n        tile_width = tile.get(\"width\", tile.get(\"size\", tile_size))\n        tile_height = tile.get(\"height\", tile.get(\"size\", tile_size))\n\n        if tile_width is None or tile_height is None:\n            raise ValueError(\n                \"Tile size not found in tile data and no tile_size parameter provided\"\n            )\n\n        # Get bounds from the tile\n        if \"bounds\" in tile:\n            # If bounds are already in geo coordinates\n            minx, miny, maxx, maxy = tile[\"bounds\"]\n        else:\n            # Try to calculate bounds from transform if available\n            if hasattr(src, \"transform\"):\n                # Convert pixel coordinates to geo coordinates\n                window_transform = src.transform\n                x, y = tile[\"x\"], tile[\"y\"]\n                minx = window_transform[2] + x * window_transform[0]\n                maxy = window_transform[5] + y * window_transform[4]\n                maxx = minx + tile_width * window_transform[0]\n                miny = maxy + tile_height * window_transform[4]\n            else:\n                raise ValueError(\n                    \"Cannot determine bounds. Neither 'bounds' in tile nor transform in src.\"\n                )\n\n        # Calculate overlap with neighboring tiles if stride is provided\n        overlap = 0\n        if stride is not None and stride &lt; tile_width:\n            overlap = tile_width - stride\n\n        # Create a polygon from the bounds\n        polygon = box(minx, miny, maxx, maxy)\n\n        # Create a GeoJSON feature\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": mapping(polygon),\n            \"properties\": {\n                \"index\": tile[\"index\"],\n                \"has_features\": tile.get(\"has_features\", False),\n                \"tile_width_px\": tile_width,\n                \"tile_height_px\": tile_height,\n            },\n        }\n\n        # Add overlap information if stride is provided\n        if stride is not None:\n            feature[\"properties\"][\"stride_px\"] = stride\n            feature[\"properties\"][\"overlap_px\"] = overlap\n\n        # Add additional properties from the tile\n        for key, value in tile.items():\n            if key not in [\"bounds\", \"geometry\"]:\n                feature[\"properties\"][key] = value\n\n        features.append(feature)\n\n    # Create the GeoJSON collection\n    geojson_collection = {\n        \"type\": \"FeatureCollection\",\n        \"features\": features,\n        \"properties\": {\n            \"crs\": (\n                src.crs.to_string() if hasattr(src.crs, \"to_string\") else str(src.crs)\n            ),\n            \"total_tiles\": len(features),\n            \"source_raster_dimensions\": (\n                [src.width, src.height] if hasattr(src, \"width\") else None\n            ),\n        },\n    }\n\n    # Create directory if it doesn't exist\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)) or \".\", exist_ok=True)\n\n    # Save to file\n    with open(output_path, \"w\") as f:\n        json.dump(geojson_collection, f)\n\n    print(f\"GeoJSON saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"sam/#geoai.sam.export_training_data","title":"<code>export_training_data(in_raster, out_folder, in_class_data, image_chip_format='GEOTIFF', tile_size_x=256, tile_size_y=256, stride_x=None, stride_y=None, output_nofeature_tiles=True, metadata_format='PASCAL_VOC', start_index=0, class_value_field='class', buffer_radius=0, in_mask_polygons=None, rotation_angle=0, reference_system=None, blacken_around_feature=False, crop_mode='FIXED_SIZE', in_raster2=None, in_instance_data=None, instance_class_value_field=None, min_polygon_overlap_ratio=0.0, all_touched=True, save_geotiff=True, quiet=False)</code>","text":"<p>Export training data for deep learning using TorchGeo with progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image.</p> required <code>out_folder</code> <code>str</code> <p>Output folder path where chips and labels will be saved.</p> required <code>in_class_data</code> <code>str</code> <p>Path to vector file containing class polygons.</p> required <code>image_chip_format</code> <code>str</code> <p>Output image format (PNG, JPEG, TIFF, GEOTIFF).</p> <code>'GEOTIFF'</code> <code>tile_size_x</code> <code>int</code> <p>Width of image chips in pixels.</p> <code>256</code> <code>tile_size_y</code> <code>int</code> <p>Height of image chips in pixels.</p> <code>256</code> <code>stride_x</code> <code>int</code> <p>Horizontal stride between chips. If None, uses tile_size_x.</p> <code>None</code> <code>stride_y</code> <code>int</code> <p>Vertical stride between chips. If None, uses tile_size_y.</p> <code>None</code> <code>output_nofeature_tiles</code> <code>bool</code> <p>Whether to export chips without features.</p> <code>True</code> <code>metadata_format</code> <code>str</code> <p>Output metadata format (PASCAL_VOC, KITTI, COCO).</p> <code>'PASCAL_VOC'</code> <code>start_index</code> <code>int</code> <p>Starting index for chip filenames.</p> <code>0</code> <code>class_value_field</code> <code>str</code> <p>Field name in in_class_data containing class values.</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer radius around features (in CRS units).</p> <code>0</code> <code>in_mask_polygons</code> <code>str</code> <p>Path to vector file containing mask polygons.</p> <code>None</code> <code>rotation_angle</code> <code>float</code> <p>Rotation angle in degrees.</p> <code>0</code> <code>reference_system</code> <code>str</code> <p>Reference system code.</p> <code>None</code> <code>blacken_around_feature</code> <code>bool</code> <p>Whether to mask areas outside of features.</p> <code>False</code> <code>crop_mode</code> <code>str</code> <p>Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).</p> <code>'FIXED_SIZE'</code> <code>in_raster2</code> <code>str</code> <p>Path to secondary raster image.</p> <code>None</code> <code>in_instance_data</code> <code>str</code> <p>Path to vector file containing instance polygons.</p> <code>None</code> <code>instance_class_value_field</code> <code>str</code> <p>Field name in in_instance_data for instance classes.</p> <code>None</code> <code>min_polygon_overlap_ratio</code> <code>float</code> <p>Minimum overlap ratio for polygons.</p> <code>0.0</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization.</p> <code>True</code> <code>save_geotiff</code> <code>bool</code> <p>Whether to save as GeoTIFF with georeferencing.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>If True, suppress most output messages.</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_training_data(\n    in_raster,\n    out_folder,\n    in_class_data,\n    image_chip_format=\"GEOTIFF\",\n    tile_size_x=256,\n    tile_size_y=256,\n    stride_x=None,\n    stride_y=None,\n    output_nofeature_tiles=True,\n    metadata_format=\"PASCAL_VOC\",\n    start_index=0,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    in_mask_polygons=None,\n    rotation_angle=0,\n    reference_system=None,\n    blacken_around_feature=False,\n    crop_mode=\"FIXED_SIZE\",  # Implemented but not fully used yet\n    in_raster2=None,\n    in_instance_data=None,\n    instance_class_value_field=None,  # Implemented but not fully used yet\n    min_polygon_overlap_ratio=0.0,\n    all_touched=True,\n    save_geotiff=True,\n    quiet=False,\n):\n    \"\"\"\n    Export training data for deep learning using TorchGeo with progress bar.\n\n    Args:\n        in_raster (str): Path to input raster image.\n        out_folder (str): Output folder path where chips and labels will be saved.\n        in_class_data (str): Path to vector file containing class polygons.\n        image_chip_format (str): Output image format (PNG, JPEG, TIFF, GEOTIFF).\n        tile_size_x (int): Width of image chips in pixels.\n        tile_size_y (int): Height of image chips in pixels.\n        stride_x (int): Horizontal stride between chips. If None, uses tile_size_x.\n        stride_y (int): Vertical stride between chips. If None, uses tile_size_y.\n        output_nofeature_tiles (bool): Whether to export chips without features.\n        metadata_format (str): Output metadata format (PASCAL_VOC, KITTI, COCO).\n        start_index (int): Starting index for chip filenames.\n        class_value_field (str): Field name in in_class_data containing class values.\n        buffer_radius (float): Buffer radius around features (in CRS units).\n        in_mask_polygons (str): Path to vector file containing mask polygons.\n        rotation_angle (float): Rotation angle in degrees.\n        reference_system (str): Reference system code.\n        blacken_around_feature (bool): Whether to mask areas outside of features.\n        crop_mode (str): Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).\n        in_raster2 (str): Path to secondary raster image.\n        in_instance_data (str): Path to vector file containing instance polygons.\n        instance_class_value_field (str): Field name in in_instance_data for instance classes.\n        min_polygon_overlap_ratio (float): Minimum overlap ratio for polygons.\n        all_touched (bool): Whether to use all_touched=True in rasterization.\n        save_geotiff (bool): Whether to save as GeoTIFF with georeferencing.\n        quiet (bool): If True, suppress most output messages.\n    \"\"\"\n    # Create output directories\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n\n    # Define annotation directories based on metadata format\n    if metadata_format == \"PASCAL_VOC\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n    elif metadata_format == \"COCO\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n        # Initialize COCO annotations dictionary\n        coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # Initialize statistics dictionary\n    stats = {\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"feature_pixels\": 0,\n        \"errors\": 0,\n    }\n\n    # Open raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Set defaults for stride if not provided\n        if stride_x is None:\n            stride_x = tile_size_x\n        if stride_y is None:\n            stride_y = tile_size_y\n\n        # Calculate number of tiles in x and y directions\n        num_tiles_x = math.ceil((src.width - tile_size_x) / stride_x) + 1\n        num_tiles_y = math.ceil((src.height - tile_size_y) / stride_y) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        # Read class data\n        gdf = gpd.read_file(in_class_data)\n        if not quiet:\n            print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n            print(f\"Available columns: {gdf.columns.tolist()}\")\n            print(f\"GeoJSON CRS: {gdf.crs}\")\n\n        # Check if class_value_field exists\n        if class_value_field not in gdf.columns:\n            if not quiet:\n                print(\n                    f\"WARNING: '{class_value_field}' field not found in the input data. Using default class value 1.\"\n                )\n            # Add a default class column\n            gdf[class_value_field] = 1\n            unique_classes = [1]\n        else:\n            # Print unique classes for debugging\n            unique_classes = gdf[class_value_field].unique()\n            if not quiet:\n                print(f\"Found {len(unique_classes)} unique classes: {unique_classes}\")\n\n        # CRITICAL: Always reproject to match raster CRS to ensure proper alignment\n        if gdf.crs != src.crs:\n            if not quiet:\n                print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n            gdf = gdf.to_crs(src.crs)\n        elif reference_system and gdf.crs != reference_system:\n            if not quiet:\n                print(\n                    f\"Reprojecting features to specified reference system {reference_system}\"\n                )\n            gdf = gdf.to_crs(reference_system)\n\n        # Check overlap between raster and vector data\n        raster_bounds = box(*src.bounds)\n        vector_bounds = box(*gdf.total_bounds)\n        if not raster_bounds.intersects(vector_bounds):\n            if not quiet:\n                print(\n                    \"WARNING: The vector data doesn't intersect with the raster extent!\"\n                )\n                print(f\"Raster bounds: {src.bounds}\")\n                print(f\"Vector bounds: {gdf.total_bounds}\")\n        else:\n            overlap = (\n                raster_bounds.intersection(vector_bounds).area / vector_bounds.area\n            )\n            if not quiet:\n                print(f\"Overlap between raster and vector: {overlap:.2%}\")\n\n        # Apply buffer if specified\n        if buffer_radius &gt; 0:\n            gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n\n        # Initialize class mapping (ensure all classes are mapped to non-zero values)\n        class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n\n        # Store category info for COCO format\n        if metadata_format == \"COCO\":\n            for cls_val in unique_classes:\n                coco_annotations[\"categories\"].append(\n                    {\n                        \"id\": class_to_id[cls_val],\n                        \"name\": str(cls_val),\n                        \"supercategory\": \"object\",\n                    }\n                )\n\n        # Load mask polygons if provided\n        mask_gdf = None\n        if in_mask_polygons:\n            mask_gdf = gpd.read_file(in_mask_polygons)\n            if reference_system:\n                mask_gdf = mask_gdf.to_crs(reference_system)\n            elif mask_gdf.crs != src.crs:\n                mask_gdf = mask_gdf.to_crs(src.crs)\n\n        # Process instance data if provided\n        instance_gdf = None\n        if in_instance_data:\n            instance_gdf = gpd.read_file(in_instance_data)\n            if reference_system:\n                instance_gdf = instance_gdf.to_crs(reference_system)\n            elif instance_gdf.crs != src.crs:\n                instance_gdf = instance_gdf.to_crs(src.crs)\n\n        # Load secondary raster if provided\n        src2 = None\n        if in_raster2:\n            src2 = rasterio.open(in_raster2)\n\n        # Set up augmentation if rotation is specified\n        augmentation = None\n        if rotation_angle != 0:\n            # Fixed: Added data_keys parameter to AugmentationSequential\n            augmentation = torchgeo.transforms.AugmentationSequential(\n                torch.nn.ModuleList([RandomRotation(rotation_angle)]),\n                data_keys=[\"image\"],  # Add data_keys parameter\n            )\n\n        # Initialize annotation ID for COCO format\n        ann_id = 0\n\n        # Create progress bar\n        pbar = tqdm(\n            total=total_tiles,\n            desc=f\"Generating tiles (with features: 0)\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Generate tiles\n        chip_index = start_index\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                # Calculate window coordinates\n                window_x = x * stride_x\n                window_y = y * stride_y\n\n                # Adjust for edge cases\n                if window_x + tile_size_x &gt; src.width:\n                    window_x = src.width - tile_size_x\n                if window_y + tile_size_y &gt; src.height:\n                    window_y = src.height - tile_size_y\n\n                # Adjust window based on crop_mode\n                if crop_mode == \"CENTERED_ON_FEATURE\" and len(gdf) &gt; 0:\n                    # Find the nearest feature to the center of this window\n                    window_center_x = window_x + tile_size_x // 2\n                    window_center_y = window_y + tile_size_y // 2\n\n                    # Convert center to world coordinates\n                    center_x, center_y = src.xy(window_center_y, window_center_x)\n                    center_point = gpd.points_from_xy([center_x], [center_y])[0]\n\n                    # Find nearest feature\n                    distances = gdf.geometry.distance(center_point)\n                    nearest_idx = distances.idxmin()\n                    nearest_feature = gdf.iloc[nearest_idx]\n\n                    # Get centroid of nearest feature\n                    feature_centroid = nearest_feature.geometry.centroid\n\n                    # Convert feature centroid to pixel coordinates\n                    feature_row, feature_col = src.index(\n                        feature_centroid.x, feature_centroid.y\n                    )\n\n                    # Adjust window to center on feature\n                    window_x = max(\n                        0, min(src.width - tile_size_x, feature_col - tile_size_x // 2)\n                    )\n                    window_y = max(\n                        0, min(src.height - tile_size_y, feature_row - tile_size_y // 2)\n                    )\n\n                # Define window\n                window = Window(window_x, window_y, tile_size_x, tile_size_y)\n\n                # Get window transform and bounds in source CRS\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds more explicitly and accurately\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size_x * window_transform[0]  # Add width\n                miny = (\n                    maxy + tile_size_y * window_transform[4]\n                )  # Add height (note: transform[4] is typically negative)\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Apply rotation if specified\n                if rotation_angle != 0:\n                    window_bounds = rotate(\n                        window_bounds, rotation_angle, origin=\"center\"\n                    )\n\n                # Find features that intersect with window\n                window_features = gdf[gdf.intersects(window_bounds)]\n\n                # Process instance data if provided\n                window_instances = None\n                if instance_gdf is not None and instance_class_value_field is not None:\n                    window_instances = instance_gdf[\n                        instance_gdf.intersects(window_bounds)\n                    ]\n                    if len(window_instances) &gt; 0:\n                        if not quiet:\n                            pbar.write(\n                                f\"Found {len(window_instances)} instances in tile {chip_index}\"\n                            )\n\n                # Skip if no features and output_nofeature_tiles is False\n                if not output_nofeature_tiles and len(window_features) == 0:\n                    pbar.update(1)  # Still update progress bar\n                    continue\n\n                # Check polygon overlap ratio if specified\n                if min_polygon_overlap_ratio &gt; 0 and len(window_features) &gt; 0:\n                    valid_features = []\n                    for _, feature in window_features.iterrows():\n                        overlap_ratio = (\n                            feature.geometry.intersection(window_bounds).area\n                            / feature.geometry.area\n                        )\n                        if overlap_ratio &gt;= min_polygon_overlap_ratio:\n                            valid_features.append(feature)\n\n                    if len(valid_features) &gt; 0:\n                        window_features = gpd.GeoDataFrame(valid_features)\n                    elif not output_nofeature_tiles:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Apply mask if provided\n                if mask_gdf is not None:\n                    mask_features = mask_gdf[mask_gdf.intersects(window_bounds)]\n                    if len(mask_features) == 0:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Read image data - keep original for GeoTIFF export\n                orig_image_data = src.read(window=window)\n\n                # Create a copy for processing\n                image_data = orig_image_data.copy().astype(np.float32)\n\n                # Normalize image data for processing\n                for band in range(image_data.shape[0]):\n                    band_min, band_max = np.percentile(image_data[band], (1, 99))\n                    if band_max &gt; band_min:\n                        image_data[band] = np.clip(\n                            (image_data[band] - band_min) / (band_max - band_min), 0, 1\n                        )\n\n                # Read secondary image data if provided\n                if src2:\n                    image_data2 = src2.read(window=window)\n                    # Stack the two images\n                    image_data = np.vstack((image_data, image_data2))\n\n                # Apply blacken_around_feature if needed\n                if blacken_around_feature and len(window_features) &gt; 0:\n                    mask = np.zeros((tile_size_y, tile_size_x), dtype=bool)\n                    for _, feature in window_features.iterrows():\n                        # Project feature to pixel coordinates\n                        feature_pixels = features.rasterize(\n                            [(feature.geometry, 1)],\n                            out_shape=(tile_size_y, tile_size_x),\n                            transform=window_transform,\n                        )\n                        mask = np.logical_or(mask, feature_pixels.astype(bool))\n\n                    # Apply mask to image\n                    for band in range(image_data.shape[0]):\n                        temp = image_data[band, :, :]\n                        temp[~mask] = 0\n                        image_data[band, :, :] = temp\n\n                # Apply rotation if specified\n                if augmentation:\n                    # Convert to torch tensor for augmentation\n                    image_tensor = torch.from_numpy(image_data).unsqueeze(\n                        0\n                    )  # Add batch dimension\n                    # Apply augmentation with proper data format\n                    augmented = augmentation({\"image\": image_tensor})\n                    image_data = (\n                        augmented[\"image\"].squeeze(0).numpy()\n                    )  # Remove batch dimension\n\n                # Create a processed version for regular image formats\n                processed_image = (image_data * 255).astype(np.uint8)\n\n                # Create label mask\n                label_mask = np.zeros((tile_size_y, tile_size_x), dtype=np.uint8)\n                has_features = False\n\n                if len(window_features) &gt; 0:\n                    for idx, feature in window_features.iterrows():\n                        # Get class value\n                        class_val = (\n                            feature[class_value_field]\n                            if class_value_field in feature\n                            else 1\n                        )\n                        if isinstance(class_val, str):\n                            # If class is a string, use its position in the unique classes list\n                            class_id = class_to_id.get(class_val, 1)\n                        else:\n                            # If class is already a number, use it directly\n                            class_id = int(class_val) if class_val &gt; 0 else 1\n\n                        # Get the geometry in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            try:\n                                # Rasterize the feature\n                                feature_mask = features.rasterize(\n                                    [(geom, class_id)],\n                                    out_shape=(tile_size_y, tile_size_x),\n                                    transform=window_transform,\n                                    fill=0,\n                                    all_touched=all_touched,\n                                )\n\n                                # Update mask with higher class values taking precedence\n                                label_mask = np.maximum(label_mask, feature_mask)\n\n                                # Check if any pixels were added\n                                if np.any(feature_mask):\n                                    has_features = True\n                            except Exception as e:\n                                if not quiet:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                stats[\"errors\"] += 1\n\n                # Save as GeoTIFF if requested\n                if save_geotiff or image_chip_format.upper() in [\n                    \"TIFF\",\n                    \"TIF\",\n                    \"GEOTIFF\",\n                ]:\n                    # Standardize extension to .tif for GeoTIFF files\n                    image_filename = f\"tile_{chip_index:06d}.tif\"\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create profile for the GeoTIFF\n                    profile = src.profile.copy()\n                    profile.update(\n                        {\n                            \"height\": tile_size_y,\n                            \"width\": tile_size_x,\n                            \"count\": orig_image_data.shape[0],\n                            \"transform\": window_transform,\n                        }\n                    )\n\n                    # Save the GeoTIFF with original data\n                    try:\n                        with rasterio.open(image_path, \"w\", **profile) as dst:\n                            dst.write(orig_image_data)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving image GeoTIFF for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n                else:\n                    # For non-GeoTIFF formats, use PIL to save the image\n                    image_filename = (\n                        f\"tile_{chip_index:06d}.{image_chip_format.lower()}\"\n                    )\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create PIL image for saving\n                    if processed_image.shape[0] == 1:\n                        img = Image.fromarray(processed_image[0])\n                    elif processed_image.shape[0] == 3:\n                        # For RGB, need to transpose and make sure it's the right data type\n                        rgb_data = np.transpose(processed_image, (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n                    else:\n                        # For multiband images, save only RGB or first three bands\n                        rgb_data = np.transpose(processed_image[:3], (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n\n                    # Save image\n                    try:\n                        img.save(image_path)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(f\"ERROR saving image for tile {chip_index}: {e}\")\n                        stats[\"errors\"] += 1\n\n                # Save label as GeoTIFF\n                label_filename = f\"tile_{chip_index:06d}.tif\"\n                label_path = os.path.join(label_dir, label_filename)\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size_y,\n                    \"width\": tile_size_x,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Save label GeoTIFF\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask, 1)\n\n                    if has_features:\n                        pixel_count = np.count_nonzero(label_mask)\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += pixel_count\n                except Exception as e:\n                    if not quiet:\n                        pbar.write(f\"ERROR saving label for tile {chip_index}: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Also save a PNG version for easy visualization if requested\n                if metadata_format == \"PASCAL_VOC\":\n                    try:\n                        # Ensure correct data type for PIL\n                        png_label = label_mask.astype(np.uint8)\n                        label_img = Image.fromarray(png_label)\n                        label_png_path = os.path.join(\n                            label_dir, f\"tile_{chip_index:06d}.png\"\n                        )\n                        label_img.save(label_png_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving PNG label for tile {chip_index}: {e}\"\n                            )\n                            pbar.write(\n                                f\"  Label mask shape: {label_mask.shape}, dtype: {label_mask.dtype}\"\n                            )\n                            # Try again with explicit conversion\n                            try:\n                                # Alternative approach for problematic arrays\n                                png_data = np.zeros(\n                                    (tile_size_y, tile_size_x), dtype=np.uint8\n                                )\n                                np.copyto(png_data, label_mask, casting=\"unsafe\")\n                                label_img = Image.fromarray(png_data)\n                                label_img.save(label_png_path)\n                                pbar.write(\n                                    f\"  Succeeded using alternative conversion method\"\n                                )\n                            except Exception as e2:\n                                pbar.write(f\"  Second attempt also failed: {e2}\")\n                                stats[\"errors\"] += 1\n\n                # Generate annotations\n                if metadata_format == \"PASCAL_VOC\" and len(window_features) &gt; 0:\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = image_filename\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size_x)\n                    ET.SubElement(size, \"height\").text = str(tile_size_y)\n                    ET.SubElement(size, \"depth\").text = str(min(image_data.shape[0], 3))\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    for _, feature in window_features.iterrows():\n                        # Convert feature geometry to pixel coordinates\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        obj = ET.SubElement(root, \"object\")\n                        ET.SubElement(obj, \"name\").text = str(\n                            feature[class_value_field]\n                        )\n                        ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                        bbox = ET.SubElement(obj, \"bndbox\")\n                        ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                        ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                        ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                        ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    try:\n                        tree = ET.ElementTree(root)\n                        xml_path = os.path.join(ann_dir, f\"tile_{chip_index:06d}.xml\")\n                        tree.write(xml_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving XML annotation for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n\n                elif metadata_format == \"COCO\" and len(window_features) &gt; 0:\n                    # Add image info\n                    image_id = chip_index\n                    coco_annotations[\"images\"].append(\n                        {\n                            \"id\": image_id,\n                            \"file_name\": image_filename,\n                            \"width\": tile_size_x,\n                            \"height\": tile_size_y,\n                            \"crs\": str(src.crs),\n                            \"transform\": str(window_transform),\n                        }\n                    )\n\n                    # Add annotations for each feature\n                    for _, feature in window_features.iterrows():\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        width = xmax - xmin\n                        height = ymax - ymin\n\n                        # Add annotation\n                        ann_id += 1\n                        category_id = class_to_id[feature[class_value_field]]\n\n                        coco_annotations[\"annotations\"].append(\n                            {\n                                \"id\": ann_id,\n                                \"image_id\": image_id,\n                                \"category_id\": category_id,\n                                \"bbox\": [xmin, ymin, width, height],\n                                \"area\": width * height,\n                                \"iscrowd\": 0,\n                            }\n                        )\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                chip_index += 1\n\n        # Close progress bar\n        pbar.close()\n\n        # Save COCO annotations if applicable\n        if metadata_format == \"COCO\":\n            try:\n                with open(os.path.join(ann_dir, \"instances.json\"), \"w\") as f:\n                    json.dump(coco_annotations, f)\n            except Exception as e:\n                if not quiet:\n                    print(f\"ERROR saving COCO annotations: {e}\")\n                stats[\"errors\"] += 1\n\n        # Close secondary raster if opened\n        if src2:\n            src2.close()\n\n    # Print summary\n    if not quiet:\n        print(\"\\n------- Export Summary -------\")\n        print(f\"Total tiles exported: {stats['total_tiles']}\")\n        print(\n            f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n        )\n        if stats[\"tiles_with_features\"] &gt; 0:\n            print(\n                f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n            )\n        if stats[\"errors\"] &gt; 0:\n            print(f\"Errors encountered: {stats['errors']}\")\n        print(f\"Output saved to: {out_folder}\")\n\n        # Verify georeference in a sample image and label\n        if stats[\"total_tiles\"] &gt; 0:\n            print(\"\\n------- Georeference Verification -------\")\n            sample_image = os.path.join(image_dir, f\"tile_{start_index}.tif\")\n            sample_label = os.path.join(label_dir, f\"tile_{start_index}.tif\")\n\n            if os.path.exists(sample_image):\n                try:\n                    with rasterio.open(sample_image) as img:\n                        print(f\"Image CRS: {img.crs}\")\n                        print(f\"Image transform: {img.transform}\")\n                        print(\n                            f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                        )\n                        print(\n                            f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying image georeference: {e}\")\n\n            if os.path.exists(sample_label):\n                try:\n                    with rasterio.open(sample_label) as lbl:\n                        print(f\"Label CRS: {lbl.crs}\")\n                        print(f\"Label transform: {lbl.transform}\")\n                        print(\n                            f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                        )\n                        print(\n                            f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying label georeference: {e}\")\n\n    # Return statistics\n    return stats, out_folder\n</code></pre>"},{"location":"sam/#geoai.sam.geojson_to_coords","title":"<code>geojson_to_coords(geojson, src_crs='epsg:4326', dst_crs='epsg:4326')</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>str | dict</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>geoai/utils.py</code> <pre><code>def geojson_to_coords(\n    geojson: str, src_crs: str = \"epsg:4326\", dst_crs: str = \"epsg:4326\"\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.\n\n    Args:\n        geojson (str | dict): The geojson file path or a dictionary of feature collection.\n        src_crs (str, optional): The source CRS. Defaults to \"epsg:4326\".\n        dst_crs (str, optional): The destination CRS. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n\n    import json\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    if isinstance(geojson, dict):\n        geojson = json.dumps(geojson)\n    gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n    centroids = gdf.geometry.centroid\n    centroid_list = [[point.x, point.y] for point in centroids]\n    if src_crs != dst_crs:\n        centroid_list = transform_coords(\n            [x[0] for x in centroid_list],\n            [x[1] for x in centroid_list],\n            src_crs,\n            dst_crs,\n        )\n        centroid_list = [[x, y] for x, y in zip(centroid_list[0], centroid_list[1])]\n    return centroid_list\n</code></pre>"},{"location":"sam/#geoai.sam.geojson_to_xy","title":"<code>geojson_to_xy(src_fp, geojson, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>geojson</code> <code>str</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>geoai/utils.py</code> <pre><code>def geojson_to_xy(\n    src_fp: str, geojson: str, coord_crs: str = \"epsg:4326\", **kwargs: Any\n) -&gt; List[List[float]]:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        geojson: The geojson file path or a dictionary of feature collection.\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    with rasterio.open(src_fp) as src:\n        src_crs = src.crs\n    coords = geojson_to_coords(geojson, coord_crs, src_crs)\n    return coords_to_xy(src_fp, coords, src_crs, **kwargs)\n</code></pre>"},{"location":"sam/#geoai.sam.get_device","title":"<code>get_device()</code>","text":"<p>Returns the best available device for deep learning in the order: CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"\n    Returns the best available device for deep learning in the order:\n    CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"sam/#geoai.sam.get_raster_info","title":"<code>get_raster_info(raster_path)</code>","text":"<p>Display basic information about a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing the basic information about the raster</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info(raster_path: str) -&gt; Dict[str, Any]:\n    \"\"\"Display basic information about a raster dataset.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster\n    \"\"\"\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Get basic metadata\n        info = {\n            \"driver\": src.driver,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"count\": src.count,\n            \"dtype\": src.dtypes[0],\n            \"crs\": src.crs.to_string() if src.crs else \"No CRS defined\",\n            \"transform\": src.transform,\n            \"bounds\": src.bounds,\n            \"resolution\": (src.transform[0], -src.transform[4]),\n            \"nodata\": src.nodata,\n        }\n\n        # Calculate statistics for each band\n        stats = []\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n            band_stats = {\n                \"band\": i,\n                \"min\": float(band.min()),\n                \"max\": float(band.max()),\n                \"mean\": float(band.mean()),\n                \"std\": float(band.std()),\n            }\n            stats.append(band_stats)\n\n        info[\"band_stats\"] = stats\n\n    return info\n</code></pre>"},{"location":"sam/#geoai.sam.get_raster_info_gdal","title":"<code>get_raster_info_gdal(raster_path)</code>","text":"<p>Get basic information about a raster dataset using GDAL.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the raster, or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info_gdal(raster_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get basic information about a raster dataset using GDAL.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster,\n            or None if the file cannot be opened\n    \"\"\"\n\n    from osgeo import gdal\n\n    # Open the dataset\n    ds = gdal.Open(raster_path)\n    if ds is None:\n        print(f\"Error: Could not open {raster_path}\")\n        return None\n\n    # Get basic information\n    info = {\n        \"driver\": ds.GetDriver().ShortName,\n        \"width\": ds.RasterXSize,\n        \"height\": ds.RasterYSize,\n        \"count\": ds.RasterCount,\n        \"projection\": ds.GetProjection(),\n        \"geotransform\": ds.GetGeoTransform(),\n    }\n\n    # Calculate resolution\n    gt = ds.GetGeoTransform()\n    if gt:\n        info[\"resolution\"] = (abs(gt[1]), abs(gt[5]))\n        info[\"origin\"] = (gt[0], gt[3])\n\n    # Get band information\n    bands_info = []\n    for i in range(1, ds.RasterCount + 1):\n        band = ds.GetRasterBand(i)\n        stats = band.GetStatistics(True, True)\n        band_info = {\n            \"band\": i,\n            \"datatype\": gdal.GetDataTypeName(band.DataType),\n            \"min\": stats[0],\n            \"max\": stats[1],\n            \"mean\": stats[2],\n            \"std\": stats[3],\n            \"nodata\": band.GetNoDataValue(),\n        }\n        bands_info.append(band_info)\n\n    info[\"bands\"] = bands_info\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"sam/#geoai.sam.get_raster_resolution","title":"<code>get_raster_resolution(image_path)</code>","text":"<p>Get pixel resolution from the raster using rasterio.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>The path to the raster image.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>A tuple of (x resolution, y resolution).</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_resolution(image_path: str) -&gt; Tuple[float, float]:\n    \"\"\"Get pixel resolution from the raster using rasterio.\n\n    Args:\n        image_path: The path to the raster image.\n\n    Returns:\n        A tuple of (x resolution, y resolution).\n    \"\"\"\n    with rasterio.open(image_path) as src:\n        res = src.res\n    return res\n</code></pre>"},{"location":"sam/#geoai.sam.get_raster_stats","title":"<code>get_raster_stats(raster_path, divide_by=1.0)</code>","text":"<p>Calculate statistics for each band in a raster dataset.</p> <p>This function computes min, max, mean, and standard deviation values for each band in the provided raster, returning results in a dictionary with lists for each statistic type.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>divide_by</code> <code>float</code> <p>Value to divide pixel values by. Defaults to 1.0, which keeps the original pixel</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing lists of statistics with keys: - 'min': List of minimum values for each band - 'max': List of maximum values for each band - 'mean': List of mean values for each band - 'std': List of standard deviation values for each band</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_stats(raster_path: str, divide_by: float = 1.0) -&gt; Dict[str, Any]:\n    \"\"\"Calculate statistics for each band in a raster dataset.\n\n    This function computes min, max, mean, and standard deviation values\n    for each band in the provided raster, returning results in a dictionary\n    with lists for each statistic type.\n\n    Args:\n        raster_path (str): Path to the raster file\n        divide_by (float, optional): Value to divide pixel values by.\n            Defaults to 1.0, which keeps the original pixel\n\n    Returns:\n        dict: Dictionary containing lists of statistics with keys:\n            - 'min': List of minimum values for each band\n            - 'max': List of maximum values for each band\n            - 'mean': List of mean values for each band\n            - 'std': List of standard deviation values for each band\n    \"\"\"\n    # Initialize the results dictionary with empty lists\n    stats = {\"min\": [], \"max\": [], \"mean\": [], \"std\": []}\n\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Calculate statistics for each band\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n\n            # Append statistics for this band to each list\n            stats[\"min\"].append(float(band.min()) / divide_by)\n            stats[\"max\"].append(float(band.max()) / divide_by)\n            stats[\"mean\"].append(float(band.mean()) / divide_by)\n            stats[\"std\"].append(float(band.std()) / divide_by)\n\n    return stats\n</code></pre>"},{"location":"sam/#geoai.sam.get_vector_info","title":"<code>get_vector_info(vector_path)</code>","text":"<p>Display basic information about a vector dataset using GeoPandas.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the vector dataset</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info(vector_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Display basic information about a vector dataset using GeoPandas.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset\n    \"\"\"\n    # Open the vector dataset\n    gdf = (\n        gpd.read_parquet(vector_path)\n        if vector_path.endswith(\".parquet\")\n        else gpd.read_file(vector_path)\n    )\n\n    # Get basic metadata\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": os.path.splitext(vector_path)[1][1:].upper(),  # Format from extension\n        \"feature_count\": len(gdf),\n        \"crs\": str(gdf.crs),\n        \"geometry_type\": str(gdf.geom_type.value_counts().to_dict()),\n        \"attribute_count\": len(gdf.columns) - 1,  # Subtract the geometry column\n        \"attribute_names\": list(gdf.columns[gdf.columns != \"geometry\"]),\n        \"bounds\": gdf.total_bounds.tolist(),\n    }\n\n    # Add statistics about numeric attributes\n    numeric_columns = gdf.select_dtypes(include=[\"number\"]).columns\n    attribute_stats = {}\n    for col in numeric_columns:\n        if col != \"geometry\":\n            attribute_stats[col] = {\n                \"min\": gdf[col].min(),\n                \"max\": gdf[col].max(),\n                \"mean\": gdf[col].mean(),\n                \"std\": gdf[col].std(),\n                \"null_count\": gdf[col].isna().sum(),\n            }\n\n    info[\"attribute_stats\"] = attribute_stats\n\n    return info\n</code></pre>"},{"location":"sam/#geoai.sam.get_vector_info_ogr","title":"<code>get_vector_info_ogr(vector_path)</code>","text":"<p>Get basic information about a vector dataset using OGR.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the vector dataset, or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info_ogr(vector_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get basic information about a vector dataset using OGR.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset,\n            or None if the file cannot be opened\n    \"\"\"\n    from osgeo import ogr\n\n    # Register all OGR drivers\n    ogr.RegisterAll()\n\n    # Open the dataset\n    ds = ogr.Open(vector_path)\n    if ds is None:\n        print(f\"Error: Could not open {vector_path}\")\n        return None\n\n    # Basic dataset information\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": ds.GetDriver().GetName(),\n        \"layer_count\": ds.GetLayerCount(),\n        \"layers\": [],\n    }\n\n    # Extract information for each layer\n    for i in range(ds.GetLayerCount()):\n        layer = ds.GetLayer(i)\n        layer_info = {\n            \"name\": layer.GetName(),\n            \"feature_count\": layer.GetFeatureCount(),\n            \"geometry_type\": ogr.GeometryTypeToName(layer.GetGeomType()),\n            \"spatial_ref\": (\n                layer.GetSpatialRef().ExportToWkt() if layer.GetSpatialRef() else \"None\"\n            ),\n            \"extent\": layer.GetExtent(),\n            \"fields\": [],\n        }\n\n        # Get field information\n        defn = layer.GetLayerDefn()\n        for j in range(defn.GetFieldCount()):\n            field_defn = defn.GetFieldDefn(j)\n            field_info = {\n                \"name\": field_defn.GetName(),\n                \"type\": field_defn.GetTypeName(),\n                \"width\": field_defn.GetWidth(),\n                \"precision\": field_defn.GetPrecision(),\n            }\n            layer_info[\"fields\"].append(field_info)\n\n        info[\"layers\"].append(layer_info)\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"sam/#geoai.sam.hybrid_regularization","title":"<code>hybrid_regularization(building_polygons)</code>","text":"<p>A comprehensive hybrid approach to building footprint regularization.</p> <p>Applies different strategies based on building characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def hybrid_regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    A comprehensive hybrid approach to building footprint regularization.\n\n    Applies different strategies based on building characteristics.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Use minimum_rotated_rectangle instead of oriented_envelope\n    try:\n        from shapely.minimum_rotated_rectangle import minimum_rotated_rectangle\n    except ImportError:\n        # For older Shapely versions\n        def minimum_rotated_rectangle(geom):\n            \"\"\"Calculate the minimum rotated rectangle for a geometry\"\"\"\n            # For older Shapely versions, implement a simple version\n            return geom.minimum_rotated_rectangle\n\n    # Determine input type for correct return\n    is_gdf = isinstance(building_polygons, gpd.GeoDataFrame)\n\n    # Extract geometries if GeoDataFrame\n    if is_gdf:\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # 1. Analyze building characteristics\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Calculate shape complexity metrics\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Calculate dominant angle\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        segment_angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Weight angles by segment length\n        hist, bins = np.histogram(\n            segment_angles % 180, bins=36, range=(0, 180), weights=segment_lengths\n        )\n        bin_centers = (bins[:-1] + bins[1:]) / 2\n        dominant_angle = bin_centers[np.argmax(hist)]\n\n        # Check if building is close to orthogonal\n        is_orthogonal = min(dominant_angle % 45, 45 - (dominant_angle % 45)) &lt; 5\n\n        # 2. Apply appropriate regularization strategy\n        if complexity &gt; 1.5:\n            # Complex buildings: use minimum rotated rectangle\n            result = minimum_rotated_rectangle(building)\n        elif is_orthogonal:\n            # Near-orthogonal buildings: orthogonalize in place\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create orthogonal hull in rotated space\n            bounds = rotated.bounds\n            ortho_hull = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            result = rotate(ortho_hull, dominant_angle, origin=\"centroid\")\n        else:\n            # Diagonal buildings: use custom approach for diagonal buildings\n            # Rotate to align with axes\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Simplify in rotated space\n            simplified = rotated.simplify(0.3, preserve_topology=True)\n\n            # Get the bounds in rotated space\n            bounds = simplified.bounds\n            min_x, min_y, max_x, max_y = bounds\n\n            # Create a rectangular hull in rotated space\n            rect_poly = Polygon(\n                [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]\n            )\n\n            # Rotate back to original orientation\n            result = rotate(rect_poly, dominant_angle, origin=\"centroid\")\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"sam/#geoai.sam.inspect_pth_file","title":"<code>inspect_pth_file(pth_path)</code>","text":"<p>Inspect a PyTorch .pth model file to determine its architecture.</p> <p>Parameters:</p> Name Type Description Default <code>pth_path</code> <code>str</code> <p>Path to the .pth file to inspect</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Information about the model architecture</p> Source code in <code>geoai/utils.py</code> <pre><code>def inspect_pth_file(pth_path: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Inspect a PyTorch .pth model file to determine its architecture.\n\n    Args:\n        pth_path: Path to the .pth file to inspect\n\n    Returns:\n        Information about the model architecture\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(pth_path):\n        print(f\"Error: File {pth_path} not found\")\n        return\n\n    # Load the checkpoint\n    try:\n        checkpoint = torch.load(pth_path, map_location=torch.device(\"cpu\"))\n        print(f\"\\n{'='*50}\")\n        print(f\"Inspecting model file: {pth_path}\")\n        print(f\"{'='*50}\\n\")\n\n        # Check if it's a state_dict or a complete model\n        if isinstance(checkpoint, OrderedDict) or isinstance(checkpoint, dict):\n            if \"state_dict\" in checkpoint:\n                print(\"Found 'state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"state_dict\"]\n            elif \"model_state_dict\" in checkpoint:\n                print(\"Found 'model_state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"model_state_dict\"]\n            else:\n                print(\"Assuming file contains a direct state_dict.\")\n                state_dict = checkpoint\n\n            # Print the keys in the checkpoint\n            print(\"\\nCheckpoint contains the following keys:\")\n            for key in checkpoint.keys():\n                if isinstance(checkpoint[key], dict):\n                    print(f\"- {key} (dictionary with {len(checkpoint[key])} items)\")\n                elif isinstance(checkpoint[key], (torch.Tensor, list, tuple)):\n                    print(\n                        f\"- {key} (shape/size: {len(checkpoint[key]) if isinstance(checkpoint[key], (list, tuple)) else checkpoint[key].shape})\"\n                    )\n                else:\n                    print(f\"- {key} ({type(checkpoint[key]).__name__})\")\n\n            # Try to infer the model architecture from the state_dict keys\n            print(\"\\nAnalyzing model architecture from state_dict...\")\n\n            # Extract layer keys for analysis\n            layer_keys = list(state_dict.keys())\n\n            # Print the first few layer keys to understand naming pattern\n            print(\"\\nFirst 10 layer names in state_dict:\")\n            for i, key in enumerate(layer_keys[:10]):\n                shape = state_dict[key].shape\n                print(f\"- {key} (shape: {shape})\")\n\n            # Look for architecture indicators in the keys\n            architecture_indicators = {\n                \"conv\": 0,\n                \"bn\": 0,\n                \"layer\": 0,\n                \"fc\": 0,\n                \"backbone\": 0,\n                \"encoder\": 0,\n                \"decoder\": 0,\n                \"unet\": 0,\n                \"resnet\": 0,\n                \"classifier\": 0,\n                \"deeplab\": 0,\n                \"fcn\": 0,\n            }\n\n            for key in layer_keys:\n                for indicator in architecture_indicators:\n                    if indicator in key.lower():\n                        architecture_indicators[indicator] += 1\n\n            print(\"\\nArchitecture indicators found in layer names:\")\n            for indicator, count in architecture_indicators.items():\n                if count &gt; 0:\n                    print(f\"- '{indicator}' appears {count} times\")\n\n            # Count total parameters\n            total_params = sum(p.numel() for p in state_dict.values())\n            print(f\"\\nTotal parameters: {total_params:,}\")\n\n            # Try to load the model with different architectures\n            print(\"\\nAttempting to match with common architectures...\")\n\n            # Try to identify if it's a segmentation model\n            if any(\"out\" in k or \"classifier\" in k for k in layer_keys):\n                print(\"Model appears to be a segmentation model.\")\n\n                # Check if it might be a UNet\n                if (\n                    architecture_indicators[\"encoder\"] &gt; 0\n                    and architecture_indicators[\"decoder\"] &gt; 0\n                ):\n                    print(\n                        \"Architecture seems to be a UNet-based model with encoder-decoder structure.\"\n                    )\n                # Check for FCN or DeepLab indicators\n                elif architecture_indicators[\"fcn\"] &gt; 0:\n                    print(\n                        \"Architecture seems to be FCN-based (Fully Convolutional Network).\"\n                    )\n                elif architecture_indicators[\"deeplab\"] &gt; 0:\n                    print(\"Architecture seems to be DeepLab-based.\")\n                elif architecture_indicators[\"backbone\"] &gt; 0:\n                    print(\n                        \"Model has a backbone architecture, likely a modern segmentation model.\"\n                    )\n\n            # Try to infer output classes from the final layer\n            output_layer_keys = [\n                k for k in layer_keys if \"classifier\" in k or k.endswith(\".out.weight\")\n            ]\n            if output_layer_keys:\n                output_shape = state_dict[output_layer_keys[0]].shape\n                if len(output_shape) &gt;= 2:\n                    num_classes = output_shape[0]\n                    print(f\"\\nModel likely has {num_classes} output classes.\")\n\n            print(\"\\nSUMMARY:\")\n            print(\"The model appears to be\", end=\" \")\n            if architecture_indicators[\"unet\"] &gt; 0:\n                print(\"a UNet architecture.\", end=\" \")\n            elif architecture_indicators[\"fcn\"] &gt; 0:\n                print(\"an FCN architecture.\", end=\" \")\n            elif architecture_indicators[\"deeplab\"] &gt; 0:\n                print(\"a DeepLab architecture.\", end=\" \")\n            elif architecture_indicators[\"resnet\"] &gt; 0:\n                print(\"ResNet-based.\", end=\" \")\n            else:\n                print(\"a custom architecture.\", end=\" \")\n\n            # Try to load with common models\n            try_common_architectures(state_dict)\n\n        else:\n            print(\n                \"The file contains an entire model object rather than just a state dictionary.\"\n            )\n            # If it's a complete model, we can directly examine its architecture\n            print(checkpoint)\n\n    except Exception as e:\n        print(f\"Error loading the model file: {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>geoai/utils.py</code> <pre><code>def install_package(package: Union[str, List[str]]) -&gt; None:\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n    elif isinstance(package, list):\n        packages = package\n    else:\n        raise ValueError(\"The package argument must be a string or a list of strings.\")\n\n    for package in packages:\n        if package.startswith(\"https\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"sam/#geoai.sam.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=1.0, mask_threshold=0.5, min_object_area=100, max_object_area=None, nms_iou_threshold=0.5)</code>","text":"<p>Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <code>str</code> <p>Path to the building masks GeoTIFF</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoJSON (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>1.0</code> <code>mask_threshold</code> <code>float</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>0.5</code> <code>min_object_area</code> <code>int</code> <p>Minimum area in pixels to keep a building (default: self.min_object_area)</p> <code>100</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Maximum area in pixels to keep a building (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>GeoDataFrame with building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def masks_to_vector(\n    mask_path: str,\n    output_path: Optional[str] = None,\n    simplify_tolerance: float = 1.0,\n    mask_threshold: float = 0.5,\n    min_object_area: int = 100,\n    max_object_area: Optional[int] = None,\n    nms_iou_threshold: float = 0.5,\n) -&gt; Any:\n    \"\"\"\n    Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.\n\n    Args:\n        mask_path: Path to the building masks GeoTIFF\n        output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep a building (default: self.min_object_area)\n        max_object_area: Maximum area in pixels to keep a building (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n\n    Returns:\n        Any: GeoDataFrame with building footprints\n    \"\"\"\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min building area: {min_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(f\"Found {num_labels-1} potential buildings\")  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this building\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this building\n            building_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                building_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Building class\n            },\n            crs=crs,\n        )\n\n        def filter_overlapping_polygons(gdf, **kwargs):\n            \"\"\"\n            Filter overlapping polygons using non-maximum suppression.\n\n            Args:\n                gdf: GeoDataFrame with polygons\n                **kwargs: Optional parameters:\n                    nms_iou_threshold: IoU threshold for filtering\n\n            Returns:\n                Filtered GeoDataFrame\n            \"\"\"\n            if len(gdf) &lt;= 1:\n                return gdf\n\n            # Get parameters from kwargs or use instance defaults\n            iou_threshold = kwargs.get(\"nms_iou_threshold\", nms_iou_threshold)\n\n            # Sort by confidence\n            gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n            # Fix any invalid geometries\n            gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n                lambda geom: geom.buffer(0) if not geom.is_valid else geom\n            )\n\n            keep_indices = []\n            polygons = gdf.geometry.values\n\n            for i in range(len(polygons)):\n                if i in keep_indices:\n                    continue\n\n                keep = True\n                for j in keep_indices:\n                    # Skip invalid geometries\n                    if not polygons[i].is_valid or not polygons[j].is_valid:\n                        continue\n\n                    # Calculate IoU\n                    try:\n                        intersection = polygons[i].intersection(polygons[j]).area\n                        union = polygons[i].area + polygons[j].area - intersection\n                        iou = intersection / union if union &gt; 0 else 0\n\n                        if iou &gt; iou_threshold:\n                            keep = False\n                            break\n                    except Exception:\n                        # Skip on topology exceptions\n                        continue\n\n                if keep:\n                    keep_indices.append(i)\n\n            return gdf.iloc[keep_indices]\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        print(f\"Final building count after filtering: {len(gdf)}\")\n\n        # Save to file\n        if output_path is not None:\n            gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"sam/#geoai.sam.mosaic_geotiffs","title":"<code>mosaic_geotiffs(input_dir, output_file, mask_file=None)</code>","text":"<p>Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).</p> <p>This function identifies all GeoTIFF files in the specified directory, creates a seamless mosaic with proper handling of nodata values, and saves as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output will be clipped to the extent of the mask.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to the directory containing GeoTIFF files.</p> required <code>output_file</code> <code>str</code> <p>Path to the output Cloud Optimized GeoTIFF file.</p> required <code>mask_file</code> <code>str</code> <p>Path to a mask file to clip the output. If provided, the output will be clipped to the extent of this mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>None</code> <p>True if the mosaic was created successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\nTrue\n&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\nTrue\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def mosaic_geotiffs(\n    input_dir: str, output_file: str, mask_file: Optional[str] = None\n) -&gt; None:\n    \"\"\"Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).\n\n    This function identifies all GeoTIFF files in the specified directory,\n    creates a seamless mosaic with proper handling of nodata values, and saves\n    as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output\n    will be clipped to the extent of the mask.\n\n    Args:\n        input_dir (str): Path to the directory containing GeoTIFF files.\n        output_file (str): Path to the output Cloud Optimized GeoTIFF file.\n        mask_file (str, optional): Path to a mask file to clip the output.\n            If provided, the output will be clipped to the extent of this mask.\n            Defaults to None.\n\n    Returns:\n        bool: True if the mosaic was created successfully, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\n        True\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\n        True\n    \"\"\"\n    import glob\n\n    from osgeo import gdal\n\n    gdal.UseExceptions()\n    # Get all tif files in the directory\n    tif_files = glob.glob(os.path.join(input_dir, \"*.tif\"))\n\n    if not tif_files:\n        print(\"No GeoTIFF files found in the specified directory.\")\n        return False\n\n    # Analyze the first input file to determine compression and nodata settings\n    ds = gdal.Open(tif_files[0])\n    if ds is None:\n        print(f\"Unable to open {tif_files[0]}\")\n        return False\n\n    # Get driver metadata from the first file\n    driver = ds.GetDriver()\n    creation_options = []\n\n    # Check compression type\n    metadata = ds.GetMetadata(\"IMAGE_STRUCTURE\")\n    if \"COMPRESSION\" in metadata:\n        compression = metadata[\"COMPRESSION\"]\n        creation_options.append(f\"COMPRESS={compression}\")\n    else:\n        # Default compression if none detected\n        creation_options.append(\"COMPRESS=LZW\")\n\n    # Add COG-specific creation options\n    creation_options.extend([\"TILED=YES\", \"BLOCKXSIZE=512\", \"BLOCKYSIZE=512\"])\n\n    # Check for nodata value in the first band of the first file\n    band = ds.GetRasterBand(1)\n    has_nodata = band.GetNoDataValue() is not None\n    nodata_value = band.GetNoDataValue() if has_nodata else None\n\n    # Close the dataset\n    ds = None\n\n    # Create a temporary VRT (Virtual Dataset)\n    vrt_path = os.path.join(input_dir, \"temp_mosaic.vrt\")\n\n    # Build VRT from input files with proper nodata handling\n    vrt_options = gdal.BuildVRTOptions(\n        resampleAlg=\"nearest\",\n        srcNodata=nodata_value if has_nodata else None,\n        VRTNodata=nodata_value if has_nodata else None,\n    )\n    vrt_dataset = gdal.BuildVRT(vrt_path, tif_files, options=vrt_options)\n\n    # Close the VRT dataset to flush it to disk\n    vrt_dataset = None\n\n    # Create temp mosaic\n    temp_mosaic = output_file + \".temp.tif\"\n\n    # Convert VRT to GeoTIFF with the same compression as input\n    translate_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=creation_options,\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(temp_mosaic, vrt_path, options=translate_options)\n\n    # Apply mask if provided\n    if mask_file and os.path.exists(mask_file):\n        print(f\"Clipping mosaic to mask: {mask_file}\")\n\n        # Create a temporary clipped file\n        clipped_mosaic = output_file + \".clipped.tif\"\n\n        # Open mask file\n        mask_ds = gdal.Open(mask_file)\n        if mask_ds is None:\n            print(f\"Unable to open mask file: {mask_file}\")\n            # Continue without clipping\n        else:\n            # Get mask extent\n            mask_geotransform = mask_ds.GetGeoTransform()\n            mask_projection = mask_ds.GetProjection()\n            mask_ulx = mask_geotransform[0]\n            mask_uly = mask_geotransform[3]\n            mask_lrx = mask_ulx + (mask_geotransform[1] * mask_ds.RasterXSize)\n            mask_lry = mask_uly + (mask_geotransform[5] * mask_ds.RasterYSize)\n\n            # Close mask dataset\n            mask_ds = None\n\n            # Use warp options to clip\n            warp_options = gdal.WarpOptions(\n                format=\"GTiff\",\n                outputBounds=[mask_ulx, mask_lry, mask_lrx, mask_uly],\n                dstSRS=mask_projection,\n                creationOptions=creation_options,\n                srcNodata=nodata_value if has_nodata else None,\n                dstNodata=nodata_value if has_nodata else None,\n            )\n\n            # Apply clipping\n            gdal.Warp(clipped_mosaic, temp_mosaic, options=warp_options)\n\n            # Remove the unclipped temp mosaic and use the clipped one\n            os.remove(temp_mosaic)\n            temp_mosaic = clipped_mosaic\n\n    # Create internal overviews for the temp mosaic\n    ds = gdal.Open(temp_mosaic, gdal.GA_Update)\n    overview_list = [2, 4, 8, 16, 32]\n    ds.BuildOverviews(\"NEAREST\", overview_list)\n    ds = None  # Close the dataset to ensure overviews are written\n\n    # Convert the temp mosaic to a proper COG\n    cog_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=[\n            \"TILED=YES\",\n            \"COPY_SRC_OVERVIEWS=YES\",\n            \"COMPRESS=DEFLATE\",\n            \"PREDICTOR=2\",\n            \"BLOCKXSIZE=512\",\n            \"BLOCKYSIZE=512\",\n        ],\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(output_file, temp_mosaic, options=cog_options)\n\n    # Clean up temporary files\n    if os.path.exists(vrt_path):\n        os.remove(vrt_path)\n    if os.path.exists(temp_mosaic):\n        os.remove(temp_mosaic)\n\n    print(f\"Cloud Optimized GeoTIFF mosaic created successfully: {output_file}\")\n    return True\n</code></pre>"},{"location":"sam/#geoai.sam.orthogonalize","title":"<code>orthogonalize(input_path, output_path=None, epsilon=0.2, min_area=10, min_segments=4, area_tolerance=0.7, detect_triangles=True)</code>","text":"<p>Orthogonalizes object masks in a GeoTIFF file.</p> <p>This function reads a GeoTIFF containing object masks (binary or labeled regions), converts the raster masks to vector polygons, applies orthogonalization to each polygon, and optionally writes the result to a GeoJSON file. The source code is adapted from the Solar Panel Detection algorithm by Esri. See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284. Credits to Esri for the original code.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output GeoJSON file. If None, no file is saved.</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>Simplification tolerance for the Douglas-Peucker algorithm. Higher values result in more simplification. Default is 0.2.</p> <code>0.2</code> <code>min_area</code> <code>float</code> <p>Minimum area of polygons to process (smaller ones are kept as-is).</p> <code>10</code> <code>min_segments</code> <code>int</code> <p>Minimum number of segments to keep after simplification. Default is 4 (for rectangular shapes).</p> <code>4</code> <code>area_tolerance</code> <code>float</code> <p>Allowed ratio of area change. Values less than 1.0 restrict area change. Default is 0.7 (allows reduction to 70% of original area).</p> <code>0.7</code> <code>detect_triangles</code> <code>bool</code> <p>If True, performs additional check to avoid creating triangular shapes.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A GeoDataFrame containing the orthogonalized features.</p> Source code in <code>geoai/utils.py</code> <pre><code>def orthogonalize(\n    input_path,\n    output_path=None,\n    epsilon=0.2,\n    min_area=10,\n    min_segments=4,\n    area_tolerance=0.7,\n    detect_triangles=True,\n) -&gt; Any:\n    \"\"\"\n    Orthogonalizes object masks in a GeoTIFF file.\n\n    This function reads a GeoTIFF containing object masks (binary or labeled regions),\n    converts the raster masks to vector polygons, applies orthogonalization to each polygon,\n    and optionally writes the result to a GeoJSON file.\n    The source code is adapted from the Solar Panel Detection algorithm by Esri.\n    See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284.\n    Credits to Esri for the original code.\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str, optional): Path to save the output GeoJSON file. If None, no file is saved.\n        epsilon (float, optional): Simplification tolerance for the Douglas-Peucker algorithm.\n            Higher values result in more simplification. Default is 0.2.\n        min_area (float, optional): Minimum area of polygons to process (smaller ones are kept as-is).\n        min_segments (int, optional): Minimum number of segments to keep after simplification.\n            Default is 4 (for rectangular shapes).\n        area_tolerance (float, optional): Allowed ratio of area change. Values less than 1.0 restrict\n            area change. Default is 0.7 (allows reduction to 70% of original area).\n        detect_triangles (bool, optional): If True, performs additional check to avoid creating triangular shapes.\n\n    Returns:\n        Any: A GeoDataFrame containing the orthogonalized features.\n    \"\"\"\n\n    from functools import partial\n\n    def orthogonalize_ring(ring, epsilon=0.2, min_segments=4):\n        \"\"\"\n        Orthogonalizes a ring (list of coordinates).\n\n        Args:\n            ring (list): List of [x, y] coordinates forming a ring\n            epsilon (float, optional): Simplification tolerance\n            min_segments (int, optional): Minimum number of segments to keep\n\n        Returns:\n            list: Orthogonalized list of coordinates\n        \"\"\"\n        if len(ring) &lt;= 3:\n            return ring\n\n        # Convert to numpy array\n        ring_arr = np.array(ring)\n\n        # Get orientation\n        angle = math.degrees(get_orientation(ring_arr))\n\n        # Simplify using Ramer-Douglas-Peucker algorithm\n        ring_arr = simplify(ring_arr, eps=epsilon)\n\n        # If simplified too much, adjust epsilon to maintain minimum segments\n        if len(ring_arr) &lt; min_segments:\n            # Try with smaller epsilon until we get at least min_segments points\n            for adjust_factor in [0.75, 0.5, 0.25, 0.1]:\n                test_arr = simplify(np.array(ring), eps=epsilon * adjust_factor)\n                if len(test_arr) &gt;= min_segments:\n                    ring_arr = test_arr\n                    break\n\n        # Convert to dataframe for processing\n        df = to_dataframe(ring_arr)\n\n        # Add orientation information\n        add_orientation(df, angle)\n\n        # Align segments to orthogonal directions\n        df = align(df)\n\n        # Merge collinear line segments\n        df = merge_lines(df)\n\n        if len(df) == 0:\n            return ring\n\n        # If we have a triangle-like result (3 segments or less), return the original shape\n        if len(df) &lt;= 3:\n            return ring\n\n        # Join the orthogonalized segments back into a ring\n        joined_ring = join_ring(df)\n\n        # If the join operation didn't produce a valid ring, return the original\n        if len(joined_ring) == 0 or len(joined_ring[0]) &lt; 3:\n            return ring\n\n        # Enhanced validation: check for triangular result and geometric validity\n        result_coords = joined_ring[0]\n\n        # If result has 3 or fewer points (triangle), use original\n        if len(result_coords) &lt;= 3:  # 2 points + closing point (degenerate)\n            return ring\n\n        # Additional validation: check for degenerate geometry\n        # Calculate area ratio to detect if the shape got severely distorted\n        def calculate_polygon_area(coords):\n            if len(coords) &lt; 3:\n                return 0\n            area = 0\n            n = len(coords)\n            for i in range(n):\n                j = (i + 1) % n\n                area += coords[i][0] * coords[j][1]\n                area -= coords[j][0] * coords[i][1]\n            return abs(area) / 2\n\n        original_area = calculate_polygon_area(ring)\n        result_area = calculate_polygon_area(result_coords)\n\n        # If the area changed dramatically (more than 30% shrinkage or 300% growth), use original\n        if original_area &gt; 0 and result_area &gt; 0:\n            area_ratio = result_area / original_area\n            if area_ratio &lt; 0.3 or area_ratio &gt; 3.0:\n                return ring\n\n        # Check for triangular spikes and problematic artifacts\n        very_acute_angle_count = 0\n        triangular_spike_detected = False\n\n        for i in range(len(result_coords) - 1):  # -1 to exclude closing point\n            p1 = result_coords[i - 1]\n            p2 = result_coords[i]\n            p3 = result_coords[(i + 1) % (len(result_coords) - 1)]\n\n            # Calculate angle at p2\n            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n\n            v1_norm = np.linalg.norm(v1)\n            v2_norm = np.linalg.norm(v2)\n\n            if v1_norm &gt; 0 and v2_norm &gt; 0:\n                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                cos_angle = np.clip(cos_angle, -1, 1)\n                angle = np.arccos(cos_angle)\n\n                # Count very acute angles (&lt; 20 degrees) - these are likely spikes\n                if angle &lt; np.pi / 9:  # 20 degrees\n                    very_acute_angle_count += 1\n                    # If it's very acute with short sides, it's definitely a spike\n                    if v1_norm &lt; 5 or v2_norm &lt; 5:\n                        triangular_spike_detected = True\n\n        # Check for excessively long edges that might be artifacts\n        edge_lengths = []\n        for i in range(len(result_coords) - 1):\n            edge_len = np.sqrt(\n                (result_coords[i + 1][0] - result_coords[i][0]) ** 2\n                + (result_coords[i + 1][1] - result_coords[i][1]) ** 2\n            )\n            edge_lengths.append(edge_len)\n\n        excessive_edge_detected = False\n        if len(edge_lengths) &gt; 0:\n            avg_edge_length = np.mean(edge_lengths)\n            max_edge_length = np.max(edge_lengths)\n            # Only reject if edge is extremely disproportionate (8x average)\n            if max_edge_length &gt; avg_edge_length * 8:\n                excessive_edge_detected = True\n\n        # Check for triangular artifacts by detecting spikes that extend beyond bounds\n        # Calculate original bounds\n        orig_xs = [p[0] for p in ring]\n        orig_ys = [p[1] for p in ring]\n        orig_min_x, orig_max_x = min(orig_xs), max(orig_xs)\n        orig_min_y, orig_max_y = min(orig_ys), max(orig_ys)\n        orig_width = orig_max_x - orig_min_x\n        orig_height = orig_max_y - orig_min_y\n\n        # Calculate result bounds\n        result_xs = [p[0] for p in result_coords]\n        result_ys = [p[1] for p in result_coords]\n        result_min_x, result_max_x = min(result_xs), max(result_xs)\n        result_min_y, result_max_y = min(result_ys), max(result_ys)\n\n        # Stricter bounds checking to catch triangular artifacts\n        bounds_extension_detected = False\n        # More conservative: only allow 10% extension\n        tolerance_x = max(orig_width * 0.1, 1.0)  # 10% tolerance, at least 1 unit\n        tolerance_y = max(orig_height * 0.1, 1.0)  # 10% tolerance, at least 1 unit\n\n        if (\n            result_min_x &lt; orig_min_x - tolerance_x\n            or result_max_x &gt; orig_max_x + tolerance_x\n            or result_min_y &lt; orig_min_y - tolerance_y\n            or result_max_y &gt; orig_max_y + tolerance_y\n        ):\n            bounds_extension_detected = True\n\n        # Reject if we detect triangular spikes, excessive edges, or bounds violations\n        if (\n            triangular_spike_detected\n            or very_acute_angle_count &gt; 2  # Multiple very acute angles\n            or excessive_edge_detected\n            or bounds_extension_detected\n        ):  # Any significant bounds extension\n            return ring\n\n        # Convert back to a list and ensure it's closed\n        result = joined_ring[0].tolist()\n        if len(result) &gt; 0 and (result[0] != result[-1]):\n            result.append(result[0])\n\n        return result\n\n    def vectorize_mask(mask, transform):\n        \"\"\"\n        Converts a binary mask to vector polygons.\n\n        Args:\n            mask (numpy.ndarray): Binary mask where non-zero values represent objects\n            transform (rasterio.transform.Affine): Affine transformation matrix\n\n        Returns:\n            list: List of GeoJSON features\n        \"\"\"\n        shapes = features.shapes(mask, transform=transform)\n        features_list = []\n\n        for shape, value in shapes:\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                features_list.append(\n                    {\n                        \"type\": \"Feature\",\n                        \"properties\": {\"value\": int(value)},\n                        \"geometry\": shape,\n                    }\n                )\n\n        return features_list\n\n    def rasterize_features(features, shape, transform, dtype=np.uint8):\n        \"\"\"\n        Converts vector features back to a raster mask.\n\n        Args:\n            features (list): List of GeoJSON features\n            shape (tuple): Shape of the output raster (height, width)\n            transform (rasterio.transform.Affine): Affine transformation matrix\n            dtype (numpy.dtype, optional): Data type of the output raster\n\n        Returns:\n            numpy.ndarray: Rasterized mask\n        \"\"\"\n        mask = features.rasterize(\n            [\n                (feature[\"geometry\"], feature[\"properties\"][\"value\"])\n                for feature in features\n            ],\n            out_shape=shape,\n            transform=transform,\n            fill=0,\n            dtype=dtype,\n        )\n\n        return mask\n\n    # The following helper functions are from the original code\n    def get_orientation(contour):\n        \"\"\"\n        Calculate the orientation angle of a contour.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            float: Orientation angle in radians\n        \"\"\"\n        box = cv2.minAreaRect(contour.astype(int))\n        (cx, cy), (w, h), angle = box\n        return math.radians(angle)\n\n    def simplify(contour, eps=0.2):\n        \"\"\"\n        Simplify a contour using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n            eps (float, optional): Epsilon value for simplification\n\n        Returns:\n            numpy.ndarray: Simplified contour\n        \"\"\"\n        return rdp(contour, epsilon=eps)\n\n    def to_dataframe(ring):\n        \"\"\"\n        Convert a ring to a pandas DataFrame with line segment information.\n\n        Args:\n            ring (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            pandas.DataFrame: DataFrame with line segment information\n        \"\"\"\n        df = pd.DataFrame(ring, columns=[\"x1\", \"y1\"])\n        df[\"x2\"] = df[\"x1\"].shift(-1)\n        df[\"y2\"] = df[\"y1\"].shift(-1)\n        df.dropna(inplace=True)\n        df[\"angle_atan\"] = np.arctan2((df[\"y2\"] - df[\"y1\"]), (df[\"x2\"] - df[\"x1\"]))\n        df[\"angle_atan_deg\"] = df[\"angle_atan\"] * 57.2958\n        df[\"len\"] = np.sqrt((df[\"y2\"] - df[\"y1\"]) ** 2 + (df[\"x2\"] - df[\"x1\"]) ** 2)\n        df[\"cx\"] = (df[\"x2\"] + df[\"x1\"]) / 2.0\n        df[\"cy\"] = (df[\"y2\"] + df[\"y1\"]) / 2.0\n        return df\n\n    def add_orientation(df, angle):\n        \"\"\"\n        Add orientation information to the DataFrame.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n            angle (float): Orientation angle in degrees\n\n        Returns:\n            None: Modifies the DataFrame in-place\n        \"\"\"\n        rtangle = angle + 90\n        is_parallel = (\n            (df[\"angle_atan_deg\"] &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] &lt; (angle + 45))\n        ) | (\n            (df[\"angle_atan_deg\"] + 180 &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] + 180 &lt; (angle + 45))\n        )\n        df[\"angle\"] = math.radians(angle)\n        df[\"angle\"] = df[\"angle\"].where(is_parallel, math.radians(rtangle))\n\n    def align(df):\n        \"\"\"\n        Align line segments to their nearest orthogonal direction.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n\n        Returns:\n            pandas.DataFrame: DataFrame with aligned line segments\n        \"\"\"\n        # Handle edge case with empty dataframe\n        if len(df) == 0:\n            return df.copy()\n\n        df_clone = df.copy()\n\n        # Ensure angle column exists and has valid values\n        if \"angle\" not in df_clone.columns or df_clone[\"angle\"].isna().any():\n            # If angle data is missing, add default angles based on atan2\n            df_clone[\"angle\"] = df_clone[\"angle_atan\"]\n\n        # Ensure length and center point data is valid\n        if \"len\" not in df_clone.columns or df_clone[\"len\"].isna().any():\n            # Recalculate lengths if missing\n            df_clone[\"len\"] = np.sqrt(\n                (df_clone[\"x2\"] - df_clone[\"x1\"]) ** 2\n                + (df_clone[\"y2\"] - df_clone[\"y1\"]) ** 2\n            )\n\n        if \"cx\" not in df_clone.columns or df_clone[\"cx\"].isna().any():\n            df_clone[\"cx\"] = (df_clone[\"x1\"] + df_clone[\"x2\"]) / 2.0\n\n        if \"cy\" not in df_clone.columns or df_clone[\"cy\"].isna().any():\n            df_clone[\"cy\"] = (df_clone[\"y1\"] + df_clone[\"y2\"]) / 2.0\n\n        # Apply orthogonal alignment\n        df_clone[\"x1\"] = df_clone[\"cx\"] - ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"x2\"] = df_clone[\"cx\"] + ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"y1\"] = df_clone[\"cy\"] - ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n        df_clone[\"y2\"] = df_clone[\"cy\"] + ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n\n        return df_clone\n\n    def merge_lines(df_aligned):\n        \"\"\"\n        Merge collinear line segments.\n\n        Args:\n            df_aligned (pandas.DataFrame): DataFrame with aligned line segments\n\n        Returns:\n            pandas.DataFrame: DataFrame with merged line segments\n        \"\"\"\n        ortho_lines = []\n        groups = df_aligned.groupby(\n            (df_aligned[\"angle\"].shift() != df_aligned[\"angle\"]).cumsum()\n        )\n        for x, y in groups:\n            group_cx = (y[\"cx\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            group_cy = (y[\"cy\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            cumlen = y[\"len\"].sum()\n\n            ortho_lines.append((group_cx, group_cy, cumlen, y[\"angle\"].iloc[0]))\n\n        ortho_list = []\n        for cx, cy, length, rot_angle in ortho_lines:\n            X1 = cx - (length / 2) * math.cos(rot_angle)\n            X2 = cx + (length / 2) * math.cos(rot_angle)\n            Y1 = cy - (length / 2) * math.sin(rot_angle)\n            Y2 = cy + (length / 2) * math.sin(rot_angle)\n\n            ortho_list.append(\n                {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": length,\n                    \"cx\": cx,\n                    \"cy\": cy,\n                    \"angle\": rot_angle,\n                }\n            )\n\n        # Improved fix: Prevent merging that would create triangular or problematic shapes\n        if (\n            len(ortho_list) &gt; 3 and ortho_list[0][\"angle\"] == ortho_list[-1][\"angle\"]\n        ):  # join first and last segment if they're in same direction\n            # Check if merging would result in 3 or 4 segments (potentially triangular)\n            resulting_segments = len(ortho_list) - 1\n            if resulting_segments &lt;= 4:\n                # For very small polygons, be extra cautious about merging\n                # Calculate the spatial relationship between first and last segments\n                first_center = np.array([ortho_list[0][\"cx\"], ortho_list[0][\"cy\"]])\n                last_center = np.array([ortho_list[-1][\"cx\"], ortho_list[-1][\"cy\"]])\n                center_distance = np.linalg.norm(first_center - last_center)\n\n                # Get average segment length for comparison\n                avg_length = sum(seg[\"len\"] for seg in ortho_list) / len(ortho_list)\n\n                # Only merge if segments are close enough and it won't create degenerate shapes\n                if center_distance &gt; avg_length * 1.5:\n                    # Skip merging - segments are too far apart\n                    pass\n                else:\n                    # Proceed with merging only for well-connected segments\n                    totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n                    merge_cx = (\n                        (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                        + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n                    ) / totlen\n\n                    merge_cy = (\n                        (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                        + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n                    ) / totlen\n\n                    rot_angle = ortho_list[0][\"angle\"]\n                    X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n                    X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n                    Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n                    Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n                    ortho_list[-1] = {\n                        \"x1\": X1,\n                        \"y1\": Y1,\n                        \"x2\": X2,\n                        \"y2\": Y2,\n                        \"len\": totlen,\n                        \"cx\": merge_cx,\n                        \"cy\": merge_cy,\n                        \"angle\": rot_angle,\n                    }\n                    ortho_list = ortho_list[1:]\n            else:\n                # For larger polygons, proceed with standard merging\n                totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n                merge_cx = (\n                    (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                    + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n                ) / totlen\n\n                merge_cy = (\n                    (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                    + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n                ) / totlen\n\n                rot_angle = ortho_list[0][\"angle\"]\n                X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n                X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n                Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n                Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n                ortho_list[-1] = {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": totlen,\n                    \"cx\": merge_cx,\n                    \"cy\": merge_cy,\n                    \"angle\": rot_angle,\n                }\n                ortho_list = ortho_list[1:]\n        ortho_df = pd.DataFrame(ortho_list)\n        return ortho_df\n\n    def find_intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n        \"\"\"\n        Find the intersection point of two line segments.\n\n        Args:\n            x1, y1, x2, y2: Coordinates of the first line segment\n            x3, y3, x4, y4: Coordinates of the second line segment\n\n        Returns:\n            list: [x, y] coordinates of the intersection point\n\n        Raises:\n            ZeroDivisionError: If the lines are parallel or collinear\n        \"\"\"\n        # Calculate the denominator of the intersection formula\n        denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n\n        # Check if lines are parallel or collinear (denominator close to zero)\n        if abs(denominator) &lt; 1e-10:\n            raise ZeroDivisionError(\"Lines are parallel or collinear\")\n\n        px = (\n            (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n        py = (\n            (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n\n        # Check if the intersection point is within a reasonable distance\n        # from both line segments to avoid extreme extrapolation\n        def point_on_segment(x, y, x1, y1, x2, y2, tolerance=2.0):\n            # Check if point (x,y) is near the line segment from (x1,y1) to (x2,y2)\n            # First check if it's near the infinite line\n            line_len = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n            if line_len &lt; 1e-10:\n                return np.sqrt((x - x1) ** 2 + (y - y1) ** 2) &lt;= tolerance\n\n            t = ((x - x1) * (x2 - x1) + (y - y1) * (y2 - y1)) / (line_len**2)\n\n            # Check distance to the infinite line\n            proj_x = x1 + t * (x2 - x1)\n            proj_y = y1 + t * (y2 - y1)\n            dist_to_line = np.sqrt((x - proj_x) ** 2 + (y - proj_y) ** 2)\n\n            # Check if the projection is near the segment, not just the infinite line\n            if t &lt; -tolerance or t &gt; 1 + tolerance:\n                # If far from the segment, compute distance to the nearest endpoint\n                dist_to_start = np.sqrt((x - x1) ** 2 + (y - y1) ** 2)\n                dist_to_end = np.sqrt((x - x2) ** 2 + (y - y2) ** 2)\n                return min(dist_to_start, dist_to_end) &lt;= tolerance * 2\n\n            return dist_to_line &lt;= tolerance\n\n        # Check if intersection is reasonably close to both line segments\n        if not (\n            point_on_segment(px, py, x1, y1, x2, y2)\n            and point_on_segment(px, py, x3, y3, x4, y4)\n        ):\n            # If intersection is far from segments, it's probably extrapolating too much\n            raise ValueError(\"Intersection point too far from line segments\")\n\n        return [px, py]\n\n    def join_ring(merged_df):\n        \"\"\"\n        Join line segments to form a closed ring.\n\n        Args:\n            merged_df (pandas.DataFrame): DataFrame with merged line segments\n\n        Returns:\n            numpy.ndarray: Array of shape (1, n, 2) containing the ring coordinates\n        \"\"\"\n        # Handle edge cases\n        if len(merged_df) &lt; 3:\n            # Not enough segments to form a valid polygon\n            return np.array([[]])\n\n        ring = []\n\n        # Find intersections between adjacent line segments\n        for i in range(len(merged_df) - 1):\n            x1, y1, x2, y2, *_ = merged_df.iloc[i]\n            x3, y3, x4, y4, *_ = merged_df.iloc[i + 1]\n\n            try:\n                intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n                # Check if the intersection point is too far from either line segment\n                # This helps prevent extending edges beyond reasonable bounds\n                dist_to_seg1 = min(\n                    np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                    np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n                )\n                dist_to_seg2 = min(\n                    np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                    np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n                )\n\n                # Use the maximum of line segment lengths as a reference\n                max_len = max(\n                    np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                    np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n                )\n\n                # Improved intersection validation\n                # Calculate angle between segments to detect sharp corners\n                v1 = np.array([x2 - x1, y2 - y1])\n                v2 = np.array([x4 - x3, y4 - y3])\n                v1_norm = np.linalg.norm(v1)\n                v2_norm = np.linalg.norm(v2)\n\n                if v1_norm &gt; 0 and v2_norm &gt; 0:\n                    cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                    cos_angle = np.clip(cos_angle, -1, 1)\n                    angle = np.arccos(cos_angle)\n\n                    # Check for very sharp angles that could create triangular artifacts\n                    is_sharp_angle = (\n                        angle &lt; np.pi / 6 or angle &gt; 5 * np.pi / 6\n                    )  # &lt;30\u00b0 or &gt;150\u00b0\n                else:\n                    is_sharp_angle = False\n\n                # Determine whether to use intersection or segment endpoint\n                if (\n                    dist_to_seg1 &gt; max_len * 0.5\n                    or dist_to_seg2 &gt; max_len * 0.5\n                    or is_sharp_angle\n                ):\n                    # Use a more conservative approach for problematic intersections\n                    # Use the closer endpoint between segments\n                    dist_x2_to_seg2 = min(\n                        np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2),\n                        np.sqrt((x2 - x4) ** 2 + (y2 - y4) ** 2),\n                    )\n                    dist_x3_to_seg1 = min(\n                        np.sqrt((x3 - x1) ** 2 + (y3 - y1) ** 2),\n                        np.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2),\n                    )\n\n                    if dist_x2_to_seg2 &lt;= dist_x3_to_seg1:\n                        ring.append([x2, y2])\n                    else:\n                        ring.append([x3, y3])\n                else:\n                    ring.append(intersection)\n            except Exception:\n                # If intersection calculation fails, use the endpoint of the first segment\n                ring.append([x2, y2])\n\n        # Connect last segment with first segment\n        x1, y1, x2, y2, *_ = merged_df.iloc[-1]\n        x3, y3, x4, y4, *_ = merged_df.iloc[0]\n\n        try:\n            intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n            # Check if the intersection point is too far from either line segment\n            dist_to_seg1 = min(\n                np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n            )\n            dist_to_seg2 = min(\n                np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n            )\n\n            max_len = max(\n                np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n            )\n\n            # Apply same sharp angle detection for closing segment\n            v1 = np.array([x2 - x1, y2 - y1])\n            v2 = np.array([x4 - x3, y4 - y3])\n            v1_norm = np.linalg.norm(v1)\n            v2_norm = np.linalg.norm(v2)\n\n            if v1_norm &gt; 0 and v2_norm &gt; 0:\n                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                cos_angle = np.clip(cos_angle, -1, 1)\n                angle = np.arccos(cos_angle)\n                is_sharp_angle = angle &lt; np.pi / 6 or angle &gt; 5 * np.pi / 6\n            else:\n                is_sharp_angle = False\n\n            if (\n                dist_to_seg1 &gt; max_len * 0.5\n                or dist_to_seg2 &gt; max_len * 0.5\n                or is_sharp_angle\n            ):\n                # Use conservative approach for closing segment\n                dist_x2_to_seg2 = min(\n                    np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2),\n                    np.sqrt((x2 - x4) ** 2 + (y2 - y4) ** 2),\n                )\n                dist_x3_to_seg1 = min(\n                    np.sqrt((x3 - x1) ** 2 + (y3 - y1) ** 2),\n                    np.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2),\n                )\n\n                if dist_x2_to_seg2 &lt;= dist_x3_to_seg1:\n                    ring.append([x2, y2])\n                else:\n                    ring.append([x3, y3])\n            else:\n                ring.append(intersection)\n        except Exception:\n            # If intersection calculation fails, use the endpoint of the last segment\n            ring.append([x2, y2])\n\n        # Ensure the ring is closed\n        if len(ring) &gt; 0 and (ring[0][0] != ring[-1][0] or ring[0][1] != ring[-1][1]):\n            ring.append(ring[0])\n\n        return np.array([ring])\n\n    def rdp(M, epsilon=0, dist=None, algo=\"iter\", return_mask=False):\n        \"\"\"\n        Simplifies a given array of points using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float, optional): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            algo (str, optional): Algorithm to use ('iter' or 'rec')\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray or list: Simplified points or mask\n        \"\"\"\n        if dist is None:\n            dist = pldist\n\n        if algo == \"iter\":\n            algo = partial(rdp_iter, return_mask=return_mask)\n        elif algo == \"rec\":\n            if return_mask:\n                raise NotImplementedError(\n                    'return_mask=True not supported with algo=\"rec\"'\n                )\n            algo = rdp_rec\n\n        if \"numpy\" in str(type(M)):\n            return algo(M, epsilon, dist)\n\n        return algo(np.array(M), epsilon, dist).tolist()\n\n    def pldist(point, start, end):\n        \"\"\"\n        Calculates the distance from 'point' to the line given by 'start' and 'end'.\n\n        Args:\n            point (numpy.ndarray): Point coordinates\n            start (numpy.ndarray): Start point of the line\n            end (numpy.ndarray): End point of the line\n\n        Returns:\n            float: Distance from point to line\n        \"\"\"\n        if np.all(np.equal(start, end)):\n            return np.linalg.norm(point - start)\n\n        # Fix for NumPy 2.0 deprecation warning - handle 2D vectors properly\n        # Instead of using cross product directly, calculate the area of the\n        # parallelogram formed by the vectors and divide by the length of the line\n        line_vec = end - start\n        point_vec = point - start\n\n        # Area of parallelogram = |a|*|b|*sin(\u03b8)\n        # For 2D vectors: |a\u00d7b| = |a|*|b|*sin(\u03b8) = determinant([ax, ay], [bx, by])\n        area = abs(line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0])\n\n        # Distance = Area / |line_vec|\n        return area / np.linalg.norm(line_vec)\n\n    def rdp_rec(M, epsilon, dist=pldist):\n        \"\"\"\n        Recursive implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Simplified points\n        \"\"\"\n        dmax = 0.0\n        index = -1\n\n        for i in range(1, M.shape[0]):\n            d = dist(M[i], M[0], M[-1])\n\n            if d &gt; dmax:\n                index = i\n                dmax = d\n\n        if dmax &gt; epsilon:\n            r1 = rdp_rec(M[: index + 1], epsilon, dist)\n            r2 = rdp_rec(M[index:], epsilon, dist)\n\n            return np.vstack((r1[:-1], r2))\n        else:\n            return np.vstack((M[0], M[-1]))\n\n    def _rdp_iter(M, start_index, last_index, epsilon, dist=pldist):\n        \"\"\"\n        Internal iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            start_index (int): Start index\n            last_index (int): Last index\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Boolean mask of points to keep\n        \"\"\"\n        stk = []\n        stk.append([start_index, last_index])\n        global_start_index = start_index\n        indices = np.ones(last_index - start_index + 1, dtype=bool)\n\n        while stk:\n            start_index, last_index = stk.pop()\n\n            dmax = 0.0\n            index = start_index\n\n            for i in range(index + 1, last_index):\n                if indices[i - global_start_index]:\n                    d = dist(M[i], M[start_index], M[last_index])\n                    if d &gt; dmax:\n                        index = i\n                        dmax = d\n\n            if dmax &gt; epsilon:\n                stk.append([start_index, index])\n                stk.append([index, last_index])\n            else:\n                for i in range(start_index + 1, last_index):\n                    indices[i - global_start_index] = False\n\n        return indices\n\n    def rdp_iter(M, epsilon, dist=pldist, return_mask=False):\n        \"\"\"\n        Iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray: Simplified points or boolean mask\n        \"\"\"\n        mask = _rdp_iter(M, 0, len(M) - 1, epsilon, dist)\n\n        if return_mask:\n            return mask\n\n        return M[mask]\n\n    # Read the raster data\n    with rasterio.open(input_path) as src:\n        # Read the first band (assuming it contains the mask)\n        mask = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Extract shapes from the raster mask\n        shapes = list(features.shapes(mask, transform=transform))\n\n        # Initialize progress bar\n        print(f\"Processing {len(shapes)} features...\")\n\n        # Convert shapes to GeoJSON features\n        features_list = []\n        for shape, value in tqdm(shapes, desc=\"Converting features\", unit=\"shape\"):\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                # Convert GeoJSON geometry to Shapely polygon\n                polygon = Polygon(shape[\"coordinates\"][0])\n\n                # Skip tiny polygons\n                if polygon.area &lt; min_area:\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n                    continue\n\n                # Check if shape is triangular and if we want to avoid triangular shapes\n                if detect_triangles:\n                    # Create a simplified version to check number of vertices\n                    simple_polygon = polygon.simplify(epsilon)\n                    if (\n                        len(simple_polygon.exterior.coords) &lt;= 4\n                    ):  # 3 points + closing point\n                        # Likely a triangular shape - skip orthogonalization\n                        features_list.append(\n                            {\n                                \"type\": \"Feature\",\n                                \"properties\": {\"value\": int(value)},\n                                \"geometry\": shape,\n                            }\n                        )\n                        continue\n\n                # Process larger, non-triangular polygons\n                try:\n                    # Convert shapely polygon to a ring format for orthogonalization\n                    exterior_ring = list(polygon.exterior.coords)\n                    interior_rings = [\n                        list(interior.coords) for interior in polygon.interiors\n                    ]\n\n                    # Calculate bounding box aspect ratio to help with parameter tuning\n                    minx, miny, maxx, maxy = polygon.bounds\n                    width = maxx - minx\n                    height = maxy - miny\n                    aspect_ratio = max(width, height) / max(1.0, min(width, height))\n\n                    # Determine if this shape is likely to be a building/rectangular object\n                    # Long thin objects might require different treatment\n                    is_rectangular = aspect_ratio &lt; 3.0\n\n                    # Rectangular objects usually need more careful orthogonalization\n                    epsilon_adjusted = epsilon\n                    min_segments_adjusted = min_segments\n\n                    if is_rectangular:\n                        # For rectangular objects, use more conservative epsilon\n                        epsilon_adjusted = epsilon * 0.75\n                        # Ensure we get at least 4 points for a proper rectangle\n                        min_segments_adjusted = max(4, min_segments)\n\n                    # Orthogonalize the exterior and interior rings\n                    orthogonalized_exterior = orthogonalize_ring(\n                        exterior_ring,\n                        epsilon=epsilon_adjusted,\n                        min_segments=min_segments_adjusted,\n                    )\n\n                    orthogonalized_interiors = [\n                        orthogonalize_ring(\n                            ring,\n                            epsilon=epsilon_adjusted,\n                            min_segments=min_segments_adjusted,\n                        )\n                        for ring in interior_rings\n                    ]\n\n                    # Validate the result - calculate area change\n                    original_area = polygon.area\n                    orthogonalized_poly = Polygon(orthogonalized_exterior)\n\n                    if orthogonalized_poly.is_valid:\n                        area_ratio = (\n                            orthogonalized_poly.area / original_area\n                            if original_area &gt; 0\n                            else 0\n                        )\n\n                        # If area changed too much, revert to original\n                        if area_ratio &lt; area_tolerance or area_ratio &gt; (\n                            1.0 / area_tolerance\n                        ):\n                            # Use original polygon instead\n                            geometry = shape\n                        else:\n                            # Create a new geometry with orthogonalized rings\n                            geometry = {\n                                \"type\": \"Polygon\",\n                                \"coordinates\": [orthogonalized_exterior],\n                            }\n\n                            # Add interior rings if they exist\n                            if orthogonalized_interiors:\n                                geometry[\"coordinates\"].extend(\n                                    [ring for ring in orthogonalized_interiors]\n                                )\n                    else:\n                        # If resulting polygon is invalid, use original\n                        geometry = shape\n\n                    # Add the feature to the list\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": geometry,\n                        }\n                    )\n                except Exception as e:\n                    # Keep the original shape if orthogonalization fails\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n\n        # Create the final GeoJSON structure\n        geojson = {\n            \"type\": \"FeatureCollection\",\n            \"crs\": {\"type\": \"name\", \"properties\": {\"name\": str(crs)}},\n            \"features\": features_list,\n        }\n\n        # Convert to GeoDataFrame and set the CRS\n        gdf = gpd.GeoDataFrame.from_features(geojson[\"features\"], crs=crs)\n\n        # Save to file if output_path is provided\n        if output_path:\n            print(f\"Saving to {output_path}...\")\n            gdf.to_file(output_path)\n            print(\"Done!\")\n\n        return gdf\n</code></pre>"},{"location":"sam/#geoai.sam.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n\n    try:\n        from torchgeo.datasets import unbind_samples\n    except ImportError as e:\n        raise ImportError(\n            \"Your torchgeo version is too old. Please upgrade to the latest version using 'pip install -U torchgeo'.\"\n        )\n\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"sam/#geoai.sam.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"sam/#geoai.sam.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"sam/#geoai.sam.plot_performance_metrics","title":"<code>plot_performance_metrics(history_path, figsize=(15, 5), verbose=True, save_path=None, kwargs=None)</code>","text":"<p>Plot performance metrics from a history object.</p> <p>Parameters:</p> Name Type Description Default <code>history_path</code> <code>str</code> <p>The history object to plot.</p> required <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size.</p> <code>(15, 5)</code> <code>verbose</code> <code>bool</code> <p>Whether to print the best and final metrics.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def plot_performance_metrics(\n    history_path: str,\n    figsize: Tuple[int, int] = (15, 5),\n    verbose: bool = True,\n    save_path: Optional[str] = None,\n    kwargs: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"Plot performance metrics from a history object.\n\n    Args:\n        history_path: The history object to plot.\n        figsize: The figure size.\n        verbose: Whether to print the best and final metrics.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    history = torch.load(history_path)\n\n    # Handle different key naming conventions\n    train_loss_key = \"train_losses\" if \"train_losses\" in history else \"train_loss\"\n    val_loss_key = \"val_losses\" if \"val_losses\" in history else \"val_loss\"\n    val_iou_key = \"val_ious\" if \"val_ious\" in history else \"val_iou\"\n    val_dice_key = \"val_dices\" if \"val_dices\" in history else \"val_dice\"\n\n    # Determine number of subplots based on available metrics\n    has_dice = val_dice_key in history\n    n_plots = 3 if has_dice else 2\n    figsize = (15, 5) if has_dice else (10, 5)\n\n    plt.figure(figsize=figsize)\n\n    # Plot loss\n    plt.subplot(1, n_plots, 1)\n    if train_loss_key in history:\n        plt.plot(history[train_loss_key], label=\"Train Loss\")\n    if val_loss_key in history:\n        plt.plot(history[val_loss_key], label=\"Val Loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot IoU\n    plt.subplot(1, n_plots, 2)\n    if val_iou_key in history:\n        plt.plot(history[val_iou_key], label=\"Val IoU\")\n    plt.title(\"IoU Score\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"IoU\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Dice if available\n    if has_dice:\n        plt.subplot(1, n_plots, 3)\n        plt.plot(history[val_dice_key], label=\"Val Dice\")\n        plt.title(\"Dice Score\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dice\")\n        plt.legend()\n        plt.grid(True)\n\n    plt.tight_layout()\n\n    if save_path:\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 150\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n        plt.savefig(save_path, **kwargs)\n\n    plt.show()\n\n    if verbose:\n        if val_iou_key in history:\n            print(f\"Best IoU: {max(history[val_iou_key]):.4f}\")\n            print(f\"Final IoU: {history[val_iou_key][-1]:.4f}\")\n        if val_dice_key in history:\n            print(f\"Best Dice: {max(history[val_dice_key]):.4f}\")\n            print(f\"Final Dice: {history[val_dice_key][-1]:.4f}\")\n</code></pre>"},{"location":"sam/#geoai.sam.plot_prediction_comparison","title":"<code>plot_prediction_comparison(original_image, prediction_image, ground_truth_image=None, titles=None, figsize=(15, 5), save_path=None, show_plot=True, prediction_colormap='gray', ground_truth_colormap='gray', original_colormap=None, indexes=None, divider=None)</code>","text":"<p>Plot original image, prediction, and optional ground truth side by side.</p> <p>Supports input as file paths, NumPy arrays, or PIL Images. For multi-band images, selected channels can be specified via <code>indexes</code>. If the image data is not normalized (e.g., Sentinel-2 [0, 10000]), the <code>divider</code> can be used to scale values for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>original_image</code> <code>Union[str, ndarray, Image]</code> <p>Original input image as a file path, NumPy array, or PIL Image.</p> required <code>prediction_image</code> <code>Union[str, ndarray, Image]</code> <p>Predicted segmentation mask image.</p> required <code>ground_truth_image</code> <code>Optional[Union[str, ndarray, Image]]</code> <p>Ground truth mask image. Defaults to None.</p> <code>None</code> <code>titles</code> <code>Optional[List[str]]</code> <p>List of titles for the subplots. If not provided, default titles are used.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Size of the entire figure in inches. Defaults to (15, 5).</p> <code>(15, 5)</code> <code>save_path</code> <code>Optional[str]</code> <p>If specified, saves the figure to this path. Defaults to None.</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Whether to display the figure using plt.show(). Defaults to True.</p> <code>True</code> <code>prediction_colormap</code> <code>str</code> <p>Colormap to use for the prediction mask. Defaults to \"gray\".</p> <code>'gray'</code> <code>ground_truth_colormap</code> <code>str</code> <p>Colormap to use for the ground truth mask. Defaults to \"gray\".</p> <code>'gray'</code> <code>original_colormap</code> <code>Optional[str]</code> <p>Colormap to use for the original image if it's grayscale. Defaults to None.</p> <code>None</code> <code>indexes</code> <code>Optional[List[int]]</code> <p>List of band/channel indexes (0-based for NumPy, 1-based for rasterio) to extract from the original image. Useful for multi-band imagery like Sentinel-2. Defaults to None.</p> <code>None</code> <code>divider</code> <code>Optional[float]</code> <p>Value to divide the original image by for normalization (e.g., 10000 for reflectance). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>matplotlib.figure.Figure: The generated matplotlib figure object.</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_prediction_comparison(\n    original_image: Union[str, np.ndarray, Image.Image],\n    prediction_image: Union[str, np.ndarray, Image.Image],\n    ground_truth_image: Optional[Union[str, np.ndarray, Image.Image]] = None,\n    titles: Optional[List[str]] = None,\n    figsize: Tuple[int, int] = (15, 5),\n    save_path: Optional[str] = None,\n    show_plot: bool = True,\n    prediction_colormap: str = \"gray\",\n    ground_truth_colormap: str = \"gray\",\n    original_colormap: Optional[str] = None,\n    indexes: Optional[List[int]] = None,\n    divider: Optional[float] = None,\n) -&gt; None:\n    \"\"\"Plot original image, prediction, and optional ground truth side by side.\n\n    Supports input as file paths, NumPy arrays, or PIL Images. For multi-band\n    images, selected channels can be specified via `indexes`. If the image data\n    is not normalized (e.g., Sentinel-2 [0, 10000]), the `divider` can be used\n    to scale values for visualization.\n\n    Args:\n        original_image (Union[str, np.ndarray, Image.Image]):\n            Original input image as a file path, NumPy array, or PIL Image.\n        prediction_image (Union[str, np.ndarray, Image.Image]):\n            Predicted segmentation mask image.\n        ground_truth_image (Optional[Union[str, np.ndarray, Image.Image]], optional):\n            Ground truth mask image. Defaults to None.\n        titles (Optional[List[str]], optional):\n            List of titles for the subplots. If not provided, default titles are used.\n        figsize (Tuple[int, int], optional):\n            Size of the entire figure in inches. Defaults to (15, 5).\n        save_path (Optional[str], optional):\n            If specified, saves the figure to this path. Defaults to None.\n        show_plot (bool, optional):\n            Whether to display the figure using plt.show(). Defaults to True.\n        prediction_colormap (str, optional):\n            Colormap to use for the prediction mask. Defaults to \"gray\".\n        ground_truth_colormap (str, optional):\n            Colormap to use for the ground truth mask. Defaults to \"gray\".\n        original_colormap (Optional[str], optional):\n            Colormap to use for the original image if it's grayscale. Defaults to None.\n        indexes (Optional[List[int]], optional):\n            List of band/channel indexes (0-based for NumPy, 1-based for rasterio) to extract from the original image.\n            Useful for multi-band imagery like Sentinel-2. Defaults to None.\n        divider (Optional[float], optional):\n            Value to divide the original image by for normalization (e.g., 10000 for reflectance). Defaults to None.\n\n    Returns:\n        matplotlib.figure.Figure:\n            The generated matplotlib figure object.\n    \"\"\"\n\n    def _load_image(img_input, indexes=None):\n        \"\"\"Helper function to load image from various input types.\"\"\"\n        if isinstance(img_input, str):\n            if img_input.lower().endswith((\".tif\", \".tiff\")):\n                with rasterio.open(img_input) as src:\n                    if indexes:\n                        img = src.read(indexes)  # 1-based\n                        img = (\n                            np.transpose(img, (1, 2, 0)) if len(indexes) &gt; 1 else img[0]\n                        )\n                    else:\n                        img = src.read()\n                        if img.shape[0] == 1:\n                            img = img[0]\n                        else:\n                            img = np.transpose(img, (1, 2, 0))\n            else:\n                img = np.array(Image.open(img_input))\n        elif isinstance(img_input, Image.Image):\n            img = np.array(img_input)\n        elif isinstance(img_input, np.ndarray):\n            img = img_input\n            if indexes is not None and img.ndim == 3:\n                img = img[:, :, indexes]\n        else:\n            raise ValueError(f\"Unsupported image type: {type(img_input)}\")\n        return img\n\n    # Load images\n    original = _load_image(original_image, indexes=indexes)\n    prediction = _load_image(prediction_image)\n    ground_truth = (\n        _load_image(ground_truth_image) if ground_truth_image is not None else None\n    )\n\n    # Apply divider normalization if requested\n    if divider is not None and isinstance(original, np.ndarray) and original.ndim == 3:\n        original = np.clip(original.astype(np.float32) / divider, 0, 1)\n\n    # Determine layout\n    num_plots = 3 if ground_truth is not None else 2\n    fig, axes = plt.subplots(1, num_plots, figsize=figsize)\n    if num_plots == 2:\n        axes = [axes[0], axes[1]]\n\n    if titles is None:\n        titles = [\"Original Image\", \"Prediction\"]\n        if ground_truth is not None:\n            titles.append(\"Ground Truth\")\n\n    # Plot original\n    if original.ndim == 3 and original.shape[2] in [3, 4]:\n        axes[0].imshow(original)\n    else:\n        axes[0].imshow(original, cmap=original_colormap)\n    axes[0].set_title(titles[0])\n    axes[0].axis(\"off\")\n\n    # Prediction\n    axes[1].imshow(prediction, cmap=prediction_colormap)\n    axes[1].set_title(titles[1])\n    axes[1].axis(\"off\")\n\n    # Ground truth\n    if ground_truth is not None:\n        axes[2].imshow(ground_truth, cmap=ground_truth_colormap)\n        axes[2].set_title(titles[2])\n        axes[2].axis(\"off\")\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Plot saved to: {save_path}\")\n\n    if show_plot:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"sam/#geoai.sam.print_raster_info","title":"<code>print_raster_info(raster_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a raster dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the raster. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing raster information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_raster_info(\n    raster_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a raster dataset and optionally show a preview.\n\n    Args:\n        raster_path (str): Path to the raster file\n        show_preview (bool, optional): Whether to display a visual preview of the raster.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing raster information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_raster_info(raster_path)\n\n        # Print basic information\n        print(f\"===== RASTER INFORMATION: {raster_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Dimensions: {info['width']} x {info['height']} pixels\")\n        print(f\"Number of bands: {info['count']}\")\n        print(f\"Data type: {info['dtype']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Georeferenced Bounds: {info['bounds']}\")\n        print(f\"Pixel Resolution: {info['resolution'][0]}, {info['resolution'][1]}\")\n        print(f\"NoData Value: {info['nodata']}\")\n\n        # Print band statistics\n        print(\"\\n----- Band Statistics -----\")\n        for band_stat in info[\"band_stats\"]:\n            print(f\"Band {band_stat['band']}:\")\n            print(f\"  Min: {band_stat['min']:.2f}\")\n            print(f\"  Max: {band_stat['max']:.2f}\")\n            print(f\"  Mean: {band_stat['mean']:.2f}\")\n            print(f\"  Std Dev: {band_stat['std']:.2f}\")\n\n        # Show a preview if requested\n        if show_preview:\n            with rasterio.open(raster_path) as src:\n                # For multi-band images, show RGB composite or first band\n                if src.count &gt;= 3:\n                    # Try to show RGB composite\n                    rgb = np.dstack([src.read(i) for i in range(1, 4)])\n                    plt.figure(figsize=figsize)\n                    plt.imshow(rgb)\n                    plt.title(f\"RGB Preview: {raster_path}\")\n                else:\n                    # Show first band for single-band images\n                    plt.figure(figsize=figsize)\n                    show(\n                        src.read(1),\n                        cmap=\"viridis\",\n                        title=f\"Band 1 Preview: {raster_path}\",\n                    )\n                    plt.colorbar(label=\"Pixel Value\")\n                plt.show()\n\n    except Exception as e:\n        print(f\"Error reading raster: {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.print_vector_info","title":"<code>print_vector_info(vector_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a vector dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the vector data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing vector information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_vector_info(\n    vector_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a vector dataset and optionally show a preview.\n\n    Args:\n        vector_path (str): Path to the vector file\n        show_preview (bool, optional): Whether to display a visual preview of the vector data.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing vector information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_vector_info(vector_path)\n\n        # Print basic information\n        print(f\"===== VECTOR INFORMATION: {vector_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Feature count: {info['feature_count']}\")\n        print(f\"Geometry types: {info['geometry_type']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Bounds: {info['bounds']}\")\n        print(f\"Number of attributes: {info['attribute_count']}\")\n        print(f\"Attribute names: {', '.join(info['attribute_names'])}\")\n\n        # Print attribute statistics\n        if info[\"attribute_stats\"]:\n            print(\"\\n----- Attribute Statistics -----\")\n            for attr, stats in info[\"attribute_stats\"].items():\n                print(f\"Attribute: {attr}\")\n                for stat_name, stat_value in stats.items():\n                    print(\n                        f\"  {stat_name}: {stat_value:.4f}\"\n                        if isinstance(stat_value, float)\n                        else f\"  {stat_name}: {stat_value}\"\n                    )\n\n        # Show a preview if requested\n        if show_preview:\n            gdf = (\n                gpd.read_parquet(vector_path)\n                if vector_path.endswith(\".parquet\")\n                else gpd.read_file(vector_path)\n            )\n            fig, ax = plt.subplots(figsize=figsize)\n            gdf.plot(ax=ax, cmap=\"viridis\")\n            ax.set_title(f\"Preview: {vector_path}\")\n            plt.tight_layout()\n            plt.show()\n\n            # # Show a sample of the attribute table\n            # if not gdf.empty:\n            #     print(\"\\n----- Sample of attribute table (first 5 rows) -----\")\n            #     print(gdf.head().to_string())\n\n    except Exception as e:\n        print(f\"Error reading vector data: {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.raster_to_vector","title":"<code>raster_to_vector(raster_path, output_path=None, threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', unique_attribute_value=False, output_format='geojson', plot_result=False)</code>","text":"<p>Convert a raster label mask to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file (e.g., GeoTIFF).</p> required <code>output_path</code> <code>str</code> <p>Path to save the output vector file. If None, returns GeoDataFrame without saving.</p> <code>None</code> <code>threshold</code> <code>int / float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>unique_attribute_value</code> <code>bool</code> <p>Whether to generate unique values for each shape within a class.</p> <code>False</code> <code>output_format</code> <code>str</code> <p>Format for output file - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting polygons overlaid on the raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector(\n    raster_path: str,\n    output_path: Optional[str] = None,\n    threshold: float = 0,\n    min_area: float = 10,\n    simplify_tolerance: Optional[float] = None,\n    class_values: Optional[List[int]] = None,\n    attribute_name: str = \"class\",\n    unique_attribute_value: bool = False,\n    output_format: str = \"geojson\",\n    plot_result: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a raster label mask to vector polygons.\n\n    Args:\n        raster_path (str): Path to the input raster file (e.g., GeoTIFF).\n        output_path (str): Path to save the output vector file. If None, returns GeoDataFrame without saving.\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        unique_attribute_value (bool): Whether to generate unique values for each shape within a class.\n        output_format (str): Format for output file - 'geojson', 'shapefile', 'gpkg'.\n        plot_result (bool): Whether to plot the resulting polygons overlaid on the raster.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.\n    \"\"\"\n    # Open the raster file\n    with rasterio.open(raster_path) as src:\n        # Read the data\n        data = src.read(1)\n\n        # Get metadata\n        transform = src.transform\n        crs = src.crs\n\n        # Create mask based on threshold and class values\n        if class_values is not None:\n            # Create a mask for each specified class value\n            masks = {val: (data == val) for val in class_values}\n        else:\n            # Create a mask for values above threshold\n            masks = {1: (data &gt; threshold)}\n            class_values = [1]  # Default class\n\n        # Initialize list to store features\n        all_features = []\n\n        # Process each class value\n        for class_val in class_values:\n            mask = masks[class_val]\n            shape_count = 1\n            # Vectorize the mask\n            for geom, value in features.shapes(\n                mask.astype(np.uint8), mask=mask, transform=transform\n            ):\n                # Convert to shapely geometry\n                geom = shape(geom)\n\n                # Skip small polygons\n                if geom.area &lt; min_area:\n                    continue\n\n                # Simplify geometry if requested\n                if simplify_tolerance is not None:\n                    geom = geom.simplify(simplify_tolerance)\n\n                # Add to features list with class value\n                if unique_attribute_value:\n                    all_features.append(\n                        {\"geometry\": geom, attribute_name: class_val * shape_count}\n                    )\n                else:\n                    all_features.append({\"geometry\": geom, attribute_name: class_val})\n\n                shape_count += 1\n\n        # Create GeoDataFrame\n        if all_features:\n            gdf = gpd.GeoDataFrame(all_features, crs=crs)\n        else:\n            print(\"Warning: No features were extracted from the raster.\")\n            # Return empty GeoDataFrame with correct CRS\n            gdf = gpd.GeoDataFrame([], geometry=[], crs=crs)\n\n        # Save to file if requested\n        if output_path is not None:\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n            # Save to file based on format\n            if output_format.lower() == \"geojson\":\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            elif output_format.lower() == \"shapefile\":\n                gdf.to_file(output_path)\n            elif output_format.lower() == \"gpkg\":\n                gdf.to_file(output_path, driver=\"GPKG\")\n            else:\n                raise ValueError(f\"Unsupported output format: {output_format}\")\n\n            print(f\"Vectorized data saved to {output_path}\")\n\n        # Plot result if requested\n        if plot_result:\n            fig, ax = plt.subplots(figsize=(12, 12))\n\n            # Plot raster\n            raster_img = src.read()\n            if raster_img.shape[0] == 1:\n                plt.imshow(raster_img[0], cmap=\"viridis\", alpha=0.7)\n            else:\n                # Use first 3 bands for RGB display\n                rgb = raster_img[:3].transpose(1, 2, 0)\n                # Normalize for display\n                rgb = np.clip(rgb / rgb.max(), 0, 1)\n                plt.imshow(rgb)\n\n            # Plot vector boundaries\n            if not gdf.empty:\n                gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n\n            plt.title(\"Raster with Vectorized Boundaries\")\n            plt.axis(\"off\")\n            plt.tight_layout()\n            plt.show()\n\n        return gdf\n</code></pre>"},{"location":"sam/#geoai.sam.raster_to_vector_batch","title":"<code>raster_to_vector_batch(input_dir, output_dir, pattern='*.tif', threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', output_format='geojson', merge_output=False, merge_filename='merged_vectors')</code>","text":"<p>Batch convert multiple raster files to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input raster files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output vector files.</p> required <code>pattern</code> <code>str</code> <p>Pattern to match raster files (e.g., '*.tif').</p> <code>'*.tif'</code> <code>threshold</code> <code>int / float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>output_format</code> <code>str</code> <p>Format for output files - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>merge_output</code> <code>bool</code> <p>Whether to merge all output vectors into a single file.</p> <code>False</code> <code>merge_filename</code> <code>str</code> <p>Filename for the merged output (without extension).</p> <code>'merged_vectors'</code> <p>Returns:</p> Type Description <code>Optional[GeoDataFrame]</code> <p>geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector_batch(\n    input_dir: str,\n    output_dir: str,\n    pattern: str = \"*.tif\",\n    threshold: float = 0,\n    min_area: float = 10,\n    simplify_tolerance: Optional[float] = None,\n    class_values: Optional[List[int]] = None,\n    attribute_name: str = \"class\",\n    output_format: str = \"geojson\",\n    merge_output: bool = False,\n    merge_filename: str = \"merged_vectors\",\n) -&gt; Optional[gpd.GeoDataFrame]:\n    \"\"\"\n    Batch convert multiple raster files to vector polygons.\n\n    Args:\n        input_dir (str): Directory containing input raster files.\n        output_dir (str): Directory to save output vector files.\n        pattern (str): Pattern to match raster files (e.g., '*.tif').\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        output_format (str): Format for output files - 'geojson', 'shapefile', 'gpkg'.\n        merge_output (bool): Whether to merge all output vectors into a single file.\n        merge_filename (str): Filename for the merged output (without extension).\n\n    Returns:\n        geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.\n    \"\"\"\n    import glob\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get list of raster files\n    raster_files = glob.glob(os.path.join(input_dir, pattern))\n\n    if not raster_files:\n        print(f\"No files matching pattern '{pattern}' found in {input_dir}\")\n        return None\n\n    print(f\"Found {len(raster_files)} raster files to process\")\n\n    # Process each raster file\n    gdfs = []\n    for raster_file in tqdm(raster_files, desc=\"Processing rasters\"):\n        # Get output filename\n        base_name = os.path.splitext(os.path.basename(raster_file))[0]\n        if output_format.lower() == \"geojson\":\n            out_file = os.path.join(output_dir, f\"{base_name}.geojson\")\n        elif output_format.lower() == \"shapefile\":\n            out_file = os.path.join(output_dir, f\"{base_name}.shp\")\n        elif output_format.lower() == \"gpkg\":\n            out_file = os.path.join(output_dir, f\"{base_name}.gpkg\")\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        # Convert raster to vector\n        if merge_output:\n            # Don't save individual files if merging\n            gdf = raster_to_vector(\n                raster_file,\n                output_path=None,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n            )\n\n            # Add filename as attribute\n            if not gdf.empty:\n                gdf[\"source_file\"] = base_name\n                gdfs.append(gdf)\n        else:\n            # Save individual files\n            raster_to_vector(\n                raster_file,\n                output_path=out_file,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n                output_format=output_format,\n            )\n\n    # Merge output if requested\n    if merge_output and gdfs:\n        merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n\n        # Set CRS to the CRS of the first GeoDataFrame\n        if merged_gdf.crs is None and gdfs:\n            merged_gdf.crs = gdfs[0].crs\n\n        # Save merged output\n        if output_format.lower() == \"geojson\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.geojson\")\n            merged_gdf.to_file(merged_file, driver=\"GeoJSON\")\n        elif output_format.lower() == \"shapefile\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.shp\")\n            merged_gdf.to_file(merged_file)\n        elif output_format.lower() == \"gpkg\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.gpkg\")\n            merged_gdf.to_file(merged_file, driver=\"GPKG\")\n\n        print(f\"Merged vector data saved to {merged_file}\")\n        return merged_gdf\n\n    return None\n</code></pre>"},{"location":"sam/#geoai.sam.read_raster","title":"<code>read_raster(source, band=None, masked=True, **kwargs)</code>","text":"<p>Reads raster data from various formats using rioxarray.</p> <p>This function reads raster data from local files or URLs into a rioxarray data structure with preserved geospatial metadata.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>String path to the raster file or URL.</p> required <code>band</code> <code>Optional[Union[int, List[int]]]</code> <p>Integer or list of integers specifying which band(s) to read. Defaults to None (all bands).</p> <code>None</code> <code>masked</code> <code>bool</code> <p>Boolean indicating whether to mask nodata values. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rioxarray.open_rasterio.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xarray.DataArray: A DataArray containing the raster data with geospatial metadata preserved.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local GeoTIFF</p> <pre><code>&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n&gt;&gt;&gt;\nRead only band 1 from a remote GeoTIFF\n&gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n&gt;&gt;&gt;\nRead a raster without masking nodata values\n&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_raster(\n    source: str,\n    band: Optional[Union[int, List[int]]] = None,\n    masked: bool = True,\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"Reads raster data from various formats using rioxarray.\n\n    This function reads raster data from local files or URLs into a rioxarray\n    data structure with preserved geospatial metadata.\n\n    Args:\n        source: String path to the raster file or URL.\n        band: Integer or list of integers specifying which band(s) to read.\n            Defaults to None (all bands).\n        masked: Boolean indicating whether to mask nodata values.\n            Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rioxarray.open_rasterio.\n\n    Returns:\n        xarray.DataArray: A DataArray containing the raster data with geospatial\n            metadata preserved.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n        &gt;&gt;&gt;\n        Read only band 1 from a remote GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n        &gt;&gt;&gt;\n        Read a raster without masking nodata values\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n    \"\"\"\n    import urllib.parse\n\n    from rasterio.errors import RasterioIOError\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"Raster file does not exist: {source}\")\n\n    try:\n        # Open the raster with rioxarray\n        raster = rxr.open_rasterio(source, masked=masked, **kwargs)\n\n        # Handle band selection if specified\n        if band is not None:\n            if isinstance(band, (list, tuple)):\n                # Convert from 1-based indexing to 0-based indexing\n                band_indices = [b - 1 for b in band]\n                raster = raster.isel(band=band_indices)\n            else:\n                # Single band selection (convert from 1-based to 0-based indexing)\n                raster = raster.isel(band=band - 1)\n\n        return raster\n\n    except RasterioIOError as e:\n        raise ValueError(f\"Could not read raster from source '{source}': {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading raster data: {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.read_vector","title":"<code>read_vector(source, layer=None, **kwargs)</code>","text":"<p>Reads vector data from various formats including GeoParquet.</p> <p>This function dynamically determines the file type based on extension and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>String path to the vector file or URL.</p> required <code>layer</code> <code>Optional[str]</code> <p>String or integer specifying which layer to read from multi-layer files (only applicable for formats like GPKG, GeoJSON, etc.). Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the underlying reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local shapefile</p> <pre><code>&gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n&gt;&gt;&gt;\nRead a GeoParquet file from URL\n&gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n&gt;&gt;&gt;\nRead a specific layer from a GeoPackage\n&gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_vector(\n    source: str, layer: Optional[str] = None, **kwargs: Any\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Reads vector data from various formats including GeoParquet.\n\n    This function dynamically determines the file type based on extension\n    and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.\n\n    Args:\n        source: String path to the vector file or URL.\n        layer: String or integer specifying which layer to read from multi-layer\n            files (only applicable for formats like GPKG, GeoJSON, etc.).\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the underlying reader.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local shapefile\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n        &gt;&gt;&gt;\n        Read a GeoParquet file from URL\n        &gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n        &gt;&gt;&gt;\n        Read a specific layer from a GeoPackage\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n    \"\"\"\n\n    import urllib.parse\n\n    import fiona\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"File does not exist: {source}\")\n\n    # Get file extension\n    _, ext = os.path.splitext(source)\n    ext = ext.lower()\n\n    # Handle GeoParquet files\n    if ext in [\".parquet\", \".pq\", \".geoparquet\"]:\n        return gpd.read_parquet(source, **kwargs)\n\n    # Handle common vector formats\n    if ext in [\".shp\", \".geojson\", \".json\", \".gpkg\", \".gml\", \".kml\", \".gpx\"]:\n        # For formats that might have multiple layers\n        if ext in [\".gpkg\", \".gml\"] and layer is not None:\n            return gpd.read_file(source, layer=layer, **kwargs)\n        return gpd.read_file(source, **kwargs)\n\n    # Try to use fiona to identify valid layers for formats that might have them\n    # Only attempt this for local files as fiona.listlayers might not work with URLs\n    if layer is None and ext in [\".gpkg\", \".gml\"] and not is_url:\n        try:\n            layers = fiona.listlayers(source)\n            if layers:\n                return gpd.read_file(source, layer=layers[0], **kwargs)\n        except Exception:\n            # If listing layers fails, we'll fall through to the generic read attempt\n            pass\n\n    # For other formats or when layer listing fails, attempt to read using GeoPandas\n    try:\n        return gpd.read_file(source, **kwargs)\n    except Exception as e:\n        raise ValueError(f\"Could not read from source '{source}': {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    import scipy.ndimage as ndi\n    from skimage import measure\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"axis_major_length\",\n            \"axis_minor_length\",\n            \"eccentricity\",\n            \"diameter_areagth\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"axis_major_length\"] / df[\"axis_minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n\n        if out_vector is not None:\n            tmp_raster = None\n            tmp_vector = None\n            try:\n                if out_image is None:\n                    tmp_raster = temp_file_path(\".tif\")\n                    da.rio.to_raster(tmp_raster, dtype=dtype)\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        tmp_raster,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                else:\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        out_image,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n            finally:\n                try:\n                    if tmp_raster is not None and os.path.exists(tmp_raster):\n                        os.remove(tmp_raster)\n                    if tmp_vector is not None and os.path.exists(tmp_vector):\n                        os.remove(tmp_vector)\n                except Exception as e:\n                    print(f\"Warning: Failed to delete temporary files: {str(e)}\")\n\n        return da, df\n</code></pre>"},{"location":"sam/#geoai.sam.regularization","title":"<code>regularization(building_polygons, angle_tolerance=10, simplify_tolerance=0.5, orthogonalize=True, preserve_topology=True)</code>","text":"<p>Regularizes building footprint polygons with multiple techniques beyond minimum rotated rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <code>angle_tolerance</code> <code>float</code> <p>Degrees within which angles will be regularized to 90/180 degrees</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Distance tolerance for Douglas-Peucker simplification</p> <code>0.5</code> <code>orthogonalize</code> <code>bool</code> <p>Whether to enforce orthogonal angles in the final polygons</p> <code>True</code> <code>preserve_topology</code> <code>bool</code> <p>Whether to preserve topology during simplification</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n    angle_tolerance: float = 10,\n    simplify_tolerance: float = 0.5,\n    orthogonalize: bool = True,\n    preserve_topology: bool = True,\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    Regularizes building footprint polygons with multiple techniques beyond minimum\n    rotated rectangles.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n        angle_tolerance: Degrees within which angles will be regularized to 90/180 degrees\n        simplify_tolerance: Distance tolerance for Douglas-Peucker simplification\n        orthogonalize: Whether to enforce orthogonal angles in the final polygons\n        preserve_topology: Whether to preserve topology during simplification\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely import wkt\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import Polygon, shape\n\n    regularized_buildings = []\n\n    # Check if we're dealing with a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    for building in geom_objects:\n        # Handle potential string representations of geometries\n        if isinstance(building, str):\n            try:\n                # Try to parse as WKT\n                building = wkt.loads(building)\n            except Exception:\n                print(f\"Failed to parse geometry string: {building[:30]}...\")\n                continue\n\n        # Ensure we have a valid geometry\n        if not hasattr(building, \"simplify\"):\n            print(f\"Invalid geometry type: {type(building)}\")\n            continue\n\n        # Step 1: Simplify to remove noise and small vertices\n        simplified = building.simplify(\n            simplify_tolerance, preserve_topology=preserve_topology\n        )\n\n        if orthogonalize:\n            # Make sure we have a valid polygon with an exterior\n            if not hasattr(simplified, \"exterior\") or simplified.exterior is None:\n                print(f\"Simplified geometry has no exterior: {simplified}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            # Step 2: Get the dominant angle to rotate building\n            coords = np.array(simplified.exterior.coords)\n\n            # Make sure we have enough coordinates for angle calculation\n            if len(coords) &lt; 3:\n                print(f\"Not enough coordinates for angle calculation: {len(coords)}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            segments = np.diff(coords, axis=0)\n            angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n            # Find most common angle classes (0, 90, 180, 270 degrees)\n            binned_angles = np.round(angles / 90) * 90\n            dominant_angle = np.bincount(binned_angles.astype(int) % 180).argmax()\n\n            # Step 3: Rotate to align with axes, regularize, then rotate back\n            rotated = rotate(simplified, -dominant_angle, origin=\"centroid\")\n\n            # Step 4: Rectify coordinates to enforce right angles\n            ext_coords = np.array(rotated.exterior.coords)\n            rect_coords = []\n\n            # Regularize each vertex to create orthogonal corners\n            for i in range(len(ext_coords) - 1):\n                rect_coords.append(ext_coords[i])\n\n                # Check if we need to add a right-angle vertex\n                angle = (\n                    np.arctan2(\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 1]\n                        - ext_coords[i, 1],\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 0]\n                        - ext_coords[i, 0],\n                    )\n                    * 180\n                    / np.pi\n                )\n\n                if abs(angle % 90) &gt; angle_tolerance and abs(angle % 90) &lt; (\n                    90 - angle_tolerance\n                ):\n                    # Add intermediate point to create right angle\n                    rect_coords.append(\n                        [\n                            ext_coords[(i + 1) % (len(ext_coords) - 1), 0],\n                            ext_coords[i, 1],\n                        ]\n                    )\n\n            # Close the polygon by adding the first point again\n            rect_coords.append(rect_coords[0])\n\n            # Create regularized polygon and rotate back\n            regularized = Polygon(rect_coords)\n            final_building = rotate(regularized, dominant_angle, origin=\"centroid\")\n        else:\n            final_building = simplified\n\n        regularized_buildings.append(final_building)\n\n    # If input was a GeoDataFrame, return a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        return gpd.GeoDataFrame(\n            geometry=regularized_buildings, crs=building_polygons.crs\n        )\n    else:\n        return regularized_buildings\n</code></pre>"},{"location":"sam/#geoai.sam.regularize","title":"<code>regularize(data, parallel_threshold=1.0, target_crs=None, simplify=True, simplify_tolerance=0.5, allow_45_degree=True, diagonal_threshold_reduction=15, allow_circles=True, circle_threshold=0.9, num_cores=1, include_metadata=False, output_path=None, **kwargs)</code>","text":"<p>Regularizes polygon geometries in a GeoDataFrame by aligning edges.</p> <p>Aligns edges to be parallel or perpendicular (optionally also 45 degrees) to their main direction. Handles reprojection, initial simplification, regularization, geometry cleanup, and parallel processing.</p> <p>This function is a wrapper around the <code>regularize_geodataframe</code> function from the <code>buildingregulariser</code> package. Credits to the original author Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[GeoDataFrame, str]</code> <p>Input GeoDataFrame with polygon or multipolygon geometries, or a file path to the GeoDataFrame.</p> required <code>parallel_threshold</code> <code>float</code> <p>Distance threshold for merging nearly parallel adjacent edges during regularization. Defaults to 1.0.</p> <code>1.0</code> <code>target_crs</code> <code>Optional[Union[str, CRS]]</code> <p>Target Coordinate Reference System for processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a projected CRS. Defaults to None.</p> <code>None</code> <code>simplify</code> <code>bool</code> <p>If True, applies initial simplification to the geometry before regularization. Defaults to True.</p> <code>True</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for the initial simplification step (if <code>simplify</code> is True). Also used for geometry cleanup steps. Defaults to 0.5.</p> <code>0.5</code> <code>allow_45_degree</code> <code>bool</code> <p>If True, allows edges to be oriented at 45-degree angles relative to the main direction during regularization. Defaults to True.</p> <code>True</code> <code>diagonal_threshold_reduction</code> <code>float</code> <p>Reduction factor in degrees to reduce the likelihood of diagonal edges being created. Larger values reduce the likelihood of diagonal edges. Defaults to 15.</p> <code>15</code> <code>allow_circles</code> <code>bool</code> <p>If True, attempts to detect polygons that are nearly circular and replaces them with perfect circles. Defaults to True.</p> <code>True</code> <code>circle_threshold</code> <code>float</code> <p>Intersection over Union (IoU) threshold used for circle detection (if <code>allow_circles</code> is True). Value between 0 and 1. Defaults to 0.9.</p> <code>0.9</code> <code>num_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. If 1, processing is done sequentially. Defaults to 1.</p> <code>1</code> <code>include_metadata</code> <code>bool</code> <p>If True, includes metadata about the regularization process in the output GeoDataFrame. Defaults to False.</p> <code>False</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoDataFrame. If None, the output is not saved. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>to_file</code> method when saving the output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are</p> <code>Any</code> <p>preserved. Geometries that failed processing might be dropped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularize(\n    data: Union[gpd.GeoDataFrame, str],\n    parallel_threshold: float = 1.0,\n    target_crs: Optional[Union[str, \"pyproj.CRS\"]] = None,\n    simplify: bool = True,\n    simplify_tolerance: float = 0.5,\n    allow_45_degree: bool = True,\n    diagonal_threshold_reduction: float = 15,\n    allow_circles: bool = True,\n    circle_threshold: float = 0.9,\n    num_cores: int = 1,\n    include_metadata: bool = False,\n    output_path: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Regularizes polygon geometries in a GeoDataFrame by aligning edges.\n\n    Aligns edges to be parallel or perpendicular (optionally also 45 degrees)\n    to their main direction. Handles reprojection, initial simplification,\n    regularization, geometry cleanup, and parallel processing.\n\n    This function is a wrapper around the `regularize_geodataframe` function\n    from the `buildingregulariser` package. Credits to the original author\n    Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.\n\n    Args:\n        data (Union[gpd.GeoDataFrame, str]): Input GeoDataFrame with polygon or multipolygon geometries,\n            or a file path to the GeoDataFrame.\n        parallel_threshold (float, optional): Distance threshold for merging nearly parallel adjacent edges\n            during regularization. Defaults to 1.0.\n        target_crs (Optional[Union[str, \"pyproj.CRS\"]], optional): Target Coordinate Reference System for\n            processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a\n            projected CRS. Defaults to None.\n        simplify (bool, optional): If True, applies initial simplification to the geometry before\n            regularization. Defaults to True.\n        simplify_tolerance (float, optional): Tolerance for the initial simplification step (if `simplify`\n            is True). Also used for geometry cleanup steps. Defaults to 0.5.\n        allow_45_degree (bool, optional): If True, allows edges to be oriented at 45-degree angles relative\n            to the main direction during regularization. Defaults to True.\n        diagonal_threshold_reduction (float, optional): Reduction factor in degrees to reduce the likelihood\n            of diagonal edges being created. Larger values reduce the likelihood of diagonal edges.\n            Defaults to 15.\n        allow_circles (bool, optional): If True, attempts to detect polygons that are nearly circular and\n            replaces them with perfect circles. Defaults to True.\n        circle_threshold (float, optional): Intersection over Union (IoU) threshold used for circle detection\n            (if `allow_circles` is True). Value between 0 and 1. Defaults to 0.9.\n        num_cores (int, optional): Number of CPU cores to use for parallel processing. If 1, processing is\n            done sequentially. Defaults to 1.\n        include_metadata (bool, optional): If True, includes metadata about the regularization process in the\n            output GeoDataFrame. Defaults to False.\n        output_path (Optional[str], optional): Path to save the output GeoDataFrame. If None, the output is\n            not saved. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the `to_file` method when saving the output.\n\n    Returns:\n        gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are\n        preserved. Geometries that failed processing might be dropped.\n\n    Raises:\n        ValueError: If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.\n    \"\"\"\n    try:\n        from buildingregulariser import regularize_geodataframe\n    except ImportError:\n        install_package(\"buildingregulariser\")\n        from buildingregulariser import regularize_geodataframe\n\n    if isinstance(data, str):\n        data = gpd.read_file(data)\n    elif not isinstance(data, gpd.GeoDataFrame):\n        raise ValueError(\"Input data must be a GeoDataFrame or a file path.\")\n\n    # Check if the input data is empty\n    if data.empty:\n        raise ValueError(\"Input GeoDataFrame is empty.\")\n\n    gdf = regularize_geodataframe(\n        data,\n        parallel_threshold=parallel_threshold,\n        target_crs=target_crs,\n        simplify=simplify,\n        simplify_tolerance=simplify_tolerance,\n        allow_45_degree=allow_45_degree,\n        diagonal_threshold_reduction=diagonal_threshold_reduction,\n        allow_circles=allow_circles,\n        circle_threshold=circle_threshold,\n        num_cores=num_cores,\n        include_metadata=include_metadata,\n    )\n\n    if output_path:\n        gdf.to_file(output_path, **kwargs)\n\n    return gdf\n</code></pre>"},{"location":"sam/#geoai.sam.rowcol_to_xy","title":"<code>rowcol_to_xy(src_fp, rows=None, cols=None, boxes=None, zs=None, offset='center', output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Converts a list of (row, col) coordinates to (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>rows</code> <code>list</code> <p>A list of row coordinates. Defaults to None.</p> <code>None</code> <code>cols</code> <code>list</code> <p>A list of col coordinates. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list</code> <p>A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]</p> <code>None</code> <code>zs</code> <code>Optional[List[float]]</code> <p>zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.</p> <code>None</code> <code>offset</code> <code>str</code> <p>Determines if the returned coordinates are for the center of the pixel or for a corner.</p> <code>'center'</code> <code>output</code> <code>str</code> <p>The output vector file path. Defaults to None.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.xy.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>A list of (x, y) coordinates.</p> Source code in <code>geoai/utils.py</code> <pre><code>def rowcol_to_xy(\n    src_fp: str,\n    rows: Optional[List[int]] = None,\n    cols: Optional[List[int]] = None,\n    boxes: Optional[List[List[int]]] = None,\n    zs: Optional[List[float]] = None,\n    offset: str = \"center\",\n    output: Optional[str] = None,\n    dst_crs: str = \"EPSG:4326\",\n    **kwargs: Any,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Converts a list of (row, col) coordinates to (x, y) coordinates.\n\n    Args:\n        src_fp (str): The source raster file path.\n        rows (list, optional): A list of row coordinates. Defaults to None.\n        cols (list, optional): A list of col coordinates. Defaults to None.\n        boxes (list, optional): A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]\n        zs: zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.\n        offset (str, optional): Determines if the returned coordinates are for the center of the pixel or for a corner.\n        output (str, optional): The output vector file path. Defaults to None.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.xy.\n\n    Returns:\n        A list of (x, y) coordinates.\n    \"\"\"\n\n    if boxes is not None:\n        rows = []\n        cols = []\n\n        for box in boxes:\n            rows.append(box[1])\n            rows.append(box[3])\n            cols.append(box[0])\n            cols.append(box[2])\n\n    if rows is None or cols is None:\n        raise ValueError(\"rows and cols must be provided.\")\n\n    with rasterio.open(src_fp) as src:\n        xs, ys = rasterio.transform.xy(src.transform, rows, cols, zs, offset, **kwargs)\n        src_crs = src.crs\n\n    if boxes is None:\n        return [[x, y] for x, y in zip(xs, ys)]\n    else:\n        result = [[xs[i], ys[i + 1], xs[i + 1], ys[i]] for i in range(0, len(xs), 2)]\n\n        if output is not None:\n            boxes_to_vector(result, src_crs, dst_crs, output)\n        else:\n            return result\n</code></pre>"},{"location":"sam/#geoai.sam.stack_bands","title":"<code>stack_bands(input_files, output_file, resolution=None, dtype=None, temp_vrt='stack.vrt', overwrite=False, compress='DEFLATE', output_format='COG', extra_gdal_translate_args=None)</code>","text":"<p>Stack bands from multiple images into a single multi-band GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>input_files</code> <code>List[str]</code> <p>List of input image paths.</p> required <code>output_file</code> <code>str</code> <p>Path to the output stacked image.</p> required <code>resolution</code> <code>float</code> <p>Output resolution. If None, inferred from first image.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Output data type (e.g., \"UInt16\", \"Float32\").</p> <code>None</code> <code>temp_vrt</code> <code>str</code> <p>Temporary VRT filename.</p> <code>'stack.vrt'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file.</p> <code>False</code> <code>compress</code> <code>str</code> <p>Compression method.</p> <code>'DEFLATE'</code> <code>output_format</code> <code>str</code> <p>GDAL output format (default is \"COG\").</p> <code>'COG'</code> <code>extra_gdal_translate_args</code> <code>List[str]</code> <p>Extra arguments for gdal_translate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def stack_bands(\n    input_files: List[str],\n    output_file: str,\n    resolution: Optional[float] = None,\n    dtype: Optional[str] = None,  # e.g., \"UInt16\", \"Float32\"\n    temp_vrt: str = \"stack.vrt\",\n    overwrite: bool = False,\n    compress: str = \"DEFLATE\",\n    output_format: str = \"COG\",\n    extra_gdal_translate_args: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Stack bands from multiple images into a single multi-band GeoTIFF.\n\n    Args:\n        input_files (List[str]): List of input image paths.\n        output_file (str): Path to the output stacked image.\n        resolution (float, optional): Output resolution. If None, inferred from first image.\n        dtype (str, optional): Output data type (e.g., \"UInt16\", \"Float32\").\n        temp_vrt (str): Temporary VRT filename.\n        overwrite (bool): Whether to overwrite the output file.\n        compress (str): Compression method.\n        output_format (str): GDAL output format (default is \"COG\").\n        extra_gdal_translate_args (List[str], optional): Extra arguments for gdal_translate.\n\n    Returns:\n        str: Path to the output file.\n    \"\"\"\n    import leafmap\n\n    if not input_files:\n        raise ValueError(\"No input files provided.\")\n    elif isinstance(input_files, str):\n        input_files = leafmap.find_files(input_files, \".tif\")\n\n    if os.path.exists(output_file) and not overwrite:\n        print(f\"Output file already exists: {output_file}\")\n        return output_file\n\n    # Infer resolution if not provided\n    if resolution is None:\n        resolution_x, resolution_y = get_raster_resolution(input_files[0])\n    else:\n        resolution_x = resolution_y = resolution\n\n    # Step 1: Build VRT\n    vrt_cmd = [\"gdalbuildvrt\", \"-separate\", temp_vrt] + input_files\n    subprocess.run(vrt_cmd, check=True)\n\n    # Step 2: Translate VRT to output GeoTIFF\n    translate_cmd = [\n        \"gdal_translate\",\n        \"-tr\",\n        str(resolution_x),\n        str(resolution_y),\n        temp_vrt,\n        output_file,\n        \"-of\",\n        output_format,\n        \"-co\",\n        f\"COMPRESS={compress}\",\n    ]\n\n    if dtype:\n        translate_cmd.insert(1, \"-ot\")\n        translate_cmd.insert(2, dtype)\n\n    if extra_gdal_translate_args:\n        translate_cmd += extra_gdal_translate_args\n\n    subprocess.run(translate_cmd, check=True)\n\n    # Step 3: Clean up VRT\n    if os.path.exists(temp_vrt):\n        os.remove(temp_vrt)\n\n    return output_file\n</code></pre>"},{"location":"sam/#geoai.sam.temp_file_path","title":"<code>temp_file_path(ext)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The temporary file path.</p> Source code in <code>geoai/utils.py</code> <pre><code>def temp_file_path(ext: str) -&gt; str:\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        ext (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not ext.startswith(\".\"):\n        ext = \".\" + ext\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{ext}\")\n\n    return file_path\n</code></pre>"},{"location":"sam/#geoai.sam.try_common_architectures","title":"<code>try_common_architectures(state_dict)</code>","text":"<p>Try to load the state_dict into common architectures to see which one fits.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>The model's state dictionary</p> required Source code in <code>geoai/utils.py</code> <pre><code>def try_common_architectures(state_dict: Dict[str, Any]) -&gt; Optional[str]:\n    \"\"\"\n    Try to load the state_dict into common architectures to see which one fits.\n\n    Args:\n        state_dict: The model's state dictionary\n    \"\"\"\n    import torchinfo\n\n    # Test models and their initializations\n    models_to_try = {\n        \"FCN-ResNet50\": lambda: fcn_resnet50(num_classes=9),\n        \"DeepLabV3-ResNet50\": lambda: deeplabv3_resnet50(num_classes=9),\n    }\n\n    print(\"\\nTrying to load state_dict into common architectures:\")\n\n    for name, model_fn in models_to_try.items():\n        try:\n            model = model_fn()\n            # Sometimes state_dict keys have 'model.' prefix\n            if all(k.startswith(\"model.\") for k in state_dict.keys()):\n                cleaned_state_dict = {k[6:]: v for k, v in state_dict.items()}\n                model.load_state_dict(cleaned_state_dict, strict=False)\n            else:\n                model.load_state_dict(state_dict, strict=False)\n\n            print(\n                f\"- {name}: Successfully loaded (may have missing or unexpected keys)\"\n            )\n\n            # Generate model summary\n            print(f\"\\nSummary of {name} architecture:\")\n            summary = torchinfo.summary(model, input_size=(1, 3, 224, 224), verbose=0)\n            print(summary)\n\n        except Exception as e:\n            print(f\"- {name}: Failed to load - {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.vector_to_geojson","title":"<code>vector_to_geojson(filename, output=None, **kwargs)</code>","text":"<p>Converts a vector file to a geojson file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <code>output</code> <code>str</code> <p>The output geojson file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>str</code> <p>The geojson dictionary.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_geojson(\n    filename: str, output: Optional[str] = None, **kwargs: Any\n) -&gt; str:\n    \"\"\"Converts a vector file to a geojson file.\n\n    Args:\n        filename (str): The vector file path.\n        output (str, optional): The output geojson file path. Defaults to None.\n\n    Returns:\n        dict: The geojson dictionary.\n    \"\"\"\n\n    if filename.startswith(\"http\"):\n        filename = download_file(filename)\n\n    gdf = gpd.read_file(filename, **kwargs)\n    if output is None:\n        return gdf.__geo_interface__\n    else:\n        gdf.to_file(output, driver=\"GeoJSON\")\n</code></pre>"},{"location":"sam/#geoai.sam.vector_to_raster","title":"<code>vector_to_raster(vector_path, output_path=None, reference_raster=None, attribute_field=None, output_shape=None, transform=None, pixel_size=None, bounds=None, crs=None, all_touched=False, fill_value=0, dtype=np.uint8, nodata=None, plot_result=False)</code>","text":"<p>Convert vector data to a raster.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output raster file. If None, returns the array without saving.</p> <code>None</code> <code>reference_raster</code> <code>str</code> <p>Path to a reference raster for dimensions, transform and CRS.</p> <code>None</code> <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values. If None, all vector features will be burned with value 1.</p> <code>None</code> <code>output_shape</code> <code>tuple</code> <p>Shape of the output raster as (height, width). Required if reference_raster is not provided.</p> <code>None</code> <code>transform</code> <code>Affine</code> <p>Affine transformation matrix. Required if reference_raster is not provided.</p> <code>None</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size (resolution) as single value or (x_res, y_res). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>bounds</code> <code>tuple</code> <p>Bounds of the output raster as (left, bottom, right, top). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>crs</code> <code>str or CRS</code> <p>Coordinate reference system of the output raster. Required if reference_raster is not provided.</p> <code>None</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in. If False, only pixels whose center is within the geometry will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type of the output raster.</p> <code>uint8</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: The rasterized data array if output_path is None, else None.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_raster(\n    vector_path: Union[str, gpd.GeoDataFrame],\n    output_path: Optional[str] = None,\n    reference_raster: Optional[str] = None,\n    attribute_field: Optional[str] = None,\n    output_shape: Optional[Tuple[int, int]] = None,\n    transform: Optional[Any] = None,\n    pixel_size: Optional[float] = None,\n    bounds: Optional[List[float]] = None,\n    crs: Optional[str] = None,\n    all_touched: bool = False,\n    fill_value: Union[int, float] = 0,\n    dtype: Any = np.uint8,\n    nodata: Optional[Union[int, float]] = None,\n    plot_result: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert vector data to a raster.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_path (str): Path to save the output raster file. If None, returns the array without saving.\n        reference_raster (str): Path to a reference raster for dimensions, transform and CRS.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n            If None, all vector features will be burned with value 1.\n        output_shape (tuple): Shape of the output raster as (height, width).\n            Required if reference_raster is not provided.\n        transform (affine.Affine): Affine transformation matrix.\n            Required if reference_raster is not provided.\n        pixel_size (float or tuple): Pixel size (resolution) as single value or (x_res, y_res).\n            Used to calculate transform if transform is not provided.\n        bounds (tuple): Bounds of the output raster as (left, bottom, right, top).\n            Used to calculate transform if transform is not provided.\n        crs (str or CRS): Coordinate reference system of the output raster.\n            Required if reference_raster is not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n            If False, only pixels whose center is within the geometry will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n        plot_result (bool): Whether to plot the resulting raster.\n\n    Returns:\n        numpy.ndarray: The rasterized data array if output_path is None, else None.\n    \"\"\"\n    # Load vector data\n    if isinstance(vector_path, gpd.GeoDataFrame):\n        gdf = vector_path\n    else:\n        gdf = gpd.read_file(vector_path)\n\n    # Check if vector data is empty\n    if gdf.empty:\n        warnings.warn(\"The input vector data is empty. Creating an empty raster.\")\n\n    # Get CRS from vector data if not provided\n    if crs is None and reference_raster is None:\n        crs = gdf.crs\n\n    # Get transform and output shape from reference raster if provided\n    if reference_raster is not None:\n        with rasterio.open(reference_raster) as src:\n            transform = src.transform\n            output_shape = src.shape\n            crs = src.crs\n            if nodata is None:\n                nodata = src.nodata\n    else:\n        # Check if we have all required parameters\n        if transform is None:\n            if pixel_size is None or bounds is None:\n                raise ValueError(\n                    \"Either reference_raster, transform, or both pixel_size and bounds must be provided.\"\n                )\n\n            # Calculate transform from pixel size and bounds\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n                y_res = abs(y_res) * -1  # Convert to negative for north-up raster\n\n            left, bottom, right, top = bounds\n            transform = rasterio.transform.from_bounds(\n                left,\n                bottom,\n                right,\n                top,\n                int((right - left) / x_res),\n                int((top - bottom) / abs(y_res)),\n            )\n\n        if output_shape is None:\n            # Calculate output shape from bounds and pixel size\n            if bounds is None or pixel_size is None:\n                raise ValueError(\n                    \"output_shape must be provided if reference_raster is not provided and \"\n                    \"cannot be calculated from bounds and pixel_size.\"\n                )\n\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n\n            left, bottom, right, top = bounds\n            width = int((right - left) / x_res)\n            height = int((top - bottom) / abs(y_res))\n            output_shape = (height, width)\n\n    # Ensure CRS is set\n    if crs is None:\n        raise ValueError(\n            \"CRS must be provided either directly, from reference_raster, or from input vector data.\"\n        )\n\n    # Reproject vector data if its CRS doesn't match the output CRS\n    if gdf.crs != crs:\n        print(f\"Reprojecting vector data from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Create empty raster filled with fill_value\n    raster_data = np.full(output_shape, fill_value, dtype=dtype)\n\n    # Burn vector features into raster\n    if not gdf.empty:\n        # Prepare shapes for burning\n        if attribute_field is not None and attribute_field in gdf.columns:\n            # Use attribute field for values\n            shapes = [\n                (geom, value) for geom, value in zip(gdf.geometry, gdf[attribute_field])\n            ]\n        else:\n            # Burn with value 1\n            shapes = [(geom, 1) for geom in gdf.geometry]\n\n        # Burn shapes into raster\n        burned = features.rasterize(\n            shapes=shapes,\n            out_shape=output_shape,\n            transform=transform,\n            fill=fill_value,\n            all_touched=all_touched,\n            dtype=dtype,\n        )\n\n        # Update raster data\n        raster_data = burned\n\n    # Save raster if output path is provided\n    if output_path is not None:\n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        # Define metadata\n        metadata = {\n            \"driver\": \"GTiff\",\n            \"height\": output_shape[0],\n            \"width\": output_shape[1],\n            \"count\": 1,\n            \"dtype\": raster_data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n        }\n\n        # Add nodata value if provided\n        if nodata is not None:\n            metadata[\"nodata\"] = nodata\n\n        # Write raster\n        with rasterio.open(output_path, \"w\", **metadata) as dst:\n            dst.write(raster_data, 1)\n\n        print(f\"Rasterized data saved to {output_path}\")\n\n    # Plot result if requested\n    if plot_result:\n        fig, ax = plt.subplots(figsize=(10, 10))\n\n        # Plot raster\n        im = ax.imshow(raster_data, cmap=\"viridis\")\n        plt.colorbar(im, ax=ax, label=attribute_field if attribute_field else \"Value\")\n\n        # Plot vector boundaries for reference\n        if output_path is not None:\n            # Get the extent of the raster\n            with rasterio.open(output_path) as src:\n                bounds = src.bounds\n                raster_bbox = box(*bounds)\n        else:\n            # Calculate extent from transform and shape\n            height, width = output_shape\n            left, top = transform * (0, 0)\n            right, bottom = transform * (width, height)\n            raster_bbox = box(left, bottom, right, top)\n\n        # Clip vector to raster extent for clarity in plot\n        if not gdf.empty:\n            gdf_clipped = gpd.clip(gdf, raster_bbox)\n            if not gdf_clipped.empty:\n                gdf_clipped.boundary.plot(ax=ax, color=\"red\", linewidth=1)\n\n        plt.title(\"Rasterized Vector Data\")\n        plt.tight_layout()\n        plt.show()\n\n    return raster_data\n</code></pre>"},{"location":"sam/#geoai.sam.view_image","title":"<code>view_image(image, transpose=False, bdx=None, clip_percentiles=(2, 98), gamma=None, figsize=(10, 5), axis_off=True, title=None, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[ndarray, Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    clip_percentiles: Optional[Tuple[float, float]] = (2, 98),\n    gamma: Optional[float] = None,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        title (Optional[str], optional): The title of the plot. Defaults to None.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n    elif isinstance(image, str):\n        image = rasterio.open(image).read().transpose(1, 2, 0)\n\n    ax = plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if clip_percentiles is not None:\n        p_low, p_high = clip_percentiles\n        lower = np.percentile(image, p_low)\n        upper = np.percentile(image, p_high)\n        image = np.clip((image - lower) / (upper - lower), 0, 1)\n\n    if gamma is not None:\n        image = np.power(image, gamma)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n\n    return ax\n</code></pre>"},{"location":"sam/#geoai.sam.view_raster","title":"<code>view_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': False}, basemap='OpenStreetMap', basemap_args=None, backend='ipyleaflet', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>basemap_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the basemap. Defaults to None.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The backend to use. Defaults to \"ipyleaflet\".</p> <code>'ipyleaflet'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>leafmap.Map: The map object with the raster layer added.</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = None,\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    basemap_args: Optional[Dict] = None,\n    backend: Optional[str] = \"ipyleaflet\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        basemap_args (Optional[Dict], optional): Additional arguments for the basemap. Defaults to None.\n        backend (Optional[str], optional): The backend to use. Defaults to \"ipyleaflet\".\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n\n    if backend == \"folium\":\n        import leafmap.foliumap as leafmap\n    else:\n        import leafmap.leafmap as leafmap\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if array_args is None:\n        array_args = {}\n\n    m = leafmap.Map()\n\n    if isinstance(basemap, str):\n        if basemap.lower().endswith(\".tif\"):\n            if basemap.lower().startswith(\"http\"):\n                if \"name\" not in basemap_args:\n                    basemap_args[\"name\"] = \"Basemap\"\n                m.add_cog_layer(basemap, **basemap_args)\n            else:\n                if \"layer_name\" not in basemap_args:\n                    basemap_args[\"layer_name\"] = \"Basemap\"\n                m.add_raster(basemap, **basemap_args)\n    else:\n        m.add_basemap(basemap, **basemap_args)\n\n    if isinstance(source, dict):\n        source = dict_to_image(source)\n\n    if (\n        isinstance(source, str)\n        and source.lower().endswith(\".tif\")\n        and source.startswith(\"http\")\n    ):\n        if indexes is not None:\n            kwargs[\"bidx\"] = indexes\n        if colormap is not None:\n            kwargs[\"colormap_name\"] = colormap\n        if attribution is None:\n            attribution = \"TiTiler\"\n\n        m.add_cog_layer(\n            source,\n            name=layer_name,\n            opacity=opacity,\n            attribution=attribution,\n            zoom_to_layer=zoom_to_layer,\n            **kwargs,\n        )\n    else:\n        m.add_raster(\n            source=source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            layer_index=layer_index,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            **kwargs,\n        )\n    return m\n</code></pre>"},{"location":"sam/#geoai.sam.view_vector","title":"<code>view_vector(vector_data, column=None, cmap='viridis', figsize=(10, 10), title=None, legend=True, basemap=False, basemap_type='streets', alpha=0.7, edge_color='black', classification='quantiles', n_classes=5, highlight_index=None, highlight_color='red', scheme=None, save_path=None, dpi=300)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>column</code> <code>str</code> <p>Column to use for choropleth mapping. If None, a single color will be used. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str or Colormap</code> <p>Colormap to use for choropleth mapping. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height) in inches. Defaults to (10, 10).</p> <code>(10, 10)</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to None.</p> <code>None</code> <code>legend</code> <code>bool</code> <p>Whether to display a legend. Defaults to True.</p> <code>True</code> <code>basemap</code> <code>bool</code> <p>Whether to add a web basemap. Requires contextily. Defaults to False.</p> <code>False</code> <code>basemap_type</code> <code>str</code> <p>Type of basemap to use. Options: 'streets', 'satellite'. Defaults to 'streets'.</p> <code>'streets'</code> <code>alpha</code> <code>float</code> <p>Transparency of the vector features, between 0-1. Defaults to 0.7.</p> <code>0.7</code> <code>edge_color</code> <code>str</code> <p>Color for feature edges. Defaults to \"black\".</p> <code>'black'</code> <code>classification</code> <code>str</code> <p>Classification method for choropleth maps. Options: \"quantiles\", \"equal_interval\", \"natural_breaks\". Defaults to \"quantiles\".</p> <code>'quantiles'</code> <code>n_classes</code> <code>int</code> <p>Number of classes for choropleth maps. Defaults to 5.</p> <code>5</code> <code>highlight_index</code> <code>list</code> <p>List of indices to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color to use for highlighted features. Defaults to \"red\".</p> <code>'red'</code> <code>scheme</code> <code>str</code> <p>MapClassify classification scheme. Overrides classification parameter if provided. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, the figure is not saved. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>DPI for saved figure. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>Any</code> <p>matplotlib.axes.Axes: The Axes object containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    column: Optional[str] = None,\n    cmap: str = \"viridis\",\n    figsize: Tuple[int, int] = (10, 10),\n    title: Optional[str] = None,\n    legend: bool = True,\n    basemap: bool = False,\n    basemap_type: str = \"streets\",\n    alpha: float = 0.7,\n    edge_color: str = \"black\",\n    classification: str = \"quantiles\",\n    n_classes: int = 5,\n    highlight_index: Optional[int] = None,\n    highlight_color: str = \"red\",\n    scheme: Optional[str] = None,\n    save_path: Optional[str] = None,\n    dpi: int = 300,\n) -&gt; Any:\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        column (str, optional): Column to use for choropleth mapping. If None,\n            a single color will be used. Defaults to None.\n        cmap (str or matplotlib.colors.Colormap, optional): Colormap to use for\n            choropleth mapping. Defaults to \"viridis\".\n        figsize (tuple, optional): Figure size as (width, height) in inches.\n            Defaults to (10, 10).\n        title (str, optional): Title for the plot. Defaults to None.\n        legend (bool, optional): Whether to display a legend. Defaults to True.\n        basemap (bool, optional): Whether to add a web basemap. Requires contextily.\n            Defaults to False.\n        basemap_type (str, optional): Type of basemap to use. Options: 'streets', 'satellite'.\n            Defaults to 'streets'.\n        alpha (float, optional): Transparency of the vector features, between 0-1.\n            Defaults to 0.7.\n        edge_color (str, optional): Color for feature edges. Defaults to \"black\".\n        classification (str, optional): Classification method for choropleth maps.\n            Options: \"quantiles\", \"equal_interval\", \"natural_breaks\".\n            Defaults to \"quantiles\".\n        n_classes (int, optional): Number of classes for choropleth maps.\n            Defaults to 5.\n        highlight_index (list, optional): List of indices to highlight.\n            Defaults to None.\n        highlight_color (str, optional): Color to use for highlighted features.\n            Defaults to \"red\".\n        scheme (str, optional): MapClassify classification scheme. Overrides\n            classification parameter if provided. Defaults to None.\n        save_path (str, optional): Path to save the figure. If None, the figure\n            is not saved. Defaults to None.\n        dpi (int, optional): DPI for saved figure. Defaults to 300.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing the plot.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n    \"\"\"\n    import contextily as ctx\n\n    if isinstance(vector_data, str):\n        vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    # Make a copy to avoid changing the original data\n    gdf = vector_data.copy()\n\n    # Set up figure and axis\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Determine geometry type\n    geom_type = gdf.geometry.iloc[0].geom_type\n\n    # Plotting parameters\n    plot_kwargs = {\"alpha\": alpha, \"ax\": ax}\n\n    # Set up keyword arguments based on geometry type\n    if \"Point\" in geom_type:\n        plot_kwargs[\"markersize\"] = 50\n        plot_kwargs[\"edgecolor\"] = edge_color\n    elif \"Line\" in geom_type:\n        plot_kwargs[\"linewidth\"] = 1\n    elif \"Polygon\" in geom_type:\n        plot_kwargs[\"edgecolor\"] = edge_color\n\n    # Classification options\n    if column is not None:\n        if scheme is not None:\n            # Use mapclassify scheme if provided\n            plot_kwargs[\"scheme\"] = scheme\n        else:\n            # Use classification parameter\n            if classification == \"quantiles\":\n                plot_kwargs[\"scheme\"] = \"quantiles\"\n            elif classification == \"equal_interval\":\n                plot_kwargs[\"scheme\"] = \"equal_interval\"\n            elif classification == \"natural_breaks\":\n                plot_kwargs[\"scheme\"] = \"fisher_jenks\"\n\n        plot_kwargs[\"k\"] = n_classes\n        plot_kwargs[\"cmap\"] = cmap\n        plot_kwargs[\"column\"] = column\n        plot_kwargs[\"legend\"] = legend\n\n    # Plot the main data\n    gdf.plot(**plot_kwargs)\n\n    # Highlight specific features if requested\n    if highlight_index is not None:\n        gdf.iloc[highlight_index].plot(\n            ax=ax, color=highlight_color, edgecolor=\"black\", linewidth=2, zorder=5\n        )\n\n    if basemap:\n        try:\n            basemap_options = {\n                \"streets\": ctx.providers.OpenStreetMap.Mapnik,\n                \"satellite\": ctx.providers.Esri.WorldImagery,\n            }\n            ctx.add_basemap(ax, crs=gdf.crs, source=basemap_options[basemap_type])\n        except Exception as e:\n            print(f\"Could not add basemap: {e}\")\n\n    # Set title if provided\n    if title:\n        ax.set_title(title, fontsize=14)\n\n    # Remove axes if not needed\n    ax.set_axis_off()\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save figure if a path is provided\n    if save_path:\n        plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n\n    return ax\n</code></pre>"},{"location":"sam/#geoai.sam.view_vector_interactive","title":"<code>view_vector_interactive(vector_data, layer_name='Vector Layer', tiles_args=None, **kwargs)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer. Defaults to \"Vector Layer\".</p> <code>'Vector Layer'</code> <code>tiles_args</code> <code>dict</code> <p>Additional arguments for the localtileserver client. get_folium_tile_layer function. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to GeoDataFrame.explore() function. See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>folium.Map: The map object with the vector data added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector_interactive(cities)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector_interactive(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    layer_name: str = \"Vector Layer\",\n    tiles_args: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        layer_name (str, optional): The name of the layer. Defaults to \"Vector Layer\".\n        tiles_args (dict, optional): Additional arguments for the localtileserver client.\n            get_folium_tile_layer function. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to GeoDataFrame.explore() function.\n            See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html\n\n    Returns:\n        folium.Map: The map object with the vector data added.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector_interactive(cities)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n    \"\"\"\n    import folium\n    import folium.plugins as plugins\n    from leafmap import cog_tile\n    from localtileserver import TileClient, get_folium_tile_layer\n\n    google_tiles = {\n        \"Roadmap\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Maps\",\n        },\n        \"Satellite\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Satellite\",\n        },\n        \"Terrain\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Terrain\",\n        },\n        \"Hybrid\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Hybrid\",\n        },\n    }\n\n    basemap_layer_name = None\n    raster_layer = None\n\n    if \"tiles\" in kwargs and isinstance(kwargs[\"tiles\"], str):\n        if kwargs[\"tiles\"].title() in google_tiles:\n            basemap_layer_name = google_tiles[kwargs[\"tiles\"].title()][\"name\"]\n            kwargs[\"tiles\"] = google_tiles[kwargs[\"tiles\"].title()][\"url\"]\n            kwargs[\"attr\"] = \"Google\"\n        elif kwargs[\"tiles\"].lower().endswith(\".tif\"):\n            if tiles_args is None:\n                tiles_args = {}\n            if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                basemap_layer_name = \"Remote Raster\"\n                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n                kwargs[\"attr\"] = \"TiTiler\"\n            else:\n                basemap_layer_name = \"Local Raster\"\n                client = TileClient(kwargs[\"tiles\"])\n                raster_layer = get_folium_tile_layer(client, **tiles_args)\n                kwargs[\"tiles\"] = raster_layer.tiles\n                kwargs[\"attr\"] = \"localtileserver\"\n\n    if \"max_zoom\" not in kwargs:\n        kwargs[\"max_zoom\"] = 30\n\n    if isinstance(vector_data, str):\n        if vector_data.endswith(\".parquet\"):\n            vector_data = gpd.read_parquet(vector_data)\n        else:\n            vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    layer_control = kwargs.pop(\"layer_control\", True)\n    fullscreen_control = kwargs.pop(\"fullscreen_control\", True)\n\n    m = vector_data.explore(**kwargs)\n\n    # Change the layer name\n    for layer in m._children.values():\n        if isinstance(layer, folium.GeoJson):\n            layer.layer_name = layer_name\n        if isinstance(layer, folium.TileLayer) and basemap_layer_name:\n            layer.layer_name = basemap_layer_name\n\n    if layer_control:\n        m.add_child(folium.LayerControl())\n\n    if fullscreen_control:\n        plugins.Fullscreen().add_to(m)\n\n    return m\n</code></pre>"},{"location":"sam/#geoai.sam.visualize_vector_by_attribute","title":"<code>visualize_vector_by_attribute(vector_path, attribute_name, cmap='viridis', figsize=(10, 8))</code>","text":"<p>Create a thematic map visualization of vector data based on an attribute.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to visualize</p> required <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if visualization was successful, False otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def visualize_vector_by_attribute(\n    vector_path: str,\n    attribute_name: str,\n    cmap: str = \"viridis\",\n    figsize: Tuple[int, int] = (10, 8),\n) -&gt; bool:\n    \"\"\"Create a thematic map visualization of vector data based on an attribute.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to visualize\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        bool: True if visualization was successful, False otherwise\n    \"\"\"\n    try:\n        # Read the vector data\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return False\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Determine plot type based on data type\n        if pd.api.types.is_numeric_dtype(gdf[attribute_name]):\n            # Continuous data\n            gdf.plot(column=attribute_name, cmap=cmap, legend=True, ax=ax)\n        else:\n            # Categorical data\n            gdf.plot(column=attribute_name, categorical=True, legend=True, ax=ax)\n\n        # Add title and labels\n        ax.set_title(f\"{os.path.basename(vector_path)} - {attribute_name}\")\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Add basemap or additional elements if available\n        # Note: Additional options could be added here for more complex maps\n\n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"Error visualizing data: {str(e)}\")\n</code></pre>"},{"location":"sam/#geoai.sam.write_colormap","title":"<code>write_colormap(image, colormap, output=None)</code>","text":"<p>Write a colormap to an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray]</code> <p>The image to write the colormap to.</p> required <code>colormap</code> <code>Union[str, Dict]</code> <p>The colormap to write to the image.</p> required <code>output</code> <code>Optional[str]</code> <p>The output file path.</p> <code>None</code> Source code in <code>geoai/utils.py</code> <pre><code>def write_colormap(\n    image: Union[str, np.ndarray],\n    colormap: Union[str, Dict],\n    output: Optional[str] = None,\n) -&gt; Optional[str]:\n    \"\"\"Write a colormap to an image.\n\n    Args:\n        image: The image to write the colormap to.\n        colormap: The colormap to write to the image.\n        output: The output file path.\n    \"\"\"\n    if isinstance(colormap, str):\n        colormap = leafmap.get_image_colormap(colormap)\n    leafmap.write_image_colormap(image, colormap, output)\n</code></pre>"},{"location":"segment/","title":"segment module","text":"<p>This module provides functionality for segmenting high-resolution satellite imagery using vision-language models.</p>"},{"location":"segment/#geoai.segment.BoundingBox","title":"<code>BoundingBox</code>  <code>dataclass</code>","text":"<p>Represents a bounding box with coordinates.</p> Source code in <code>geoai/segment.py</code> <pre><code>@dataclass\nclass BoundingBox:\n    \"\"\"Represents a bounding box with coordinates.\"\"\"\n\n    xmin: int\n    ymin: int\n    xmax: int\n    ymax: int\n\n    @property\n    def xyxy(self) -&gt; List[float]:\n        return [self.xmin, self.ymin, self.xmax, self.ymax]\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation","title":"<code>CLIPSegmentation</code>","text":"<p>A class for segmenting high-resolution satellite imagery using text prompts with CLIP-based models.</p> <p>This segmenter utilizes the CLIP-Seg model to perform semantic segmentation based on text prompts. It can process large GeoTIFF files by tiling them and handles proper georeferencing in the output.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".</p> <code>'CIDAS/clipseg-rd64-refined'</code> <code>device</code> <code>str</code> <p>Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 352.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 16.</p> <code>32</code> <p>Attributes:</p> Name Type Description <code>processor</code> <code>CLIPSegProcessor</code> <p>The processor for the CLIP-Seg model.</p> <code>model</code> <code>CLIPSegForImageSegmentation</code> <p>The CLIP-Seg model for segmentation.</p> <code>device</code> <code>str</code> <p>The device being used ('cuda' or 'cpu').</p> <code>tile_size</code> <code>int</code> <p>Size of tiles for processing.</p> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> Source code in <code>geoai/segment.py</code> <pre><code>class CLIPSegmentation:\n    \"\"\"\n    A class for segmenting high-resolution satellite imagery using text prompts with CLIP-based models.\n\n    This segmenter utilizes the CLIP-Seg model to perform semantic segmentation based on text prompts.\n    It can process large GeoTIFF files by tiling them and handles proper georeferencing in the output.\n\n    Args:\n        model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n        device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 352.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 16.\n\n    Attributes:\n        processor (CLIPSegProcessor): The processor for the CLIP-Seg model.\n        model (CLIPSegForImageSegmentation): The CLIP-Seg model for segmentation.\n        device (str): The device being used ('cuda' or 'cpu').\n        tile_size (int): Size of tiles for processing.\n        overlap (int): Overlap between tiles.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"CIDAS/clipseg-rd64-refined\",\n        device: Optional[str] = None,\n        tile_size: int = 512,\n        overlap: int = 32,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the ImageSegmenter with the specified model and settings.\n\n        Args:\n            model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n            device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n            tile_size (int): Size of tiles to process the image in chunks. Defaults to 512.\n            overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 32.\n        \"\"\"\n        self.tile_size = tile_size\n        self.overlap = overlap\n\n        # Set device\n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n\n        # Load model and processor\n        self.processor = CLIPSegProcessor.from_pretrained(model_name)\n        self.model = CLIPSegForImageSegmentation.from_pretrained(model_name).to(\n            self.device\n        )\n\n        print(f\"Model loaded on {self.device}\")\n\n    def segment_image(\n        self,\n        input_path: str,\n        output_path: str,\n        text_prompt: str,\n        threshold: float = 0.5,\n        smoothing_sigma: float = 1.0,\n    ) -&gt; str:\n        \"\"\"\n        Segment a GeoTIFF image using the provided text prompt.\n\n        The function processes the image in tiles and saves the result as a GeoTIFF with two bands:\n        - Band 1: Binary segmentation mask (0 or 1)\n        - Band 2: Probability scores (0.0 to 1.0)\n\n        Args:\n            input_path (str): Path to the input GeoTIFF file.\n            output_path (str): Path where the output GeoTIFF will be saved.\n            text_prompt (str): Text description of what to segment (e.g., \"water\", \"buildings\").\n            threshold (float): Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n\n        Returns:\n            str: Path to the saved output file.\n        \"\"\"\n        # Open the input GeoTIFF\n        with rasterio.open(input_path) as src:\n            # Get metadata\n            meta = src.meta\n            height = src.height\n            width = src.width\n\n            # Create output metadata\n            out_meta = meta.copy()\n            out_meta.update({\"count\": 2, \"dtype\": \"float32\", \"nodata\": None})\n\n            # Create arrays for results\n            segmentation = np.zeros((height, width), dtype=np.float32)\n            probabilities = np.zeros((height, width), dtype=np.float32)\n\n            # Calculate effective tile size (accounting for overlap)\n            effective_tile_size = self.tile_size - 2 * self.overlap\n\n            # Calculate number of tiles\n            n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n            n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n            total_tiles = n_tiles_x * n_tiles_y\n\n            # Process tiles with tqdm progress bar\n            with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n                # Iterate through tiles\n                for y in range(n_tiles_y):\n                    for x in range(n_tiles_x):\n                        # Calculate tile coordinates with overlap\n                        x_start = max(0, x * effective_tile_size - self.overlap)\n                        y_start = max(0, y * effective_tile_size - self.overlap)\n                        x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                        y_end = min(\n                            height, (y + 1) * effective_tile_size + self.overlap\n                        )\n\n                        tile_width = x_end - x_start\n                        tile_height = y_end - y_start\n\n                        # Read the tile\n                        window = Window(x_start, y_start, tile_width, tile_height)\n                        tile_data = src.read(window=window)\n\n                        # Process the tile\n                        try:\n                            # Convert to RGB if necessary (handling different satellite bands)\n                            if tile_data.shape[0] &gt; 3:\n                                # Use first three bands for RGB representation\n                                rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                                # Normalize data to 0-255 range if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n                            elif tile_data.shape[0] == 1:\n                                # Create RGB from grayscale\n                                rgb_tile = np.repeat(\n                                    tile_data[0][:, :, np.newaxis], 3, axis=2\n                                )\n                                # Normalize if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n                            else:\n                                # Already 3-channel, assume RGB\n                                rgb_tile = tile_data.transpose(1, 2, 0)\n                                # Normalize if needed\n                                if rgb_tile.max() &gt; 0:\n                                    rgb_tile = (\n                                        (rgb_tile - rgb_tile.min())\n                                        / (rgb_tile.max() - rgb_tile.min())\n                                        * 255\n                                    ).astype(np.uint8)\n\n                            # Convert to PIL Image\n                            pil_image = Image.fromarray(rgb_tile)\n\n                            # Resize if needed to match model's requirements\n                            if (\n                                pil_image.width &gt; self.tile_size\n                                or pil_image.height &gt; self.tile_size\n                            ):\n                                # Keep aspect ratio - use LANCZOS resampling instead of deprecated constant\n                                pil_image.thumbnail(\n                                    (self.tile_size, self.tile_size),\n                                    Image.Resampling.LANCZOS,\n                                )\n\n                            # Process with CLIP-Seg\n                            inputs = self.processor(\n                                text=text_prompt, images=pil_image, return_tensors=\"pt\"\n                            ).to(self.device)\n\n                            # Forward pass\n                            with torch.no_grad():\n                                outputs = self.model(**inputs)\n\n                            # Get logits and resize to original tile size\n                            logits = outputs.logits[0]\n\n                            # Convert logits to probabilities with sigmoid\n                            probs = torch.sigmoid(logits).cpu().numpy()\n\n                            # Resize back to original tile size if needed\n                            if probs.shape != (tile_height, tile_width):\n                                # Use bicubic interpolation for smoother results\n                                probs_resized = np.array(\n                                    Image.fromarray(probs).resize(\n                                        (tile_width, tile_height),\n                                        Image.Resampling.BICUBIC,\n                                    )\n                                )\n                            else:\n                                probs_resized = probs\n\n                            # Apply gaussian blur to reduce blockiness\n                            try:\n                                from scipy.ndimage import gaussian_filter\n\n                                probs_resized = gaussian_filter(\n                                    probs_resized, sigma=smoothing_sigma\n                                )\n                            except ImportError:\n                                pass  # Continue without smoothing if scipy is not available\n\n                            # Store results in the full arrays\n                            # Only store the non-overlapping part (except at edges)\n                            valid_x_start = self.overlap if x &gt; 0 else 0\n                            valid_y_start = self.overlap if y &gt; 0 else 0\n                            valid_x_end = (\n                                tile_width - self.overlap\n                                if x &lt; n_tiles_x - 1\n                                else tile_width\n                            )\n                            valid_y_end = (\n                                tile_height - self.overlap\n                                if y &lt; n_tiles_y - 1\n                                else tile_height\n                            )\n\n                            dest_x_start = x_start + valid_x_start\n                            dest_y_start = y_start + valid_y_start\n                            dest_x_end = x_start + valid_x_end\n                            dest_y_end = y_start + valid_y_end\n\n                            # Store probabilities\n                            probabilities[\n                                dest_y_start:dest_y_end, dest_x_start:dest_x_end\n                            ] = probs_resized[\n                                valid_y_start:valid_y_end, valid_x_start:valid_x_end\n                            ]\n\n                        except Exception as e:\n                            print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                            # Continue with next tile\n\n                        # Update progress bar\n                        pbar.update(1)\n\n            # Create binary segmentation from probabilities\n            segmentation = (probabilities &gt;= threshold).astype(np.float32)\n\n            # Write the output GeoTIFF\n            with rasterio.open(output_path, \"w\", **out_meta) as dst:\n                dst.write(segmentation, 1)\n                dst.write(probabilities, 2)\n\n                # Add descriptions to bands\n                dst.set_band_description(1, \"Binary Segmentation\")\n                dst.set_band_description(2, \"Probability Scores\")\n\n            print(f\"Segmentation saved to {output_path}\")\n            return output_path\n\n    def segment_image_batch(\n        self,\n        input_paths: List[str],\n        output_dir: str,\n        text_prompt: str,\n        threshold: float = 0.5,\n        smoothing_sigma: float = 1.0,\n        suffix: str = \"_segmented\",\n    ) -&gt; List[str]:\n        \"\"\"\n        Segment multiple GeoTIFF images using the provided text prompt.\n\n        Args:\n            input_paths (list): List of paths to input GeoTIFF files.\n            output_dir (str): Directory where output GeoTIFFs will be saved.\n            text_prompt (str): Text description of what to segment.\n            threshold (float): Threshold for binary segmentation. Defaults to 0.5.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n            suffix (str): Suffix to add to output filenames. Defaults to \"_segmented\".\n\n        Returns:\n            list: Paths to all saved output files.\n        \"\"\"\n        # Create output directory if it doesn't exist\n        os.makedirs(output_dir, exist_ok=True)\n\n        output_paths = []\n\n        # Process each input file\n        for input_path in tqdm(input_paths, desc=\"Processing files\"):\n            # Generate output path\n            filename = os.path.basename(input_path)\n            base_name, ext = os.path.splitext(filename)\n            output_path = os.path.join(output_dir, f\"{base_name}{suffix}{ext}\")\n\n            # Segment the image\n            result_path = self.segment_image(\n                input_path, output_path, text_prompt, threshold, smoothing_sigma\n            )\n            output_paths.append(result_path)\n\n        return output_paths\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation.__init__","title":"<code>__init__(model_name='CIDAS/clipseg-rd64-refined', device=None, tile_size=512, overlap=32)</code>","text":"<p>Initialize the ImageSegmenter with the specified model and settings.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".</p> <code>'CIDAS/clipseg-rd64-refined'</code> <code>device</code> <code>str</code> <p>Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 512.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 32.</p> <code>32</code> Source code in <code>geoai/segment.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"CIDAS/clipseg-rd64-refined\",\n    device: Optional[str] = None,\n    tile_size: int = 512,\n    overlap: int = 32,\n) -&gt; None:\n    \"\"\"\n    Initialize the ImageSegmenter with the specified model and settings.\n\n    Args:\n        model_name (str): Name of the CLIP-Seg model to use. Defaults to \"CIDAS/clipseg-rd64-refined\".\n        device (str): Device to run the model on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 512.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 32.\n    \"\"\"\n    self.tile_size = tile_size\n    self.overlap = overlap\n\n    # Set device\n    if device is None:\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    else:\n        self.device = device\n\n    # Load model and processor\n    self.processor = CLIPSegProcessor.from_pretrained(model_name)\n    self.model = CLIPSegForImageSegmentation.from_pretrained(model_name).to(\n        self.device\n    )\n\n    print(f\"Model loaded on {self.device}\")\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation.segment_image","title":"<code>segment_image(input_path, output_path, text_prompt, threshold=0.5, smoothing_sigma=1.0)</code>","text":"<p>Segment a GeoTIFF image using the provided text prompt.</p> <p>The function processes the image in tiles and saves the result as a GeoTIFF with two bands: - Band 1: Binary segmentation mask (0 or 1) - Band 2: Probability scores (0.0 to 1.0)</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output GeoTIFF will be saved.</p> required <code>text_prompt</code> <code>str</code> <p>Text description of what to segment (e.g., \"water\", \"buildings\").</p> required <code>threshold</code> <code>float</code> <p>Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.</p> <code>0.5</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved output file.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image(\n    self,\n    input_path: str,\n    output_path: str,\n    text_prompt: str,\n    threshold: float = 0.5,\n    smoothing_sigma: float = 1.0,\n) -&gt; str:\n    \"\"\"\n    Segment a GeoTIFF image using the provided text prompt.\n\n    The function processes the image in tiles and saves the result as a GeoTIFF with two bands:\n    - Band 1: Binary segmentation mask (0 or 1)\n    - Band 2: Probability scores (0.0 to 1.0)\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str): Path where the output GeoTIFF will be saved.\n        text_prompt (str): Text description of what to segment (e.g., \"water\", \"buildings\").\n        threshold (float): Threshold for binary segmentation (0.0 to 1.0). Defaults to 0.5.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n\n    Returns:\n        str: Path to the saved output file.\n    \"\"\"\n    # Open the input GeoTIFF\n    with rasterio.open(input_path) as src:\n        # Get metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Create output metadata\n        out_meta = meta.copy()\n        out_meta.update({\"count\": 2, \"dtype\": \"float32\", \"nodata\": None})\n\n        # Create arrays for results\n        segmentation = np.zeros((height, width), dtype=np.float32)\n        probabilities = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate effective tile size (accounting for overlap)\n        effective_tile_size = self.tile_size - 2 * self.overlap\n\n        # Calculate number of tiles\n        n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n        n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n        total_tiles = n_tiles_x * n_tiles_y\n\n        # Process tiles with tqdm progress bar\n        with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n            # Iterate through tiles\n            for y in range(n_tiles_y):\n                for x in range(n_tiles_x):\n                    # Calculate tile coordinates with overlap\n                    x_start = max(0, x * effective_tile_size - self.overlap)\n                    y_start = max(0, y * effective_tile_size - self.overlap)\n                    x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                    y_end = min(\n                        height, (y + 1) * effective_tile_size + self.overlap\n                    )\n\n                    tile_width = x_end - x_start\n                    tile_height = y_end - y_start\n\n                    # Read the tile\n                    window = Window(x_start, y_start, tile_width, tile_height)\n                    tile_data = src.read(window=window)\n\n                    # Process the tile\n                    try:\n                        # Convert to RGB if necessary (handling different satellite bands)\n                        if tile_data.shape[0] &gt; 3:\n                            # Use first three bands for RGB representation\n                            rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                            # Normalize data to 0-255 range if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n                        elif tile_data.shape[0] == 1:\n                            # Create RGB from grayscale\n                            rgb_tile = np.repeat(\n                                tile_data[0][:, :, np.newaxis], 3, axis=2\n                            )\n                            # Normalize if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n                        else:\n                            # Already 3-channel, assume RGB\n                            rgb_tile = tile_data.transpose(1, 2, 0)\n                            # Normalize if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n\n                        # Convert to PIL Image\n                        pil_image = Image.fromarray(rgb_tile)\n\n                        # Resize if needed to match model's requirements\n                        if (\n                            pil_image.width &gt; self.tile_size\n                            or pil_image.height &gt; self.tile_size\n                        ):\n                            # Keep aspect ratio - use LANCZOS resampling instead of deprecated constant\n                            pil_image.thumbnail(\n                                (self.tile_size, self.tile_size),\n                                Image.Resampling.LANCZOS,\n                            )\n\n                        # Process with CLIP-Seg\n                        inputs = self.processor(\n                            text=text_prompt, images=pil_image, return_tensors=\"pt\"\n                        ).to(self.device)\n\n                        # Forward pass\n                        with torch.no_grad():\n                            outputs = self.model(**inputs)\n\n                        # Get logits and resize to original tile size\n                        logits = outputs.logits[0]\n\n                        # Convert logits to probabilities with sigmoid\n                        probs = torch.sigmoid(logits).cpu().numpy()\n\n                        # Resize back to original tile size if needed\n                        if probs.shape != (tile_height, tile_width):\n                            # Use bicubic interpolation for smoother results\n                            probs_resized = np.array(\n                                Image.fromarray(probs).resize(\n                                    (tile_width, tile_height),\n                                    Image.Resampling.BICUBIC,\n                                )\n                            )\n                        else:\n                            probs_resized = probs\n\n                        # Apply gaussian blur to reduce blockiness\n                        try:\n                            from scipy.ndimage import gaussian_filter\n\n                            probs_resized = gaussian_filter(\n                                probs_resized, sigma=smoothing_sigma\n                            )\n                        except ImportError:\n                            pass  # Continue without smoothing if scipy is not available\n\n                        # Store results in the full arrays\n                        # Only store the non-overlapping part (except at edges)\n                        valid_x_start = self.overlap if x &gt; 0 else 0\n                        valid_y_start = self.overlap if y &gt; 0 else 0\n                        valid_x_end = (\n                            tile_width - self.overlap\n                            if x &lt; n_tiles_x - 1\n                            else tile_width\n                        )\n                        valid_y_end = (\n                            tile_height - self.overlap\n                            if y &lt; n_tiles_y - 1\n                            else tile_height\n                        )\n\n                        dest_x_start = x_start + valid_x_start\n                        dest_y_start = y_start + valid_y_start\n                        dest_x_end = x_start + valid_x_end\n                        dest_y_end = y_start + valid_y_end\n\n                        # Store probabilities\n                        probabilities[\n                            dest_y_start:dest_y_end, dest_x_start:dest_x_end\n                        ] = probs_resized[\n                            valid_y_start:valid_y_end, valid_x_start:valid_x_end\n                        ]\n\n                    except Exception as e:\n                        print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                        # Continue with next tile\n\n                    # Update progress bar\n                    pbar.update(1)\n\n        # Create binary segmentation from probabilities\n        segmentation = (probabilities &gt;= threshold).astype(np.float32)\n\n        # Write the output GeoTIFF\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(segmentation, 1)\n            dst.write(probabilities, 2)\n\n            # Add descriptions to bands\n            dst.set_band_description(1, \"Binary Segmentation\")\n            dst.set_band_description(2, \"Probability Scores\")\n\n        print(f\"Segmentation saved to {output_path}\")\n        return output_path\n</code></pre>"},{"location":"segment/#geoai.segment.CLIPSegmentation.segment_image_batch","title":"<code>segment_image_batch(input_paths, output_dir, text_prompt, threshold=0.5, smoothing_sigma=1.0, suffix='_segmented')</code>","text":"<p>Segment multiple GeoTIFF images using the provided text prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>list</code> <p>List of paths to input GeoTIFF files.</p> required <code>output_dir</code> <code>str</code> <p>Directory where output GeoTIFFs will be saved.</p> required <code>text_prompt</code> <code>str</code> <p>Text description of what to segment.</p> required <code>threshold</code> <code>float</code> <p>Threshold for binary segmentation. Defaults to 0.5.</p> <code>0.5</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.</p> <code>1.0</code> <code>suffix</code> <code>str</code> <p>Suffix to add to output filenames. Defaults to \"_segmented\".</p> <code>'_segmented'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>Paths to all saved output files.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image_batch(\n    self,\n    input_paths: List[str],\n    output_dir: str,\n    text_prompt: str,\n    threshold: float = 0.5,\n    smoothing_sigma: float = 1.0,\n    suffix: str = \"_segmented\",\n) -&gt; List[str]:\n    \"\"\"\n    Segment multiple GeoTIFF images using the provided text prompt.\n\n    Args:\n        input_paths (list): List of paths to input GeoTIFF files.\n        output_dir (str): Directory where output GeoTIFFs will be saved.\n        text_prompt (str): Text description of what to segment.\n        threshold (float): Threshold for binary segmentation. Defaults to 0.5.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness. Defaults to 1.0.\n        suffix (str): Suffix to add to output filenames. Defaults to \"_segmented\".\n\n    Returns:\n        list: Paths to all saved output files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    output_paths = []\n\n    # Process each input file\n    for input_path in tqdm(input_paths, desc=\"Processing files\"):\n        # Generate output path\n        filename = os.path.basename(input_path)\n        base_name, ext = os.path.splitext(filename)\n        output_path = os.path.join(output_dir, f\"{base_name}{suffix}{ext}\")\n\n        # Segment the image\n        result_path = self.segment_image(\n            input_path, output_path, text_prompt, threshold, smoothing_sigma\n        )\n        output_paths.append(result_path)\n\n    return output_paths\n</code></pre>"},{"location":"segment/#geoai.segment.DetectionResult","title":"<code>DetectionResult</code>  <code>dataclass</code>","text":"<p>Represents a detection result with score, label, bounding box, and optional mask.</p> Source code in <code>geoai/segment.py</code> <pre><code>@dataclass\nclass DetectionResult:\n    \"\"\"Represents a detection result with score, label, bounding box, and optional mask.\"\"\"\n\n    score: float\n    label: str\n    box: BoundingBox\n    mask: Optional[np.array] = None\n\n    @classmethod\n    def from_dict(cls, detection_dict: Dict) -&gt; \"DetectionResult\":\n        return cls(\n            score=detection_dict[\"score\"],\n            label=detection_dict[\"label\"],\n            box=BoundingBox(\n                xmin=detection_dict[\"box\"][\"xmin\"],\n                ymin=detection_dict[\"box\"][\"ymin\"],\n                xmax=detection_dict[\"box\"][\"xmax\"],\n                ymax=detection_dict[\"box\"][\"ymax\"],\n            ),\n        )\n</code></pre>"},{"location":"segment/#geoai.segment.GroundedSAM","title":"<code>GroundedSAM</code>","text":"<p>A class for segmenting remote sensing imagery using text prompts with Grounding DINO + SAM.</p> <p>This class combines Grounding DINO for object detection and Segment Anything Model (SAM) for precise segmentation based on text prompts. It can process large GeoTIFF files by tiling them and handles proper georeferencing in the outputs.</p> <p>Parameters:</p> Name Type Description Default <code>detector_id</code> <code>str</code> <p>Hugging Face model ID for Grounding DINO. Defaults to \"IDEA-Research/grounding-dino-tiny\".</p> <code>'IDEA-Research/grounding-dino-tiny'</code> <code>segmenter_id</code> <code>str</code> <p>Hugging Face model ID for SAM. Defaults to \"facebook/sam-vit-base\".</p> <code>'facebook/sam-vit-base'</code> <code>device</code> <code>str</code> <p>Device to run the models on ('cuda', 'cpu'). If None, will use CUDA if available.</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks. Defaults to 1024.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts. Defaults to 128.</p> <code>128</code> <code>threshold</code> <code>float</code> <p>Detection threshold for Grounding DINO. Defaults to 0.3.</p> <code>0.3</code> <p>Attributes:</p> Name Type Description <code>detector_id</code> <code>str</code> <p>The Grounding DINO model ID.</p> <code>segmenter_id</code> <code>str</code> <p>The SAM model ID.</p> <code>device</code> <code>str</code> <p>The device being used ('cuda' or 'cpu').</p> <code>tile_size</code> <code>int</code> <p>Size of tiles for processing.</p> <code>overlap</code> <code>int</code> <p>Overlap between tiles.</p> <code>threshold</code> <code>float</code> <p>Detection threshold.</p> <code>object_detector</code> <code>float</code> <p>The Grounding DINO pipeline.</p> <code>segmentator</code> <code>float</code> <p>The SAM model.</p> <code>processor</code> <code>float</code> <p>The SAM processor.</p> Source code in <code>geoai/segment.py</code> <pre><code>class GroundedSAM:\n    \"\"\"\n    A class for segmenting remote sensing imagery using text prompts with Grounding DINO + SAM.\n\n    This class combines Grounding DINO for object detection and Segment Anything Model (SAM) for\n    precise segmentation based on text prompts. It can process large GeoTIFF files by tiling them\n    and handles proper georeferencing in the outputs.\n\n    Args:\n        detector_id (str): Hugging Face model ID for Grounding DINO. Defaults to \"IDEA-Research/grounding-dino-tiny\".\n        segmenter_id (str): Hugging Face model ID for SAM. Defaults to \"facebook/sam-vit-base\".\n        device (str): Device to run the models on ('cuda', 'cpu'). If None, will use CUDA if available.\n        tile_size (int): Size of tiles to process the image in chunks. Defaults to 1024.\n        overlap (int): Overlap between tiles to avoid edge artifacts. Defaults to 128.\n        threshold (float): Detection threshold for Grounding DINO. Defaults to 0.3.\n\n    Attributes:\n        detector_id (str): The Grounding DINO model ID.\n        segmenter_id (str): The SAM model ID.\n        device (str): The device being used ('cuda' or 'cpu').\n        tile_size (int): Size of tiles for processing.\n        overlap (int): Overlap between tiles.\n        threshold (float): Detection threshold.\n        object_detector: The Grounding DINO pipeline.\n        segmentator: The SAM model.\n        processor: The SAM processor.\n    \"\"\"\n\n    def __init__(\n        self,\n        detector_id: str = \"IDEA-Research/grounding-dino-tiny\",\n        segmenter_id: str = \"facebook/sam-vit-base\",\n        device: Optional[str] = None,\n        tile_size: int = 1024,\n        overlap: int = 128,\n        threshold: float = 0.3,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the GroundedSAM with the specified models and settings.\n\n        Args:\n            detector_id (str): Hugging Face model ID for Grounding DINO.\n            segmenter_id (str): Hugging Face model ID for SAM.\n            device (str): Device to run the models on ('cuda', 'cpu').\n            tile_size (int): Size of tiles to process the image in chunks.\n            overlap (int): Overlap between tiles to avoid edge artifacts.\n            threshold (float): Detection threshold for Grounding DINO.\n        \"\"\"\n        self.detector_id = detector_id\n        self.segmenter_id = segmenter_id\n        self.tile_size = tile_size\n        self.overlap = overlap\n        self.threshold = threshold\n\n        # Set device\n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n\n        # Load models\n        self._load_models()\n\n        print(f\"GroundedSAM initialized on {self.device}\")\n\n    def _load_models(self) -&gt; None:\n        \"\"\"Load the Grounding DINO and SAM models.\"\"\"\n        # Load Grounding DINO\n        self.object_detector = pipeline(\n            model=self.detector_id,\n            task=\"zero-shot-object-detection\",\n            device=self.device,\n        )\n\n        # Load SAM\n        self.segmentator = AutoModelForMaskGeneration.from_pretrained(\n            self.segmenter_id\n        ).to(self.device)\n        self.processor = AutoProcessor.from_pretrained(self.segmenter_id)\n\n    def _detect(self, image: Image.Image, labels: List[str]) -&gt; List[DetectionResult]:\n        \"\"\"\n        Use Grounding DINO to detect objects in an image.\n\n        Args:\n            image (Image.Image): PIL image to detect objects in.\n            labels (List[str]): List of text labels to detect.\n\n        Returns:\n            List[DetectionResult]: List of detection results.\n        \"\"\"\n        # Ensure labels end with periods\n        labels = [label if label.endswith(\".\") else label + \".\" for label in labels]\n\n        results = self.object_detector(\n            image, candidate_labels=labels, threshold=self.threshold\n        )\n        results = [DetectionResult.from_dict(result) for result in results]\n\n        return results\n\n    def _apply_nms(\n        self, detections: List[DetectionResult], iou_threshold: float = 0.5\n    ) -&gt; List[DetectionResult]:\n        \"\"\"\n        Apply Non-Maximum Suppression to remove overlapping detections.\n\n        Args:\n            detections (List[DetectionResult]): List of detection results.\n            iou_threshold (float): IoU threshold for NMS.\n\n        Returns:\n            List[DetectionResult]: Filtered detection results.\n        \"\"\"\n        if not detections:\n            return detections\n\n        # Convert to format for NMS\n        boxes = []\n        scores = []\n\n        for detection in detections:\n            boxes.append(\n                [\n                    detection.box.xmin,\n                    detection.box.ymin,\n                    detection.box.xmax,\n                    detection.box.ymax,\n                ]\n            )\n            scores.append(detection.score)\n\n        boxes = np.array(boxes, dtype=np.float32)\n        scores = np.array(scores, dtype=np.float32)\n\n        # Apply NMS using OpenCV\n        indices = cv2.dnn.NMSBoxes(boxes, scores, self.threshold, iou_threshold)\n\n        if len(indices) &gt; 0:\n            indices = indices.flatten()\n            return [detections[i] for i in indices]\n        else:\n            return []\n\n    def _get_boxes(self, results: List[DetectionResult]) -&gt; List[List[List[float]]]:\n        \"\"\"Extract bounding boxes from detection results.\"\"\"\n        boxes = []\n        for result in results:\n            xyxy = result.box.xyxy\n            boxes.append(xyxy)\n        return [boxes]\n\n    def _refine_masks(\n        self, masks: torch.BoolTensor, polygon_refinement: bool = False\n    ) -&gt; List[np.ndarray]:\n        \"\"\"Refine masks from SAM output.\"\"\"\n        masks = masks.cpu().float()\n        masks = masks.permute(0, 2, 3, 1)\n        masks = masks.mean(axis=-1)\n        masks = (masks &gt; 0).int()\n        masks = masks.numpy().astype(np.uint8)\n        masks = list(masks)\n\n        if polygon_refinement:\n            for idx, mask in enumerate(masks):\n                shape = mask.shape\n                polygon = self._mask_to_polygon(mask)\n                if polygon:\n                    mask = self._polygon_to_mask(polygon, shape)\n                    masks[idx] = mask\n\n        return masks\n\n    def _mask_to_polygon(self, mask: np.ndarray) -&gt; List[List[int]]:\n        \"\"\"Convert mask to polygon coordinates.\"\"\"\n        # Find contours in the binary mask\n        contours, _ = cv2.findContours(\n            mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        if not contours:\n            return []\n\n        # Find the contour with the largest area\n        largest_contour = max(contours, key=cv2.contourArea)\n\n        # Extract the vertices of the contour\n        polygon = largest_contour.reshape(-1, 2).tolist()\n\n        return polygon\n\n    def _polygon_to_mask(\n        self, polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]\n    ) -&gt; np.ndarray:\n        \"\"\"Convert polygon to mask.\"\"\"\n        # Create an empty mask\n        mask = np.zeros(image_shape, dtype=np.uint8)\n\n        # Convert polygon to an array of points\n        pts = np.array(polygon, dtype=np.int32)\n\n        # Fill the polygon with white color (255)\n        cv2.fillPoly(mask, [pts], color=(255,))\n\n        return mask\n\n    def _separate_instances(\n        self, mask: np.ndarray, min_area: int = 50\n    ) -&gt; List[np.ndarray]:\n        \"\"\"\n        Separate individual instances from a combined mask using connected components.\n\n        Args:\n            mask (np.ndarray): Combined binary mask.\n            min_area (int): Minimum area threshold for valid instances.\n\n        Returns:\n            List[np.ndarray]: List of individual instance masks.\n        \"\"\"\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            mask.astype(np.uint8), connectivity=8\n        )\n\n        instances = []\n        for i in range(1, num_labels):  # Skip background (label 0)\n            # Get area of the component\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Filter by minimum area\n            if area &gt;= min_area:\n                # Create mask for this instance\n                instance_mask = (labels == i).astype(np.uint8) * 255\n                instances.append(instance_mask)\n\n        return instances\n\n    def _mask_to_polygons(\n        self,\n        mask: np.ndarray,\n        transform,\n        x_offset: int = 0,\n        y_offset: int = 0,\n        min_area: int = 50,\n        simplify_tolerance: float = 1.0,\n    ) -&gt; List[Dict]:\n        \"\"\"\n        Convert mask to individual polygons with geospatial coordinates.\n\n        Args:\n            mask (np.ndarray): Binary mask.\n            transform: Rasterio transform object.\n            x_offset (int): X offset for tile position.\n            y_offset (int): Y offset for tile position.\n            min_area (int): Minimum area threshold for valid polygons.\n            simplify_tolerance (float): Tolerance for polygon simplification.\n\n        Returns:\n            List[Dict]: List of polygon dictionaries with geometry and properties.\n        \"\"\"\n        polygons = []\n\n        # Get individual instances\n        instances = self._separate_instances(mask, min_area)\n\n        for instance_mask in instances:\n            # Find contours\n            contours, _ = cv2.findContours(\n                instance_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            for contour in contours:\n                # Filter by minimum area\n                area = cv2.contourArea(contour)\n                if area &lt; min_area:\n                    continue\n\n                # Simplify contour\n                epsilon = simplify_tolerance\n                simplified_contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to pixel coordinates (add offsets)\n                pixel_coords = simplified_contour.reshape(-1, 2)\n                pixel_coords = pixel_coords + [x_offset, y_offset]\n\n                # Convert to geographic coordinates\n                geo_coords = []\n                for x, y in pixel_coords:\n                    geo_x, geo_y = transform * (x, y)\n                    geo_coords.append([geo_x, geo_y])\n\n                # Close the polygon if needed\n                if len(geo_coords) &gt; 2:\n                    if geo_coords[0] != geo_coords[-1]:\n                        geo_coords.append(geo_coords[0])\n\n                    # Create Shapely polygon\n                    try:\n                        polygon = Polygon(geo_coords)\n                        if polygon.is_valid and polygon.area &gt; 0:\n                            polygons.append({\"geometry\": polygon, \"area_pixels\": area})\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n                        continue\n\n        return polygons\n\n    def _segment(\n        self,\n        image: Image.Image,\n        detection_results: List[DetectionResult],\n        polygon_refinement: bool = False,\n    ) -&gt; List[DetectionResult]:\n        \"\"\"\n        Use SAM to generate masks for detected objects.\n\n        Args:\n            image (Image.Image): PIL image.\n            detection_results (List[DetectionResult]): Detection results from Grounding DINO.\n            polygon_refinement (bool): Whether to refine masks using polygon fitting.\n\n        Returns:\n            List[DetectionResult]: Detection results with masks.\n        \"\"\"\n        if not detection_results:\n            return detection_results\n\n        boxes = self._get_boxes(detection_results)\n        inputs = self.processor(\n            images=image, input_boxes=boxes, return_tensors=\"pt\"\n        ).to(self.device)\n\n        outputs = self.segmentator(**inputs)\n        masks = self.processor.post_process_masks(\n            masks=outputs.pred_masks,\n            original_sizes=inputs.original_sizes,\n            reshaped_input_sizes=inputs.reshaped_input_sizes,\n        )[0]\n\n        masks = self._refine_masks(masks, polygon_refinement)\n\n        for detection_result, mask in zip(detection_results, masks):\n            detection_result.mask = mask\n\n        return detection_results\n\n    def segment_image(\n        self,\n        input_path: str,\n        output_path: str,\n        text_prompts: Union[str, List[str]],\n        polygon_refinement: bool = False,\n        export_boxes: bool = False,\n        export_polygons: bool = True,\n        smoothing_sigma: float = 1.0,\n        nms_threshold: float = 0.5,\n        min_polygon_area: int = 50,\n        simplify_tolerance: float = 2.0,\n    ) -&gt; str:\n        \"\"\"\n        Segment a GeoTIFF image using text prompts with improved instance segmentation.\n\n        Args:\n            input_path (str): Path to the input GeoTIFF file.\n            output_path (str): Path where the output GeoTIFF will be saved.\n            text_prompts (Union[str, List[str]]): Text prompt(s) describing what to segment.\n            polygon_refinement (bool): Whether to refine masks using polygon fitting.\n            export_boxes (bool): Whether to export bounding boxes as a separate vector file.\n            export_polygons (bool): Whether to export segmentation polygons as vector file.\n            smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness.\n            nms_threshold (float): Non-maximum suppression threshold for removing overlapping detections.\n            min_polygon_area (int): Minimum area in pixels for valid polygons.\n            simplify_tolerance (float): Tolerance for polygon simplification.\n\n        Returns:\n            Dict: Dictionary containing paths to output files.\n        \"\"\"\n        if isinstance(text_prompts, str):\n            text_prompts = [text_prompts]\n\n        # Open the input GeoTIFF\n        with rasterio.open(input_path) as src:\n            # Get metadata\n            meta = src.meta\n            height = src.height\n            width = src.width\n            transform = src.transform\n            crs = src.crs\n\n            # Create output metadata for segmentation masks\n            out_meta = meta.copy()\n            out_meta.update(\n                {\"count\": len(text_prompts) + 1, \"dtype\": \"uint8\", \"nodata\": 0}\n            )\n\n            # Create arrays for results\n            all_masks = np.zeros((len(text_prompts), height, width), dtype=np.uint8)\n            all_boxes = []\n            all_polygons = []\n\n            # Calculate effective tile size (accounting for overlap)\n            effective_tile_size = self.tile_size - 2 * self.overlap\n\n            # Calculate number of tiles\n            n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n            n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n            total_tiles = n_tiles_x * n_tiles_y\n\n            print(f\"Processing {total_tiles} tiles ({n_tiles_x}x{n_tiles_y})\")\n\n            # Process tiles with tqdm progress bar\n            with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n                # Iterate through tiles\n                for y in range(n_tiles_y):\n                    for x in range(n_tiles_x):\n                        # Calculate tile coordinates with overlap\n                        x_start = max(0, x * effective_tile_size - self.overlap)\n                        y_start = max(0, y * effective_tile_size - self.overlap)\n                        x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                        y_end = min(\n                            height, (y + 1) * effective_tile_size + self.overlap\n                        )\n\n                        tile_width = x_end - x_start\n                        tile_height = y_end - y_start\n\n                        # Read the tile\n                        window = Window(x_start, y_start, tile_width, tile_height)\n                        tile_data = src.read(window=window)\n\n                        # Process the tile\n                        try:\n                            # Convert to RGB format for processing\n                            if tile_data.shape[0] &gt;= 3:\n                                # Use first three bands for RGB representation\n                                rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                            elif tile_data.shape[0] == 1:\n                                # Create RGB from grayscale\n                                rgb_tile = np.repeat(\n                                    tile_data[0][:, :, np.newaxis], 3, axis=2\n                                )\n                            else:\n                                print(\n                                    f\"Unsupported number of bands: {tile_data.shape[0]}\"\n                                )\n                                continue\n\n                            # Normalize to 0-255 range if needed\n                            if rgb_tile.max() &gt; 0:\n                                rgb_tile = (\n                                    (rgb_tile - rgb_tile.min())\n                                    / (rgb_tile.max() - rgb_tile.min())\n                                    * 255\n                                ).astype(np.uint8)\n\n                            # Convert to PIL Image\n                            pil_image = Image.fromarray(rgb_tile)\n\n                            # Detect objects\n                            detections = self._detect(pil_image, text_prompts)\n\n                            if detections:\n                                # Apply Non-Maximum Suppression to reduce overlapping detections\n                                detections = self._apply_nms(detections, nms_threshold)\n\n                                if detections:\n                                    # Segment objects\n                                    detections = self._segment(\n                                        pil_image, detections, polygon_refinement\n                                    )\n\n                                    # Process results\n                                    for i, prompt in enumerate(text_prompts):\n                                        prompt_polygons = []\n                                        prompt_mask = np.zeros(\n                                            (tile_height, tile_width), dtype=np.uint8\n                                        )\n\n                                        for detection in detections:\n                                            if (\n                                                detection.label.replace(\".\", \"\")\n                                                .strip()\n                                                .lower()\n                                                == prompt.lower()\n                                            ):\n                                                if detection.mask is not None:\n                                                    # Apply gaussian blur to reduce blockiness\n                                                    try:\n                                                        from scipy.ndimage import (\n                                                            gaussian_filter,\n                                                        )\n\n                                                        smoothed_mask = gaussian_filter(\n                                                            detection.mask.astype(\n                                                                float\n                                                            ),\n                                                            sigma=smoothing_sigma,\n                                                        )\n                                                        detection.mask = (\n                                                            smoothed_mask &gt; 0.5\n                                                        ).astype(np.uint8)\n                                                    except ImportError:\n                                                        pass\n\n                                                    # Add to combined mask for this prompt\n                                                    prompt_mask = np.maximum(\n                                                        prompt_mask, detection.mask\n                                                    )\n\n                                                # Store bounding box with geospatial coordinates\n                                                if export_boxes:\n                                                    bbox = detection.box\n                                                    x_geo_min, y_geo_min = transform * (\n                                                        x_start + bbox.xmin,\n                                                        y_start + bbox.ymin,\n                                                    )\n                                                    x_geo_max, y_geo_max = transform * (\n                                                        x_start + bbox.xmax,\n                                                        y_start + bbox.ymax,\n                                                    )\n\n                                                    geo_box = {\n                                                        \"label\": detection.label,\n                                                        \"score\": detection.score,\n                                                        \"prompt\": prompt,\n                                                        \"geometry\": box(\n                                                            x_geo_min,\n                                                            y_geo_max,\n                                                            x_geo_max,\n                                                            y_geo_min,\n                                                        ),\n                                                    }\n                                                    all_boxes.append(geo_box)\n\n                                        # Convert masks to individual polygons\n                                        if export_polygons and np.any(prompt_mask):\n                                            tile_polygons = self._mask_to_polygons(\n                                                prompt_mask,\n                                                transform,\n                                                x_start,\n                                                y_start,\n                                                min_polygon_area,\n                                                simplify_tolerance,\n                                            )\n\n                                            # Add metadata to polygons\n                                            for poly_data in tile_polygons:\n                                                poly_data.update(\n                                                    {\n                                                        \"label\": prompt,\n                                                        \"score\": max(\n                                                            [\n                                                                d.score\n                                                                for d in detections\n                                                                if d.label.replace(\n                                                                    \".\", \"\"\n                                                                )\n                                                                .strip()\n                                                                .lower()\n                                                                == prompt.lower()\n                                                            ],\n                                                            default=0.0,\n                                                        ),\n                                                        \"tile_x\": x,\n                                                        \"tile_y\": y,\n                                                    }\n                                                )\n                                                all_polygons.append(poly_data)\n\n                                        # Store mask in the global array\n                                        valid_x_start = self.overlap if x &gt; 0 else 0\n                                        valid_y_start = self.overlap if y &gt; 0 else 0\n                                        valid_x_end = (\n                                            tile_width - self.overlap\n                                            if x &lt; n_tiles_x - 1\n                                            else tile_width\n                                        )\n                                        valid_y_end = (\n                                            tile_height - self.overlap\n                                            if y &lt; n_tiles_y - 1\n                                            else tile_height\n                                        )\n\n                                        dest_x_start = x_start + valid_x_start\n                                        dest_y_start = y_start + valid_y_start\n                                        dest_x_end = x_start + valid_x_end\n                                        dest_y_end = y_start + valid_y_end\n\n                                        mask_slice = prompt_mask[\n                                            valid_y_start:valid_y_end,\n                                            valid_x_start:valid_x_end,\n                                        ]\n                                        all_masks[\n                                            i,\n                                            dest_y_start:dest_y_end,\n                                            dest_x_start:dest_x_end,\n                                        ] = np.maximum(\n                                            all_masks[\n                                                i,\n                                                dest_y_start:dest_y_end,\n                                                dest_x_start:dest_x_end,\n                                            ],\n                                            mask_slice,\n                                        )\n\n                        except Exception as e:\n                            print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                            continue\n\n                        # Update progress bar\n                        pbar.update(1)\n\n            # Create combined mask (union of all individual masks)\n            combined_mask = np.any(all_masks, axis=0).astype(np.uint8)\n\n            # Write the output GeoTIFF\n            with rasterio.open(output_path, \"w\", **out_meta) as dst:\n                # Write combined mask as first band\n                dst.write(combined_mask, 1)\n\n                # Write individual masks for each prompt\n                for i, mask in enumerate(all_masks):\n                    dst.write(mask, i + 2)\n\n                # Add descriptions to bands\n                dst.set_band_description(1, \"Combined Segmentation\")\n                for i, prompt in enumerate(text_prompts):\n                    dst.set_band_description(i + 2, f\"Segmentation: {prompt}\")\n\n            result_files = {\"segmentation\": output_path}\n\n            # Export bounding boxes if requested\n            if export_boxes and all_boxes:\n                boxes_path = output_path.replace(\".tif\", \"_boxes.geojson\")\n                gdf = gpd.GeoDataFrame(all_boxes, crs=crs)\n                gdf.to_file(boxes_path, driver=\"GeoJSON\")\n                result_files[\"boxes\"] = boxes_path\n                print(f\"Exported {len(all_boxes)} bounding boxes to {boxes_path}\")\n\n            # Export instance polygons if requested\n            if export_polygons and all_polygons:\n                polygons_path = output_path.replace(\".tif\", \"_polygons.geojson\")\n                gdf = gpd.GeoDataFrame(all_polygons, crs=crs)\n                gdf.to_file(polygons_path, driver=\"GeoJSON\")\n                result_files[\"polygons\"] = polygons_path\n                print(\n                    f\"Exported {len(all_polygons)} instance polygons to {polygons_path}\"\n                )\n\n            print(f\"Segmentation saved to {output_path}\")\n            print(\n                f\"Found {len(all_polygons)} individual building instances\"\n                if export_polygons\n                else \"\"\n            )\n\n            return result_files\n</code></pre>"},{"location":"segment/#geoai.segment.GroundedSAM.__init__","title":"<code>__init__(detector_id='IDEA-Research/grounding-dino-tiny', segmenter_id='facebook/sam-vit-base', device=None, tile_size=1024, overlap=128, threshold=0.3)</code>","text":"<p>Initialize the GroundedSAM with the specified models and settings.</p> <p>Parameters:</p> Name Type Description Default <code>detector_id</code> <code>str</code> <p>Hugging Face model ID for Grounding DINO.</p> <code>'IDEA-Research/grounding-dino-tiny'</code> <code>segmenter_id</code> <code>str</code> <p>Hugging Face model ID for SAM.</p> <code>'facebook/sam-vit-base'</code> <code>device</code> <code>str</code> <p>Device to run the models on ('cuda', 'cpu').</p> <code>None</code> <code>tile_size</code> <code>int</code> <p>Size of tiles to process the image in chunks.</p> <code>1024</code> <code>overlap</code> <code>int</code> <p>Overlap between tiles to avoid edge artifacts.</p> <code>128</code> <code>threshold</code> <code>float</code> <p>Detection threshold for Grounding DINO.</p> <code>0.3</code> Source code in <code>geoai/segment.py</code> <pre><code>def __init__(\n    self,\n    detector_id: str = \"IDEA-Research/grounding-dino-tiny\",\n    segmenter_id: str = \"facebook/sam-vit-base\",\n    device: Optional[str] = None,\n    tile_size: int = 1024,\n    overlap: int = 128,\n    threshold: float = 0.3,\n) -&gt; None:\n    \"\"\"\n    Initialize the GroundedSAM with the specified models and settings.\n\n    Args:\n        detector_id (str): Hugging Face model ID for Grounding DINO.\n        segmenter_id (str): Hugging Face model ID for SAM.\n        device (str): Device to run the models on ('cuda', 'cpu').\n        tile_size (int): Size of tiles to process the image in chunks.\n        overlap (int): Overlap between tiles to avoid edge artifacts.\n        threshold (float): Detection threshold for Grounding DINO.\n    \"\"\"\n    self.detector_id = detector_id\n    self.segmenter_id = segmenter_id\n    self.tile_size = tile_size\n    self.overlap = overlap\n    self.threshold = threshold\n\n    # Set device\n    if device is None:\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    else:\n        self.device = device\n\n    # Load models\n    self._load_models()\n\n    print(f\"GroundedSAM initialized on {self.device}\")\n</code></pre>"},{"location":"segment/#geoai.segment.GroundedSAM.segment_image","title":"<code>segment_image(input_path, output_path, text_prompts, polygon_refinement=False, export_boxes=False, export_polygons=True, smoothing_sigma=1.0, nms_threshold=0.5, min_polygon_area=50, simplify_tolerance=2.0)</code>","text":"<p>Segment a GeoTIFF image using text prompts with improved instance segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path where the output GeoTIFF will be saved.</p> required <code>text_prompts</code> <code>Union[str, List[str]]</code> <p>Text prompt(s) describing what to segment.</p> required <code>polygon_refinement</code> <code>bool</code> <p>Whether to refine masks using polygon fitting.</p> <code>False</code> <code>export_boxes</code> <code>bool</code> <p>Whether to export bounding boxes as a separate vector file.</p> <code>False</code> <code>export_polygons</code> <code>bool</code> <p>Whether to export segmentation polygons as vector file.</p> <code>True</code> <code>smoothing_sigma</code> <code>float</code> <p>Sigma value for Gaussian smoothing to reduce blockiness.</p> <code>1.0</code> <code>nms_threshold</code> <code>float</code> <p>Non-maximum suppression threshold for removing overlapping detections.</p> <code>0.5</code> <code>min_polygon_area</code> <code>int</code> <p>Minimum area in pixels for valid polygons.</p> <code>50</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for polygon simplification.</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>str</code> <p>Dictionary containing paths to output files.</p> Source code in <code>geoai/segment.py</code> <pre><code>def segment_image(\n    self,\n    input_path: str,\n    output_path: str,\n    text_prompts: Union[str, List[str]],\n    polygon_refinement: bool = False,\n    export_boxes: bool = False,\n    export_polygons: bool = True,\n    smoothing_sigma: float = 1.0,\n    nms_threshold: float = 0.5,\n    min_polygon_area: int = 50,\n    simplify_tolerance: float = 2.0,\n) -&gt; str:\n    \"\"\"\n    Segment a GeoTIFF image using text prompts with improved instance segmentation.\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str): Path where the output GeoTIFF will be saved.\n        text_prompts (Union[str, List[str]]): Text prompt(s) describing what to segment.\n        polygon_refinement (bool): Whether to refine masks using polygon fitting.\n        export_boxes (bool): Whether to export bounding boxes as a separate vector file.\n        export_polygons (bool): Whether to export segmentation polygons as vector file.\n        smoothing_sigma (float): Sigma value for Gaussian smoothing to reduce blockiness.\n        nms_threshold (float): Non-maximum suppression threshold for removing overlapping detections.\n        min_polygon_area (int): Minimum area in pixels for valid polygons.\n        simplify_tolerance (float): Tolerance for polygon simplification.\n\n    Returns:\n        Dict: Dictionary containing paths to output files.\n    \"\"\"\n    if isinstance(text_prompts, str):\n        text_prompts = [text_prompts]\n\n    # Open the input GeoTIFF\n    with rasterio.open(input_path) as src:\n        # Get metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n        transform = src.transform\n        crs = src.crs\n\n        # Create output metadata for segmentation masks\n        out_meta = meta.copy()\n        out_meta.update(\n            {\"count\": len(text_prompts) + 1, \"dtype\": \"uint8\", \"nodata\": 0}\n        )\n\n        # Create arrays for results\n        all_masks = np.zeros((len(text_prompts), height, width), dtype=np.uint8)\n        all_boxes = []\n        all_polygons = []\n\n        # Calculate effective tile size (accounting for overlap)\n        effective_tile_size = self.tile_size - 2 * self.overlap\n\n        # Calculate number of tiles\n        n_tiles_x = max(1, int(np.ceil(width / effective_tile_size)))\n        n_tiles_y = max(1, int(np.ceil(height / effective_tile_size)))\n        total_tiles = n_tiles_x * n_tiles_y\n\n        print(f\"Processing {total_tiles} tiles ({n_tiles_x}x{n_tiles_y})\")\n\n        # Process tiles with tqdm progress bar\n        with tqdm(total=total_tiles, desc=\"Processing tiles\") as pbar:\n            # Iterate through tiles\n            for y in range(n_tiles_y):\n                for x in range(n_tiles_x):\n                    # Calculate tile coordinates with overlap\n                    x_start = max(0, x * effective_tile_size - self.overlap)\n                    y_start = max(0, y * effective_tile_size - self.overlap)\n                    x_end = min(width, (x + 1) * effective_tile_size + self.overlap)\n                    y_end = min(\n                        height, (y + 1) * effective_tile_size + self.overlap\n                    )\n\n                    tile_width = x_end - x_start\n                    tile_height = y_end - y_start\n\n                    # Read the tile\n                    window = Window(x_start, y_start, tile_width, tile_height)\n                    tile_data = src.read(window=window)\n\n                    # Process the tile\n                    try:\n                        # Convert to RGB format for processing\n                        if tile_data.shape[0] &gt;= 3:\n                            # Use first three bands for RGB representation\n                            rgb_tile = tile_data[:3].transpose(1, 2, 0)\n                        elif tile_data.shape[0] == 1:\n                            # Create RGB from grayscale\n                            rgb_tile = np.repeat(\n                                tile_data[0][:, :, np.newaxis], 3, axis=2\n                            )\n                        else:\n                            print(\n                                f\"Unsupported number of bands: {tile_data.shape[0]}\"\n                            )\n                            continue\n\n                        # Normalize to 0-255 range if needed\n                        if rgb_tile.max() &gt; 0:\n                            rgb_tile = (\n                                (rgb_tile - rgb_tile.min())\n                                / (rgb_tile.max() - rgb_tile.min())\n                                * 255\n                            ).astype(np.uint8)\n\n                        # Convert to PIL Image\n                        pil_image = Image.fromarray(rgb_tile)\n\n                        # Detect objects\n                        detections = self._detect(pil_image, text_prompts)\n\n                        if detections:\n                            # Apply Non-Maximum Suppression to reduce overlapping detections\n                            detections = self._apply_nms(detections, nms_threshold)\n\n                            if detections:\n                                # Segment objects\n                                detections = self._segment(\n                                    pil_image, detections, polygon_refinement\n                                )\n\n                                # Process results\n                                for i, prompt in enumerate(text_prompts):\n                                    prompt_polygons = []\n                                    prompt_mask = np.zeros(\n                                        (tile_height, tile_width), dtype=np.uint8\n                                    )\n\n                                    for detection in detections:\n                                        if (\n                                            detection.label.replace(\".\", \"\")\n                                            .strip()\n                                            .lower()\n                                            == prompt.lower()\n                                        ):\n                                            if detection.mask is not None:\n                                                # Apply gaussian blur to reduce blockiness\n                                                try:\n                                                    from scipy.ndimage import (\n                                                        gaussian_filter,\n                                                    )\n\n                                                    smoothed_mask = gaussian_filter(\n                                                        detection.mask.astype(\n                                                            float\n                                                        ),\n                                                        sigma=smoothing_sigma,\n                                                    )\n                                                    detection.mask = (\n                                                        smoothed_mask &gt; 0.5\n                                                    ).astype(np.uint8)\n                                                except ImportError:\n                                                    pass\n\n                                                # Add to combined mask for this prompt\n                                                prompt_mask = np.maximum(\n                                                    prompt_mask, detection.mask\n                                                )\n\n                                            # Store bounding box with geospatial coordinates\n                                            if export_boxes:\n                                                bbox = detection.box\n                                                x_geo_min, y_geo_min = transform * (\n                                                    x_start + bbox.xmin,\n                                                    y_start + bbox.ymin,\n                                                )\n                                                x_geo_max, y_geo_max = transform * (\n                                                    x_start + bbox.xmax,\n                                                    y_start + bbox.ymax,\n                                                )\n\n                                                geo_box = {\n                                                    \"label\": detection.label,\n                                                    \"score\": detection.score,\n                                                    \"prompt\": prompt,\n                                                    \"geometry\": box(\n                                                        x_geo_min,\n                                                        y_geo_max,\n                                                        x_geo_max,\n                                                        y_geo_min,\n                                                    ),\n                                                }\n                                                all_boxes.append(geo_box)\n\n                                    # Convert masks to individual polygons\n                                    if export_polygons and np.any(prompt_mask):\n                                        tile_polygons = self._mask_to_polygons(\n                                            prompt_mask,\n                                            transform,\n                                            x_start,\n                                            y_start,\n                                            min_polygon_area,\n                                            simplify_tolerance,\n                                        )\n\n                                        # Add metadata to polygons\n                                        for poly_data in tile_polygons:\n                                            poly_data.update(\n                                                {\n                                                    \"label\": prompt,\n                                                    \"score\": max(\n                                                        [\n                                                            d.score\n                                                            for d in detections\n                                                            if d.label.replace(\n                                                                \".\", \"\"\n                                                            )\n                                                            .strip()\n                                                            .lower()\n                                                            == prompt.lower()\n                                                        ],\n                                                        default=0.0,\n                                                    ),\n                                                    \"tile_x\": x,\n                                                    \"tile_y\": y,\n                                                }\n                                            )\n                                            all_polygons.append(poly_data)\n\n                                    # Store mask in the global array\n                                    valid_x_start = self.overlap if x &gt; 0 else 0\n                                    valid_y_start = self.overlap if y &gt; 0 else 0\n                                    valid_x_end = (\n                                        tile_width - self.overlap\n                                        if x &lt; n_tiles_x - 1\n                                        else tile_width\n                                    )\n                                    valid_y_end = (\n                                        tile_height - self.overlap\n                                        if y &lt; n_tiles_y - 1\n                                        else tile_height\n                                    )\n\n                                    dest_x_start = x_start + valid_x_start\n                                    dest_y_start = y_start + valid_y_start\n                                    dest_x_end = x_start + valid_x_end\n                                    dest_y_end = y_start + valid_y_end\n\n                                    mask_slice = prompt_mask[\n                                        valid_y_start:valid_y_end,\n                                        valid_x_start:valid_x_end,\n                                    ]\n                                    all_masks[\n                                        i,\n                                        dest_y_start:dest_y_end,\n                                        dest_x_start:dest_x_end,\n                                    ] = np.maximum(\n                                        all_masks[\n                                            i,\n                                            dest_y_start:dest_y_end,\n                                            dest_x_start:dest_x_end,\n                                        ],\n                                        mask_slice,\n                                    )\n\n                    except Exception as e:\n                        print(f\"Error processing tile at ({x}, {y}): {str(e)}\")\n                        continue\n\n                    # Update progress bar\n                    pbar.update(1)\n\n        # Create combined mask (union of all individual masks)\n        combined_mask = np.any(all_masks, axis=0).astype(np.uint8)\n\n        # Write the output GeoTIFF\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            # Write combined mask as first band\n            dst.write(combined_mask, 1)\n\n            # Write individual masks for each prompt\n            for i, mask in enumerate(all_masks):\n                dst.write(mask, i + 2)\n\n            # Add descriptions to bands\n            dst.set_band_description(1, \"Combined Segmentation\")\n            for i, prompt in enumerate(text_prompts):\n                dst.set_band_description(i + 2, f\"Segmentation: {prompt}\")\n\n        result_files = {\"segmentation\": output_path}\n\n        # Export bounding boxes if requested\n        if export_boxes and all_boxes:\n            boxes_path = output_path.replace(\".tif\", \"_boxes.geojson\")\n            gdf = gpd.GeoDataFrame(all_boxes, crs=crs)\n            gdf.to_file(boxes_path, driver=\"GeoJSON\")\n            result_files[\"boxes\"] = boxes_path\n            print(f\"Exported {len(all_boxes)} bounding boxes to {boxes_path}\")\n\n        # Export instance polygons if requested\n        if export_polygons and all_polygons:\n            polygons_path = output_path.replace(\".tif\", \"_polygons.geojson\")\n            gdf = gpd.GeoDataFrame(all_polygons, crs=crs)\n            gdf.to_file(polygons_path, driver=\"GeoJSON\")\n            result_files[\"polygons\"] = polygons_path\n            print(\n                f\"Exported {len(all_polygons)} instance polygons to {polygons_path}\"\n            )\n\n        print(f\"Segmentation saved to {output_path}\")\n        print(\n            f\"Found {len(all_polygons)} individual building instances\"\n            if export_polygons\n            else \"\"\n        )\n\n        return result_files\n</code></pre>"},{"location":"segmentation/","title":"segmentation module","text":""},{"location":"segmentation/#geoai.segmentation.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Custom Dataset for loading images and masks.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"Custom Dataset for loading images and masks.\"\"\"\n\n    def __init__(\n        self,\n        images_dir: str,\n        masks_dir: str,\n        transform: Optional[A.Compose] = None,\n        target_size: Tuple[int, int] = (256, 256),\n        num_classes: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            images_dir (str): Directory containing images.\n            masks_dir (str): Directory containing masks.\n            transform (A.Compose, optional): Transformations to be applied on the images and masks.\n            target_size (tuple, optional): Target size for resizing images and masks.\n            num_classes (int, optional): Number of classes in the masks.\n        \"\"\"\n        self.images_dir = images_dir\n        self.masks_dir = masks_dir\n        self.transform = transform\n        self.target_size = target_size\n        self.num_classes = num_classes\n        self.images = sorted(os.listdir(images_dir))\n        self.masks = sorted(os.listdir(masks_dir))\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the total number of samples.\"\"\"\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"\n        Args:\n            idx (int): Index of the sample to fetch.\n\n        Returns:\n            dict: A dictionary with 'pixel_values' and 'labels'.\n        \"\"\"\n        img_path = os.path.join(self.images_dir, self.images[idx])\n        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n\n        image = image.resize(self.target_size)\n        mask = mask.resize(self.target_size)\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        mask = (mask &gt; 127).astype(np.uint8)\n\n        if self.transform:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        assert (\n            mask.max() &lt; self.num_classes\n        ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n        assert (\n            mask.min() &gt;= 0\n        ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n        mask = mask.clone().detach().long()\n\n        return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the sample to fetch.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Tensor]</code> <p>A dictionary with 'pixel_values' and 'labels'.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"\n    Args:\n        idx (int): Index of the sample to fetch.\n\n    Returns:\n        dict: A dictionary with 'pixel_values' and 'labels'.\n    \"\"\"\n    img_path = os.path.join(self.images_dir, self.images[idx])\n    mask_path = os.path.join(self.masks_dir, self.masks[idx])\n    image = Image.open(img_path).convert(\"RGB\")\n    mask = Image.open(mask_path).convert(\"L\")\n\n    image = image.resize(self.target_size)\n    mask = mask.resize(self.target_size)\n\n    image = np.array(image)\n    mask = np.array(mask)\n\n    mask = (mask &gt; 127).astype(np.uint8)\n\n    if self.transform:\n        transformed = self.transform(image=image, mask=mask)\n        image = transformed[\"image\"]\n        mask = transformed[\"mask\"]\n\n    assert (\n        mask.max() &lt; self.num_classes\n    ), f\"Mask values should be less than {self.num_classes}, but found {mask.max()}\"\n    assert (\n        mask.min() &gt;= 0\n    ), f\"Mask values should be greater than or equal to 0, but found {mask.min()}\"\n\n    mask = mask.clone().detach().long()\n\n    return {\"pixel_values\": image, \"labels\": mask}\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__init__","title":"<code>__init__(images_dir, masks_dir, transform=None, target_size=(256, 256), num_classes=2)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>Compose</code> <p>Transformations to be applied on the images and masks.</p> <code>None</code> <code>target_size</code> <code>tuple</code> <p>Target size for resizing images and masks.</p> <code>(256, 256)</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the masks.</p> <code>2</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def __init__(\n    self,\n    images_dir: str,\n    masks_dir: str,\n    transform: Optional[A.Compose] = None,\n    target_size: Tuple[int, int] = (256, 256),\n    num_classes: int = 2,\n) -&gt; None:\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose, optional): Transformations to be applied on the images and masks.\n        target_size (tuple, optional): Target size for resizing images and masks.\n        num_classes (int, optional): Number of classes in the masks.\n    \"\"\"\n    self.images_dir = images_dir\n    self.masks_dir = masks_dir\n    self.transform = transform\n    self.target_size = target_size\n    self.num_classes = num_classes\n    self.images = sorted(os.listdir(images_dir))\n    self.masks = sorted(os.listdir(masks_dir))\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.CustomDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of samples.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the total number of samples.\"\"\"\n    return len(self.images)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.get_transform","title":"<code>get_transform()</code>","text":"<p>Returns:</p> Type Description <code>Compose</code> <p>A.Compose: A composition of image transformations.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def get_transform() -&gt; A.Compose:\n    \"\"\"\n    Returns:\n        A.Compose: A composition of image transformations.\n    \"\"\"\n    return A.Compose(\n        [\n            A.Resize(256, 256),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.load_model","title":"<code>load_model(model_path, device)</code>","text":"<p>Loads the fine-tuned model from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model.</p> required <code>device</code> <code>device</code> <p>Device to load the model on.</p> required <p>Returns:</p> Name Type Description <code>SegformerForSemanticSegmentation</code> <code>SegformerForSemanticSegmentation</code> <p>Loaded model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def load_model(\n    model_path: str, device: torch.device\n) -&gt; SegformerForSemanticSegmentation:\n    \"\"\"\n    Loads the fine-tuned model from the specified path.\n\n    Args:\n        model_path (str): Path to the model.\n        device (torch.device): Device to load the model on.\n\n    Returns:\n        SegformerForSemanticSegmentation: Loaded model.\n    \"\"\"\n    model = SegformerForSemanticSegmentation.from_pretrained(model_path)\n    model.to(device)\n    model.eval()\n    return model\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.predict_image","title":"<code>predict_image(model, image_tensor, original_size, device)</code>","text":"<p>Predicts the segmentation mask for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>SegformerForSemanticSegmentation</code> <p>Fine-tuned model.</p> required <code>image_tensor</code> <code>Tensor</code> <p>Preprocessed image tensor.</p> required <code>original_size</code> <code>tuple</code> <p>Original size of the image (width, height).</p> required <code>device</code> <code>device</code> <p>Device to perform inference on.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def predict_image(\n    model: SegformerForSemanticSegmentation,\n    image_tensor: torch.Tensor,\n    original_size: Tuple[int, int],\n    device: torch.device,\n) -&gt; np.ndarray:\n    \"\"\"\n    Predicts the segmentation mask for the input image.\n\n    Args:\n        model (SegformerForSemanticSegmentation): Fine-tuned model.\n        image_tensor (torch.Tensor): Preprocessed image tensor.\n        original_size (tuple): Original size of the image (width, height).\n        device (torch.device): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    with torch.no_grad():\n        image_tensor = image_tensor.to(device)\n        outputs = model(pixel_values=image_tensor)\n        logits = outputs.logits\n        upsampled_logits = F.interpolate(\n            logits, size=original_size[::-1], mode=\"bilinear\", align_corners=False\n        )\n        predictions = torch.argmax(upsampled_logits, dim=1).cpu().numpy()\n    return predictions[0]\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.prepare_datasets","title":"<code>prepare_datasets(images_dir, masks_dir, transform, test_size=0.2, random_state=42)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing images.</p> required <code>masks_dir</code> <code>str</code> <p>Directory containing masks.</p> required <code>transform</code> <code>Compose</code> <p>Transformations to be applied.</p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the validation split.</p> <code>0.2</code> <code>random_state</code> <code>int</code> <p>Random seed for shuffling the dataset.</p> <code>42</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Subset, Subset]</code> <p>Training and validation datasets.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def prepare_datasets(\n    images_dir: str,\n    masks_dir: str,\n    transform: A.Compose,\n    test_size: float = 0.2,\n    random_state: int = 42,\n) -&gt; Tuple[Subset, Subset]:\n    \"\"\"\n    Args:\n        images_dir (str): Directory containing images.\n        masks_dir (str): Directory containing masks.\n        transform (A.Compose): Transformations to be applied.\n        test_size (float, optional): Proportion of the dataset to include in the validation split.\n        random_state (int, optional): Random seed for shuffling the dataset.\n\n    Returns:\n        tuple: Training and validation datasets.\n    \"\"\"\n    dataset = CustomDataset(images_dir, masks_dir, transform)\n    train_indices, val_indices = train_test_split(\n        list(range(len(dataset))), test_size=test_size, random_state=random_state\n    )\n    train_dataset = Subset(dataset, train_indices)\n    val_dataset = Subset(dataset, val_indices)\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.preprocess_image","title":"<code>preprocess_image(image_path, target_size=(256, 256))</code>","text":"<p>Preprocesses the input image for prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Preprocessed image tensor.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def preprocess_image(image_path: str, target_size: tuple = (256, 256)) -&gt; torch.Tensor:\n    \"\"\"\n    Preprocesses the input image for prediction.\n\n    Args:\n        image_path (str): Path to the input image.\n        target_size (tuple, optional): Target size for resizing the image.\n\n    Returns:\n        torch.Tensor: Preprocessed image tensor.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    transform = A.Compose(\n        [\n            A.Resize(target_size[0], target_size[1]),\n            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            ToTensorV2(),\n        ]\n    )\n    image = np.array(image)\n    transformed = transform(image=image)\n    return transformed[\"image\"].unsqueeze(0)\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.segment_image","title":"<code>segment_image(image_path, model_path, target_size=(256, 256), device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))</code>","text":"<p>Segments the input image using the fine-tuned model.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the input image.</p> required <code>model_path</code> <code>str</code> <p>Path to the fine-tuned model.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing the image.</p> <code>(256, 256)</code> <code>device</code> <code>device</code> <p>Device to perform inference on.</p> <code>device('cuda' if is_available() else 'cpu')</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Predicted segmentation mask.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def segment_image(\n    image_path: str,\n    model_path: str,\n    target_size: Tuple[int, int] = (256, 256),\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n) -&gt; np.ndarray:\n    \"\"\"\n    Segments the input image using the fine-tuned model.\n\n    Args:\n        image_path (str): Path to the input image.\n        model_path (str): Path to the fine-tuned model.\n        target_size (tuple, optional): Target size for resizing the image.\n        device (torch.device, optional): Device to perform inference on.\n\n    Returns:\n        np.ndarray: Predicted segmentation mask.\n    \"\"\"\n    model = load_model(model_path, device)\n    image = Image.open(image_path).convert(\"RGB\")\n    original_size = image.size\n    image_tensor = preprocess_image(image_path, target_size)\n    predictions = predict_image(model, image_tensor, original_size, device)\n    return predictions\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.train_model","title":"<code>train_model(train_dataset, val_dataset, pretrained_model='nvidia/segformer-b0-finetuned-ade-512-512', model_save_path='./model', output_dir='./results', num_epochs=10, batch_size=8, learning_rate=5e-05)</code>","text":"<p>Trains the model and saves the fine-tuned model to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> required <code>pretrained_model</code> <code>str</code> <p>Pretrained model to fine-tune.</p> <code>'nvidia/segformer-b0-finetuned-ade-512-512'</code> <code>model_save_path</code> <code>str</code> <p>Path to save the fine-tuned model. Defaults to './model'.</p> <code>'./model'</code> <code>output_dir</code> <code>str</code> <p>Directory to save training outputs.</p> <code>'./results'</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>10</code> <code>batch_size</code> <code>int</code> <p>Batch size for training and evaluation.</p> <code>8</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>5e-05</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved fine-tuned model.</p> Source code in <code>geoai/segmentation.py</code> <pre><code>def train_model(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    pretrained_model: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n    model_save_path: str = \"./model\",\n    output_dir: str = \"./results\",\n    num_epochs: int = 10,\n    batch_size: int = 8,\n    learning_rate: float = 5e-5,\n) -&gt; str:\n    \"\"\"\n    Trains the model and saves the fine-tuned model to the specified path.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset): Validation dataset.\n        pretrained_model (str, optional): Pretrained model to fine-tune.\n        model_save_path (str): Path to save the fine-tuned model. Defaults to './model'.\n        output_dir (str, optional): Directory to save training outputs.\n        num_epochs (int, optional): Number of training epochs.\n        batch_size (int, optional): Batch size for training and evaluation.\n        learning_rate (float, optional): Learning rate for training.\n\n    Returns:\n        str: Path to the saved fine-tuned model.\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model).to(\n        device\n    )\n    data_collator = DefaultDataCollator(return_tensors=\"pt\")\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_dir=\"./logs\",\n        learning_rate=learning_rate,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n\n    trainer.train()\n    model.save_pretrained(model_save_path)\n    print(f\"Model saved to {model_save_path}\")\n    return model_save_path\n</code></pre>"},{"location":"segmentation/#geoai.segmentation.visualize_predictions","title":"<code>visualize_predictions(image_path, segmented_mask, target_size=(256, 256), reference_image_path=None)</code>","text":"<p>Visualizes the original image, segmented mask, and optionally the reference image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the original image.</p> required <code>segmented_mask</code> <code>ndarray</code> <p>Predicted segmentation mask.</p> required <code>target_size</code> <code>tuple</code> <p>Target size for resizing images.</p> <code>(256, 256)</code> <code>reference_image_path</code> <code>str</code> <p>Path to the reference image.</p> <code>None</code> Source code in <code>geoai/segmentation.py</code> <pre><code>def visualize_predictions(\n    image_path: str,\n    segmented_mask: np.ndarray,\n    target_size: Tuple[int, int] = (256, 256),\n    reference_image_path: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Visualizes the original image, segmented mask, and optionally the reference image.\n\n    Args:\n        image_path (str): Path to the original image.\n        segmented_mask (np.ndarray): Predicted segmentation mask.\n        target_size (tuple, optional): Target size for resizing images.\n        reference_image_path (str, optional): Path to the reference image.\n    \"\"\"\n    original_image = Image.open(image_path).convert(\"RGB\")\n    original_image = original_image.resize(target_size)\n    segmented_image = Image.fromarray((segmented_mask * 255).astype(np.uint8))\n\n    if reference_image_path:\n        reference_image = Image.open(reference_image_path).convert(\"RGB\")\n        reference_image = reference_image.resize(target_size)\n        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n        axes[1].imshow(reference_image)\n        axes[1].set_title(\"Reference Image\")\n        axes[1].axis(\"off\")\n    else:\n        fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\n    axes[0].imshow(original_image)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis(\"off\")\n\n    if reference_image_path:\n        axes[2].imshow(segmented_image, cmap=\"gray\")\n        axes[2].set_title(\"Segmented Image\")\n        axes[2].axis(\"off\")\n    else:\n        axes[1].imshow(segmented_image, cmap=\"gray\")\n        axes[1].set_title(\"Segmented Image\")\n        axes[1].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"timm_segment/","title":"timm_segment module","text":"<p>Module for training semantic segmentation models using timm encoders with PyTorch Lightning.</p>"},{"location":"timm_segment/#geoai.timm_segment.SegmentationDataset","title":"<code>SegmentationDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for semantic segmentation with remote sensing imagery.</p> Source code in <code>geoai/timm_segment.py</code> <pre><code>class SegmentationDataset(Dataset):\n    \"\"\"\n    Dataset for semantic segmentation with remote sensing imagery.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_paths: List[str],\n        mask_paths: List[str],\n        transform: Optional[Callable] = None,\n        num_channels: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize SegmentationDataset.\n\n        Args:\n            image_paths (List[str]): List of paths to image files.\n            mask_paths (List[str]): List of paths to mask files.\n            transform (callable, optional): Transform to apply to images and masks.\n            num_channels (int, optional): Number of channels to use. If None, uses all.\n        \"\"\"\n        self.image_paths = image_paths\n        self.mask_paths = mask_paths\n        self.transform = transform\n        self.num_channels = num_channels\n\n        if len(image_paths) != len(mask_paths):\n            raise ValueError(\"Number of images must match number of masks\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        import rasterio\n\n        # Load image\n        with rasterio.open(self.image_paths[idx]) as src:\n            image = src.read()  # Shape: (C, H, W)\n\n            # Handle channel selection\n            if self.num_channels is not None and image.shape[0] != self.num_channels:\n                if image.shape[0] &gt; self.num_channels:\n                    image = image[: self.num_channels]\n                else:\n                    # Pad with zeros if needed\n                    padded = np.zeros(\n                        (self.num_channels, image.shape[1], image.shape[2])\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n            # Normalize to [0, 1]\n            if image.max() &gt; 1.0:\n                image = image / 255.0\n\n            image = image.astype(np.float32)\n\n        # Load mask\n        with rasterio.open(self.mask_paths[idx]) as src:\n            mask = src.read(1)  # Shape: (H, W)\n            mask = mask.astype(np.int64)\n\n        # Convert to tensors\n        image = torch.from_numpy(image)\n        mask = torch.from_numpy(mask)\n\n        # Apply transforms if provided\n        if self.transform is not None:\n            image, mask = self.transform(image, mask)\n\n        return image, mask\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.SegmentationDataset.__init__","title":"<code>__init__(image_paths, mask_paths, transform=None, num_channels=None)</code>","text":"<p>Initialize SegmentationDataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of paths to image files.</p> required <code>mask_paths</code> <code>List[str]</code> <p>List of paths to mask files.</p> required <code>transform</code> <code>callable</code> <p>Transform to apply to images and masks.</p> <code>None</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use. If None, uses all.</p> <code>None</code> Source code in <code>geoai/timm_segment.py</code> <pre><code>def __init__(\n    self,\n    image_paths: List[str],\n    mask_paths: List[str],\n    transform: Optional[Callable] = None,\n    num_channels: Optional[int] = None,\n):\n    \"\"\"\n    Initialize SegmentationDataset.\n\n    Args:\n        image_paths (List[str]): List of paths to image files.\n        mask_paths (List[str]): List of paths to mask files.\n        transform (callable, optional): Transform to apply to images and masks.\n        num_channels (int, optional): Number of channels to use. If None, uses all.\n    \"\"\"\n    self.image_paths = image_paths\n    self.mask_paths = mask_paths\n    self.transform = transform\n    self.num_channels = num_channels\n\n    if len(image_paths) != len(mask_paths):\n        raise ValueError(\"Number of images must match number of masks\")\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.TimmSegmentationModel","title":"<code>TimmSegmentationModel</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for semantic segmentation using timm encoders with SMP decoders, or pure timm models from Hugging Face Hub.</p> Source code in <code>geoai/timm_segment.py</code> <pre><code>class TimmSegmentationModel(pl.LightningModule):\n    \"\"\"\n    PyTorch Lightning module for semantic segmentation using timm encoders with SMP decoders,\n    or pure timm models from Hugging Face Hub.\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder_name: str = \"resnet50\",\n        architecture: str = \"unet\",\n        num_classes: int = 2,\n        in_channels: int = 3,\n        encoder_weights: str = \"imagenet\",\n        learning_rate: float = 1e-3,\n        weight_decay: float = 1e-4,\n        freeze_encoder: bool = False,\n        loss_fn: Optional[nn.Module] = None,\n        class_weights: Optional[torch.Tensor] = None,\n        use_timm_model: bool = False,\n        timm_model_name: Optional[str] = None,\n        **decoder_kwargs: Any,\n    ):\n        \"\"\"\n        Initialize TimmSegmentationModel.\n\n        Args:\n            encoder_name (str): Name of encoder (e.g., 'resnet50', 'efficientnet_b0').\n            architecture (str): Segmentation architecture ('unet', 'unetplusplus', 'deeplabv3',\n                'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet', 'pan').\n                Ignored if use_timm_model=True.\n            num_classes (int): Number of output classes.\n            in_channels (int): Number of input channels.\n            encoder_weights (str): Pretrained weights for encoder ('imagenet', 'ssl', 'swsl', None).\n            learning_rate (float): Learning rate for optimizer.\n            weight_decay (float): Weight decay for optimizer.\n            freeze_encoder (bool): Freeze encoder weights during training.\n            loss_fn (nn.Module, optional): Custom loss function. Defaults to CrossEntropyLoss.\n            class_weights (torch.Tensor, optional): Class weights for loss function.\n            use_timm_model (bool): If True, load a complete segmentation model from timm/HF Hub\n                instead of using SMP architecture. Defaults to False.\n            timm_model_name (str, optional): Name or path of timm model from HF Hub\n                (e.g., 'hf-hub:timm/segformer_b0.ade_512x512' or 'nvidia/mit-b0').\n                Only used if use_timm_model=True.\n            **decoder_kwargs: Additional arguments for decoder (only used with SMP).\n        \"\"\"\n        super().__init__()\n\n        if not TIMM_AVAILABLE:\n            raise ImportError(\"timm is required. Install it with: pip install timm\")\n\n        self.save_hyperparameters()\n\n        # Check if using a pure timm model from HF Hub\n        if use_timm_model:\n            if timm_model_name is None:\n                timm_model_name = encoder_name\n\n            # Load model from timm (supports HF Hub with 'hf-hub:' prefix)\n            try:\n                self.model = timm.create_model(\n                    timm_model_name,\n                    pretrained=True if encoder_weights else False,\n                    num_classes=num_classes,\n                    in_chans=in_channels,\n                )\n                print(f\"Loaded timm model: {timm_model_name}\")\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to load timm model '{timm_model_name}'. \"\n                    f\"Error: {str(e)}. \"\n                    f\"For HF Hub models, use format 'hf-hub:username/model-name' or 'hf_hub:username/model-name'.\"\n                )\n        else:\n            # Use SMP architecture with timm encoder\n            if not SMP_AVAILABLE:\n                raise ImportError(\n                    \"segmentation-models-pytorch is required. \"\n                    \"Install it with: pip install segmentation-models-pytorch\"\n                )\n\n            # Create segmentation model with timm encoder using smp.create_model\n            try:\n                self.model = smp.create_model(\n                    arch=architecture,\n                    encoder_name=encoder_name,\n                    encoder_weights=encoder_weights,\n                    in_channels=in_channels,\n                    classes=num_classes,\n                    **decoder_kwargs,\n                )\n            except Exception as e:\n                # Provide helpful error message\n                available_archs = [\n                    \"unet\",\n                    \"unetplusplus\",\n                    \"manet\",\n                    \"linknet\",\n                    \"fpn\",\n                    \"pspnet\",\n                    \"deeplabv3\",\n                    \"deeplabv3plus\",\n                    \"pan\",\n                    \"upernet\",\n                ]\n                raise ValueError(\n                    f\"Failed to create model with architecture '{architecture}' and encoder '{encoder_name}'. \"\n                    f\"Error: {str(e)}. \"\n                    f\"Available architectures include: {', '.join(available_archs)}. \"\n                    f\"Please check the segmentation-models-pytorch documentation for supported combinations.\"\n                )\n\n        if freeze_encoder:\n            self._freeze_encoder()\n\n        # Set up loss function\n        if loss_fn is not None:\n            self.loss_fn = loss_fn\n        elif class_weights is not None:\n            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n        else:\n            self.loss_fn = nn.CrossEntropyLoss()\n\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n\n    def _freeze_encoder(self):\n        \"\"\"Freeze encoder weights.\"\"\"\n        if hasattr(self.model, \"encoder\"):\n            for param in self.model.encoder.parameters():\n                param.requires_grad = False\n        else:\n            # For pure timm models without separate encoder\n            if not self.hparams.use_timm_model:\n                raise ValueError(\"Model does not have an encoder attribute to freeze\")\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n\n        # Calculate IoU\n        pred = torch.argmax(logits, dim=1)\n        iou = self._compute_iou(pred, y)\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_iou\", iou, on_step=True, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n\n        # Calculate IoU\n        pred = torch.argmax(logits, dim=1)\n        iou = self._compute_iou(pred, y)\n\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n        self.log(\"val_iou\", iou, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n\n        # Calculate IoU\n        pred = torch.argmax(logits, dim=1)\n        iou = self._compute_iou(pred, y)\n\n        self.log(\"test_loss\", loss, on_epoch=True)\n        self.log(\"test_iou\", iou, on_epoch=True)\n\n        return loss\n\n    def _compute_iou(self, pred, target, smooth=1e-6):\n        \"\"\"Compute mean IoU across all classes.\"\"\"\n        num_classes = self.hparams.num_classes\n        ious = []\n\n        for cls in range(num_classes):\n            pred_cls = pred == cls\n            target_cls = target == cls\n\n            intersection = (pred_cls &amp; target_cls).float().sum()\n            union = (pred_cls | target_cls).float().sum()\n\n            if union == 0:\n                continue\n\n            iou = (intersection + smooth) / (union + smooth)\n            ious.append(iou)\n\n        return (\n            torch.stack(ious).mean() if ious else torch.tensor(0.0, device=pred.device)\n        )\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n        )\n\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode=\"min\",\n            factor=0.5,\n            patience=5,\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n            },\n        }\n\n    def predict_step(self, batch, batch_idx):\n        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n        logits = self(x)\n        probs = torch.softmax(logits, dim=1)\n        preds = torch.argmax(probs, dim=1)\n        return {\"predictions\": preds, \"probabilities\": probs}\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.TimmSegmentationModel.__init__","title":"<code>__init__(encoder_name='resnet50', architecture='unet', num_classes=2, in_channels=3, encoder_weights='imagenet', learning_rate=0.001, weight_decay=0.0001, freeze_encoder=False, loss_fn=None, class_weights=None, use_timm_model=False, timm_model_name=None, **decoder_kwargs)</code>","text":"<p>Initialize TimmSegmentationModel.</p> <p>Parameters:</p> Name Type Description Default <code>encoder_name</code> <code>str</code> <p>Name of encoder (e.g., 'resnet50', 'efficientnet_b0').</p> <code>'resnet50'</code> <code>architecture</code> <code>str</code> <p>Segmentation architecture ('unet', 'unetplusplus', 'deeplabv3', 'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet', 'pan'). Ignored if use_timm_model=True.</p> <code>'unet'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>encoder_weights</code> <code>str</code> <p>Pretrained weights for encoder ('imagenet', 'ssl', 'swsl', None).</p> <code>'imagenet'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for optimizer.</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer.</p> <code>0.0001</code> <code>freeze_encoder</code> <code>bool</code> <p>Freeze encoder weights during training.</p> <code>False</code> <code>loss_fn</code> <code>Module</code> <p>Custom loss function. Defaults to CrossEntropyLoss.</p> <code>None</code> <code>class_weights</code> <code>Tensor</code> <p>Class weights for loss function.</p> <code>None</code> <code>use_timm_model</code> <code>bool</code> <p>If True, load a complete segmentation model from timm/HF Hub instead of using SMP architecture. Defaults to False.</p> <code>False</code> <code>timm_model_name</code> <code>str</code> <p>Name or path of timm model from HF Hub (e.g., 'hf-hub:timm/segformer_b0.ade_512x512' or 'nvidia/mit-b0'). Only used if use_timm_model=True.</p> <code>None</code> <code>**decoder_kwargs</code> <code>Any</code> <p>Additional arguments for decoder (only used with SMP).</p> <code>{}</code> Source code in <code>geoai/timm_segment.py</code> <pre><code>def __init__(\n    self,\n    encoder_name: str = \"resnet50\",\n    architecture: str = \"unet\",\n    num_classes: int = 2,\n    in_channels: int = 3,\n    encoder_weights: str = \"imagenet\",\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-4,\n    freeze_encoder: bool = False,\n    loss_fn: Optional[nn.Module] = None,\n    class_weights: Optional[torch.Tensor] = None,\n    use_timm_model: bool = False,\n    timm_model_name: Optional[str] = None,\n    **decoder_kwargs: Any,\n):\n    \"\"\"\n    Initialize TimmSegmentationModel.\n\n    Args:\n        encoder_name (str): Name of encoder (e.g., 'resnet50', 'efficientnet_b0').\n        architecture (str): Segmentation architecture ('unet', 'unetplusplus', 'deeplabv3',\n            'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet', 'pan').\n            Ignored if use_timm_model=True.\n        num_classes (int): Number of output classes.\n        in_channels (int): Number of input channels.\n        encoder_weights (str): Pretrained weights for encoder ('imagenet', 'ssl', 'swsl', None).\n        learning_rate (float): Learning rate for optimizer.\n        weight_decay (float): Weight decay for optimizer.\n        freeze_encoder (bool): Freeze encoder weights during training.\n        loss_fn (nn.Module, optional): Custom loss function. Defaults to CrossEntropyLoss.\n        class_weights (torch.Tensor, optional): Class weights for loss function.\n        use_timm_model (bool): If True, load a complete segmentation model from timm/HF Hub\n            instead of using SMP architecture. Defaults to False.\n        timm_model_name (str, optional): Name or path of timm model from HF Hub\n            (e.g., 'hf-hub:timm/segformer_b0.ade_512x512' or 'nvidia/mit-b0').\n            Only used if use_timm_model=True.\n        **decoder_kwargs: Additional arguments for decoder (only used with SMP).\n    \"\"\"\n    super().__init__()\n\n    if not TIMM_AVAILABLE:\n        raise ImportError(\"timm is required. Install it with: pip install timm\")\n\n    self.save_hyperparameters()\n\n    # Check if using a pure timm model from HF Hub\n    if use_timm_model:\n        if timm_model_name is None:\n            timm_model_name = encoder_name\n\n        # Load model from timm (supports HF Hub with 'hf-hub:' prefix)\n        try:\n            self.model = timm.create_model(\n                timm_model_name,\n                pretrained=True if encoder_weights else False,\n                num_classes=num_classes,\n                in_chans=in_channels,\n            )\n            print(f\"Loaded timm model: {timm_model_name}\")\n        except Exception as e:\n            raise ValueError(\n                f\"Failed to load timm model '{timm_model_name}'. \"\n                f\"Error: {str(e)}. \"\n                f\"For HF Hub models, use format 'hf-hub:username/model-name' or 'hf_hub:username/model-name'.\"\n            )\n    else:\n        # Use SMP architecture with timm encoder\n        if not SMP_AVAILABLE:\n            raise ImportError(\n                \"segmentation-models-pytorch is required. \"\n                \"Install it with: pip install segmentation-models-pytorch\"\n            )\n\n        # Create segmentation model with timm encoder using smp.create_model\n        try:\n            self.model = smp.create_model(\n                arch=architecture,\n                encoder_name=encoder_name,\n                encoder_weights=encoder_weights,\n                in_channels=in_channels,\n                classes=num_classes,\n                **decoder_kwargs,\n            )\n        except Exception as e:\n            # Provide helpful error message\n            available_archs = [\n                \"unet\",\n                \"unetplusplus\",\n                \"manet\",\n                \"linknet\",\n                \"fpn\",\n                \"pspnet\",\n                \"deeplabv3\",\n                \"deeplabv3plus\",\n                \"pan\",\n                \"upernet\",\n            ]\n            raise ValueError(\n                f\"Failed to create model with architecture '{architecture}' and encoder '{encoder_name}'. \"\n                f\"Error: {str(e)}. \"\n                f\"Available architectures include: {', '.join(available_archs)}. \"\n                f\"Please check the segmentation-models-pytorch documentation for supported combinations.\"\n            )\n\n    if freeze_encoder:\n        self._freeze_encoder()\n\n    # Set up loss function\n    if loss_fn is not None:\n        self.loss_fn = loss_fn\n    elif class_weights is not None:\n        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n    else:\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    self.learning_rate = learning_rate\n    self.weight_decay = weight_decay\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.predict_segmentation","title":"<code>predict_segmentation(model, image_paths, batch_size=8, num_workers=4, device=None, return_probabilities=False)</code>","text":"<p>Make predictions on images using a trained segmentation model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[TimmSegmentationModel, Module]</code> <p>Trained model.</p> required <code>image_paths</code> <code>List[str]</code> <p>List of paths to images.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>8</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>4</code> <code>device</code> <code>Optional[str]</code> <p>Device to use ('cuda', 'cpu', etc.). Auto-detected if None.</p> <code>None</code> <code>return_probabilities</code> <code>bool</code> <p>If True, return both predictions and probabilities.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>Array of predicted segmentation masks.</p> <code>probabilities</code> <code>optional</code> <p>Array of class probabilities if return_probabilities=True.</p> Source code in <code>geoai/timm_segment.py</code> <pre><code>def predict_segmentation(\n    model: Union[TimmSegmentationModel, nn.Module],\n    image_paths: List[str],\n    batch_size: int = 8,\n    num_workers: int = 4,\n    device: Optional[str] = None,\n    return_probabilities: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Make predictions on images using a trained segmentation model.\n\n    Args:\n        model: Trained model.\n        image_paths: List of paths to images.\n        batch_size: Batch size for inference.\n        num_workers: Number of data loading workers.\n        device: Device to use ('cuda', 'cpu', etc.). Auto-detected if None.\n        return_probabilities: If True, return both predictions and probabilities.\n\n    Returns:\n        predictions: Array of predicted segmentation masks.\n        probabilities (optional): Array of class probabilities if return_probabilities=True.\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Create dummy masks for dataset\n    dummy_masks = image_paths  # Use image paths as placeholders\n    dataset = SegmentationDataset(image_paths, dummy_masks)\n\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    model.eval()\n    model = model.to(device)\n\n    all_preds = []\n    all_probs = []\n\n    with torch.no_grad():\n        for images, _ in tqdm(loader, desc=\"Making predictions\"):\n            images = images.to(device)\n\n            if isinstance(model, TimmSegmentationModel):\n                logits = model(images)\n            else:\n                logits = model(images)\n\n            probs = torch.softmax(logits, dim=1)\n            preds = torch.argmax(probs, dim=1)\n\n            all_preds.append(preds.cpu().numpy())\n            if return_probabilities:\n                all_probs.append(probs.cpu().numpy())\n\n    predictions = np.concatenate(all_preds)\n\n    if return_probabilities:\n        probabilities = np.concatenate(all_probs)\n        return predictions, probabilities\n\n    return predictions\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.push_timm_model_to_hub","title":"<code>push_timm_model_to_hub(model_path, repo_id, encoder_name='resnet50', architecture='unet', num_channels=3, num_classes=2, use_timm_model=False, timm_model_name=None, commit_message=None, private=False, token=None, **kwargs)</code>","text":"<p>Push a trained timm segmentation model to Hugging Face Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to trained model checkpoint (.ckpt or .pth).</p> required <code>repo_id</code> <code>str</code> <p>Repository ID on HF Hub (e.g., 'username/model-name').</p> required <code>encoder_name</code> <code>str</code> <p>Name of timm encoder used in training.</p> <code>'resnet50'</code> <code>architecture</code> <code>str</code> <p>Segmentation architecture used in training.</p> <code>'unet'</code> <code>num_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> <code>use_timm_model</code> <code>bool</code> <p>If True, model was trained with pure timm model.</p> <code>False</code> <code>timm_model_name</code> <code>str</code> <p>Model name from HF Hub used during training.</p> <code>None</code> <code>commit_message</code> <code>str</code> <p>Commit message for the upload.</p> <code>None</code> <code>private</code> <code>bool</code> <p>Whether to make the repository private.</p> <code>False</code> <code>token</code> <code>str</code> <p>HuggingFace API token. If None, uses logged-in token.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for push_to_hub.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>URL of the uploaded model on HF Hub.</p> Source code in <code>geoai/timm_segment.py</code> <pre><code>def push_timm_model_to_hub(\n    model_path: str,\n    repo_id: str,\n    encoder_name: str = \"resnet50\",\n    architecture: str = \"unet\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    use_timm_model: bool = False,\n    timm_model_name: Optional[str] = None,\n    commit_message: Optional[str] = None,\n    private: bool = False,\n    token: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Push a trained timm segmentation model to Hugging Face Hub.\n\n    Args:\n        model_path (str): Path to trained model checkpoint (.ckpt or .pth).\n        repo_id (str): Repository ID on HF Hub (e.g., 'username/model-name').\n        encoder_name (str): Name of timm encoder used in training.\n        architecture (str): Segmentation architecture used in training.\n        num_channels (int): Number of input channels.\n        num_classes (int): Number of output classes.\n        use_timm_model (bool): If True, model was trained with pure timm model.\n        timm_model_name (str, optional): Model name from HF Hub used during training.\n        commit_message (str, optional): Commit message for the upload.\n        private (bool): Whether to make the repository private.\n        token (str, optional): HuggingFace API token. If None, uses logged-in token.\n        **kwargs: Additional arguments for push_to_hub.\n\n    Returns:\n        str: URL of the uploaded model on HF Hub.\n    \"\"\"\n    try:\n        from huggingface_hub import HfApi, create_repo\n    except ImportError:\n        raise ImportError(\n            \"huggingface_hub is required to push models. \"\n            \"Install it with: pip install huggingface-hub\"\n        )\n\n    # Load model\n    if model_path.endswith(\".ckpt\"):\n        lightning_model = TimmSegmentationModel.load_from_checkpoint(\n            model_path,\n            encoder_name=encoder_name,\n            architecture=architecture,\n            num_classes=num_classes,\n            in_channels=num_channels,\n            use_timm_model=use_timm_model,\n            timm_model_name=timm_model_name,\n        )\n        model = lightning_model.model\n    else:\n        # Load state dict\n        if use_timm_model:\n            if timm_model_name is None:\n                timm_model_name = encoder_name\n\n            model = timm.create_model(\n                timm_model_name,\n                pretrained=False,\n                num_classes=num_classes,\n                in_chans=num_channels,\n            )\n        else:\n            import segmentation_models_pytorch as smp\n\n            model = smp.create_model(\n                arch=architecture,\n                encoder_name=encoder_name,\n                encoder_weights=None,\n                in_channels=num_channels,\n                classes=num_classes,\n            )\n\n        model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n\n    # Create repository if it doesn't exist\n    api = HfApi(token=token)\n    try:\n        create_repo(repo_id, private=private, token=token, exist_ok=True)\n    except Exception as e:\n        print(f\"Repository creation note: {e}\")\n\n    # Save model configuration\n    config = {\n        \"encoder_name\": encoder_name,\n        \"architecture\": architecture,\n        \"num_channels\": num_channels,\n        \"num_classes\": num_classes,\n        \"use_timm_model\": use_timm_model,\n        \"timm_model_name\": timm_model_name,\n        \"model_type\": \"timm_segmentation\",\n    }\n\n    # Save model state dict to temporary file\n    import tempfile\n    import json\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Save model state dict\n        model_save_path = os.path.join(tmpdir, \"model.pth\")\n        torch.save(model.state_dict(), model_save_path)\n\n        # Save config\n        config_path = os.path.join(tmpdir, \"config.json\")\n        with open(config_path, \"w\") as f:\n            json.dump(config, f, indent=2)\n\n        # Upload files\n        if commit_message is None:\n            commit_message = f\"Upload {architecture} with {encoder_name} encoder\"\n\n        api.upload_folder(\n            folder_path=tmpdir,\n            repo_id=repo_id,\n            commit_message=commit_message,\n            token=token,\n            **kwargs,\n        )\n\n    url = f\"https://huggingface.co/{repo_id}\"\n    print(f\"Model successfully pushed to: {url}\")\n    return url\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.timm_semantic_segmentation","title":"<code>timm_semantic_segmentation(input_path, output_path, model_path, encoder_name='resnet50', architecture='unet', num_channels=3, num_classes=2, window_size=512, overlap=256, batch_size=4, device=None, quiet=False, use_timm_model=False, timm_model_name=None, **kwargs)</code>","text":"<p>Perform semantic segmentation on a raster using a trained timm model.</p> <p>This function performs inference on a GeoTIFF using a sliding window approach and saves the result as a georeferenced raster.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model checkpoint (.ckpt or .pth).</p> required <code>encoder_name</code> <code>str</code> <p>Name of timm encoder used in training.</p> <code>'resnet50'</code> <code>architecture</code> <code>str</code> <p>Segmentation architecture used in training.</p> <code>'unet'</code> <code>num_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>device</code> <code>str</code> <p>Device to use. Auto-detected if None.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress progress messages.</p> <code>False</code> <code>use_timm_model</code> <code>bool</code> <p>If True, model was trained with timm model from HF Hub.</p> <code>False</code> <code>timm_model_name</code> <code>str</code> <p>Model name from HF Hub used during training.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>geoai/timm_segment.py</code> <pre><code>def timm_semantic_segmentation(\n    input_path: str,\n    output_path: str,\n    model_path: str,\n    encoder_name: str = \"resnet50\",\n    architecture: str = \"unet\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    window_size: int = 512,\n    overlap: int = 256,\n    batch_size: int = 4,\n    device: Optional[str] = None,\n    quiet: bool = False,\n    use_timm_model: bool = False,\n    timm_model_name: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform semantic segmentation on a raster using a trained timm model.\n\n    This function performs inference on a GeoTIFF using a sliding window approach\n    and saves the result as a georeferenced raster.\n\n    Args:\n        input_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask.\n        model_path (str): Path to trained model checkpoint (.ckpt or .pth).\n        encoder_name (str): Name of timm encoder used in training.\n        architecture (str): Segmentation architecture used in training.\n        num_channels (int): Number of input channels.\n        num_classes (int): Number of output classes.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        batch_size (int): Batch size for inference.\n        device (str, optional): Device to use. Auto-detected if None.\n        quiet (bool): If True, suppress progress messages.\n        use_timm_model (bool): If True, model was trained with timm model from HF Hub.\n        timm_model_name (str, optional): Model name from HF Hub used during training.\n        **kwargs: Additional arguments.\n    \"\"\"\n    import rasterio\n    from rasterio.windows import Window\n\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Load model\n    if model_path.endswith(\".ckpt\"):\n        model = TimmSegmentationModel.load_from_checkpoint(\n            model_path,\n            encoder_name=encoder_name,\n            architecture=architecture,\n            num_classes=num_classes,\n            in_channels=num_channels,\n            use_timm_model=use_timm_model,\n            timm_model_name=timm_model_name,\n        )\n        model = model.model  # Get underlying model\n    else:\n        # Load state dict\n        if use_timm_model:\n            # Load pure timm model\n            if timm_model_name is None:\n                timm_model_name = encoder_name\n\n            model = timm.create_model(\n                timm_model_name,\n                pretrained=False,\n                num_classes=num_classes,\n                in_chans=num_channels,\n            )\n        else:\n            # Load SMP model\n            import segmentation_models_pytorch as smp\n\n            try:\n                model = smp.create_model(\n                    arch=architecture,\n                    encoder_name=encoder_name,\n                    encoder_weights=None,\n                    in_channels=num_channels,\n                    classes=num_classes,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to create model with architecture '{architecture}' and encoder '{encoder_name}'. \"\n                    f\"Error: {str(e)}\"\n                )\n\n        model.load_state_dict(torch.load(model_path, map_location=device))\n\n    model.eval()\n    model = model.to(device)\n\n    # Read input raster\n    with rasterio.open(input_path) as src:\n        meta = src.meta.copy()\n        height, width = src.shape\n\n        # Calculate number of windows\n        stride = window_size - overlap\n        n_rows = int(np.ceil((height - overlap) / stride))\n        n_cols = int(np.ceil((width - overlap) / stride))\n\n        if not quiet:\n            print(f\"Processing {n_rows} x {n_cols} = {n_rows * n_cols} windows\")\n\n        # Initialize output array (use int32 to avoid overflow during accumulation)\n        output = np.zeros((height, width), dtype=np.int32)\n        count = np.zeros((height, width), dtype=np.int32)\n\n        # Process windows\n        with torch.no_grad():\n            for i in tqdm(range(n_rows), disable=quiet, desc=\"Processing rows\"):\n                for j in range(n_cols):\n                    # Calculate window bounds\n                    row_start = i * stride\n                    col_start = j * stride\n                    row_end = min(row_start + window_size, height)\n                    col_end = min(col_start + window_size, width)\n\n                    # Read window\n                    window = Window(\n                        col_start, row_start, col_end - col_start, row_end - row_start\n                    )\n                    img = src.read(window=window)\n\n                    # Handle channel selection\n                    if img.shape[0] &gt; num_channels:\n                        img = img[:num_channels]\n                    elif img.shape[0] &lt; num_channels:\n                        padded = np.zeros((num_channels, img.shape[1], img.shape[2]))\n                        padded[: img.shape[0]] = img\n                        img = padded\n\n                    # Normalize\n                    if img.max() &gt; 1.0:\n                        img = img / 255.0\n                    img = img.astype(np.float32)\n\n                    # Pad if necessary\n                    h, w = img.shape[1], img.shape[2]\n                    if h &lt; window_size or w &lt; window_size:\n                        padded = np.zeros(\n                            (num_channels, window_size, window_size), dtype=np.float32\n                        )\n                        padded[:, :h, :w] = img\n                        img = padded\n\n                    # Predict\n                    img_tensor = torch.from_numpy(img).unsqueeze(0).to(device)\n                    logits = model(img_tensor)\n                    pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n\n                    # Crop to actual size\n                    pred = pred[:h, :w]\n\n                    # Add to output\n                    output[row_start:row_end, col_start:col_end] += pred\n                    count[row_start:row_end, col_start:col_end] += 1\n\n        # Average overlapping predictions\n        output = (output / np.maximum(count, 1)).astype(np.uint8)\n\n    # Save output\n    meta.update({\"count\": 1, \"dtype\": \"uint8\", \"compress\": \"lzw\"})\n\n    with rasterio.open(output_path, \"w\", **meta) as dst:\n        dst.write(output, 1)\n\n    if not quiet:\n        print(f\"Segmentation saved to {output_path}\")\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.train_timm_segmentation","title":"<code>train_timm_segmentation(train_dataset, val_dataset=None, test_dataset=None, encoder_name='resnet50', architecture='unet', num_classes=2, in_channels=3, encoder_weights='imagenet', output_dir='output', batch_size=8, num_epochs=50, learning_rate=0.001, weight_decay=0.0001, num_workers=4, freeze_encoder=False, class_weights=None, accelerator='auto', devices='auto', monitor_metric='val_loss', mode='min', patience=10, save_top_k=1, checkpoint_path=None, use_timm_model=False, timm_model_name=None, **kwargs)</code>","text":"<p>Train a semantic segmentation model using timm encoder.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> <code>None</code> <code>test_dataset</code> <code>Dataset</code> <p>Test dataset.</p> <code>None</code> <code>encoder_name</code> <code>str</code> <p>Name of timm encoder.</p> <code>'resnet50'</code> <code>architecture</code> <code>str</code> <p>Segmentation architecture.</p> <code>'unet'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>encoder_weights</code> <code>str</code> <p>Pretrained weights for encoder.</p> <code>'imagenet'</code> <code>output_dir</code> <code>str</code> <p>Directory to save outputs.</p> <code>'output'</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>8</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer.</p> <code>0.0001</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>4</code> <code>freeze_encoder</code> <code>bool</code> <p>Freeze encoder during training.</p> <code>False</code> <code>class_weights</code> <code>List[float]</code> <p>Class weights for loss.</p> <code>None</code> <code>accelerator</code> <code>str</code> <p>Accelerator type ('auto', 'gpu', 'cpu').</p> <code>'auto'</code> <code>devices</code> <code>str</code> <p>Devices to use.</p> <code>'auto'</code> <code>monitor_metric</code> <code>str</code> <p>Metric to monitor for checkpointing.</p> <code>'val_loss'</code> <code>mode</code> <code>str</code> <p>'min' or 'max' for monitor_metric.</p> <code>'min'</code> <code>patience</code> <code>int</code> <p>Early stopping patience.</p> <code>10</code> <code>save_top_k</code> <code>int</code> <p>Number of best models to save.</p> <code>1</code> <code>checkpoint_path</code> <code>str</code> <p>Path to checkpoint to resume from.</p> <code>None</code> <code>use_timm_model</code> <code>bool</code> <p>Load complete segmentation model from timm/HF Hub.</p> <code>False</code> <code>timm_model_name</code> <code>str</code> <p>Model name from HF Hub (e.g., 'hf-hub:nvidia/mit-b0').</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for PyTorch Lightning Trainer.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>TimmSegmentationModel</code> <code>TimmSegmentationModel</code> <p>Trained model.</p> Source code in <code>geoai/timm_segment.py</code> <pre><code>def train_timm_segmentation(\n    train_dataset: Dataset,\n    val_dataset: Optional[Dataset] = None,\n    test_dataset: Optional[Dataset] = None,\n    encoder_name: str = \"resnet50\",\n    architecture: str = \"unet\",\n    num_classes: int = 2,\n    in_channels: int = 3,\n    encoder_weights: str = \"imagenet\",\n    output_dir: str = \"output\",\n    batch_size: int = 8,\n    num_epochs: int = 50,\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-4,\n    num_workers: int = 4,\n    freeze_encoder: bool = False,\n    class_weights: Optional[List[float]] = None,\n    accelerator: str = \"auto\",\n    devices: str = \"auto\",\n    monitor_metric: str = \"val_loss\",\n    mode: str = \"min\",\n    patience: int = 10,\n    save_top_k: int = 1,\n    checkpoint_path: Optional[str] = None,\n    use_timm_model: bool = False,\n    timm_model_name: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; TimmSegmentationModel:\n    \"\"\"\n    Train a semantic segmentation model using timm encoder.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset, optional): Validation dataset.\n        test_dataset (Dataset, optional): Test dataset.\n        encoder_name (str): Name of timm encoder.\n        architecture (str): Segmentation architecture.\n        num_classes (int): Number of output classes.\n        in_channels (int): Number of input channels.\n        encoder_weights (str): Pretrained weights for encoder.\n        output_dir (str): Directory to save outputs.\n        batch_size (int): Batch size for training.\n        num_epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate.\n        weight_decay (float): Weight decay for optimizer.\n        num_workers (int): Number of data loading workers.\n        freeze_encoder (bool): Freeze encoder during training.\n        class_weights (List[float], optional): Class weights for loss.\n        accelerator (str): Accelerator type ('auto', 'gpu', 'cpu').\n        devices (str): Devices to use.\n        monitor_metric (str): Metric to monitor for checkpointing.\n        mode (str): 'min' or 'max' for monitor_metric.\n        patience (int): Early stopping patience.\n        save_top_k (int): Number of best models to save.\n        checkpoint_path (str, optional): Path to checkpoint to resume from.\n        use_timm_model (bool): Load complete segmentation model from timm/HF Hub.\n        timm_model_name (str, optional): Model name from HF Hub (e.g., 'hf-hub:nvidia/mit-b0').\n        **kwargs: Additional arguments for PyTorch Lightning Trainer.\n\n    Returns:\n        TimmSegmentationModel: Trained model.\n    \"\"\"\n    if not LIGHTNING_AVAILABLE:\n        raise ImportError(\n            \"PyTorch Lightning is required. Install it with: pip install lightning\"\n        )\n\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    model_dir = os.path.join(output_dir, \"models\")\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Convert class weights to tensor if provided\n    weight_tensor = None\n    if class_weights is not None:\n        weight_tensor = torch.tensor(class_weights, dtype=torch.float32)\n\n    # Create model\n    model = TimmSegmentationModel(\n        encoder_name=encoder_name,\n        architecture=architecture,\n        num_classes=num_classes,\n        in_channels=in_channels,\n        encoder_weights=encoder_weights,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        freeze_encoder=freeze_encoder,\n        class_weights=weight_tensor,\n        use_timm_model=use_timm_model,\n        timm_model_name=timm_model_name,\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    val_loader = None\n    if val_dataset is not None:\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n\n    # Set up callbacks\n    callbacks = []\n\n    # Model checkpoint\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=model_dir,\n        filename=f\"{encoder_name}_{architecture}_{{epoch:02d}}_{{val_loss:.4f}}\",\n        monitor=monitor_metric,\n        mode=mode,\n        save_top_k=save_top_k,\n        save_last=True,\n        verbose=True,\n    )\n    callbacks.append(checkpoint_callback)\n\n    # Early stopping\n    early_stop_callback = EarlyStopping(\n        monitor=monitor_metric,\n        patience=patience,\n        mode=mode,\n        verbose=True,\n    )\n    callbacks.append(early_stop_callback)\n\n    # Set up logger\n    logger = CSVLogger(model_dir, name=\"lightning_logs\")\n\n    # Create trainer\n    trainer = pl.Trainer(\n        max_epochs=num_epochs,\n        accelerator=accelerator,\n        devices=devices,\n        callbacks=callbacks,\n        logger=logger,\n        log_every_n_steps=10,\n        **kwargs,\n    )\n\n    # Train model\n    print(f\"Training {encoder_name} {architecture} for {num_epochs} epochs...\")\n    trainer.fit(\n        model,\n        train_dataloaders=train_loader,\n        val_dataloaders=val_loader,\n        ckpt_path=checkpoint_path,\n    )\n\n    # Test if test dataset provided\n    if test_dataset is not None:\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n        print(\"\\nTesting model on test set...\")\n        trainer.test(model, dataloaders=test_loader)\n\n    print(f\"\\nBest model saved at: {checkpoint_callback.best_model_path}\")\n\n    # Save training history in compatible format\n    metrics = trainer.logged_metrics\n    history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_iou\": [],\n        \"epochs\": [],\n    }\n\n    # Extract metrics from logger\n    import pandas as pd\n    import glob\n\n    csv_files = glob.glob(\n        os.path.join(model_dir, \"lightning_logs\", \"**\", \"metrics.csv\"), recursive=True\n    )\n    if csv_files:\n        df = pd.read_csv(csv_files[0])\n\n        # Group by epoch to get epoch-level metrics\n        epoch_data = df.groupby(\"epoch\").last().reset_index()\n\n        if \"train_loss_epoch\" in epoch_data.columns:\n            history[\"train_loss\"] = epoch_data[\"train_loss_epoch\"].dropna().tolist()\n        if \"val_loss\" in epoch_data.columns:\n            history[\"val_loss\"] = epoch_data[\"val_loss\"].dropna().tolist()\n        if \"val_iou\" in epoch_data.columns:\n            history[\"val_iou\"] = epoch_data[\"val_iou\"].dropna().tolist()\n        if \"epoch\" in epoch_data.columns:\n            history[\"epochs\"] = epoch_data[\"epoch\"].dropna().tolist()\n\n    # Save history\n    history_path = os.path.join(model_dir, \"training_history.pth\")\n    torch.save(history, history_path)\n    print(f\"Training history saved to: {history_path}\")\n\n    return model\n</code></pre>"},{"location":"timm_segment/#geoai.timm_segment.train_timm_segmentation_model","title":"<code>train_timm_segmentation_model(images_dir, labels_dir, output_dir, input_format='directory', encoder_name='resnet50', architecture='unet', encoder_weights='imagenet', num_channels=3, num_classes=2, batch_size=8, num_epochs=50, learning_rate=0.001, weight_decay=0.0001, val_split=0.2, seed=42, num_workers=4, freeze_encoder=False, monitor_metric='val_iou', mode='max', patience=10, save_top_k=1, verbose=True, device=None, use_timm_model=False, timm_model_name=None, **kwargs)</code>","text":"<p>Train a semantic segmentation model using timm encoder (simplified interface).</p> <p>This is a simplified function that takes image and label directories and handles the dataset creation automatically, similar to train_segmentation_model.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing image GeoTIFF files (for 'directory' format), or root directory containing images/ subdirectory (for 'yolo' format), or directory containing images (for 'coco' format).</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing label GeoTIFF files (for 'directory' format), or path to COCO annotations JSON file (for 'coco' format), or not used (for 'yolo' format - labels are in images_dir/labels/).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model checkpoints and results.</p> required <code>input_format</code> <code>str</code> <p>Input data format - 'directory' (default), 'coco', or 'yolo'. - 'directory': Standard directory structure with separate images_dir and labels_dir - 'coco': COCO JSON format (labels_dir should be path to instances.json) - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)</p> <code>'directory'</code> <code>encoder_name</code> <code>str</code> <p>Name of timm encoder (e.g., 'resnet50', 'efficientnet_b3').</p> <code>'resnet50'</code> <code>architecture</code> <code>str</code> <p>Segmentation architecture ('unet', 'unetplusplus', 'deeplabv3', 'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet', 'pan').</p> <code>'unet'</code> <code>encoder_weights</code> <code>str</code> <p>Pretrained weights ('imagenet', 'ssl', 'swsl', None).</p> <code>'imagenet'</code> <code>num_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>8</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer.</p> <code>0.0001</code> <code>val_split</code> <code>float</code> <p>Validation split ratio (0-1).</p> <code>0.2</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility.</p> <code>42</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>4</code> <code>freeze_encoder</code> <code>bool</code> <p>Freeze encoder during training.</p> <code>False</code> <code>monitor_metric</code> <code>str</code> <p>Metric to monitor ('val_loss' or 'val_iou').</p> <code>'val_iou'</code> <code>mode</code> <code>str</code> <p>'min' for loss, 'max' for metrics.</p> <code>'max'</code> <code>patience</code> <code>int</code> <p>Early stopping patience.</p> <code>10</code> <code>save_top_k</code> <code>int</code> <p>Number of best models to save.</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Print training progress.</p> <code>True</code> <code>device</code> <code>str</code> <p>Device to use. Auto-detected if None.</p> <code>None</code> <code>use_timm_model</code> <code>bool</code> <p>Load complete segmentation model from timm/HF Hub.</p> <code>False</code> <code>timm_model_name</code> <code>str</code> <p>Model name from HF Hub (e.g., 'hf-hub:nvidia/mit-b0').</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for training.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: Trained model.</p> Source code in <code>geoai/timm_segment.py</code> <pre><code>def train_timm_segmentation_model(\n    images_dir: str,\n    labels_dir: str,\n    output_dir: str,\n    input_format: str = \"directory\",\n    encoder_name: str = \"resnet50\",\n    architecture: str = \"unet\",\n    encoder_weights: str = \"imagenet\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    batch_size: int = 8,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001,\n    weight_decay: float = 1e-4,\n    val_split: float = 0.2,\n    seed: int = 42,\n    num_workers: int = 4,\n    freeze_encoder: bool = False,\n    monitor_metric: str = \"val_iou\",\n    mode: str = \"max\",\n    patience: int = 10,\n    save_top_k: int = 1,\n    verbose: bool = True,\n    device: Optional[str] = None,\n    use_timm_model: bool = False,\n    timm_model_name: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Train a semantic segmentation model using timm encoder (simplified interface).\n\n    This is a simplified function that takes image and label directories and handles\n    the dataset creation automatically, similar to train_segmentation_model.\n\n    Args:\n        images_dir (str): Directory containing image GeoTIFF files (for 'directory' format),\n            or root directory containing images/ subdirectory (for 'yolo' format),\n            or directory containing images (for 'coco' format).\n        labels_dir (str): Directory containing label GeoTIFF files (for 'directory' format),\n            or path to COCO annotations JSON file (for 'coco' format),\n            or not used (for 'yolo' format - labels are in images_dir/labels/).\n        output_dir (str): Directory to save model checkpoints and results.\n        input_format (str): Input data format - 'directory' (default), 'coco', or 'yolo'.\n            - 'directory': Standard directory structure with separate images_dir and labels_dir\n            - 'coco': COCO JSON format (labels_dir should be path to instances.json)\n            - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)\n        encoder_name (str): Name of timm encoder (e.g., 'resnet50', 'efficientnet_b3').\n        architecture (str): Segmentation architecture ('unet', 'unetplusplus', 'deeplabv3',\n            'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet', 'pan').\n        encoder_weights (str): Pretrained weights ('imagenet', 'ssl', 'swsl', None).\n        num_channels (int): Number of input channels.\n        num_classes (int): Number of output classes.\n        batch_size (int): Batch size for training.\n        num_epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate.\n        weight_decay (float): Weight decay for optimizer.\n        val_split (float): Validation split ratio (0-1).\n        seed (int): Random seed for reproducibility.\n        num_workers (int): Number of data loading workers.\n        freeze_encoder (bool): Freeze encoder during training.\n        monitor_metric (str): Metric to monitor ('val_loss' or 'val_iou').\n        mode (str): 'min' for loss, 'max' for metrics.\n        patience (int): Early stopping patience.\n        save_top_k (int): Number of best models to save.\n        verbose (bool): Print training progress.\n        device (str, optional): Device to use. Auto-detected if None.\n        use_timm_model (bool): Load complete segmentation model from timm/HF Hub.\n        timm_model_name (str, optional): Model name from HF Hub (e.g., 'hf-hub:nvidia/mit-b0').\n        **kwargs: Additional arguments for training.\n\n    Returns:\n        torch.nn.Module: Trained model.\n    \"\"\"\n    import glob\n    from sklearn.model_selection import train_test_split\n    from .train import parse_coco_annotations, parse_yolo_annotations\n\n    if not LIGHTNING_AVAILABLE:\n        raise ImportError(\n            \"PyTorch Lightning is required. Install it with: pip install lightning\"\n        )\n\n    # Set random seed\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    # Get image and label paths based on input format\n    if input_format.lower() == \"coco\":\n        # Parse COCO format annotations\n        if verbose:\n            print(f\"Loading COCO format annotations from {labels_dir}\")\n        # For COCO format, labels_dir is path to instances.json\n        # Labels are typically in a \"labels\" directory parallel to \"annotations\"\n        coco_root = os.path.dirname(os.path.dirname(labels_dir))  # Go up two levels\n        labels_directory = os.path.join(coco_root, \"labels\")\n        image_paths, label_paths = parse_coco_annotations(\n            labels_dir, images_dir, labels_directory\n        )\n    elif input_format.lower() == \"yolo\":\n        # Parse YOLO format annotations\n        if verbose:\n            print(f\"Loading YOLO format data from {images_dir}\")\n        image_paths, label_paths = parse_yolo_annotations(images_dir)\n    else:\n        # Default: directory format\n        image_paths = sorted(\n            glob.glob(os.path.join(images_dir, \"*.tif\"))\n            + glob.glob(os.path.join(images_dir, \"*.tiff\"))\n        )\n        label_paths = sorted(\n            glob.glob(os.path.join(labels_dir, \"*.tif\"))\n            + glob.glob(os.path.join(labels_dir, \"*.tiff\"))\n        )\n\n    if len(image_paths) == 0:\n        raise ValueError(f\"No images found\")\n    if len(label_paths) == 0:\n        raise ValueError(f\"No labels found\")\n    if len(image_paths) != len(label_paths):\n        raise ValueError(\n            f\"Number of images ({len(image_paths)}) doesn't match \"\n            f\"number of labels ({len(label_paths)})\"\n        )\n\n    if verbose:\n        print(f\"Found {len(image_paths)} image-label pairs\")\n\n    # Split into train and validation\n    train_images, val_images, train_labels, val_labels = train_test_split(\n        image_paths, label_paths, test_size=val_split, random_state=seed\n    )\n\n    if verbose:\n        print(f\"Training samples: {len(train_images)}\")\n        print(f\"Validation samples: {len(val_images)}\")\n\n    # Create datasets\n    train_dataset = SegmentationDataset(\n        image_paths=train_images,\n        mask_paths=train_labels,\n        num_channels=num_channels,\n    )\n\n    val_dataset = SegmentationDataset(\n        image_paths=val_images,\n        mask_paths=val_labels,\n        num_channels=num_channels,\n    )\n\n    # Train model\n    model = train_timm_segmentation(\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        test_dataset=None,\n        encoder_name=encoder_name,\n        architecture=architecture,\n        num_classes=num_classes,\n        in_channels=num_channels,\n        encoder_weights=encoder_weights,\n        output_dir=output_dir,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        num_workers=num_workers,\n        freeze_encoder=freeze_encoder,\n        accelerator=\"auto\" if device is None else device,\n        monitor_metric=monitor_metric,\n        mode=mode,\n        patience=patience,\n        save_top_k=save_top_k,\n        use_timm_model=use_timm_model,\n        timm_model_name=timm_model_name,\n        **kwargs,\n    )\n\n    if verbose:\n        print(f\"\\nTraining completed. Model saved to {output_dir}\")\n\n    return model.model  # Return the underlying model\n</code></pre>"},{"location":"timm_train/","title":"timm_train module","text":"<p>Module for training and fine-tuning models using timm (PyTorch Image Models) with remote sensing imagery.</p>"},{"location":"timm_train/#geoai.timm_train.RemoteSensingDataset","title":"<code>RemoteSensingDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for remote sensing imagery classification.</p> <p>This dataset handles loading raster images and their corresponding labels for training classification models.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>class RemoteSensingDataset(Dataset):\n    \"\"\"\n    Dataset for remote sensing imagery classification.\n\n    This dataset handles loading raster images and their corresponding labels\n    for training classification models.\n    \"\"\"\n\n    def __init__(\n        self,\n        image_paths: List[str],\n        labels: List[int],\n        transform: Optional[Callable] = None,\n        num_channels: Optional[int] = None,\n    ):\n        \"\"\"\n        Initialize RemoteSensingDataset.\n\n        Args:\n            image_paths (List[str]): List of paths to image files.\n            labels (List[int]): List of integer labels corresponding to images.\n            transform (callable, optional): Transform to apply to images.\n            num_channels (int, optional): Number of channels to use. If None, uses all.\n        \"\"\"\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n        self.num_channels = num_channels\n\n        if len(image_paths) != len(labels):\n            raise ValueError(\"Number of images must match number of labels\")\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        import rasterio\n\n        # Load image\n        with rasterio.open(self.image_paths[idx]) as src:\n            image = src.read()  # Shape: (C, H, W)\n\n            # Handle channel selection\n            if self.num_channels is not None and image.shape[0] != self.num_channels:\n                if image.shape[0] &gt; self.num_channels:\n                    image = image[: self.num_channels]\n                else:\n                    # Pad with zeros if needed\n                    padded = np.zeros(\n                        (self.num_channels, image.shape[1], image.shape[2])\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n            # Normalize to [0, 1]\n            if image.max() &gt; 1.0:\n                image = image / 255.0\n\n            image = image.astype(np.float32)\n\n        # Get label\n        label = self.labels[idx]\n\n        # Convert to tensor\n        image = torch.from_numpy(image)\n        label = torch.tensor(label, dtype=torch.long)\n\n        # Apply transforms if provided\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, label\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.RemoteSensingDataset.__init__","title":"<code>__init__(image_paths, labels, transform=None, num_channels=None)</code>","text":"<p>Initialize RemoteSensingDataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of paths to image files.</p> required <code>labels</code> <code>List[int]</code> <p>List of integer labels corresponding to images.</p> required <code>transform</code> <code>callable</code> <p>Transform to apply to images.</p> <code>None</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use. If None, uses all.</p> <code>None</code> Source code in <code>geoai/timm_train.py</code> <pre><code>def __init__(\n    self,\n    image_paths: List[str],\n    labels: List[int],\n    transform: Optional[Callable] = None,\n    num_channels: Optional[int] = None,\n):\n    \"\"\"\n    Initialize RemoteSensingDataset.\n\n    Args:\n        image_paths (List[str]): List of paths to image files.\n        labels (List[int]): List of integer labels corresponding to images.\n        transform (callable, optional): Transform to apply to images.\n        num_channels (int, optional): Number of channels to use. If None, uses all.\n    \"\"\"\n    self.image_paths = image_paths\n    self.labels = labels\n    self.transform = transform\n    self.num_channels = num_channels\n\n    if len(image_paths) != len(labels):\n        raise ValueError(\"Number of images must match number of labels\")\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.TimmClassifier","title":"<code>TimmClassifier</code>","text":"<p>               Bases: <code>LightningModule</code></p> <p>PyTorch Lightning module for image classification using timm models.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>class TimmClassifier(pl.LightningModule):\n    \"\"\"\n    PyTorch Lightning module for image classification using timm models.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"resnet50\",\n        num_classes: int = 10,\n        in_channels: int = 3,\n        pretrained: bool = True,\n        learning_rate: float = 1e-3,\n        weight_decay: float = 1e-4,\n        freeze_backbone: bool = False,\n        loss_fn: Optional[nn.Module] = None,\n        class_weights: Optional[torch.Tensor] = None,\n        **model_kwargs: Any,\n    ):\n        \"\"\"\n        Initialize TimmClassifier.\n\n        Args:\n            model_name (str): Name of timm model.\n            num_classes (int): Number of output classes.\n            in_channels (int): Number of input channels.\n            pretrained (bool): Use pretrained weights.\n            learning_rate (float): Learning rate for optimizer.\n            weight_decay (float): Weight decay for optimizer.\n            freeze_backbone (bool): Freeze backbone weights during training.\n            loss_fn (nn.Module, optional): Custom loss function. Defaults to CrossEntropyLoss.\n            class_weights (torch.Tensor, optional): Class weights for loss function.\n            **model_kwargs: Additional arguments for timm model.\n        \"\"\"\n        super().__init__()\n\n        if not TIMM_AVAILABLE:\n            raise ImportError(\"timm is required. Install it with: pip install timm\")\n\n        self.save_hyperparameters()\n\n        self.model = get_timm_model(\n            model_name=model_name,\n            num_classes=num_classes,\n            in_channels=in_channels,\n            pretrained=pretrained,\n            **model_kwargs,\n        )\n\n        if freeze_backbone:\n            self._freeze_backbone()\n\n        # Set up loss function\n        if loss_fn is not None:\n            self.loss_fn = loss_fn\n        elif class_weights is not None:\n            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n        else:\n            self.loss_fn = nn.CrossEntropyLoss()\n\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n\n    def _freeze_backbone(self):\n        \"\"\"Freeze all layers except the classifier head.\"\"\"\n        for name, param in self.model.named_parameters():\n            if \"fc\" not in name and \"head\" not in name and \"classifier\" not in name:\n                param.requires_grad = False\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n\n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n\n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n\n        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n\n        # Calculate accuracy\n        preds = torch.argmax(logits, dim=1)\n        acc = (preds == y).float().mean()\n\n        self.log(\"test_loss\", loss, on_epoch=True)\n        self.log(\"test_acc\", acc, on_epoch=True)\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(\n            self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay\n        )\n\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, mode=\"min\", factor=0.5, patience=5, verbose=True\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n            },\n        }\n\n    def predict_step(self, batch, batch_idx):\n        x = batch[0] if isinstance(batch, (list, tuple)) else batch\n        logits = self(x)\n        probs = torch.softmax(logits, dim=1)\n        preds = torch.argmax(probs, dim=1)\n        return {\"predictions\": preds, \"probabilities\": probs}\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.TimmClassifier.__init__","title":"<code>__init__(model_name='resnet50', num_classes=10, in_channels=3, pretrained=True, learning_rate=0.001, weight_decay=0.0001, freeze_backbone=False, loss_fn=None, class_weights=None, **model_kwargs)</code>","text":"<p>Initialize TimmClassifier.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of timm model.</p> <code>'resnet50'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>10</code> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Use pretrained weights.</p> <code>True</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for optimizer.</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer.</p> <code>0.0001</code> <code>freeze_backbone</code> <code>bool</code> <p>Freeze backbone weights during training.</p> <code>False</code> <code>loss_fn</code> <code>Module</code> <p>Custom loss function. Defaults to CrossEntropyLoss.</p> <code>None</code> <code>class_weights</code> <code>Tensor</code> <p>Class weights for loss function.</p> <code>None</code> <code>**model_kwargs</code> <code>Any</code> <p>Additional arguments for timm model.</p> <code>{}</code> Source code in <code>geoai/timm_train.py</code> <pre><code>def __init__(\n    self,\n    model_name: str = \"resnet50\",\n    num_classes: int = 10,\n    in_channels: int = 3,\n    pretrained: bool = True,\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-4,\n    freeze_backbone: bool = False,\n    loss_fn: Optional[nn.Module] = None,\n    class_weights: Optional[torch.Tensor] = None,\n    **model_kwargs: Any,\n):\n    \"\"\"\n    Initialize TimmClassifier.\n\n    Args:\n        model_name (str): Name of timm model.\n        num_classes (int): Number of output classes.\n        in_channels (int): Number of input channels.\n        pretrained (bool): Use pretrained weights.\n        learning_rate (float): Learning rate for optimizer.\n        weight_decay (float): Weight decay for optimizer.\n        freeze_backbone (bool): Freeze backbone weights during training.\n        loss_fn (nn.Module, optional): Custom loss function. Defaults to CrossEntropyLoss.\n        class_weights (torch.Tensor, optional): Class weights for loss function.\n        **model_kwargs: Additional arguments for timm model.\n    \"\"\"\n    super().__init__()\n\n    if not TIMM_AVAILABLE:\n        raise ImportError(\"timm is required. Install it with: pip install timm\")\n\n    self.save_hyperparameters()\n\n    self.model = get_timm_model(\n        model_name=model_name,\n        num_classes=num_classes,\n        in_channels=in_channels,\n        pretrained=pretrained,\n        **model_kwargs,\n    )\n\n    if freeze_backbone:\n        self._freeze_backbone()\n\n    # Set up loss function\n    if loss_fn is not None:\n        self.loss_fn = loss_fn\n    elif class_weights is not None:\n        self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n    else:\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    self.learning_rate = learning_rate\n    self.weight_decay = weight_decay\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.get_timm_model","title":"<code>get_timm_model(model_name='resnet50', num_classes=10, in_channels=3, pretrained=True, features_only=False, **kwargs)</code>","text":"<p>Create a timm model with custom input channels for remote sensing imagery.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the timm model (e.g., 'resnet50', 'efficientnet_b0', 'vit_base_patch16_224', 'convnext_base').</p> <code>'resnet50'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes for classification.</p> <code>10</code> <code>in_channels</code> <code>int</code> <p>Number of input channels (3 for RGB, 4 for RGBN, etc.).</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained weights.</p> <code>True</code> <code>features_only</code> <code>bool</code> <p>If True, return feature extraction model without classifier.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to timm.create_model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: Configured timm model.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If timm is not installed.</p> <code>ValueError</code> <p>If model_name is not available in timm.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>def get_timm_model(\n    model_name: str = \"resnet50\",\n    num_classes: int = 10,\n    in_channels: int = 3,\n    pretrained: bool = True,\n    features_only: bool = False,\n    **kwargs: Any,\n) -&gt; nn.Module:\n    \"\"\"\n    Create a timm model with custom input channels for remote sensing imagery.\n\n    Args:\n        model_name (str): Name of the timm model (e.g., 'resnet50', 'efficientnet_b0',\n            'vit_base_patch16_224', 'convnext_base').\n        num_classes (int): Number of output classes for classification.\n        in_channels (int): Number of input channels (3 for RGB, 4 for RGBN, etc.).\n        pretrained (bool): Whether to use pretrained weights.\n        features_only (bool): If True, return feature extraction model without classifier.\n        **kwargs: Additional arguments to pass to timm.create_model.\n\n    Returns:\n        nn.Module: Configured timm model.\n\n    Raises:\n        ImportError: If timm is not installed.\n        ValueError: If model_name is not available in timm.\n    \"\"\"\n    if not TIMM_AVAILABLE:\n        raise ImportError(\"timm is required. Install it with: pip install timm\")\n\n    # Check if model exists\n    if model_name not in timm.list_models():\n        available_models = timm.list_models(pretrained=True)[:10]\n        raise ValueError(\n            f\"Model '{model_name}' not found in timm. \"\n            f\"First 10 available models: {available_models}. \"\n            f\"See all models at: https://github.com/huggingface/pytorch-image-models\"\n        )\n\n    # Create base model\n    model = timm.create_model(\n        model_name,\n        pretrained=pretrained,\n        num_classes=num_classes if not features_only else 0,\n        in_chans=in_channels,\n        features_only=features_only,\n        **kwargs,\n    )\n\n    return model\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.list_timm_models","title":"<code>list_timm_models(filter='', pretrained=False, limit=None)</code>","text":"<p>List available timm models.</p> <p>Parameters:</p> Name Type Description Default <code>filter</code> <code>str</code> <p>Filter models by name pattern (e.g., 'resnet', 'efficientnet'). The filter supports wildcards. If no wildcards are provided, '*' is added automatically.</p> <code>''</code> <code>pretrained</code> <code>bool</code> <p>Only show models with pretrained weights.</p> <code>False</code> <code>limit</code> <code>int</code> <p>Maximum number of models to return.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of model names.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If timm is not installed.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>def list_timm_models(\n    filter: str = \"\",\n    pretrained: bool = False,\n    limit: Optional[int] = None,\n) -&gt; List[str]:\n    \"\"\"\n    List available timm models.\n\n    Args:\n        filter (str): Filter models by name pattern (e.g., 'resnet', 'efficientnet').\n            The filter supports wildcards. If no wildcards are provided, '*' is added automatically.\n        pretrained (bool): Only show models with pretrained weights.\n        limit (int, optional): Maximum number of models to return.\n\n    Returns:\n        List of model names.\n\n    Raises:\n        ImportError: If timm is not installed.\n    \"\"\"\n    if not TIMM_AVAILABLE:\n        raise ImportError(\"timm is required. Install it with: pip install timm\")\n\n    # Add wildcards if not present in filter\n    if filter and \"*\" not in filter:\n        filter = f\"*{filter}*\"\n\n    models = timm.list_models(filter=filter, pretrained=pretrained)\n\n    if limit is not None:\n        models = models[:limit]\n\n    return models\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.modify_first_conv_for_channels","title":"<code>modify_first_conv_for_channels(model, in_channels, pretrained_channels=3)</code>","text":"<p>Modify the first convolutional layer of a model to accept different number of input channels.</p> <p>This is useful when you have a pretrained model with 3 input channels but want to use imagery with more channels (e.g., 4 for RGBN, or more for multispectral).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model to modify.</p> required <code>in_channels</code> <code>int</code> <p>Desired number of input channels.</p> required <code>pretrained_channels</code> <code>int</code> <p>Number of channels in pretrained weights (usually 3).</p> <code>3</code> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: Modified model with updated first conv layer.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>def modify_first_conv_for_channels(\n    model: nn.Module,\n    in_channels: int,\n    pretrained_channels: int = 3,\n) -&gt; nn.Module:\n    \"\"\"\n    Modify the first convolutional layer of a model to accept different number of input channels.\n\n    This is useful when you have a pretrained model with 3 input channels but want to use\n    imagery with more channels (e.g., 4 for RGBN, or more for multispectral).\n\n    Args:\n        model (nn.Module): PyTorch model to modify.\n        in_channels (int): Desired number of input channels.\n        pretrained_channels (int): Number of channels in pretrained weights (usually 3).\n\n    Returns:\n        nn.Module: Modified model with updated first conv layer.\n    \"\"\"\n    if in_channels == pretrained_channels:\n        return model\n\n    # Find the first conv layer (different models have different architectures)\n    first_conv_name = None\n    first_conv = None\n\n    # Common patterns for first conv layers\n    possible_names = [\"conv1\", \"conv_stem\", \"patch_embed.proj\", \"stem.conv1\"]\n\n    for name in possible_names:\n        try:\n            parts = name.split(\".\")\n            module = model\n            for part in parts:\n                module = getattr(module, part)\n            if isinstance(module, nn.Conv2d):\n                first_conv_name = name\n                first_conv = module\n                break\n        except AttributeError:\n            continue\n\n    if first_conv is None:\n        # Fallback: search recursively\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Conv2d):\n                first_conv_name = name\n                first_conv = module\n                break\n\n    if first_conv is None:\n        raise ValueError(\"Could not find first convolutional layer in model\")\n\n    # Create new conv layer with desired input channels\n    new_conv = nn.Conv2d(\n        in_channels,\n        first_conv.out_channels,\n        kernel_size=first_conv.kernel_size,\n        stride=first_conv.stride,\n        padding=first_conv.padding,\n        bias=first_conv.bias is not None,\n    )\n\n    # Initialize weights\n    with torch.no_grad():\n        if pretrained_channels == 3 and in_channels &gt; 3:\n            # Copy RGB weights\n            new_conv.weight[:, :3, :, :] = first_conv.weight\n\n            # Initialize additional channels with mean of RGB weights\n            mean_weight = first_conv.weight.mean(dim=1, keepdim=True)\n            for i in range(3, in_channels):\n                new_conv.weight[:, i : i + 1, :, :] = mean_weight\n        else:\n            # Generic initialization\n            nn.init.kaiming_normal_(\n                new_conv.weight, mode=\"fan_out\", nonlinearity=\"relu\"\n            )\n\n        if first_conv.bias is not None:\n            new_conv.bias = first_conv.bias\n\n    # Replace the first conv layer\n    parts = first_conv_name.split(\".\")\n    if len(parts) == 1:\n        setattr(model, first_conv_name, new_conv)\n    else:\n        parent = model\n        for part in parts[:-1]:\n            parent = getattr(parent, part)\n        setattr(parent, parts[-1], new_conv)\n\n    return model\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.predict_with_timm","title":"<code>predict_with_timm(model, image_paths, batch_size=32, num_workers=4, device=None, return_probabilities=False)</code>","text":"<p>Make predictions on images using a trained timm model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Union[TimmClassifier, Module]</code> <p>Trained model (TimmClassifier or nn.Module).</p> required <code>image_paths</code> <code>List[str]</code> <p>List of paths to images.</p> required <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>32</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>4</code> <code>device</code> <code>Optional[str]</code> <p>Device to use ('cuda', 'cpu', etc.). Auto-detected if None.</p> <code>None</code> <code>return_probabilities</code> <code>bool</code> <p>If True, return both predictions and probabilities.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>predictions</code> <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>Array of predicted class indices.</p> <code>probabilities</code> <code>optional</code> <p>Array of class probabilities if return_probabilities=True.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>def predict_with_timm(\n    model: Union[TimmClassifier, nn.Module],\n    image_paths: List[str],\n    batch_size: int = 32,\n    num_workers: int = 4,\n    device: Optional[str] = None,\n    return_probabilities: bool = False,\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Make predictions on images using a trained timm model.\n\n    Args:\n        model: Trained model (TimmClassifier or nn.Module).\n        image_paths: List of paths to images.\n        batch_size: Batch size for inference.\n        num_workers: Number of data loading workers.\n        device: Device to use ('cuda', 'cpu', etc.). Auto-detected if None.\n        return_probabilities: If True, return both predictions and probabilities.\n\n    Returns:\n        predictions: Array of predicted class indices.\n        probabilities (optional): Array of class probabilities if return_probabilities=True.\n    \"\"\"\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    # Create dummy labels for dataset\n    dummy_labels = [0] * len(image_paths)\n    dataset = RemoteSensingDataset(image_paths, dummy_labels)\n\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    model.eval()\n    model = model.to(device)\n\n    all_preds = []\n    all_probs = []\n\n    with torch.no_grad():\n        for images, _ in tqdm(loader, desc=\"Making predictions\"):\n            images = images.to(device)\n\n            if isinstance(model, TimmClassifier):\n                logits = model(images)\n            else:\n                logits = model(images)\n\n            probs = torch.softmax(logits, dim=1)\n            preds = torch.argmax(probs, dim=1)\n\n            all_preds.append(preds.cpu().numpy())\n            if return_probabilities:\n                all_probs.append(probs.cpu().numpy())\n\n    predictions = np.concatenate(all_preds)\n\n    if return_probabilities:\n        probabilities = np.concatenate(all_probs)\n        return predictions, probabilities\n\n    return predictions\n</code></pre>"},{"location":"timm_train/#geoai.timm_train.train_timm_classifier","title":"<code>train_timm_classifier(train_dataset, val_dataset=None, test_dataset=None, model_name='resnet50', num_classes=10, in_channels=3, pretrained=True, output_dir='output', batch_size=32, num_epochs=50, learning_rate=0.001, weight_decay=0.0001, num_workers=4, freeze_backbone=False, class_weights=None, accelerator='auto', devices='auto', monitor_metric='val_loss', mode='min', patience=10, save_top_k=1, checkpoint_path=None, **kwargs)</code>","text":"<p>Train a timm-based classifier on remote sensing imagery.</p> <p>Parameters:</p> Name Type Description Default <code>train_dataset</code> <code>Dataset</code> <p>Training dataset.</p> required <code>val_dataset</code> <code>Dataset</code> <p>Validation dataset.</p> <code>None</code> <code>test_dataset</code> <code>Dataset</code> <p>Test dataset.</p> <code>None</code> <code>model_name</code> <code>str</code> <p>Name of timm model to use.</p> <code>'resnet50'</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>10</code> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Use pretrained weights.</p> <code>True</code> <code>output_dir</code> <code>str</code> <p>Directory to save outputs.</p> <code>'output'</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Learning rate.</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer.</p> <code>0.0001</code> <code>num_workers</code> <code>int</code> <p>Number of data loading workers.</p> <code>4</code> <code>freeze_backbone</code> <code>bool</code> <p>Freeze backbone during training.</p> <code>False</code> <code>class_weights</code> <code>List[float]</code> <p>Class weights for loss.</p> <code>None</code> <code>accelerator</code> <code>str</code> <p>Accelerator type ('auto', 'gpu', 'cpu').</p> <code>'auto'</code> <code>devices</code> <code>str</code> <p>Devices to use.</p> <code>'auto'</code> <code>monitor_metric</code> <code>str</code> <p>Metric to monitor for checkpointing.</p> <code>'val_loss'</code> <code>mode</code> <code>str</code> <p>'min' or 'max' for monitor_metric.</p> <code>'min'</code> <code>patience</code> <code>int</code> <p>Early stopping patience.</p> <code>10</code> <code>save_top_k</code> <code>int</code> <p>Number of best models to save.</p> <code>1</code> <code>checkpoint_path</code> <code>str</code> <p>Path to checkpoint to resume from.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments for PyTorch Lightning Trainer.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>TimmClassifier</code> <code>TimmClassifier</code> <p>Trained model.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If PyTorch Lightning is not installed.</p> Source code in <code>geoai/timm_train.py</code> <pre><code>def train_timm_classifier(\n    train_dataset: Dataset,\n    val_dataset: Optional[Dataset] = None,\n    test_dataset: Optional[Dataset] = None,\n    model_name: str = \"resnet50\",\n    num_classes: int = 10,\n    in_channels: int = 3,\n    pretrained: bool = True,\n    output_dir: str = \"output\",\n    batch_size: int = 32,\n    num_epochs: int = 50,\n    learning_rate: float = 1e-3,\n    weight_decay: float = 1e-4,\n    num_workers: int = 4,\n    freeze_backbone: bool = False,\n    class_weights: Optional[List[float]] = None,\n    accelerator: str = \"auto\",\n    devices: str = \"auto\",\n    monitor_metric: str = \"val_loss\",\n    mode: str = \"min\",\n    patience: int = 10,\n    save_top_k: int = 1,\n    checkpoint_path: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; TimmClassifier:\n    \"\"\"\n    Train a timm-based classifier on remote sensing imagery.\n\n    Args:\n        train_dataset (Dataset): Training dataset.\n        val_dataset (Dataset, optional): Validation dataset.\n        test_dataset (Dataset, optional): Test dataset.\n        model_name (str): Name of timm model to use.\n        num_classes (int): Number of output classes.\n        in_channels (int): Number of input channels.\n        pretrained (bool): Use pretrained weights.\n        output_dir (str): Directory to save outputs.\n        batch_size (int): Batch size for training.\n        num_epochs (int): Number of training epochs.\n        learning_rate (float): Learning rate.\n        weight_decay (float): Weight decay for optimizer.\n        num_workers (int): Number of data loading workers.\n        freeze_backbone (bool): Freeze backbone during training.\n        class_weights (List[float], optional): Class weights for loss.\n        accelerator (str): Accelerator type ('auto', 'gpu', 'cpu').\n        devices (str): Devices to use.\n        monitor_metric (str): Metric to monitor for checkpointing.\n        mode (str): 'min' or 'max' for monitor_metric.\n        patience (int): Early stopping patience.\n        save_top_k (int): Number of best models to save.\n        checkpoint_path (str, optional): Path to checkpoint to resume from.\n        **kwargs: Additional arguments for PyTorch Lightning Trainer.\n\n    Returns:\n        TimmClassifier: Trained model.\n\n    Raises:\n        ImportError: If PyTorch Lightning is not installed.\n    \"\"\"\n    if not LIGHTNING_AVAILABLE:\n        raise ImportError(\n            \"PyTorch Lightning is required. Install it with: pip install lightning\"\n        )\n\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    model_dir = os.path.join(output_dir, \"models\")\n    os.makedirs(model_dir, exist_ok=True)\n\n    # Convert class weights to tensor if provided\n    weight_tensor = None\n    if class_weights is not None:\n        weight_tensor = torch.tensor(class_weights, dtype=torch.float32)\n\n    # Create model\n    model = TimmClassifier(\n        model_name=model_name,\n        num_classes=num_classes,\n        in_channels=in_channels,\n        pretrained=pretrained,\n        learning_rate=learning_rate,\n        weight_decay=weight_decay,\n        freeze_backbone=freeze_backbone,\n        class_weights=weight_tensor,\n    )\n\n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    val_loader = None\n    if val_dataset is not None:\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n\n    # Set up callbacks\n    callbacks = []\n\n    # Model checkpoint\n    checkpoint_callback = ModelCheckpoint(\n        dirpath=model_dir,\n        filename=f\"{model_name}_{{epoch:02d}}_{{val_loss:.4f}}\",\n        monitor=monitor_metric,\n        mode=mode,\n        save_top_k=save_top_k,\n        save_last=True,\n        verbose=True,\n    )\n    callbacks.append(checkpoint_callback)\n\n    # Early stopping\n    early_stop_callback = EarlyStopping(\n        monitor=monitor_metric,\n        patience=patience,\n        mode=mode,\n        verbose=True,\n    )\n    callbacks.append(early_stop_callback)\n\n    # Set up logger\n    logger = CSVLogger(model_dir, name=\"lightning_logs\")\n\n    # Create trainer\n    trainer = pl.Trainer(\n        max_epochs=num_epochs,\n        accelerator=accelerator,\n        devices=devices,\n        callbacks=callbacks,\n        logger=logger,\n        log_every_n_steps=10,\n        **kwargs,\n    )\n\n    # Train model\n    print(f\"Training {model_name} for {num_epochs} epochs...\")\n    trainer.fit(\n        model,\n        train_dataloaders=train_loader,\n        val_dataloaders=val_loader,\n        ckpt_path=checkpoint_path,\n    )\n\n    # Test if test dataset provided\n    if test_dataset is not None:\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n        print(\"\\nTesting model on test set...\")\n        trainer.test(model, dataloaders=test_loader)\n\n    print(f\"\\nBest model saved at: {checkpoint_callback.best_model_path}\")\n\n    return model\n</code></pre>"},{"location":"train/","title":"train module","text":""},{"location":"train/#geoai.train.Compose","title":"<code>Compose</code>","text":"<p>Custom compose transform that works with image and target.</p> Source code in <code>geoai/train.py</code> <pre><code>class Compose:\n    \"\"\"Custom compose transform that works with image and target.\"\"\"\n\n    def __init__(self, transforms: List[Callable]) -&gt; None:\n        \"\"\"\n        Initialize compose transform.\n\n        Args:\n            transforms (list): List of transforms to apply.\n        \"\"\"\n        self.transforms = transforms\n\n    def __call__(\n        self, image: torch.Tensor, target: Dict[str, torch.Tensor]\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.Compose.__init__","title":"<code>__init__(transforms)</code>","text":"<p>Initialize compose transform.</p> <p>Parameters:</p> Name Type Description Default <code>transforms</code> <code>list</code> <p>List of transforms to apply.</p> required Source code in <code>geoai/train.py</code> <pre><code>def __init__(self, transforms: List[Callable]) -&gt; None:\n    \"\"\"\n    Initialize compose transform.\n\n    Args:\n        transforms (list): List of transforms to apply.\n    \"\"\"\n    self.transforms = transforms\n</code></pre>"},{"location":"train/#geoai.train.ObjectDetectionDataset","title":"<code>ObjectDetectionDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for object detection from GeoTIFF images and labels.</p> Source code in <code>geoai/train.py</code> <pre><code>class ObjectDetectionDataset(Dataset):\n    \"\"\"Dataset for object detection from GeoTIFF images and labels.\"\"\"\n\n    def __init__(\n        self,\n        image_paths: List[str],\n        label_paths: List[str],\n        transforms: Optional[Callable] = None,\n        num_channels: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize dataset.\n\n        Args:\n            image_paths (list): List of paths to image GeoTIFF files.\n            label_paths (list): List of paths to label GeoTIFF files.\n            transforms (callable, optional): Transformations to apply to images and masks.\n            num_channels (int, optional): Number of channels to use from images. If None,\n                auto-detected from the first image.\n        \"\"\"\n        self.image_paths = image_paths\n        self.label_paths = label_paths\n        self.transforms = transforms\n\n        # Auto-detect the number of channels if not specified\n        if num_channels is None:\n            with rasterio.open(self.image_paths[0]) as src:\n                self.num_channels = src.count\n        else:\n            self.num_channels = num_channels\n\n    def __len__(self) -&gt; int:\n        return len(self.image_paths)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        # Load image\n        with rasterio.open(self.image_paths[idx]) as src:\n            # Read as [C, H, W] format\n            image = src.read().astype(np.float32)\n\n            # Normalize image to [0, 1] range\n            image = image / 255.0\n\n            # Handle different number of channels\n            if image.shape[0] &gt; self.num_channels:\n                image = image[\n                    : self.num_channels\n                ]  # Keep only first 4 bands if more exist\n            elif image.shape[0] &lt; self.num_channels:\n                # Pad with zeros if less than 4 bands\n                padded = np.zeros(\n                    (self.num_channels, image.shape[1], image.shape[2]),\n                    dtype=np.float32,\n                )\n                padded[: image.shape[0]] = image\n                image = padded\n\n            # Convert to CHW tensor\n            image = torch.as_tensor(image, dtype=torch.float32)\n\n        # Load label mask\n        with rasterio.open(self.label_paths[idx]) as src:\n            label_mask = src.read(1)\n            binary_mask = (label_mask &gt; 0).astype(np.uint8)\n\n        # Find all building instances using connected components\n        labeled_mask, num_instances = measure.label(\n            binary_mask, return_num=True, connectivity=2\n        )\n\n        # Create list to hold masks for each building instance\n        masks = []\n        boxes = []\n        labels = []\n\n        for i in range(1, num_instances + 1):\n            # Create mask for this instance\n            instance_mask = (labeled_mask == i).astype(np.uint8)\n\n            # Calculate area and filter out tiny instances (noise)\n            area = instance_mask.sum()\n            if area &lt; 10:  # Minimum area threshold\n                continue\n\n            # Find bounding box coordinates\n            pos = np.where(instance_mask)\n            if len(pos[0]) == 0:  # Skip if mask is empty\n                continue\n\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n\n            # Skip invalid boxes\n            if xmax &lt;= xmin or ymax &lt;= ymin:\n                continue\n\n            # Add small padding to ensure the mask is within the box\n            xmin = max(0, xmin - 1)\n            ymin = max(0, ymin - 1)\n            xmax = min(binary_mask.shape[1] - 1, xmax + 1)\n            ymax = min(binary_mask.shape[0] - 1, ymax + 1)\n\n            boxes.append([xmin, ymin, xmax, ymax])\n            masks.append(instance_mask)\n            labels.append(1)  # 1 for building class\n\n        # Handle case with no valid instances\n        if len(boxes) == 0:\n            # Create a dummy target with minimal required fields\n            target = {\n                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n                \"labels\": torch.zeros((0), dtype=torch.int64),\n                \"masks\": torch.zeros(\n                    (0, binary_mask.shape[0], binary_mask.shape[1]), dtype=torch.uint8\n                ),\n                \"image_id\": torch.tensor([idx]),\n                \"area\": torch.zeros((0), dtype=torch.float32),\n                \"iscrowd\": torch.zeros((0), dtype=torch.int64),\n            }\n        else:\n            # Convert to tensors\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n\n            # Calculate area of boxes\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n            # Prepare target dictionary\n            target = {\n                \"boxes\": boxes,\n                \"labels\": labels,\n                \"masks\": masks,\n                \"image_id\": torch.tensor([idx]),\n                \"area\": area,\n                \"iscrowd\": torch.zeros_like(labels),  # Assume no crowd instances\n            }\n\n        # Apply transforms if specified\n        if self.transforms is not None:\n            image, target = self.transforms(image, target)\n\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.ObjectDetectionDataset.__init__","title":"<code>__init__(image_paths, label_paths, transforms=None, num_channels=None)</code>","text":"<p>Initialize dataset.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>list</code> <p>List of paths to image GeoTIFF files.</p> required <code>label_paths</code> <code>list</code> <p>List of paths to label GeoTIFF files.</p> required <code>transforms</code> <code>callable</code> <p>Transformations to apply to images and masks.</p> <code>None</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from images. If None, auto-detected from the first image.</p> <code>None</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(\n    self,\n    image_paths: List[str],\n    label_paths: List[str],\n    transforms: Optional[Callable] = None,\n    num_channels: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize dataset.\n\n    Args:\n        image_paths (list): List of paths to image GeoTIFF files.\n        label_paths (list): List of paths to label GeoTIFF files.\n        transforms (callable, optional): Transformations to apply to images and masks.\n        num_channels (int, optional): Number of channels to use from images. If None,\n            auto-detected from the first image.\n    \"\"\"\n    self.image_paths = image_paths\n    self.label_paths = label_paths\n    self.transforms = transforms\n\n    # Auto-detect the number of channels if not specified\n    if num_channels is None:\n        with rasterio.open(self.image_paths[0]) as src:\n            self.num_channels = src.count\n    else:\n        self.num_channels = num_channels\n</code></pre>"},{"location":"train/#geoai.train.RandomHorizontalFlip","title":"<code>RandomHorizontalFlip</code>","text":"<p>Random horizontal flip transform.</p> Source code in <code>geoai/train.py</code> <pre><code>class RandomHorizontalFlip:\n    \"\"\"Random horizontal flip transform.\"\"\"\n\n    def __init__(self, prob: float = 0.5) -&gt; None:\n        \"\"\"\n        Initialize random horizontal flip.\n\n        Args:\n            prob (float): Probability of applying the flip.\n        \"\"\"\n        self.prob = prob\n\n    def __call__(\n        self, image: torch.Tensor, target: Dict[str, torch.Tensor]\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        if random.random() &lt; self.prob:\n            # Flip image\n            image = torch.flip(image, dims=[2])  # Flip along width dimension\n\n            # Flip masks\n            if \"masks\" in target and len(target[\"masks\"]) &gt; 0:\n                target[\"masks\"] = torch.flip(target[\"masks\"], dims=[2])\n\n            # Update boxes\n            if \"boxes\" in target and len(target[\"boxes\"]) &gt; 0:\n                boxes = target[\"boxes\"]\n                width = image.shape[2]\n                boxes[:, 0], boxes[:, 2] = width - boxes[:, 2], width - boxes[:, 0]\n                target[\"boxes\"] = boxes\n\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.RandomHorizontalFlip.__init__","title":"<code>__init__(prob=0.5)</code>","text":"<p>Initialize random horizontal flip.</p> <p>Parameters:</p> Name Type Description Default <code>prob</code> <code>float</code> <p>Probability of applying the flip.</p> <code>0.5</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(self, prob: float = 0.5) -&gt; None:\n    \"\"\"\n    Initialize random horizontal flip.\n\n    Args:\n        prob (float): Probability of applying the flip.\n    \"\"\"\n    self.prob = prob\n</code></pre>"},{"location":"train/#geoai.train.SemanticBrightnessAdjustment","title":"<code>SemanticBrightnessAdjustment</code>","text":"<p>Random brightness adjustment transform for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticBrightnessAdjustment:\n    \"\"\"Random brightness adjustment transform for semantic segmentation.\"\"\"\n\n    def __init__(\n        self, brightness_range: Tuple[float, float] = (0.8, 1.2), prob: float = 0.5\n    ) -&gt; None:\n        \"\"\"\n        Initialize brightness adjustment transform.\n\n        Args:\n            brightness_range: Tuple of (min, max) brightness factors.\n            prob: Probability of applying the transform.\n        \"\"\"\n        self.brightness_range = brightness_range\n        self.prob = prob\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        if random.random() &lt; self.prob:\n            # Apply random brightness adjustment\n            factor = self.brightness_range[0] + random.random() * (\n                self.brightness_range[1] - self.brightness_range[0]\n            )\n            image = torch.clamp(image * factor, 0, 1)\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticBrightnessAdjustment.__init__","title":"<code>__init__(brightness_range=(0.8, 1.2), prob=0.5)</code>","text":"<p>Initialize brightness adjustment transform.</p> <p>Parameters:</p> Name Type Description Default <code>brightness_range</code> <code>Tuple[float, float]</code> <p>Tuple of (min, max) brightness factors.</p> <code>(0.8, 1.2)</code> <code>prob</code> <code>float</code> <p>Probability of applying the transform.</p> <code>0.5</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(\n    self, brightness_range: Tuple[float, float] = (0.8, 1.2), prob: float = 0.5\n) -&gt; None:\n    \"\"\"\n    Initialize brightness adjustment transform.\n\n    Args:\n        brightness_range: Tuple of (min, max) brightness factors.\n        prob: Probability of applying the transform.\n    \"\"\"\n    self.brightness_range = brightness_range\n    self.prob = prob\n</code></pre>"},{"location":"train/#geoai.train.SemanticContrastAdjustment","title":"<code>SemanticContrastAdjustment</code>","text":"<p>Random contrast adjustment transform for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticContrastAdjustment:\n    \"\"\"Random contrast adjustment transform for semantic segmentation.\"\"\"\n\n    def __init__(\n        self, contrast_range: Tuple[float, float] = (0.8, 1.2), prob: float = 0.5\n    ) -&gt; None:\n        \"\"\"\n        Initialize contrast adjustment transform.\n\n        Args:\n            contrast_range: Tuple of (min, max) contrast factors.\n            prob: Probability of applying the transform.\n        \"\"\"\n        self.contrast_range = contrast_range\n        self.prob = prob\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        if random.random() &lt; self.prob:\n            # Apply random contrast adjustment\n            factor = self.contrast_range[0] + random.random() * (\n                self.contrast_range[1] - self.contrast_range[0]\n            )\n            mean = image.mean(dim=(1, 2), keepdim=True)\n            image = torch.clamp((image - mean) * factor + mean, 0, 1)\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticContrastAdjustment.__init__","title":"<code>__init__(contrast_range=(0.8, 1.2), prob=0.5)</code>","text":"<p>Initialize contrast adjustment transform.</p> <p>Parameters:</p> Name Type Description Default <code>contrast_range</code> <code>Tuple[float, float]</code> <p>Tuple of (min, max) contrast factors.</p> <code>(0.8, 1.2)</code> <code>prob</code> <code>float</code> <p>Probability of applying the transform.</p> <code>0.5</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(\n    self, contrast_range: Tuple[float, float] = (0.8, 1.2), prob: float = 0.5\n) -&gt; None:\n    \"\"\"\n    Initialize contrast adjustment transform.\n\n    Args:\n        contrast_range: Tuple of (min, max) contrast factors.\n        prob: Probability of applying the transform.\n    \"\"\"\n    self.contrast_range = contrast_range\n    self.prob = prob\n</code></pre>"},{"location":"train/#geoai.train.SemanticRandomHorizontalFlip","title":"<code>SemanticRandomHorizontalFlip</code>","text":"<p>Random horizontal flip transform for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticRandomHorizontalFlip:\n    \"\"\"Random horizontal flip transform for semantic segmentation.\"\"\"\n\n    def __init__(self, prob: float = 0.5) -&gt; None:\n        self.prob = prob\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        if random.random() &lt; self.prob:\n            # Flip image and mask along width dimension\n            image = torch.flip(image, dims=[2])\n            mask = torch.flip(mask, dims=[1])\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticRandomRotation90","title":"<code>SemanticRandomRotation90</code>","text":"<p>Random 90-degree rotation transform for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticRandomRotation90:\n    \"\"\"Random 90-degree rotation transform for semantic segmentation.\"\"\"\n\n    def __init__(self, prob: float = 0.5) -&gt; None:\n        self.prob = prob\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        if random.random() &lt; self.prob:\n            # Randomly rotate by 90, 180, or 270 degrees\n            k = random.randint(1, 3)\n            image = torch.rot90(image, k, dims=[1, 2])\n            mask = torch.rot90(mask, k, dims=[0, 1])\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticRandomVerticalFlip","title":"<code>SemanticRandomVerticalFlip</code>","text":"<p>Random vertical flip transform for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticRandomVerticalFlip:\n    \"\"\"Random vertical flip transform for semantic segmentation.\"\"\"\n\n    def __init__(self, prob: float = 0.5) -&gt; None:\n        self.prob = prob\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        if random.random() &lt; self.prob:\n            # Flip image and mask along height dimension\n            image = torch.flip(image, dims=[1])\n            mask = torch.flip(mask, dims=[0])\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticSegmentationDataset","title":"<code>SemanticSegmentationDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Dataset for semantic segmentation from GeoTIFF, PNG, JPG, and other image formats.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticSegmentationDataset(Dataset):\n    \"\"\"Dataset for semantic segmentation from GeoTIFF, PNG, JPG, and other image formats.\"\"\"\n\n    def __init__(\n        self,\n        image_paths: List[str],\n        label_paths: List[str],\n        transforms: Optional[Callable] = None,\n        num_channels: Optional[int] = None,\n        target_size: Optional[Tuple[int, int]] = None,\n        resize_mode: str = \"resize\",\n        num_classes: int = 2,\n    ) -&gt; None:\n        \"\"\"\n        Initialize dataset for semantic segmentation.\n\n        Args:\n            image_paths (list): List of paths to image files (GeoTIFF, PNG, JPG, etc.).\n            label_paths (list): List of paths to label files (GeoTIFF, PNG, JPG, etc.).\n            transforms (callable, optional): Transformations to apply to images and masks.\n            num_channels (int, optional): Number of channels to use from images. If None,\n                auto-detected from the first image.\n            target_size (tuple, optional): Target size (height, width) for standardizing images.\n                If None, images will keep their original sizes.\n            resize_mode (str): How to handle size standardization. Options:\n                'resize' - Resize images to target_size (may change aspect ratio)\n                'pad' - Pad images to target_size (preserves aspect ratio)\n            num_classes (int): Number of classes for segmentation. Used for mask normalization.\n        \"\"\"\n        self.image_paths = image_paths\n        self.label_paths = label_paths\n        self.transforms = transforms\n        self.target_size = target_size\n        self.resize_mode = resize_mode\n        self.num_classes = num_classes\n\n        # Auto-detect the number of channels if not specified\n        if num_channels is None:\n            self.num_channels = self._get_num_channels(self.image_paths[0])\n        else:\n            self.num_channels = num_channels\n\n    def _is_geotiff(self, file_path: str) -&gt; bool:\n        \"\"\"Check if file is a GeoTIFF based on extension.\"\"\"\n        return file_path.lower().endswith((\".tif\", \".tiff\"))\n\n    def _get_num_channels(self, image_path: str) -&gt; int:\n        \"\"\"Get number of channels from an image file.\"\"\"\n        if self._is_geotiff(image_path):\n            with rasterio.open(image_path) as src:\n                return src.count\n        else:\n            # For standard image formats, use PIL\n            with Image.open(image_path) as img:\n                if img.mode == \"RGB\":\n                    return 3\n                elif img.mode == \"RGBA\":\n                    return 4\n                elif img.mode == \"L\":\n                    return 1\n                else:\n                    # Convert to RGB and return 3 channels\n                    return 3\n\n    def _resize_image_and_mask(\n        self, image: np.ndarray, mask: np.ndarray\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Resize image and mask to target size.\"\"\"\n        if self.target_size is None:\n            return image, mask\n\n        target_h, target_w = self.target_size\n\n        if self.resize_mode == \"resize\":\n            # Direct resize (may change aspect ratio)\n            image = F.interpolate(\n                image.unsqueeze(0),\n                size=(target_h, target_w),\n                mode=\"bilinear\",\n                align_corners=False,\n            ).squeeze(0)\n\n            mask = (\n                F.interpolate(\n                    mask.unsqueeze(0).unsqueeze(0).float(),\n                    size=(target_h, target_w),\n                    mode=\"nearest\",\n                )\n                .squeeze(0)\n                .squeeze(0)\n                .long()\n            )\n            # Clamp mask values to ensure they're within valid range [0, num_classes-1]\n            mask = torch.clamp(mask, 0, self.num_classes - 1)\n\n        elif self.resize_mode == \"pad\":\n            # Pad to target size (preserves aspect ratio)\n            image = self._pad_to_size(image, (target_h, target_w))\n            mask = self._pad_to_size(mask.unsqueeze(0), (target_h, target_w)).squeeze(0)\n            # Clamp mask values to ensure they're within valid range [0, num_classes-1]\n            mask = torch.clamp(mask, 0, self.num_classes - 1)\n\n        return image, mask\n\n    def _pad_to_size(\n        self, tensor: torch.Tensor, target_size: Tuple[int, int]\n    ) -&gt; torch.Tensor:\n        \"\"\"Pad tensor to target size with zeros.\"\"\"\n        target_h, target_w = target_size\n\n        if tensor.dim() == 3:  # Image [C, H, W]\n            _, h, w = tensor.shape\n        elif tensor.dim() == 2:  # Mask [H, W]\n            h, w = tensor.shape\n        else:\n            raise ValueError(f\"Unexpected tensor dimensions: {tensor.shape}\")\n\n        # Calculate padding\n        pad_h = max(0, target_h - h)\n        pad_w = max(0, target_w - w)\n\n        # Pad equally on both sides\n        pad_top = pad_h // 2\n        pad_bottom = pad_h - pad_top\n        pad_left = pad_w // 2\n        pad_right = pad_w - pad_left\n\n        # Apply padding (left, right, top, bottom)\n        padded = F.pad(tensor, (pad_left, pad_right, pad_top, pad_bottom), value=0)\n\n        # Crop if tensor is larger than target\n        if tensor.dim() == 3:\n            padded = padded[:, :target_h, :target_w]\n        else:\n            padded = padded[:target_h, :target_w]\n\n        return padded\n\n    def __len__(self) -&gt; int:\n        return len(self.image_paths)\n\n    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        # Load image\n        image_path = self.image_paths[idx]\n        if self._is_geotiff(image_path):\n            # Load GeoTIFF using rasterio\n            with rasterio.open(image_path) as src:\n                # Read as [C, H, W] format\n                image = src.read().astype(np.float32)\n                # Normalize image to [0, 1] range\n                image = image / 255.0\n        else:\n            # Load standard image formats using PIL\n            with Image.open(image_path) as img:\n                # Convert to RGB if needed\n                if img.mode != \"RGB\":\n                    img = img.convert(\"RGB\")\n                # Convert to numpy array [H, W, C]\n                image = np.array(img, dtype=np.float32)\n                # Normalize to [0, 1] range\n                image = image / 255.0\n                # Convert to [C, H, W] format\n                image = np.transpose(image, (2, 0, 1))\n\n        # Handle different number of channels\n        if image.shape[0] &gt; self.num_channels:\n            image = image[: self.num_channels]  # Keep only specified bands\n        elif image.shape[0] &lt; self.num_channels:\n            # Pad with zeros if less than specified bands\n            padded = np.zeros(\n                (self.num_channels, image.shape[1], image.shape[2]),\n                dtype=np.float32,\n            )\n            padded[: image.shape[0]] = image\n            image = padded\n\n        # Convert to CHW tensor\n        image = torch.as_tensor(image, dtype=torch.float32)\n\n        # Load label mask\n        label_path = self.label_paths[idx]\n        if self._is_geotiff(label_path):\n            # Load GeoTIFF label using rasterio\n            with rasterio.open(label_path) as src:\n                label_mask = src.read(1).astype(np.int64)\n        else:\n            # Load standard image format label using PIL\n            with Image.open(label_path) as img:\n                # Convert to grayscale if needed\n                if img.mode != \"L\":\n                    img = img.convert(\"L\")\n                label_mask = np.array(img, dtype=np.int64)\n\n        # Normalize mask values to expected class range [0, num_classes-1]\n        # This handles cases where masks contain pixel values outside the expected range\n        unique_vals = np.unique(label_mask)\n        if len(unique_vals) &gt; 2:\n            # For multi-class case, we need to map values to proper class indices\n            # For now, we'll use a simple thresholding approach for binary segmentation\n            if self.num_classes == 2:\n                # Binary segmentation: convert to 0 (background) and 1 (foreground)\n                label_mask = (label_mask &gt; 0).astype(np.int64)\n            else:\n                # For multi-class, we could implement more sophisticated mapping\n                # For now, just ensure values are in valid range\n                label_mask = np.clip(label_mask, 0, self.num_classes - 1)\n        elif len(unique_vals) == 2 and unique_vals.max() &gt; 1:\n            # Binary mask with values not in [0,1] range - normalize to [0,1]\n            label_mask = (label_mask &gt; 0).astype(np.int64)\n\n        # Convert to tensor\n        mask = torch.as_tensor(label_mask, dtype=torch.long)\n\n        # Resize image and mask to target size if specified\n        image, mask = self._resize_image_and_mask(image, mask)\n\n        # Apply transforms if specified\n        if self.transforms is not None:\n            image, mask = self.transforms(image, mask)\n\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticSegmentationDataset.__init__","title":"<code>__init__(image_paths, label_paths, transforms=None, num_channels=None, target_size=None, resize_mode='resize', num_classes=2)</code>","text":"<p>Initialize dataset for semantic segmentation.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>list</code> <p>List of paths to image files (GeoTIFF, PNG, JPG, etc.).</p> required <code>label_paths</code> <code>list</code> <p>List of paths to label files (GeoTIFF, PNG, JPG, etc.).</p> required <code>transforms</code> <code>callable</code> <p>Transformations to apply to images and masks.</p> <code>None</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from images. If None, auto-detected from the first image.</p> <code>None</code> <code>target_size</code> <code>tuple</code> <p>Target size (height, width) for standardizing images. If None, images will keep their original sizes.</p> <code>None</code> <code>resize_mode</code> <code>str</code> <p>How to handle size standardization. Options: 'resize' - Resize images to target_size (may change aspect ratio) 'pad' - Pad images to target_size (preserves aspect ratio)</p> <code>'resize'</code> <code>num_classes</code> <code>int</code> <p>Number of classes for segmentation. Used for mask normalization.</p> <code>2</code> Source code in <code>geoai/train.py</code> <pre><code>def __init__(\n    self,\n    image_paths: List[str],\n    label_paths: List[str],\n    transforms: Optional[Callable] = None,\n    num_channels: Optional[int] = None,\n    target_size: Optional[Tuple[int, int]] = None,\n    resize_mode: str = \"resize\",\n    num_classes: int = 2,\n) -&gt; None:\n    \"\"\"\n    Initialize dataset for semantic segmentation.\n\n    Args:\n        image_paths (list): List of paths to image files (GeoTIFF, PNG, JPG, etc.).\n        label_paths (list): List of paths to label files (GeoTIFF, PNG, JPG, etc.).\n        transforms (callable, optional): Transformations to apply to images and masks.\n        num_channels (int, optional): Number of channels to use from images. If None,\n            auto-detected from the first image.\n        target_size (tuple, optional): Target size (height, width) for standardizing images.\n            If None, images will keep their original sizes.\n        resize_mode (str): How to handle size standardization. Options:\n            'resize' - Resize images to target_size (may change aspect ratio)\n            'pad' - Pad images to target_size (preserves aspect ratio)\n        num_classes (int): Number of classes for segmentation. Used for mask normalization.\n    \"\"\"\n    self.image_paths = image_paths\n    self.label_paths = label_paths\n    self.transforms = transforms\n    self.target_size = target_size\n    self.resize_mode = resize_mode\n    self.num_classes = num_classes\n\n    # Auto-detect the number of channels if not specified\n    if num_channels is None:\n        self.num_channels = self._get_num_channels(self.image_paths[0])\n    else:\n        self.num_channels = num_channels\n</code></pre>"},{"location":"train/#geoai.train.SemanticToTensor","title":"<code>SemanticToTensor</code>","text":"<p>Convert numpy.ndarray to tensor for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticToTensor:\n    \"\"\"Convert numpy.ndarray to tensor for semantic segmentation.\"\"\"\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.SemanticTransforms","title":"<code>SemanticTransforms</code>","text":"<p>Custom transforms for semantic segmentation.</p> Source code in <code>geoai/train.py</code> <pre><code>class SemanticTransforms:\n    \"\"\"Custom transforms for semantic segmentation.\"\"\"\n\n    def __init__(self, transforms: List[Callable]) -&gt; None:\n        self.transforms = transforms\n\n    def __call__(\n        self, image: torch.Tensor, mask: torch.Tensor\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        for t in self.transforms:\n            image, mask = t(image, mask)\n        return image, mask\n</code></pre>"},{"location":"train/#geoai.train.ToTensor","title":"<code>ToTensor</code>","text":"<p>Convert numpy.ndarray to tensor.</p> Source code in <code>geoai/train.py</code> <pre><code>class ToTensor:\n    \"\"\"Convert numpy.ndarray to tensor.\"\"\"\n\n    def __call__(\n        self, image: torch.Tensor, target: Dict[str, torch.Tensor]\n    ) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n        \"\"\"\n        Apply transform to image and target.\n\n        Args:\n            image (torch.Tensor): Input image.\n            target (dict): Target annotations.\n\n        Returns:\n            tuple: Transformed image and target.\n        \"\"\"\n        return image, target\n</code></pre>"},{"location":"train/#geoai.train.ToTensor.__call__","title":"<code>__call__(image, target)</code>","text":"<p>Apply transform to image and target.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Tensor</code> <p>Input image.</p> required <code>target</code> <code>dict</code> <p>Target annotations.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tensor, Dict[str, Tensor]]</code> <p>Transformed image and target.</p> Source code in <code>geoai/train.py</code> <pre><code>def __call__(\n    self, image: torch.Tensor, target: Dict[str, torch.Tensor]\n) -&gt; Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n    \"\"\"\n    Apply transform to image and target.\n\n    Args:\n        image (torch.Tensor): Input image.\n        target (dict): Target annotations.\n\n    Returns:\n        tuple: Transformed image and target.\n    \"\"\"\n    return image, target\n</code></pre>"},{"location":"train/#geoai.train.collate_fn","title":"<code>collate_fn(batch)</code>","text":"<p>Custom collate function for batching samples.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list</code> <p>List of (image, target) tuples.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tuple[Tensor, ...], Tuple[Dict[str, Tensor], ...]]</code> <p>Tuple of images and targets.</p> Source code in <code>geoai/train.py</code> <pre><code>def collate_fn(\n    batch: List[Tuple[torch.Tensor, Dict[str, torch.Tensor]]],\n) -&gt; Tuple[Tuple[torch.Tensor, ...], Tuple[Dict[str, torch.Tensor], ...]]:\n    \"\"\"\n    Custom collate function for batching samples.\n\n    Args:\n        batch (list): List of (image, target) tuples.\n\n    Returns:\n        tuple: Tuple of images and targets.\n    \"\"\"\n    return tuple(zip(*batch))\n</code></pre>"},{"location":"train/#geoai.train.evaluate","title":"<code>evaluate(model, data_loader, device)</code>","text":"<p>Evaluate the model on the validation set.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to evaluate.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>device</code> <code>device</code> <p>Device to evaluate on.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>Evaluation metrics including loss and IoU.</p> Source code in <code>geoai/train.py</code> <pre><code>def evaluate(\n    model: torch.nn.Module, data_loader: DataLoader, device: torch.device\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the model on the validation set.\n\n    Args:\n        model (torch.nn.Module): The model to evaluate.\n        data_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        device (torch.device): Device to evaluate on.\n\n    Returns:\n        dict: Evaluation metrics including loss and IoU.\n    \"\"\"\n    model.eval()\n\n    # Initialize metrics\n    total_loss = 0\n    iou_scores = []\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            # Move to device\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            # During evaluation, Mask R-CNN directly returns predictions, not losses\n            # So we'll only get loss when we provide targets explicitly\n            if len(targets) &gt; 0:\n                try:\n                    # Try to get loss dict (this works in some implementations)\n                    loss_dict = model(images, targets)\n                    if isinstance(loss_dict, dict):\n                        losses = sum(loss for loss in loss_dict.values())\n                        total_loss += losses.item()\n                except Exception as e:\n                    print(f\"Warning: Could not compute loss during evaluation: {e}\")\n                    # If we can't compute loss, we'll just focus on IoU\n                    pass\n\n            # Get predictions\n            outputs = model(images)\n\n            # Calculate IoU for each image\n            for i, output in enumerate(outputs):\n                if len(output[\"masks\"]) == 0 or len(targets[i][\"masks\"]) == 0:\n                    continue\n\n                # Convert predicted masks to binary (threshold at 0.5)\n                pred_masks = (output[\"masks\"].squeeze(1) &gt; 0.5).float()\n\n                # Combine all instance masks into a single binary mask\n                pred_combined = (\n                    torch.max(pred_masks, dim=0)[0]\n                    if pred_masks.shape[0] &gt; 0\n                    else torch.zeros_like(targets[i][\"masks\"][0])\n                )\n                target_combined = (\n                    torch.max(targets[i][\"masks\"], dim=0)[0]\n                    if targets[i][\"masks\"].shape[0] &gt; 0\n                    else torch.zeros_like(pred_combined)\n                )\n\n                # Calculate IoU\n                intersection = (pred_combined * target_combined).sum().item()\n                union = ((pred_combined + target_combined) &gt; 0).sum().item()\n\n                if union &gt; 0:\n                    iou = intersection / union\n                    iou_scores.append(iou)\n\n    # Calculate metrics\n    avg_loss = total_loss / len(data_loader) if total_loss &gt; 0 else float(\"inf\")\n    avg_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n\n    return {\"loss\": avg_loss, \"IoU\": avg_iou}\n</code></pre>"},{"location":"train/#geoai.train.evaluate_semantic","title":"<code>evaluate_semantic(model, data_loader, device, criterion, num_classes=2)</code>","text":"<p>Evaluate the semantic segmentation model on the validation set.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to evaluate.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for validation data.</p> required <code>device</code> <code>device</code> <p>Device to evaluate on.</p> required <code>criterion</code> <code>Any</code> <p>Loss function.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes for evaluation metrics.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>Evaluation metrics including loss, IoU, F1, precision, and recall.</p> Source code in <code>geoai/train.py</code> <pre><code>def evaluate_semantic(\n    model: torch.nn.Module,\n    data_loader: DataLoader,\n    device: torch.device,\n    criterion: Any,\n    num_classes: int = 2,\n) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the semantic segmentation model on the validation set.\n\n    Args:\n        model (torch.nn.Module): The model to evaluate.\n        data_loader (torch.utils.data.DataLoader): DataLoader for validation data.\n        device (torch.device): Device to evaluate on.\n        criterion: Loss function.\n        num_classes (int): Number of classes for evaluation metrics.\n\n    Returns:\n        dict: Evaluation metrics including loss, IoU, F1, precision, and recall.\n    \"\"\"\n    model.eval()\n\n    total_loss = 0\n    f1_scores = []\n    iou_scores = []\n    precision_scores = []\n    recall_scores = []\n    num_batches = len(data_loader)\n\n    with torch.no_grad():\n        for images, targets in data_loader:\n            # Move to device\n            images = images.to(device)\n            targets = targets.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            total_loss += loss.item()\n\n            # Calculate metrics for each sample in the batch\n            for pred, target in zip(outputs, targets):\n                f1 = f1_score(pred, target, num_classes=num_classes)\n                iou = iou_coefficient(pred, target, num_classes=num_classes)\n                precision = precision_score(pred, target, num_classes=num_classes)\n                recall = recall_score(pred, target, num_classes=num_classes)\n                f1_scores.append(f1)\n                iou_scores.append(iou)\n                precision_scores.append(precision)\n                recall_scores.append(recall)\n\n    # Calculate metrics\n    avg_loss = total_loss / num_batches\n    avg_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0\n    avg_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n    avg_precision = (\n        sum(precision_scores) / len(precision_scores) if precision_scores else 0\n    )\n    avg_recall = sum(recall_scores) / len(recall_scores) if recall_scores else 0\n\n    return {\n        \"loss\": avg_loss,\n        \"F1\": avg_f1,\n        \"IoU\": avg_iou,\n        \"Precision\": avg_precision,\n        \"Recall\": avg_recall,\n    }\n</code></pre>"},{"location":"train/#geoai.train.f1_score","title":"<code>f1_score(pred, target, smooth=1e-06, num_classes=None)</code>","text":"<p>Calculate F1 score (also known as Dice coefficient) for segmentation (binary or multi-class).</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask with shape [H, W].</p> required <code>smooth</code> <code>float</code> <p>Smoothing factor to avoid division by zero.</p> <code>1e-06</code> <code>num_classes</code> <code>int</code> <p>Number of classes. If None, auto-detected.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Mean F1 score across all classes.</p> Source code in <code>geoai/train.py</code> <pre><code>def f1_score(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    smooth: float = 1e-6,\n    num_classes: Optional[int] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate F1 score (also known as Dice coefficient) for segmentation (binary or multi-class).\n\n    Args:\n        pred (torch.Tensor): Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].\n        target (torch.Tensor): Ground truth mask with shape [H, W].\n        smooth (float): Smoothing factor to avoid division by zero.\n        num_classes (int, optional): Number of classes. If None, auto-detected.\n\n    Returns:\n        float: Mean F1 score across all classes.\n    \"\"\"\n    # Convert predictions to class predictions\n    if pred.dim() == 3:  # [C, H, W] format\n        pred = torch.softmax(pred, dim=0)\n        pred_classes = torch.argmax(pred, dim=0)\n    elif pred.dim() == 2:  # [H, W] format\n        pred_classes = pred\n    else:\n        raise ValueError(f\"Unexpected prediction dimensions: {pred.shape}\")\n\n    # Auto-detect number of classes if not provided\n    if num_classes is None:\n        num_classes = max(pred_classes.max().item(), target.max().item()) + 1\n\n    # Calculate F1 score for each class and average\n    f1_scores = []\n    for class_id in range(num_classes):\n        pred_class = (pred_classes == class_id).float()\n        target_class = (target == class_id).float()\n\n        intersection = (pred_class * target_class).sum()\n        union = pred_class.sum() + target_class.sum()\n\n        if union &gt; 0:\n            f1 = (2.0 * intersection + smooth) / (union + smooth)\n            f1_scores.append(f1.item())\n\n    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\n</code></pre>"},{"location":"train/#geoai.train.get_instance_segmentation_model","title":"<code>get_instance_segmentation_model(num_classes=2, num_channels=3, pretrained=True)</code>","text":"<p>Get Mask R-CNN model with custom input channels and output classes.</p> <p>Parameters:</p> Name Type Description Default <code>num_classes</code> <code>int</code> <p>Number of output classes (including background).</p> <code>2</code> <code>num_channels</code> <code>int</code> <p>Number of input channels (3 for RGB, 4 for RGBN).</p> <code>3</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone.</p> <code>True</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: Mask R-CNN model with specified input channels and output classes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If num_channels is less than 3.</p> Source code in <code>geoai/train.py</code> <pre><code>def get_instance_segmentation_model(\n    num_classes: int = 2, num_channels: int = 3, pretrained: bool = True\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Get Mask R-CNN model with custom input channels and output classes.\n\n    Args:\n        num_classes (int): Number of output classes (including background).\n        num_channels (int): Number of input channels (3 for RGB, 4 for RGBN).\n        pretrained (bool): Whether to use pretrained backbone.\n\n    Returns:\n        torch.nn.Module: Mask R-CNN model with specified input channels and output classes.\n\n    Raises:\n        ValueError: If num_channels is less than 3.\n    \"\"\"\n    # Validate num_channels\n    if num_channels &lt; 3:\n        raise ValueError(\"num_channels must be at least 3\")\n\n    # Load pre-trained model\n    model = maskrcnn_resnet50_fpn(\n        pretrained=pretrained,\n        progress=True,\n        weights=(\n            torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n            if pretrained\n            else None\n        ),\n    )\n\n    # Modify transform if num_channels is different from 3\n    if num_channels != 3:\n        # Get the transform\n        transform = model.transform\n\n        # Default values are [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225]\n        # Calculate means and stds for additional channels\n        rgb_mean = [0.485, 0.456, 0.406]\n        rgb_std = [0.229, 0.224, 0.225]\n\n        # Extend them to num_channels (use the mean value for additional channels)\n        mean_of_means = sum(rgb_mean) / len(rgb_mean)\n        mean_of_stds = sum(rgb_std) / len(rgb_std)\n\n        # Create new lists with appropriate length\n        transform.image_mean = rgb_mean + [mean_of_means] * (num_channels - 3)\n        transform.image_std = rgb_std + [mean_of_stds] * (num_channels - 3)\n\n    # Get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # Replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # Get number of input features for mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n\n    # Replace mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n        in_features_mask, hidden_layer, num_classes\n    )\n\n    # Modify the first layer if num_channels is different from 3\n    if num_channels != 3:\n        original_layer = model.backbone.body.conv1\n        model.backbone.body.conv1 = torch.nn.Conv2d(\n            num_channels,\n            original_layer.out_channels,\n            kernel_size=original_layer.kernel_size,\n            stride=original_layer.stride,\n            padding=original_layer.padding,\n            bias=original_layer.bias is not None,\n        )\n\n        # Copy weights from the original 3 channels to the new layer\n        with torch.no_grad():\n            # Copy the weights for the first 3 channels\n            model.backbone.body.conv1.weight[:, :3, :, :] = original_layer.weight\n\n            # Initialize additional channels with the mean of the first 3 channels\n            mean_weight = original_layer.weight.mean(dim=1, keepdim=True)\n            for i in range(3, num_channels):\n                model.backbone.body.conv1.weight[:, i : i + 1, :, :] = mean_weight\n\n            # Copy bias if it exists\n            if original_layer.bias is not None:\n                model.backbone.body.conv1.bias = original_layer.bias\n\n    return model\n</code></pre>"},{"location":"train/#geoai.train.get_semantic_transform","title":"<code>get_semantic_transform(train)</code>","text":"<p>Get transforms for semantic segmentation data augmentation.</p> <p>This function returns default data augmentation transforms for training semantic segmentation models. The transforms include geometric transformations (horizontal/vertical flips, rotations) and photometric adjustments (brightness, contrast) that are commonly used in remote sensing tasks.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>bool</code> <p>Whether to include training-specific transforms. If True, applies augmentations (flips, rotations, brightness/contrast adjustments). If False, only converts to tensor (for validation).</p> required <p>Returns:</p> Name Type Description <code>SemanticTransforms</code> <code>Any</code> <p>Composed transforms.</p> Example <p>train_transform = get_semantic_transform(train=True) val_transform = get_semantic_transform(train=False)</p> Source code in <code>geoai/train.py</code> <pre><code>def get_semantic_transform(train: bool) -&gt; Any:\n    \"\"\"\n    Get transforms for semantic segmentation data augmentation.\n\n    This function returns default data augmentation transforms for training\n    semantic segmentation models. The transforms include geometric transformations\n    (horizontal/vertical flips, rotations) and photometric adjustments (brightness,\n    contrast) that are commonly used in remote sensing tasks.\n\n    Args:\n        train (bool): Whether to include training-specific transforms.\n            If True, applies augmentations (flips, rotations, brightness/contrast adjustments).\n            If False, only converts to tensor (for validation).\n\n    Returns:\n        SemanticTransforms: Composed transforms.\n\n    Example:\n        &gt;&gt;&gt; train_transform = get_semantic_transform(train=True)\n        &gt;&gt;&gt; val_transform = get_semantic_transform(train=False)\n    \"\"\"\n    transforms = []\n    transforms.append(SemanticToTensor())\n\n    if train:\n        # Geometric transforms - preserve spatial structure\n        transforms.append(SemanticRandomHorizontalFlip(0.5))\n        transforms.append(SemanticRandomVerticalFlip(0.5))\n        transforms.append(SemanticRandomRotation90(0.5))\n\n        # Photometric transforms - improve model robustness\n        transforms.append(\n            SemanticBrightnessAdjustment(brightness_range=(0.8, 1.2), prob=0.5)\n        )\n        transforms.append(\n            SemanticContrastAdjustment(contrast_range=(0.8, 1.2), prob=0.5)\n        )\n\n    return SemanticTransforms(transforms)\n</code></pre>"},{"location":"train/#geoai.train.get_smp_model","title":"<code>get_smp_model(architecture='unet', encoder_name='resnet34', encoder_weights='imagenet', in_channels=3, classes=2, activation=None, **kwargs)</code>","text":"<p>Get a segmentation model from segmentation-models-pytorch using the generic create_model function.</p> <p>Parameters:</p> Name Type Description Default <code>architecture</code> <code>str</code> <p>Model architecture (e.g., 'unet', 'deeplabv3', 'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet', 'pan', 'upernet', etc.). Case insensitive.</p> <code>'unet'</code> <code>encoder_name</code> <code>str</code> <p>Encoder backbone name (e.g., 'resnet34', 'efficientnet-b0', 'mit_b0', etc.).</p> <code>'resnet34'</code> <code>encoder_weights</code> <code>str</code> <p>Encoder weights ('imagenet' or None).</p> <code>'imagenet'</code> <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> <code>3</code> <code>classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> <code>activation</code> <code>str</code> <p>Activation function for output layer.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to smp.create_model().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: Segmentation model.</p> Note <p>This function uses smp.create_model() which supports all architectures available in segmentation-models-pytorch, making it future-proof for new model additions.</p> Source code in <code>geoai/train.py</code> <pre><code>def get_smp_model(\n    architecture: str = \"unet\",\n    encoder_name: str = \"resnet34\",\n    encoder_weights: Optional[str] = \"imagenet\",\n    in_channels: int = 3,\n    classes: int = 2,\n    activation: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Get a segmentation model from segmentation-models-pytorch using the generic create_model function.\n\n    Args:\n        architecture (str): Model architecture (e.g., 'unet', 'deeplabv3', 'deeplabv3plus', 'fpn',\n            'pspnet', 'linknet', 'manet', 'pan', 'upernet', etc.). Case insensitive.\n        encoder_name (str): Encoder backbone name (e.g., 'resnet34', 'efficientnet-b0', 'mit_b0', etc.).\n        encoder_weights (str): Encoder weights ('imagenet' or None).\n        in_channels (int): Number of input channels.\n        classes (int): Number of output classes.\n        activation (str): Activation function for output layer.\n        **kwargs: Additional arguments passed to smp.create_model().\n\n    Returns:\n        torch.nn.Module: Segmentation model.\n\n    Note:\n        This function uses smp.create_model() which supports all architectures available in\n        segmentation-models-pytorch, making it future-proof for new model additions.\n    \"\"\"\n    if not SMP_AVAILABLE:\n        raise ImportError(\n            \"segmentation-models-pytorch is not installed. \"\n            \"Please install it with: pip install segmentation-models-pytorch\"\n        )\n\n    try:\n        # Use the generic create_model function - supports all SMP architectures\n        model = smp.create_model(\n            arch=architecture,  # Case insensitive\n            encoder_name=encoder_name,\n            encoder_weights=encoder_weights,\n            in_channels=in_channels,\n            classes=classes,\n            **kwargs,\n        )\n\n        # Apply activation if specified (note: activation is handled differently in create_model)\n        if activation is not None:\n            import warnings\n\n            warnings.warn(\n                \"The 'activation' parameter is deprecated when using smp.create_model(). \"\n                \"Apply activation manually after model creation if needed.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        return model\n\n    except Exception as e:\n        # Provide helpful error message\n        available_archs = []\n        try:\n            # Try to get available architectures from smp\n            if hasattr(smp, \"get_available_models\"):\n                available_archs = smp.get_available_models()\n            else:\n                available_archs = [\n                    \"unet\",\n                    \"unetplusplus\",\n                    \"manet\",\n                    \"linknet\",\n                    \"fpn\",\n                    \"pspnet\",\n                    \"deeplabv3\",\n                    \"deeplabv3plus\",\n                    \"pan\",\n                    \"upernet\",\n                ]\n        except:\n            available_archs = [\n                \"unet\",\n                \"fpn\",\n                \"deeplabv3plus\",\n                \"pspnet\",\n                \"linknet\",\n                \"manet\",\n            ]\n\n        raise ValueError(\n            f\"Failed to create model with architecture '{architecture}' and encoder '{encoder_name}'. \"\n            f\"Error: {str(e)}. \"\n            f\"Available architectures include: {', '.join(available_archs)}. \"\n            f\"Please check the segmentation-models-pytorch documentation for supported combinations.\"\n        )\n</code></pre>"},{"location":"train/#geoai.train.get_transform","title":"<code>get_transform(train)</code>","text":"<p>Get transforms for data augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>train</code> <code>bool</code> <p>Whether to include training-specific transforms.</p> required <p>Returns:</p> Name Type Description <code>Compose</code> <code>Compose</code> <p>Composed transforms.</p> Source code in <code>geoai/train.py</code> <pre><code>def get_transform(train: bool) -&gt; torchvision.transforms.Compose:\n    \"\"\"\n    Get transforms for data augmentation.\n\n    Args:\n        train (bool): Whether to include training-specific transforms.\n\n    Returns:\n        Compose: Composed transforms.\n    \"\"\"\n    transforms = []\n    transforms.append(ToTensor())\n\n    if train:\n        transforms.append(RandomHorizontalFlip(0.5))\n\n    return Compose(transforms)\n</code></pre>"},{"location":"train/#geoai.train.inference_on_geotiff","title":"<code>inference_on_geotiff(model, geotiff_path, output_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, device=None, **kwargs)</code>","text":"<p>Perform inference on a large GeoTIFF using a sliding window approach with improved blending.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained model for inference.</p> required <code>geotiff_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask GeoTIFF.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1).</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from the input image.</p> <code>3</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[ndarray, ndarray]</code> <p>Tuple containing output path and inference time in seconds.</p> Source code in <code>geoai/train.py</code> <pre><code>def inference_on_geotiff(\n    model: torch.nn.Module,\n    geotiff_path: str,\n    output_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    device: Optional[torch.device] = None,\n    **kwargs: Any,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Perform inference on a large GeoTIFF using a sliding window approach with improved blending.\n\n    Args:\n        model (torch.nn.Module): Trained model for inference.\n        geotiff_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask GeoTIFF.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        confidence_threshold (float): Confidence threshold for predictions (0-1).\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels to use from the input image.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments.\n\n    Returns:\n        tuple: Tuple containing output path and inference time in seconds.\n    \"\"\"\n    if device is None:\n        device = get_device()\n\n    # Put model in evaluation mode\n    model.to(device)\n    model.eval()\n\n    # Open the GeoTIFF\n    with rasterio.open(geotiff_path) as src:\n        # Read metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Update metadata for output raster\n        out_meta = meta.copy()\n        out_meta.update(\n            {\"count\": 1, \"dtype\": \"uint8\"}  # Single band for mask  # Binary mask\n        )\n\n        # We'll use two arrays:\n        # 1. For accumulating predictions\n        pred_accumulator = np.zeros((height, width), dtype=np.float32)\n        # 2. For tracking how many predictions contribute to each pixel\n        count_accumulator = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate the number of windows needed to cover the entire image\n        steps_y = math.ceil((height - overlap) / (window_size - overlap))\n        steps_x = math.ceil((width - overlap) / (window_size - overlap))\n\n        # Ensure we cover the entire image\n        last_y = height - window_size\n        last_x = width - window_size\n\n        total_windows = steps_y * steps_x\n        print(\n            f\"Processing {total_windows} windows with size {window_size}x{window_size} and overlap {overlap}...\"\n        )\n\n        # Create progress bar\n        pbar = tqdm(total=total_windows)\n\n        # Process in batches\n        batch_inputs = []\n        batch_positions = []\n        batch_count = 0\n\n        start_time = time.time()\n\n        # Slide window over the image - make sure we cover the entire image\n        for i in range(steps_y + 1):  # +1 to ensure we reach the edge\n            y = min(i * (window_size - overlap), last_y)\n            y = max(0, y)  # Prevent negative indices\n\n            if y &gt; last_y and i &gt; 0:  # Skip if we've already covered the entire height\n                continue\n\n            for j in range(steps_x + 1):  # +1 to ensure we reach the edge\n                x = min(j * (window_size - overlap), last_x)\n                x = max(0, x)  # Prevent negative indices\n\n                if (\n                    x &gt; last_x and j &gt; 0\n                ):  # Skip if we've already covered the entire width\n                    continue\n\n                # Read window\n                window = src.read(window=Window(x, y, window_size, window_size))\n\n                # Check if window is valid\n                if window.shape[1] != window_size or window.shape[2] != window_size:\n                    # This can happen at image edges - adjust window size\n                    current_height = window.shape[1]\n                    current_width = window.shape[2]\n                    if current_height == 0 or current_width == 0:\n                        continue  # Skip empty windows\n                else:\n                    current_height = window_size\n                    current_width = window_size\n\n                # Normalize and prepare input\n                image = window.astype(np.float32) / 255.0\n\n                # Handle different number of bands\n                if image.shape[0] &gt; num_channels:\n                    image = image[:num_channels]\n                elif image.shape[0] &lt; num_channels:\n                    padded = np.zeros(\n                        (num_channels, current_height, current_width), dtype=np.float32\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n                # Convert to tensor\n                image_tensor = torch.tensor(image, device=device)\n\n                # Add to batch\n                batch_inputs.append(image_tensor)\n                batch_positions.append((y, x, current_height, current_width))\n                batch_count += 1\n\n                # Process batch when it reaches the batch size or at the end\n                if batch_count == batch_size or (i == steps_y and j == steps_x):\n                    # Forward pass\n                    with torch.no_grad():\n                        outputs = model(batch_inputs)\n\n                    # Process each output in the batch\n                    for idx, output in enumerate(outputs):\n                        y_pos, x_pos, h, w = batch_positions[idx]\n\n                        # Create weight matrix that gives higher weight to center pixels\n                        # This helps with smooth blending at boundaries\n                        y_grid, x_grid = np.mgrid[0:h, 0:w]\n\n                        # Calculate distance from each edge\n                        dist_from_left = x_grid\n                        dist_from_right = w - x_grid - 1\n                        dist_from_top = y_grid\n                        dist_from_bottom = h - y_grid - 1\n\n                        # Combine distances (minimum distance to any edge)\n                        edge_distance = np.minimum.reduce(\n                            [\n                                dist_from_left,\n                                dist_from_right,\n                                dist_from_top,\n                                dist_from_bottom,\n                            ]\n                        )\n\n                        # Convert to weight (higher weight for center pixels)\n                        # Normalize to [0, 1]\n                        edge_distance = np.minimum(edge_distance, overlap / 2)\n                        weight = edge_distance / (overlap / 2)\n\n                        # Get masks for predictions above threshold\n                        if len(output[\"scores\"]) &gt; 0:\n                            # Get all instances that meet confidence threshold\n                            keep = output[\"scores\"] &gt; confidence_threshold\n                            masks = output[\"masks\"][keep].squeeze(1)\n\n                            # Combine all instances into one mask\n                            if len(masks) &gt; 0:\n                                combined_mask = torch.max(masks, dim=0)[0] &gt; 0.5\n                                combined_mask = (\n                                    combined_mask.cpu().numpy().astype(np.float32)\n                                )\n\n                                # Apply weight to prediction\n                                weighted_pred = combined_mask * weight\n\n                                # Add to accumulators\n                                pred_accumulator[\n                                    y_pos : y_pos + h, x_pos : x_pos + w\n                                ] += weighted_pred\n                                count_accumulator[\n                                    y_pos : y_pos + h, x_pos : x_pos + w\n                                ] += weight\n\n                    # Reset batch\n                    batch_inputs = []\n                    batch_positions = []\n                    batch_count = 0\n\n                    # Update progress bar\n                    pbar.update(len(outputs))\n\n        # Close progress bar\n        pbar.close()\n\n        # Calculate final mask by dividing accumulated predictions by counts\n        # Handle division by zero\n        mask = np.zeros((height, width), dtype=np.uint8)\n        valid_pixels = count_accumulator &gt; 0\n        if np.any(valid_pixels):\n            # Average predictions where we have data\n            mask[valid_pixels] = (\n                pred_accumulator[valid_pixels] / count_accumulator[valid_pixels] &gt; 0.5\n            ).astype(np.uint8)\n\n        # Record time\n        inference_time = time.time() - start_time\n        print(f\"Inference completed in {inference_time:.2f} seconds\")\n\n        # Save output\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(mask, 1)\n\n        print(f\"Saved prediction to {output_path}\")\n\n        return output_path, inference_time\n</code></pre>"},{"location":"train/#geoai.train.instance_segmentation","title":"<code>instance_segmentation(input_path, output_path, model_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, num_classes=2, device=None, **kwargs)</code>","text":"<p>Perform instance segmentation on a GeoTIFF using a pre-trained Mask R-CNN model.</p> <p>This is a wrapper function for object_detection with clearer naming.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask GeoTIFF.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference. Defaults to 512.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows. Defaults to 256.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1). Defaults to 0.5.</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model. Defaults to 3.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes (including background). Defaults to 2.</p> <code>2</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to object_detection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Output mask is saved to output_path.</p> Source code in <code>geoai/train.py</code> <pre><code>def instance_segmentation(\n    input_path: str,\n    output_path: str,\n    model_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    num_classes: int = 2,\n    device: Optional[torch.device] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform instance segmentation on a GeoTIFF using a pre-trained Mask R-CNN model.\n\n    This is a wrapper function for object_detection with clearer naming.\n\n    Args:\n        input_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask GeoTIFF.\n        model_path (str): Path to trained model weights.\n        window_size (int): Size of sliding window for inference. Defaults to 512.\n        overlap (int): Overlap between adjacent windows. Defaults to 256.\n        confidence_threshold (float): Confidence threshold for predictions (0-1). Defaults to 0.5.\n        batch_size (int): Batch size for inference. Defaults to 4.\n        num_channels (int): Number of channels in the input image and model. Defaults to 3.\n        num_classes (int): Number of classes (including background). Defaults to 2.\n        device (torch.device): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments passed to object_detection.\n\n    Returns:\n        None: Output mask is saved to output_path.\n    \"\"\"\n    # Create model with the specified number of classes\n    model = get_instance_segmentation_model(\n        num_classes=num_classes, num_channels=num_channels, pretrained=True\n    )\n\n    # Load the trained model\n    if device is None:\n        device = get_device()\n\n    # Load state dict and handle DataParallel module prefix\n    state_dict = torch.load(model_path, map_location=device)\n\n    # Remove 'module.' prefix if present (from DataParallel training)\n    if any(key.startswith(\"module.\") for key in state_dict.keys()):\n        state_dict = {\n            key.replace(\"module.\", \"\"): value for key, value in state_dict.items()\n        }\n\n    model.load_state_dict(state_dict)\n    model.to(device)\n\n    # Use the proper instance segmentation inference function\n    return instance_segmentation_inference_on_geotiff(\n        model=model,\n        geotiff_path=input_path,\n        output_path=output_path,\n        window_size=window_size,\n        overlap=overlap,\n        confidence_threshold=confidence_threshold,\n        batch_size=batch_size,\n        num_channels=num_channels,\n        device=device,\n        **kwargs,\n    )\n</code></pre>"},{"location":"train/#geoai.train.instance_segmentation_batch","title":"<code>instance_segmentation_batch(input_dir, output_dir, model_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, num_classes=2, device=None, **kwargs)</code>","text":"<p>Perform instance segmentation on multiple GeoTIFF files using a pre-trained Mask R-CNN model.</p> <p>This is a wrapper function for object_detection_batch with clearer naming.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input GeoTIFF files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output mask GeoTIFF files.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference. Defaults to 512.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows. Defaults to 256.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1). Defaults to 0.5.</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model. Defaults to 3.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes (including background). Defaults to 2.</p> <code>2</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to object_detection_batch.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Output masks are saved to output_dir.</p> Source code in <code>geoai/train.py</code> <pre><code>def instance_segmentation_batch(\n    input_dir: str,\n    output_dir: str,\n    model_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    num_classes: int = 2,\n    device: Optional[torch.device] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform instance segmentation on multiple GeoTIFF files using a pre-trained Mask R-CNN model.\n\n    This is a wrapper function for object_detection_batch with clearer naming.\n\n    Args:\n        input_dir (str): Directory containing input GeoTIFF files.\n        output_dir (str): Directory to save output mask GeoTIFF files.\n        model_path (str): Path to trained model weights.\n        window_size (int): Size of sliding window for inference. Defaults to 512.\n        overlap (int): Overlap between adjacent windows. Defaults to 256.\n        confidence_threshold (float): Confidence threshold for predictions (0-1). Defaults to 0.5.\n        batch_size (int): Batch size for inference. Defaults to 4.\n        num_channels (int): Number of channels in the input image and model. Defaults to 3.\n        num_classes (int): Number of classes (including background). Defaults to 2.\n        device (torch.device): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments passed to object_detection_batch.\n\n    Returns:\n        None: Output masks are saved to output_dir.\n    \"\"\"\n    # Create model with the specified number of classes\n    model = get_instance_segmentation_model(\n        num_classes=num_classes, num_channels=num_channels, pretrained=True\n    )\n\n    # Load the trained model\n    if device is None:\n        device = get_device()\n\n    # Load state dict and handle DataParallel module prefix\n    state_dict = torch.load(model_path, map_location=device)\n\n    # Remove 'module.' prefix if present (from DataParallel training)\n    if any(key.startswith(\"module.\") for key in state_dict.keys()):\n        state_dict = {\n            key.replace(\"module.\", \"\"): value for key, value in state_dict.items()\n        }\n\n    model.load_state_dict(state_dict)\n    model.to(device)\n\n    # Process all GeoTIFF files in the input directory\n    import glob\n\n    input_files = glob.glob(os.path.join(input_dir, \"*.tif\")) + glob.glob(\n        os.path.join(input_dir, \"*.tiff\")\n    )\n\n    if not input_files:\n        print(f\"No GeoTIFF files found in {input_dir}\")\n        return\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(f\"Processing {len(input_files)} files...\")\n\n    for input_file in input_files:\n        try:\n            # Generate output filename\n            base_name = os.path.splitext(os.path.basename(input_file))[0]\n            output_file = os.path.join(output_dir, f\"{base_name}_instances.tif\")\n\n            print(f\"Processing {input_file}...\")\n\n            # Run instance segmentation inference\n            instance_segmentation_inference_on_geotiff(\n                model=model,\n                geotiff_path=input_file,\n                output_path=output_file,\n                window_size=window_size,\n                overlap=overlap,\n                confidence_threshold=confidence_threshold,\n                batch_size=batch_size,\n                num_channels=num_channels,\n                device=device,\n                **kwargs,\n            )\n\n            print(f\"Saved result to {output_file}\")\n\n        except Exception as e:\n            print(f\"Error processing {input_file}: {str(e)}\")\n            continue\n\n    print(f\"Batch processing completed. Results saved to {output_dir}\")\n</code></pre>"},{"location":"train/#geoai.train.instance_segmentation_inference_on_geotiff","title":"<code>instance_segmentation_inference_on_geotiff(model, geotiff_path, output_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, device=None, **kwargs)</code>","text":"<p>Perform instance segmentation inference on a large GeoTIFF using a sliding window approach.</p> <p>This function collects all detections first, then applies non-maximum suppression to handle overlapping detections from different windows, preventing artifacts.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained model for inference.</p> required <code>geotiff_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output instance mask GeoTIFF.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1).</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from the input image.</p> <code>3</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, float]</code> <p>Tuple containing output path and inference time in seconds.</p> Source code in <code>geoai/train.py</code> <pre><code>def instance_segmentation_inference_on_geotiff(\n    model: torch.nn.Module,\n    geotiff_path: str,\n    output_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    device: Optional[torch.device] = None,\n    **kwargs: Any,\n) -&gt; Tuple[str, float]:\n    \"\"\"\n    Perform instance segmentation inference on a large GeoTIFF using a sliding window approach.\n\n    This function collects all detections first, then applies non-maximum suppression\n    to handle overlapping detections from different windows, preventing artifacts.\n\n    Args:\n        model (torch.nn.Module): Trained model for inference.\n        geotiff_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output instance mask GeoTIFF.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        confidence_threshold (float): Confidence threshold for predictions (0-1).\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels to use from the input image.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments.\n\n    Returns:\n        tuple: Tuple containing output path and inference time in seconds.\n    \"\"\"\n    if device is None:\n        device = get_device()\n\n    # Put model in evaluation mode\n    model.to(device)\n    model.eval()\n\n    # Open the GeoTIFF\n    with rasterio.open(geotiff_path) as src:\n        # Read metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Update metadata for output raster\n        out_meta = meta.copy()\n        out_meta.update(\n            {\"count\": 1, \"dtype\": \"uint16\"}  # uint16 to support many instances\n        )\n\n        # Store all detections globally for NMS\n        all_detections = []\n\n        # Calculate the number of windows needed to cover the entire image\n        steps_y = math.ceil((height - overlap) / (window_size - overlap))\n        steps_x = math.ceil((width - overlap) / (window_size - overlap))\n\n        # Ensure we cover the entire image\n        last_y = height - window_size\n        last_x = width - window_size\n\n        total_windows = steps_y * steps_x\n        print(\n            f\"Processing {total_windows} windows with size {window_size}x{window_size} and overlap {overlap}...\"\n        )\n\n        # Create progress bar\n        pbar = tqdm(total=total_windows)\n\n        # Process in batches\n        batch_inputs = []\n        batch_positions = []\n        batch_count = 0\n\n        start_time = time.time()\n\n        # Slide window over the image\n        for i in range(steps_y + 1):  # +1 to ensure we reach the edge\n            y = min(i * (window_size - overlap), last_y)\n            y = max(0, y)  # Prevent negative indices\n\n            if y &gt; last_y and i &gt; 0:  # Skip if we've already covered the entire height\n                continue\n\n            for j in range(steps_x + 1):  # +1 to ensure we reach the edge\n                x = min(j * (window_size - overlap), last_x)\n                x = max(0, x)  # Prevent negative indices\n\n                if (\n                    x &gt; last_x and j &gt; 0\n                ):  # Skip if we've already covered the entire width\n                    continue\n\n                # Read window\n                window = src.read(window=Window(x, y, window_size, window_size))\n\n                # Check if window is valid\n                if window.shape[1] == 0 or window.shape[2] == 0:\n                    continue\n\n                # Handle edge cases where window might be smaller than expected\n                actual_height, actual_width = window.shape[1], window.shape[2]\n\n                # Convert to [C, H, W] format and normalize\n                image = window.astype(np.float32) / 255.0\n\n                # Handle different number of channels\n                if image.shape[0] &gt; num_channels:\n                    image = image[:num_channels]\n                elif image.shape[0] &lt; num_channels:\n                    # Pad with zeros if less than expected channels\n                    padded = np.zeros(\n                        (num_channels, image.shape[1], image.shape[2]), dtype=np.float32\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n                # Convert to tensor\n                image_tensor = torch.tensor(image, device=device)\n\n                # Add to batch\n                batch_inputs.append(image_tensor)\n                batch_positions.append((y, x, actual_height, actual_width))\n                batch_count += 1\n\n                # Process batch when it reaches the batch size or at the end\n                if batch_count == batch_size or (i == steps_y and j == steps_x):\n                    # Forward pass\n                    with torch.no_grad():\n                        outputs = model(batch_inputs)\n\n                    # Process each output in the batch\n                    for idx, output in enumerate(outputs):\n                        y_pos, x_pos, h, w = batch_positions[idx]\n\n                        # Process each detected instance\n                        if len(output[\"scores\"]) &gt; 0:\n                            # Get instances that meet confidence threshold\n                            keep = output[\"scores\"] &gt; confidence_threshold\n                            masks = output[\"masks\"][keep].squeeze(1)\n                            scores = output[\"scores\"][keep]\n                            boxes = output[\"boxes\"][keep]\n\n                            # Convert to global coordinates and store\n                            for k in range(len(masks)):\n                                mask = masks[k].cpu().numpy() &gt; 0.5\n                                score = scores[k].cpu().item()\n                                box = boxes[k].cpu().numpy()\n\n                                # Convert box to global coordinates\n                                global_box = [\n                                    box[0] + x_pos,\n                                    box[1] + y_pos,\n                                    box[2] + x_pos,\n                                    box[3] + y_pos,\n                                ]\n\n                                # Create global mask\n                                global_mask = np.zeros((height, width), dtype=bool)\n                                global_mask[y_pos : y_pos + h, x_pos : x_pos + w] = mask\n\n                                all_detections.append(\n                                    {\n                                        \"mask\": global_mask,\n                                        \"score\": score,\n                                        \"box\": global_box,\n                                    }\n                                )\n\n                    # Reset batch\n                    batch_inputs = []\n                    batch_positions = []\n                    batch_count = 0\n\n                    # Update progress bar\n                    pbar.update(len(outputs))\n\n        # Close progress bar\n        pbar.close()\n\n        print(f\"Collected {len(all_detections)} detections before NMS\")\n\n        # Apply Non-Maximum Suppression to handle overlapping detections\n        if len(all_detections) &gt; 0:\n            # Convert to tensors for NMS\n            boxes = torch.tensor(\n                [det[\"box\"] for det in all_detections], dtype=torch.float32\n            )\n            scores = torch.tensor(\n                [det[\"score\"] for det in all_detections], dtype=torch.float32\n            )\n\n            # Apply NMS with IoU threshold\n            nms_threshold = 0.3  # IoU threshold for NMS\n            keep_indices = torchvision.ops.nms(boxes, scores, nms_threshold)\n\n            # Keep only the selected detections\n            final_detections = [all_detections[i] for i in keep_indices]\n            print(f\"After NMS: {len(final_detections)} detections\")\n\n            # Create final instance mask\n            instance_mask = np.zeros((height, width), dtype=np.uint16)\n\n            # Sort by score (highest first) for consistent ordering\n            final_detections.sort(key=lambda x: x[\"score\"], reverse=True)\n\n            # Assign unique IDs to each detection\n            for instance_id, detection in enumerate(final_detections, 1):\n                mask = detection[\"mask\"]\n                # Only assign to pixels that are not already assigned\n                available_pixels = (instance_mask == 0) &amp; mask\n                instance_mask[available_pixels] = instance_id\n        else:\n            # No detections found\n            instance_mask = np.zeros((height, width), dtype=np.uint16)\n\n        # Record time\n        inference_time = time.time() - start_time\n        print(f\"Instance segmentation completed in {inference_time:.2f} seconds\")\n        print(\n            f\"Final instances: {len(final_detections) if len(all_detections) &gt; 0 else 0}\"\n        )\n\n        # Save output\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(instance_mask, 1)\n\n        print(f\"Saved instance segmentation to {output_path}\")\n\n        return output_path, inference_time\n</code></pre>"},{"location":"train/#geoai.train.iou_coefficient","title":"<code>iou_coefficient(pred, target, smooth=1e-06, num_classes=None)</code>","text":"<p>Calculate IoU coefficient for segmentation (binary or multi-class).</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask with shape [H, W].</p> required <code>smooth</code> <code>float</code> <p>Smoothing factor to avoid division by zero.</p> <code>1e-06</code> <code>num_classes</code> <code>int</code> <p>Number of classes. If None, auto-detected.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Mean IoU coefficient across all classes.</p> Source code in <code>geoai/train.py</code> <pre><code>def iou_coefficient(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    smooth: float = 1e-6,\n    num_classes: Optional[int] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate IoU coefficient for segmentation (binary or multi-class).\n\n    Args:\n        pred (torch.Tensor): Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].\n        target (torch.Tensor): Ground truth mask with shape [H, W].\n        smooth (float): Smoothing factor to avoid division by zero.\n        num_classes (int, optional): Number of classes. If None, auto-detected.\n\n    Returns:\n        float: Mean IoU coefficient across all classes.\n    \"\"\"\n    # Convert predictions to class predictions\n    if pred.dim() == 3:  # [C, H, W] format\n        pred = torch.softmax(pred, dim=0)\n        pred_classes = torch.argmax(pred, dim=0)\n    elif pred.dim() == 2:  # [H, W] format\n        pred_classes = pred\n    else:\n        raise ValueError(f\"Unexpected prediction dimensions: {pred.shape}\")\n\n    # Auto-detect number of classes if not provided\n    if num_classes is None:\n        num_classes = max(pred_classes.max().item(), target.max().item()) + 1\n\n    # Calculate IoU for each class and average\n    iou_scores = []\n    for class_id in range(num_classes):\n        pred_class = (pred_classes == class_id).float()\n        target_class = (target == class_id).float()\n\n        intersection = (pred_class * target_class).sum()\n        union = pred_class.sum() + target_class.sum() - intersection\n\n        if union &gt; 0:\n            iou = (intersection + smooth) / (union + smooth)\n            iou_scores.append(iou.item())\n\n    return sum(iou_scores) / len(iou_scores) if iou_scores else 0.0\n</code></pre>"},{"location":"train/#geoai.train.lightly_embed_images","title":"<code>lightly_embed_images(data_dir, model_path, output_path, model_architecture=None, batch_size=64, **kwargs)</code>","text":"<p>Generate embeddings for images using a Lightly Train pretrained model.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory containing images to embed.</p> required <code>model_path</code> <code>str</code> <p>Path to the pretrained model checkpoint file (.ckpt).</p> required <code>output_path</code> <code>str</code> <p>Path to save the embeddings (as .pt file).</p> required <code>model_architecture</code> <code>str</code> <p>Architecture of the pretrained model (deprecated, kept for backwards compatibility but not used). The model architecture is automatically loaded from the checkpoint.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for embedding generation. Default is 64.</p> <code>64</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to lightly_train.embed(). Supported kwargs include: image_size, num_workers, accelerator, etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved embeddings file.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If lightly-train is not installed.</p> <code>FileNotFoundError</code> <p>If data_dir or model_path does not exist.</p> Note <p>The model_path should point to a .ckpt file from the training output, typically located at: output_dir/checkpoints/last.ckpt</p> Example <p>embeddings_path = lightly_embed_images( ...     data_dir=\"path/to/images\", ...     model_path=\"output_dir/checkpoints/last.ckpt\", ...     output_path=\"embeddings.pt\", ...     batch_size=32 ... ) print(f\"Embeddings saved to: {embeddings_path}\")</p> Source code in <code>geoai/train.py</code> <pre><code>def lightly_embed_images(\n    data_dir: str,\n    model_path: str,\n    output_path: str,\n    model_architecture: str = None,  # Deprecated, kept for backwards compatibility\n    batch_size: int = 64,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Generate embeddings for images using a Lightly Train pretrained model.\n\n    Args:\n        data_dir (str): Directory containing images to embed.\n        model_path (str): Path to the pretrained model checkpoint file (.ckpt).\n        output_path (str): Path to save the embeddings (as .pt file).\n        model_architecture (str): Architecture of the pretrained model (deprecated,\n            kept for backwards compatibility but not used). The model architecture\n            is automatically loaded from the checkpoint.\n        batch_size (int): Batch size for embedding generation. Default is 64.\n        **kwargs: Additional arguments passed to lightly_train.embed().\n            Supported kwargs include: image_size, num_workers, accelerator, etc.\n\n    Returns:\n        str: Path to the saved embeddings file.\n\n    Raises:\n        ImportError: If lightly-train is not installed.\n        FileNotFoundError: If data_dir or model_path does not exist.\n\n    Note:\n        The model_path should point to a .ckpt file from the training output,\n        typically located at: output_dir/checkpoints/last.ckpt\n\n    Example:\n        &gt;&gt;&gt; embeddings_path = lightly_embed_images(\n        ...     data_dir=\"path/to/images\",\n        ...     model_path=\"output_dir/checkpoints/last.ckpt\",\n        ...     output_path=\"embeddings.pt\",\n        ...     batch_size=32\n        ... )\n        &gt;&gt;&gt; print(f\"Embeddings saved to: {embeddings_path}\")\n    \"\"\"\n    if not LIGHTLY_TRAIN_AVAILABLE:\n        raise ImportError(\n            \"lightly-train is not installed. Please install it with: \"\n            \"pip install lightly-train\"\n        )\n\n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"Data directory does not exist: {data_dir}\")\n\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file does not exist: {model_path}\")\n\n    print(f\"Generating embeddings for images in: {data_dir}\")\n    print(f\"Using pretrained model: {model_path}\")\n\n    output_dir = os.path.dirname(output_path)\n    if output_dir:\n        os.makedirs(output_dir, exist_ok=True)\n\n    # Generate embeddings using Lightly Train\n    # Note: model_architecture is not used - it's inferred from the checkpoint\n    lightly_train.embed(\n        out=output_path,\n        data=data_dir,\n        checkpoint=model_path,\n        batch_size=batch_size,\n        **kwargs,\n    )\n\n    print(f\"Embeddings saved to: {output_path}\")\n    return output_path\n</code></pre>"},{"location":"train/#geoai.train.lightly_train_model","title":"<code>lightly_train_model(data_dir, output_dir, model='torchvision/resnet50', method='dinov2_distillation', epochs=100, batch_size=64, learning_rate=0.0001, **kwargs)</code>","text":"<p>Train a model using Lightly Train for self-supervised pretraining.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Directory containing unlabeled images for training.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save training outputs and model checkpoints.</p> required <code>model</code> <code>str</code> <p>Model architecture to train. Supports models from torchvision, timm, ultralytics, etc. Default is \"torchvision/resnet50\".</p> <code>'torchvision/resnet50'</code> <code>method</code> <code>str</code> <p>Self-supervised learning method. Options include: - \"simclr\": Works with CNN models (ResNet, EfficientNet, etc.) - \"dino\": Works with both CNNs and ViTs - \"dinov2\": Requires ViT models only - \"dinov2_distillation\": Requires ViT models only (recommended for ViTs) Default is \"dinov2_distillation\".</p> <code>'dinov2_distillation'</code> <code>epochs</code> <code>int</code> <p>Number of training epochs. Default is 100.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Default is 64.</p> <code>64</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training. Default is 1e-4.</p> <code>0.0001</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to lightly_train.train().</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the exported model file.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If lightly-train is not installed.</p> <code>ValueError</code> <p>If data_dir does not exist, is empty, or incompatible model/method.</p> Note <p>Model/Method compatibility: - CNN models (ResNet, EfficientNet): Use \"simclr\" or \"dino\" - ViT models: Use \"dinov2\", \"dinov2_distillation\", or \"dino\"</p> Example Source code in <code>geoai/train.py</code> <pre><code>def lightly_train_model(\n    data_dir: str,\n    output_dir: str,\n    model: str = \"torchvision/resnet50\",\n    method: str = \"dinov2_distillation\",\n    epochs: int = 100,\n    batch_size: int = 64,\n    learning_rate: float = 1e-4,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"\n    Train a model using Lightly Train for self-supervised pretraining.\n\n    Args:\n        data_dir (str): Directory containing unlabeled images for training.\n        output_dir (str): Directory to save training outputs and model checkpoints.\n        model (str): Model architecture to train. Supports models from torchvision,\n            timm, ultralytics, etc. Default is \"torchvision/resnet50\".\n        method (str): Self-supervised learning method. Options include:\n            - \"simclr\": Works with CNN models (ResNet, EfficientNet, etc.)\n            - \"dino\": Works with both CNNs and ViTs\n            - \"dinov2\": Requires ViT models only\n            - \"dinov2_distillation\": Requires ViT models only (recommended for ViTs)\n            Default is \"dinov2_distillation\".\n        epochs (int): Number of training epochs. Default is 100.\n        batch_size (int): Batch size for training. Default is 64.\n        learning_rate (float): Learning rate for training. Default is 1e-4.\n        **kwargs: Additional arguments passed to lightly_train.train().\n\n    Returns:\n        str: Path to the exported model file.\n\n    Raises:\n        ImportError: If lightly-train is not installed.\n        ValueError: If data_dir does not exist, is empty, or incompatible model/method.\n\n    Note:\n        Model/Method compatibility:\n        - CNN models (ResNet, EfficientNet): Use \"simclr\" or \"dino\"\n        - ViT models: Use \"dinov2\", \"dinov2_distillation\", or \"dino\"\n\n    Example:\n        &gt;&gt;&gt; # For CNN models (ResNet, EfficientNet)\n        &gt;&gt;&gt; model_path = lightly_train_model(\n        ...     data_dir=\"path/to/unlabeled/images\",\n        ...     output_dir=\"path/to/output\",\n        ...     model=\"torchvision/resnet50\",\n        ...     method=\"simclr\",  # Use simclr for CNNs\n        ...     epochs=50\n        ... )\n        &gt;&gt;&gt; # For ViT models\n        &gt;&gt;&gt; model_path = lightly_train_model(\n        ...     data_dir=\"path/to/unlabeled/images\",\n        ...     output_dir=\"path/to/output\",\n        ...     model=\"timm/vit_base_patch16_224\",\n        ...     method=\"dinov2\",  # dinov2 requires ViT\n        ...     epochs=50\n        ... )\n    \"\"\"\n    if not LIGHTLY_TRAIN_AVAILABLE:\n        raise ImportError(\n            \"lightly-train is not installed. Please install it with: \"\n            \"pip install lightly-train\"\n        )\n\n    if not os.path.exists(data_dir):\n        raise ValueError(f\"Data directory does not exist: {data_dir}\")\n\n    # Check if data directory contains images\n    image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.tif\", \"*.tiff\", \"*.bmp\"]\n    image_files = []\n    for ext in image_extensions:\n        image_files.extend(glob.glob(os.path.join(data_dir, \"**\", ext), recursive=True))\n\n    if not image_files:\n        raise ValueError(f\"No image files found in {data_dir}\")\n\n    # Validate model/method compatibility\n    is_vit_model = \"vit\" in model.lower() or \"vision_transformer\" in model.lower()\n\n    if method in [\"dinov2\", \"dinov2_distillation\"] and not is_vit_model:\n        raise ValueError(\n            f\"Method '{method}' requires a Vision Transformer (ViT) model, but got '{model}'.\\n\"\n            f\"Solutions:\\n\"\n            f\"  1. Use a ViT model: model='timm/vit_base_patch16_224'\\n\"\n            f\"  2. Use a CNN-compatible method: method='simclr' or method='dino'\\n\"\n            f\"\\nFor CNN models (ResNet, EfficientNet), use 'simclr' or 'dino'.\\n\"\n            f\"For ViT models, use 'dinov2', 'dinov2_distillation', or 'dino'.\"\n        )\n\n    print(f\"Found {len(image_files)} images in {data_dir}\")\n    print(f\"Starting self-supervised pretraining with {method} method...\")\n    print(f\"Model: {model}\")\n\n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Detect if running in notebook environment and set appropriate configuration\n    def is_notebook():\n        try:\n            from IPython import get_ipython\n\n            if get_ipython() is not None:\n                return True\n        except (ImportError, NameError):\n            pass\n        return False\n\n    # Force single-device training in notebooks to avoid DDP strategy issues\n    if is_notebook():\n        # Only override if not explicitly set by user\n        if \"accelerator\" not in kwargs:\n            # Use CPU in notebooks to avoid DDP incompatibility\n            # Users can still override by passing accelerator='gpu'\n            kwargs[\"accelerator\"] = \"cpu\"\n        if \"devices\" not in kwargs:\n            kwargs[\"devices\"] = 1  # Force single device\n\n    # Train the model using Lightly Train\n    lightly_train.train(\n        out=output_dir,\n        data=data_dir,\n        model=model,\n        method=method,\n        epochs=epochs,\n        batch_size=batch_size,\n        **kwargs,\n    )\n\n    # Return path to the exported model\n    exported_model_path = os.path.join(\n        output_dir, \"exported_models\", \"exported_last.pt\"\n    )\n\n    if os.path.exists(exported_model_path):\n        print(\n            f\"Model training completed. Exported model saved to: {exported_model_path}\"\n        )\n        return exported_model_path\n    else:\n        # Check for alternative export paths\n        possible_paths = [\n            os.path.join(output_dir, \"exported_models\", \"exported_best.pt\"),\n            os.path.join(output_dir, \"checkpoints\", \"last.ckpt\"),\n        ]\n\n        for path in possible_paths:\n            if os.path.exists(path):\n                print(f\"Model training completed. Exported model saved to: {path}\")\n                return path\n\n        print(f\"Model training completed. Output saved to: {output_dir}\")\n        return output_dir\n</code></pre>"},{"location":"train/#geoai.train.lightly_train_model--for-cnn-models-resnet-efficientnet","title":"For CNN models (ResNet, EfficientNet)","text":"<p>model_path = lightly_train_model( ...     data_dir=\"path/to/unlabeled/images\", ...     output_dir=\"path/to/output\", ...     model=\"torchvision/resnet50\", ...     method=\"simclr\",  # Use simclr for CNNs ...     epochs=50 ... )</p>"},{"location":"train/#geoai.train.lightly_train_model--for-vit-models","title":"For ViT models","text":"<p>model_path = lightly_train_model( ...     data_dir=\"path/to/unlabeled/images\", ...     output_dir=\"path/to/output\", ...     model=\"timm/vit_base_patch16_224\", ...     method=\"dinov2\",  # dinov2 requires ViT ...     epochs=50 ... )</p>"},{"location":"train/#geoai.train.load_lightly_pretrained_model","title":"<code>load_lightly_pretrained_model(model_path, model_architecture='torchvision/resnet50', device=None)</code>","text":"<p>Load a pretrained model from Lightly Train.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the pretrained model file (.pt format).</p> required <code>model_architecture</code> <code>str</code> <p>Architecture of the model to load. Default is \"torchvision/resnet50\".</p> <code>'torchvision/resnet50'</code> <code>device</code> <code>str</code> <p>Device to load the model on. If None, uses CPU.</p> <code>None</code> <p>Returns:</p> Type Description <code>Module</code> <p>torch.nn.Module: Loaded pretrained model ready for fine-tuning.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If model_path does not exist.</p> <code>ImportError</code> <p>If required libraries are not available.</p> Example <p>model = load_lightly_pretrained_model( ...     model_path=\"path/to/pretrained_model.pt\", ...     model_architecture=\"torchvision/resnet50\", ...     device=\"cuda\" ... )</p> Source code in <code>geoai/train.py</code> <pre><code>def load_lightly_pretrained_model(\n    model_path: str,\n    model_architecture: str = \"torchvision/resnet50\",\n    device: str = None,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Load a pretrained model from Lightly Train.\n\n    Args:\n        model_path (str): Path to the pretrained model file (.pt format).\n        model_architecture (str): Architecture of the model to load.\n            Default is \"torchvision/resnet50\".\n        device (str): Device to load the model on. If None, uses CPU.\n\n    Returns:\n        torch.nn.Module: Loaded pretrained model ready for fine-tuning.\n\n    Raises:\n        FileNotFoundError: If model_path does not exist.\n        ImportError: If required libraries are not available.\n\n    Example:\n        &gt;&gt;&gt; model = load_lightly_pretrained_model(\n        ...     model_path=\"path/to/pretrained_model.pt\",\n        ...     model_architecture=\"torchvision/resnet50\",\n        ...     device=\"cuda\"\n        ... )\n        &gt;&gt;&gt; # Fine-tune the model with your existing training pipeline\n    \"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    print(f\"Loading pretrained model from: {model_path}\")\n\n    # Load the model based on architecture\n    if model_architecture.startswith(\"torchvision/\"):\n        model_name = model_architecture.replace(\"torchvision/\", \"\")\n\n        # Import the model from torchvision\n        if hasattr(torchvision.models, model_name):\n            model = getattr(torchvision.models, model_name)()\n        else:\n            raise ValueError(f\"Unknown torchvision model: {model_name}\")\n\n    elif model_architecture.startswith(\"timm/\"):\n        try:\n            import timm\n\n            model_name = model_architecture.replace(\"timm/\", \"\")\n            model = timm.create_model(model_name)\n        except ImportError:\n            raise ImportError(\n                \"timm is required for TIMM models. Install with: pip install timm\"\n            )\n\n    else:\n        # For other architectures, try to import from torchvision as default\n        try:\n            model = getattr(torchvision.models, model_architecture)()\n        except AttributeError:\n            raise ValueError(f\"Unsupported model architecture: {model_architecture}\")\n\n    # Load the pretrained weights\n    try:\n        state_dict = torch.load(model_path, map_location=device, weights_only=True)\n    except TypeError:\n        # For backward compatibility with older PyTorch versions\n        state_dict = torch.load(model_path, map_location=device)\n    model.load_state_dict(state_dict)\n\n    print(f\"Successfully loaded pretrained model: {model_architecture}\")\n    return model\n</code></pre>"},{"location":"train/#geoai.train.load_lightly_pretrained_model--fine-tune-the-model-with-your-existing-training-pipeline","title":"Fine-tune the model with your existing training pipeline","text":""},{"location":"train/#geoai.train.object_detection","title":"<code>object_detection(input_path, output_path, model_path, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, num_channels=3, model=None, pretrained=True, device=None, **kwargs)</code>","text":"<p>Perform object detection on a GeoTIFF using a pre-trained Mask R-CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask GeoTIFF.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1).</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model.</p> <code>3</code> <code>model</code> <code>Module</code> <p>Predefined model. If None, a new model is created.</p> <code>None</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone for model loading.</p> <code>True</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to inference_on_geotiff.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Output mask is saved to output_path.</p> Source code in <code>geoai/train.py</code> <pre><code>def object_detection(\n    input_path: str,\n    output_path: str,\n    model_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    model: Optional[torch.nn.Module] = None,\n    pretrained: bool = True,\n    device: Optional[torch.device] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform object detection on a GeoTIFF using a pre-trained Mask R-CNN model.\n\n    Args:\n        input_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask GeoTIFF.\n        model_path (str): Path to trained model weights.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        confidence_threshold (float): Confidence threshold for predictions (0-1).\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels in the input image and model.\n        model (torch.nn.Module, optional): Predefined model. If None, a new model is created.\n        pretrained (bool): Whether to use pretrained backbone for model loading.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments passed to inference_on_geotiff.\n\n    Returns:\n        None: Output mask is saved to output_path.\n    \"\"\"\n    # Load your trained model\n    if device is None:\n        device = get_device()\n    if model is None:\n        model = get_instance_segmentation_model(\n            num_classes=2, num_channels=num_channels, pretrained=pretrained\n        )\n\n    if not os.path.exists(model_path):\n        try:\n            model_path = download_model_from_hf(model_path)\n        except Exception as e:\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    # Load state dict and handle DataParallel module prefix\n    state_dict = torch.load(model_path, map_location=device)\n\n    # Remove 'module.' prefix if present (from DataParallel training)\n    if any(key.startswith(\"module.\") for key in state_dict.keys()):\n        state_dict = {\n            key.replace(\"module.\", \"\"): value for key, value in state_dict.items()\n        }\n\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n\n    inference_on_geotiff(\n        model=model,\n        geotiff_path=input_path,\n        output_path=output_path,\n        window_size=window_size,  # Adjust based on your model and memory\n        overlap=overlap,  # Overlap to avoid edge artifacts\n        confidence_threshold=confidence_threshold,\n        batch_size=batch_size,  # Adjust based on your GPU memory\n        num_channels=num_channels,\n        device=device,\n        **kwargs,\n    )\n</code></pre>"},{"location":"train/#geoai.train.object_detection_batch","title":"<code>object_detection_batch(input_paths, output_dir, model_path, filenames=None, window_size=512, overlap=256, confidence_threshold=0.5, batch_size=4, model=None, num_channels=3, pretrained=True, device=None, **kwargs)</code>","text":"<p>Perform object detection on a GeoTIFF using a pre-trained Mask R-CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>str or list</code> <p>Path(s) to input GeoTIFF file(s). If a directory is provided, all .tif files in that directory will be processed.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output mask GeoTIFF files.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>filenames</code> <code>list</code> <p>List of output filenames. If None, defaults to \"_mask.tif\" for each input file. If provided, must match the number of input files. <code>None</code> <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>confidence_threshold</code> <code>float</code> <p>Confidence threshold for predictions (0-1).</p> <code>0.5</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model.</p> <code>3</code> <code>model</code> <code>Module</code> <p>Predefined model. If None, a new model is created.</p> <code>None</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone for model loading.</p> <code>True</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to inference_on_geotiff.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Output mask is saved to output_path.</p> Source code in <code>geoai/train.py</code> <pre><code>def object_detection_batch(\n    input_paths: Union[str, List[str]],\n    output_dir: str,\n    model_path: str,\n    filenames: Optional[List[str]] = None,\n    window_size: int = 512,\n    overlap: int = 256,\n    confidence_threshold: float = 0.5,\n    batch_size: int = 4,\n    model: Optional[torch.nn.Module] = None,\n    num_channels: int = 3,\n    pretrained: bool = True,\n    device: Optional[torch.device] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform object detection on a GeoTIFF using a pre-trained Mask R-CNN model.\n\n    Args:\n        input_paths (str or list): Path(s) to input GeoTIFF file(s). If a directory is provided,\n            all .tif files in that directory will be processed.\n        output_dir (str): Directory to save output mask GeoTIFF files.\n        model_path (str): Path to trained model weights.\n        filenames (list, optional): List of output filenames. If None, defaults to\n            \"&lt;input_filename&gt;_mask.tif\" for each input file.\n            If provided, must match the number of input files.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        confidence_threshold (float): Confidence threshold for predictions (0-1).\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels in the input image and model.\n        model (torch.nn.Module, optional): Predefined model. If None, a new model is created.\n        pretrained (bool): Whether to use pretrained backbone for model loading.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        **kwargs: Additional arguments passed to inference_on_geotiff.\n\n    Returns:\n        None: Output mask is saved to output_path.\n    \"\"\"\n    # Load your trained model\n    if device is None:\n        device = get_device()\n\n    if model is None:\n        model = get_instance_segmentation_model(\n            num_classes=2, num_channels=num_channels, pretrained=pretrained\n        )\n\n    if not os.path.exists(output_dir):\n        os.makedirs(os.path.abspath(output_dir), exist_ok=True)\n\n    if not os.path.exists(model_path):\n        try:\n            model_path = download_model_from_hf(model_path)\n        except Exception as e:\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    # Load state dict and handle DataParallel module prefix\n    state_dict = torch.load(model_path, map_location=device)\n\n    # Remove 'module.' prefix if present (from DataParallel training)\n    if any(key.startswith(\"module.\") for key in state_dict.keys()):\n        state_dict = {\n            key.replace(\"module.\", \"\"): value for key, value in state_dict.items()\n        }\n\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n\n    if isinstance(input_paths, str) and (not input_paths.endswith(\".tif\")):\n        files = glob.glob(os.path.join(input_paths, \"*.tif\"))\n        files.sort()\n    elif isinstance(input_paths, str):\n        files = [input_paths]\n\n    if filenames is None:\n        filenames = [\n            os.path.join(output_dir, os.path.basename(f).replace(\".tif\", \"_mask.tif\"))\n            for f in files\n        ]\n    else:\n        if len(filenames) != len(files):\n            raise ValueError(\"Number of filenames must match number of input files.\")\n\n    for index, file in enumerate(files):\n        print(f\"Processing file {index + 1}/{len(files)}: {file}\")\n        inference_on_geotiff(\n            model=model,\n            geotiff_path=file,\n            output_path=filenames[index],\n            window_size=window_size,  # Adjust based on your model and memory\n            overlap=overlap,  # Overlap to avoid edge artifacts\n            confidence_threshold=confidence_threshold,\n            batch_size=batch_size,  # Adjust based on your GPU memory\n            num_channels=num_channels,\n            device=device,\n            **kwargs,\n        )\n</code></pre>"},{"location":"train/#geoai.train.parse_coco_annotations","title":"<code>parse_coco_annotations(coco_json_path, images_dir, labels_dir)</code>","text":"<p>Parse COCO format annotations and return lists of image and label paths.</p> <p>Parameters:</p> Name Type Description Default <code>coco_json_path</code> <code>str</code> <p>Path to COCO annotations JSON file (instances.json).</p> required <code>images_dir</code> <code>str</code> <p>Directory containing image files.</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing label mask files.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>Tuple[List[str], List[str]]: Lists of image paths and corresponding label paths.</p> Source code in <code>geoai/train.py</code> <pre><code>def parse_coco_annotations(\n    coco_json_path: str, images_dir: str, labels_dir: str\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Parse COCO format annotations and return lists of image and label paths.\n\n    Args:\n        coco_json_path (str): Path to COCO annotations JSON file (instances.json).\n        images_dir (str): Directory containing image files.\n        labels_dir (str): Directory containing label mask files.\n\n    Returns:\n        Tuple[List[str], List[str]]: Lists of image paths and corresponding label paths.\n    \"\"\"\n    import json\n\n    with open(coco_json_path, \"r\") as f:\n        coco_data = json.load(f)\n\n    # Create mapping from image_id to filename\n    image_files = []\n    label_files = []\n\n    for img_info in coco_data[\"images\"]:\n        img_filename = img_info[\"file_name\"]\n        img_path = os.path.join(images_dir, img_filename)\n\n        # Derive label filename (same as image filename)\n        label_path = os.path.join(labels_dir, img_filename)\n\n        if os.path.exists(img_path) and os.path.exists(label_path):\n            image_files.append(img_path)\n            label_files.append(label_path)\n\n    return image_files, label_files\n</code></pre>"},{"location":"train/#geoai.train.parse_yolo_annotations","title":"<code>parse_yolo_annotations(data_dir, images_subdir='images', labels_subdir='labels')</code>","text":"<p>Parse YOLO format annotations and return lists of image and label paths.</p> <p>YOLO format structure: - data_dir/images/: Contains image files (.tif, .png, .jpg) - data_dir/labels/: Contains label masks (.tif, .png) and YOLO .txt files - data_dir/classes.txt: Class names (one per line)</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Root directory containing YOLO-format data.</p> required <code>images_subdir</code> <code>str</code> <p>Subdirectory name for images. Defaults to 'images'.</p> <code>'images'</code> <code>labels_subdir</code> <code>str</code> <p>Subdirectory name for labels. Defaults to 'labels'.</p> <code>'labels'</code> <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>Tuple[List[str], List[str]]: Lists of image paths and corresponding label paths.</p> Source code in <code>geoai/train.py</code> <pre><code>def parse_yolo_annotations(\n    data_dir: str, images_subdir: str = \"images\", labels_subdir: str = \"labels\"\n) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"\n    Parse YOLO format annotations and return lists of image and label paths.\n\n    YOLO format structure:\n    - data_dir/images/: Contains image files (.tif, .png, .jpg)\n    - data_dir/labels/: Contains label masks (.tif, .png) and YOLO .txt files\n    - data_dir/classes.txt: Class names (one per line)\n\n    Args:\n        data_dir (str): Root directory containing YOLO-format data.\n        images_subdir (str): Subdirectory name for images. Defaults to 'images'.\n        labels_subdir (str): Subdirectory name for labels. Defaults to 'labels'.\n\n    Returns:\n        Tuple[List[str], List[str]]: Lists of image paths and corresponding label paths.\n    \"\"\"\n    images_dir = os.path.join(data_dir, images_subdir)\n    labels_dir = os.path.join(data_dir, labels_subdir)\n\n    if not os.path.exists(images_dir):\n        raise FileNotFoundError(f\"Images directory not found: {images_dir}\")\n    if not os.path.exists(labels_dir):\n        raise FileNotFoundError(f\"Labels directory not found: {labels_dir}\")\n\n    # Get all image files\n    image_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n    image_files = []\n    label_files = []\n\n    for img_file in os.listdir(images_dir):\n        if img_file.lower().endswith(image_extensions):\n            img_path = os.path.join(images_dir, img_file)\n\n            # Find corresponding label mask (same filename)\n            label_path = os.path.join(labels_dir, img_file)\n\n            if os.path.exists(label_path):\n                image_files.append(img_path)\n                label_files.append(label_path)\n\n    return sorted(image_files), sorted(label_files)\n</code></pre>"},{"location":"train/#geoai.train.precision_score","title":"<code>precision_score(pred, target, smooth=1e-06, num_classes=None)</code>","text":"<p>Calculate precision score for segmentation (binary or multi-class).</p> <p>Precision = TP / (TP + FP), where: - TP (True Positives): Correctly predicted positive pixels - FP (False Positives): Incorrectly predicted positive pixels</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask with shape [H, W].</p> required <code>smooth</code> <code>float</code> <p>Smoothing factor to avoid division by zero.</p> <code>1e-06</code> <code>num_classes</code> <code>int</code> <p>Number of classes. If None, auto-detected.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Mean precision score across all classes.</p> Source code in <code>geoai/train.py</code> <pre><code>def precision_score(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    smooth: float = 1e-6,\n    num_classes: Optional[int] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate precision score for segmentation (binary or multi-class).\n\n    Precision = TP / (TP + FP), where:\n    - TP (True Positives): Correctly predicted positive pixels\n    - FP (False Positives): Incorrectly predicted positive pixels\n\n    Args:\n        pred (torch.Tensor): Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].\n        target (torch.Tensor): Ground truth mask with shape [H, W].\n        smooth (float): Smoothing factor to avoid division by zero.\n        num_classes (int, optional): Number of classes. If None, auto-detected.\n\n    Returns:\n        float: Mean precision score across all classes.\n    \"\"\"\n    # Convert predictions to class predictions\n    if pred.dim() == 3:  # [C, H, W] format\n        pred = torch.softmax(pred, dim=0)\n        pred_classes = torch.argmax(pred, dim=0)\n    elif pred.dim() == 2:  # [H, W] format\n        pred_classes = pred\n    else:\n        raise ValueError(f\"Unexpected prediction dimensions: {pred.shape}\")\n\n    # Auto-detect number of classes if not provided\n    if num_classes is None:\n        num_classes = max(pred_classes.max().item(), target.max().item()) + 1\n\n    # Calculate precision for each class and average\n    precision_scores = []\n    for class_id in range(num_classes):\n        pred_class = (pred_classes == class_id).float()\n        target_class = (target == class_id).float()\n\n        true_positives = (pred_class * target_class).sum()\n        predicted_positives = pred_class.sum()\n\n        if predicted_positives &gt; 0:\n            precision = (true_positives + smooth) / (predicted_positives + smooth)\n            precision_scores.append(precision.item())\n\n    return sum(precision_scores) / len(precision_scores) if precision_scores else 0.0\n</code></pre>"},{"location":"train/#geoai.train.recall_score","title":"<code>recall_score(pred, target, smooth=1e-06, num_classes=None)</code>","text":"<p>Calculate recall score (also known as sensitivity) for segmentation (binary or multi-class).</p> <p>Recall = TP / (TP + FN), where: - TP (True Positives): Correctly predicted positive pixels - FN (False Negatives): Incorrectly predicted negative pixels</p> <p>Parameters:</p> Name Type Description Default <code>pred</code> <code>Tensor</code> <p>Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].</p> required <code>target</code> <code>Tensor</code> <p>Ground truth mask with shape [H, W].</p> required <code>smooth</code> <code>float</code> <p>Smoothing factor to avoid division by zero.</p> <code>1e-06</code> <code>num_classes</code> <code>int</code> <p>Number of classes. If None, auto-detected.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Mean recall score across all classes.</p> Source code in <code>geoai/train.py</code> <pre><code>def recall_score(\n    pred: torch.Tensor,\n    target: torch.Tensor,\n    smooth: float = 1e-6,\n    num_classes: Optional[int] = None,\n) -&gt; float:\n    \"\"\"\n    Calculate recall score (also known as sensitivity) for segmentation (binary or multi-class).\n\n    Recall = TP / (TP + FN), where:\n    - TP (True Positives): Correctly predicted positive pixels\n    - FN (False Negatives): Incorrectly predicted negative pixels\n\n    Args:\n        pred (torch.Tensor): Predicted mask (probabilities or logits) with shape [C, H, W] or [H, W].\n        target (torch.Tensor): Ground truth mask with shape [H, W].\n        smooth (float): Smoothing factor to avoid division by zero.\n        num_classes (int, optional): Number of classes. If None, auto-detected.\n\n    Returns:\n        float: Mean recall score across all classes.\n    \"\"\"\n    # Convert predictions to class predictions\n    if pred.dim() == 3:  # [C, H, W] format\n        pred = torch.softmax(pred, dim=0)\n        pred_classes = torch.argmax(pred, dim=0)\n    elif pred.dim() == 2:  # [H, W] format\n        pred_classes = pred\n    else:\n        raise ValueError(f\"Unexpected prediction dimensions: {pred.shape}\")\n\n    # Auto-detect number of classes if not provided\n    if num_classes is None:\n        num_classes = max(pred_classes.max().item(), target.max().item()) + 1\n\n    # Calculate recall for each class and average\n    recall_scores = []\n    for class_id in range(num_classes):\n        pred_class = (pred_classes == class_id).float()\n        target_class = (target == class_id).float()\n\n        true_positives = (pred_class * target_class).sum()\n        actual_positives = target_class.sum()\n\n        if actual_positives &gt; 0:\n            recall = (true_positives + smooth) / (actual_positives + smooth)\n            recall_scores.append(recall.item())\n\n    return sum(recall_scores) / len(recall_scores) if recall_scores else 0.0\n</code></pre>"},{"location":"train/#geoai.train.semantic_inference_on_geotiff","title":"<code>semantic_inference_on_geotiff(model, geotiff_path, output_path, window_size=512, overlap=256, batch_size=4, num_channels=3, num_classes=2, device=None, probability_path=None, probability_threshold=None, save_class_probabilities=False, quiet=False, **kwargs)</code>","text":"<p>Perform semantic segmentation inference on a large GeoTIFF using a sliding window approach.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained semantic segmentation model.</p> required <code>geotiff_path</code> <code>str</code> <p>Path to input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask GeoTIFF.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from the input image.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the model output.</p> <code>2</code> <code>device</code> <code>device</code> <p>Device to run inference on.</p> <code>None</code> <code>probability_path</code> <code>str</code> <p>Path to save probability map. If provided, the normalized class probabilities will be saved as a multi-band raster.</p> <code>None</code> <code>probability_threshold</code> <code>float</code> <p>Probability threshold for binary classification. Only used when num_classes=2. If provided, pixels with class 1 probability &gt;= threshold are classified as class 1, otherwise class 0. If None (default), uses argmax.</p> <code>None</code> <code>save_class_probabilities</code> <code>bool</code> <p>If True and probability_path is provided, saves each class probability as a separate single-band file. Defaults to False.</p> <code>False</code> <code>quiet</code> <code>bool</code> <p>If True, suppress progress bar. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, float]</code> <p>Tuple containing output path and inference time in seconds.</p> Source code in <code>geoai/train.py</code> <pre><code>def semantic_inference_on_geotiff(\n    model: torch.nn.Module,\n    geotiff_path: str,\n    output_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    num_classes: int = 2,\n    device: Optional[torch.device] = None,\n    probability_path: Optional[str] = None,\n    probability_threshold: Optional[float] = None,\n    save_class_probabilities: bool = False,\n    quiet: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[str, float]:\n    \"\"\"\n    Perform semantic segmentation inference on a large GeoTIFF using a sliding window approach.\n\n    Args:\n        model (torch.nn.Module): Trained semantic segmentation model.\n        geotiff_path (str): Path to input GeoTIFF file.\n        output_path (str): Path to save output mask GeoTIFF.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels to use from the input image.\n        num_classes (int): Number of classes in the model output.\n        device (torch.device, optional): Device to run inference on.\n        probability_path (str, optional): Path to save probability map. If provided,\n            the normalized class probabilities will be saved as a multi-band raster.\n        probability_threshold (float, optional): Probability threshold for binary classification.\n            Only used when num_classes=2. If provided, pixels with class 1 probability &gt;= threshold\n            are classified as class 1, otherwise class 0. If None (default), uses argmax.\n        save_class_probabilities (bool): If True and probability_path is provided, saves each\n            class probability as a separate single-band file. Defaults to False.\n        quiet (bool): If True, suppress progress bar. Defaults to False.\n        **kwargs: Additional arguments.\n\n    Returns:\n        tuple: Tuple containing output path and inference time in seconds.\n    \"\"\"\n    if device is None:\n        device = get_device()\n\n    # Put model in evaluation mode\n    model.to(device)\n    model.eval()\n\n    # Open the GeoTIFF\n    with rasterio.open(geotiff_path) as src:\n        # Read metadata\n        meta = src.meta\n        height = src.height\n        width = src.width\n\n        # Update metadata for output raster\n        out_meta = meta.copy()\n        out_meta.update({\"count\": 1, \"dtype\": \"uint8\"})\n\n        # Initialize accumulator arrays for multi-class probability blending\n        # We'll accumulate probabilities for each class and then take argmax\n        prob_accumulator = np.zeros((num_classes, height, width), dtype=np.float32)\n        count_accumulator = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate steps\n        steps_y = math.ceil((height - overlap) / (window_size - overlap))\n        steps_x = math.ceil((width - overlap) / (window_size - overlap))\n        last_y = height - window_size\n        last_x = width - window_size\n\n        total_windows = steps_y * steps_x\n        if not quiet:\n            print(f\"Processing {total_windows} windows...\")\n\n        if not quiet:\n            pbar = tqdm(total=total_windows)\n        else:\n            pbar = None\n\n        batch_inputs = []\n        batch_positions = []\n        batch_count = 0\n\n        start_time = time.time()\n\n        for i in range(steps_y + 1):\n            y = min(i * (window_size - overlap), last_y)\n            y = max(0, y)\n\n            if y &gt; last_y and i &gt; 0:\n                continue\n\n            for j in range(steps_x + 1):\n                x = min(j * (window_size - overlap), last_x)\n                x = max(0, x)\n\n                if x &gt; last_x and j &gt; 0:\n                    continue\n\n                # Read window\n                window = src.read(window=Window(x, y, window_size, window_size))\n\n                if window.shape[1] == 0 or window.shape[2] == 0:\n                    continue\n\n                current_height = window.shape[1]\n                current_width = window.shape[2]\n\n                # Normalize and prepare input\n                image = window.astype(np.float32) / 255.0\n\n                # Handle different number of bands\n                if image.shape[0] &gt; num_channels:\n                    image = image[:num_channels]\n                elif image.shape[0] &lt; num_channels:\n                    padded = np.zeros(\n                        (num_channels, current_height, current_width), dtype=np.float32\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n                # Convert to tensor\n                image_tensor = torch.tensor(image, device=device)\n\n                # Add to batch\n                batch_inputs.append(image_tensor)\n                batch_positions.append((y, x, current_height, current_width))\n                batch_count += 1\n\n                # Process batch\n                if batch_count == batch_size or (i == steps_y and j == steps_x):\n                    with torch.no_grad():\n                        batch_tensor = torch.stack(batch_inputs)\n                        outputs = model(batch_tensor)\n\n                        # Apply softmax to get class probabilities\n                        probs = torch.softmax(outputs, dim=1)\n\n                    # Process each output in the batch\n                    for idx, prob in enumerate(probs):\n                        y_pos, x_pos, h, w = batch_positions[idx]\n\n                        # Create weight matrix for blending\n                        y_grid, x_grid = np.mgrid[0:h, 0:w]\n                        dist_from_left = x_grid\n                        dist_from_right = w - x_grid - 1\n                        dist_from_top = y_grid\n                        dist_from_bottom = h - y_grid - 1\n\n                        edge_distance = np.minimum.reduce(\n                            [\n                                dist_from_left,\n                                dist_from_right,\n                                dist_from_top,\n                                dist_from_bottom,\n                            ]\n                        )\n                        edge_distance = np.minimum(edge_distance, overlap / 2)\n\n                        # Avoid zero weights - use minimum weight of 0.1\n                        weight = np.maximum(edge_distance / (overlap / 2), 0.1)\n\n                        # For non-overlapping windows, use uniform weight\n                        if overlap == 0:\n                            weight = np.ones_like(weight)\n\n                        # Convert probabilities to numpy [C, H, W]\n                        prob_np = prob.cpu().numpy()\n\n                        # Accumulate weighted probabilities for each class\n                        y_slice = slice(y_pos, y_pos + h)\n                        x_slice = slice(x_pos, x_pos + w)\n\n                        # Add weighted probabilities for each class\n                        for class_idx in range(num_classes):\n                            prob_accumulator[class_idx, y_slice, x_slice] += (\n                                prob_np[class_idx] * weight\n                            )\n\n                        # Update weight accumulator\n                        count_accumulator[y_slice, x_slice] += weight\n\n                    # Reset batch\n                    batch_inputs = []\n                    batch_positions = []\n                    batch_count = 0\n                    if pbar is not None:\n                        pbar.update(len(probs))\n\n        if pbar is not None:\n            pbar.close()\n\n        # Calculate final mask by taking argmax of accumulated probabilities\n        mask = np.zeros((height, width), dtype=np.uint8)\n        valid_pixels = count_accumulator &gt; 0\n\n        if np.any(valid_pixels):\n            # Normalize accumulated probabilities by weights\n            normalized_probs = np.zeros_like(prob_accumulator)\n            for class_idx in range(num_classes):\n                normalized_probs[class_idx, valid_pixels] = (\n                    prob_accumulator[class_idx, valid_pixels]\n                    / count_accumulator[valid_pixels]\n                )\n\n            # Apply threshold for binary classification or use argmax\n            if probability_threshold is not None and num_classes == 2:\n                # Use threshold: classify as class 1 if probability &gt;= threshold\n                mask[valid_pixels] = (\n                    normalized_probs[1, valid_pixels] &gt;= probability_threshold\n                ).astype(np.uint8)\n                if not quiet:\n                    print(f\"Using probability threshold: {probability_threshold}\")\n            else:\n                # Take argmax to get final class predictions\n                mask[valid_pixels] = np.argmax(\n                    normalized_probs[:, valid_pixels], axis=0\n                ).astype(np.uint8)\n\n            # Check class distribution in predictions (summary only)\n            unique_classes, class_counts = np.unique(\n                mask[valid_pixels], return_counts=True\n            )\n            bg_ratio = np.sum(mask == 0) / mask.size\n            if not quiet:\n                print(\n                    f\"Predicted classes: {len(unique_classes)} classes, Background: {bg_ratio:.1%}\"\n                )\n\n        inference_time = time.time() - start_time\n        if not quiet:\n            print(f\"Inference completed in {inference_time:.2f} seconds\")\n\n        # Save output\n        out_dir = os.path.abspath(os.path.dirname(output_path))\n        os.makedirs(out_dir, exist_ok=True)\n        with rasterio.open(output_path, \"w\", **out_meta) as dst:\n            dst.write(mask, 1)\n\n        if not quiet:\n            print(f\"Saved prediction to {output_path}\")\n\n        # Save probability map if requested\n        if probability_path is not None:\n            prob_dir = os.path.abspath(os.path.dirname(probability_path))\n            os.makedirs(prob_dir, exist_ok=True)\n\n            # Prepare probability output metadata\n            prob_meta = meta.copy()\n            prob_meta.update({\"count\": num_classes, \"dtype\": \"float32\"})\n\n            # Save normalized probabilities as multi-band raster\n            with rasterio.open(probability_path, \"w\", **prob_meta) as dst:\n                for class_idx in range(num_classes):\n                    # Normalize probabilities\n                    prob_band = np.zeros((height, width), dtype=np.float32)\n                    prob_band[valid_pixels] = (\n                        prob_accumulator[class_idx, valid_pixels]\n                        / count_accumulator[valid_pixels]\n                    )\n                    dst.write(prob_band, class_idx + 1)\n\n            if not quiet:\n                print(f\"Saved probability map to {probability_path}\")\n\n            # Save individual class probabilities if requested\n            if save_class_probabilities:\n                # Prepare single-band metadata\n                single_band_meta = meta.copy()\n                single_band_meta.update({\"count\": 1, \"dtype\": \"float32\"})\n\n                # Get base filename and extension\n                prob_base = os.path.splitext(probability_path)[0]\n                prob_ext = os.path.splitext(probability_path)[1]\n\n                for class_idx in range(num_classes):\n                    # Create filename for this class\n                    class_prob_path = f\"{prob_base}_class_{class_idx}{prob_ext}\"\n\n                    # Normalize probabilities\n                    prob_band = np.zeros((height, width), dtype=np.float32)\n                    prob_band[valid_pixels] = (\n                        prob_accumulator[class_idx, valid_pixels]\n                        / count_accumulator[valid_pixels]\n                    )\n\n                    # Save single-band file\n                    with rasterio.open(class_prob_path, \"w\", **single_band_meta) as dst:\n                        dst.write(prob_band, 1)\n\n                    if not quiet:\n                        print(\n                            f\"Saved class {class_idx} probability to {class_prob_path}\"\n                        )\n\n        return output_path, inference_time\n</code></pre>"},{"location":"train/#geoai.train.semantic_inference_on_image","title":"<code>semantic_inference_on_image(model, image_path, output_path, window_size=512, overlap=256, batch_size=4, num_channels=3, num_classes=2, device=None, binary_output=True, probability_path=None, probability_threshold=None, save_class_probabilities=False, quiet=False, **kwargs)</code>","text":"<p>Perform semantic segmentation inference on a regular image (JPG, PNG, etc.) using a sliding window approach.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained semantic segmentation model.</p> required <code>image_path</code> <code>str</code> <p>Path to input image file (JPG, PNG, etc.).</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask image.</p> required <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>num_channels</code> <code>int</code> <p>Number of channels to use from the input image.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the model output.</p> <code>2</code> <code>device</code> <code>device</code> <p>Device to run inference on.</p> <code>None</code> <code>binary_output</code> <code>bool</code> <p>If True, convert multi-class output to binary (class &gt; 0).</p> <code>True</code> <code>probability_path</code> <code>str</code> <p>Path to save probability map. If provided, the normalized class probabilities will be saved as a multi-band raster.</p> <code>None</code> <code>probability_threshold</code> <code>float</code> <p>Probability threshold for binary classification. Only used when num_classes=2. If provided, pixels with class 1 probability &gt;= threshold are classified as class 1, otherwise class 0. If None (default), uses argmax.</p> <code>None</code> <code>save_class_probabilities</code> <code>bool</code> <p>If True and probability_path is provided, saves each class probability as a separate single-band file. Defaults to False.</p> <code>False</code> <code>quiet</code> <code>bool</code> <p>If True, suppress progress bar. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[str, float]</code> <p>Tuple containing output path and inference time in seconds.</p> Source code in <code>geoai/train.py</code> <pre><code>def semantic_inference_on_image(\n    model: torch.nn.Module,\n    image_path: str,\n    output_path: str,\n    window_size: int = 512,\n    overlap: int = 256,\n    batch_size: int = 4,\n    num_channels: int = 3,\n    num_classes: int = 2,\n    device: Optional[torch.device] = None,\n    binary_output: bool = True,\n    probability_path: Optional[str] = None,\n    probability_threshold: Optional[float] = None,\n    save_class_probabilities: bool = False,\n    quiet: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[str, float]:\n    \"\"\"\n    Perform semantic segmentation inference on a regular image (JPG, PNG, etc.) using a sliding window approach.\n\n    Args:\n        model (torch.nn.Module): Trained semantic segmentation model.\n        image_path (str): Path to input image file (JPG, PNG, etc.).\n        output_path (str): Path to save output mask image.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        batch_size (int): Batch size for inference.\n        num_channels (int): Number of channels to use from the input image.\n        num_classes (int): Number of classes in the model output.\n        device (torch.device, optional): Device to run inference on.\n        binary_output (bool): If True, convert multi-class output to binary (class &gt; 0).\n        probability_path (str, optional): Path to save probability map. If provided,\n            the normalized class probabilities will be saved as a multi-band raster.\n        probability_threshold (float, optional): Probability threshold for binary classification.\n            Only used when num_classes=2. If provided, pixels with class 1 probability &gt;= threshold\n            are classified as class 1, otherwise class 0. If None (default), uses argmax.\n        save_class_probabilities (bool): If True and probability_path is provided, saves each\n            class probability as a separate single-band file. Defaults to False.\n        quiet (bool): If True, suppress progress bar. Defaults to False.\n        **kwargs: Additional arguments.\n\n    Returns:\n        tuple: Tuple containing output path and inference time in seconds.\n    \"\"\"\n    from PIL import Image\n\n    if device is None:\n        device = get_device()\n\n    # Put model in evaluation mode\n    model.to(device)\n    model.eval()\n\n    # Open the image using PIL\n    with Image.open(image_path) as pil_img:\n        # Convert to RGB if needed\n        if pil_img.mode != \"RGB\":\n            pil_img = pil_img.convert(\"RGB\")\n\n        # Convert to numpy array [H, W, C]\n        img_array = np.array(pil_img, dtype=np.uint8)\n        height, width = img_array.shape[:2]\n\n        # Convert to [C, H, W] format like rasterio\n        img_array = np.transpose(img_array, (2, 0, 1))\n\n        if not quiet:\n            print(f\"Processing image: {width}x{height}\")\n\n        # Initialize accumulator arrays for multi-class probability blending\n        prob_accumulator = np.zeros((num_classes, height, width), dtype=np.float32)\n        count_accumulator = np.zeros((height, width), dtype=np.float32)\n\n        # Calculate steps\n        steps_y = math.ceil((height - overlap) / (window_size - overlap))\n        steps_x = math.ceil((width - overlap) / (window_size - overlap))\n        last_y = height - window_size\n        last_x = width - window_size\n\n        total_windows = steps_y * steps_x\n        if not quiet:\n            print(f\"Processing {total_windows} windows...\")\n\n        if not quiet:\n            pbar = tqdm(total=total_windows)\n        else:\n            pbar = None\n\n        batch_inputs = []\n        batch_positions = []\n        batch_count = 0\n\n        start_time = time.time()\n\n        for i in range(steps_y + 1):\n            y = min(i * (window_size - overlap), last_y)\n            y = max(0, y)\n\n            if y &gt; last_y and i &gt; 0:\n                continue\n\n            for j in range(steps_x + 1):\n                x = min(j * (window_size - overlap), last_x)\n                x = max(0, x)\n\n                if x &gt; last_x and j &gt; 0:\n                    continue\n\n                # Extract window from image array\n                y_end = min(y + window_size, height)\n                x_end = min(x + window_size, width)\n                window = img_array[:, y:y_end, x:x_end]\n\n                if window.shape[1] == 0 or window.shape[2] == 0:\n                    continue\n\n                current_height = window.shape[1]\n                current_width = window.shape[2]\n\n                # Pad window to window_size if needed\n                if current_height &lt; window_size or current_width &lt; window_size:\n                    padded_window = np.zeros(\n                        (window.shape[0], window_size, window_size), dtype=window.dtype\n                    )\n                    padded_window[:, :current_height, :current_width] = window\n                    window = padded_window\n\n                # Normalize and prepare input\n                image = window.astype(np.float32) / 255.0\n\n                # Handle different number of channels\n                if image.shape[0] &gt; num_channels:\n                    image = image[:num_channels]\n                elif image.shape[0] &lt; num_channels:\n                    padded = np.zeros(\n                        (num_channels, image.shape[1], image.shape[2]), dtype=np.float32\n                    )\n                    padded[: image.shape[0]] = image\n                    image = padded\n\n                # Convert to tensor\n                image_tensor = torch.tensor(image, device=device)\n\n                # Add to batch\n                batch_inputs.append(image_tensor)\n                batch_positions.append((y, x, current_height, current_width))\n                batch_count += 1\n\n                # Process batch\n                if batch_count == batch_size or (i == steps_y and j == steps_x):\n                    with torch.no_grad():\n                        batch_tensor = torch.stack(batch_inputs)\n                        outputs = model(batch_tensor)\n\n                        # Apply softmax to get class probabilities\n                        probs = torch.softmax(outputs, dim=1)\n\n                    # Process each output in the batch\n                    for idx, prob in enumerate(probs):\n                        y_pos, x_pos, h, w = batch_positions[idx]\n\n                        # Create weight matrix for blending\n                        y_grid, x_grid = np.mgrid[0:h, 0:w]\n                        dist_from_left = x_grid\n                        dist_from_right = w - x_grid - 1\n                        dist_from_top = y_grid\n                        dist_from_bottom = h - y_grid - 1\n\n                        edge_distance = np.minimum.reduce(\n                            [\n                                dist_from_left,\n                                dist_from_right,\n                                dist_from_top,\n                                dist_from_bottom,\n                            ]\n                        )\n                        edge_distance = np.minimum(edge_distance, overlap / 2)\n\n                        # Avoid zero weights - use minimum weight of 0.1\n                        weight = np.maximum(edge_distance / (overlap / 2), 0.1)\n\n                        # For non-overlapping windows, use uniform weight\n                        if overlap == 0:\n                            weight = np.ones_like(weight)\n\n                        # Convert probabilities to numpy [C, H, W] - crop to actual size\n                        prob_np = prob.cpu().numpy()[:, :h, :w]\n\n                        # Accumulate weighted probabilities for each class\n                        y_slice = slice(y_pos, y_pos + h)\n                        x_slice = slice(x_pos, x_pos + w)\n\n                        # Add weighted probabilities for each class\n                        for class_idx in range(num_classes):\n                            prob_accumulator[class_idx, y_slice, x_slice] += (\n                                prob_np[class_idx] * weight\n                            )\n\n                        # Update weight accumulator\n                        count_accumulator[y_slice, x_slice] += weight\n\n                    # Reset batch\n                    batch_inputs = []\n                    batch_positions = []\n                    batch_count = 0\n                    if pbar is not None:\n                        pbar.update(len(probs))\n\n        if pbar is not None:\n            pbar.close()\n\n        # Calculate final mask by taking argmax of accumulated probabilities\n        mask = np.zeros((height, width), dtype=np.uint8)\n        valid_pixels = count_accumulator &gt; 0\n\n        if np.any(valid_pixels):\n            # Normalize accumulated probabilities by weights\n            normalized_probs = np.zeros_like(prob_accumulator)\n            for class_idx in range(num_classes):\n                normalized_probs[class_idx, valid_pixels] = (\n                    prob_accumulator[class_idx, valid_pixels]\n                    / count_accumulator[valid_pixels]\n                )\n\n            # Apply threshold for binary classification or use argmax\n            if probability_threshold is not None and num_classes == 2:\n                # Use threshold: classify as class 1 if probability &gt;= threshold\n                mask[valid_pixels] = (\n                    normalized_probs[1, valid_pixels] &gt;= probability_threshold\n                ).astype(np.uint8)\n                if not quiet:\n                    print(f\"Using probability threshold: {probability_threshold}\")\n            else:\n                # Take argmax to get final class predictions\n                mask[valid_pixels] = np.argmax(\n                    normalized_probs[:, valid_pixels], axis=0\n                ).astype(np.uint8)\n\n            # Check class distribution in predictions before binary conversion\n            unique_classes, class_counts = np.unique(mask, return_counts=True)\n            # Convert numpy types to regular Python types for cleaner output\n            class_distribution = {\n                int(cls): int(count) for cls, count in zip(unique_classes, class_counts)\n            }\n            if not quiet:\n                print(f\"Raw predicted classes and counts: {class_distribution}\")\n\n            # Convert to binary if requested and num_classes == 2\n            if binary_output and num_classes == 2:\n                # For binary segmentation, convert class 1 to 255 (white) and class 0 to 0 (black)\n                # Use proper thresholding to ensure only 0 and 255 values\n                binary_mask = np.zeros_like(mask)\n                binary_mask[mask &gt; 0] = 255\n                mask = binary_mask\n\n                # Final check\n                unique_classes, class_counts = np.unique(mask, return_counts=True)\n                # Convert numpy types to regular Python types for cleaner output\n                binary_distribution = {\n                    int(cls): int(count)\n                    for cls, count in zip(unique_classes, class_counts)\n                }\n                if not quiet:\n                    print(f\"Binary predicted classes and counts: {binary_distribution}\")\n\n        inference_time = time.time() - start_time\n        if not quiet:\n            print(f\"Inference completed in {inference_time:.2f} seconds\")\n\n        # Save output as image\n        # For binary masks, use PNG to avoid JPEG compression artifacts\n        if binary_output and num_classes == 2:\n            # Change extension to PNG if binary output to preserve exact values\n            output_path_png = os.path.splitext(output_path)[0] + \".png\"\n            output_img = Image.fromarray(mask, mode=\"L\")\n            out_dir = os.path.abspath(os.path.dirname(output_path))\n            os.makedirs(out_dir, exist_ok=True)\n            output_img.save(output_path_png)\n            if not quiet:\n                print(\n                    f\"Saved binary prediction to {output_path_png} (PNG format to preserve exact values)\"\n                )\n\n            # Also save the original requested format for compatibility\n            if output_path != output_path_png:\n                output_img.save(output_path)\n                print(f\"Also saved to {output_path} (may have compression artifacts)\")\n        else:\n            output_img = Image.fromarray(mask, mode=\"L\")\n            output_img.save(output_path)\n            if not quiet:\n                print(f\"Saved prediction to {output_path}\")\n\n        # Save probability map if requested\n        if probability_path is not None:\n            prob_dir = os.path.abspath(os.path.dirname(probability_path))\n            os.makedirs(prob_dir, exist_ok=True)\n\n            # For regular images, we'll save as a multi-channel TIFF\n            # since we need to preserve floating point values\n            import rasterio\n            from rasterio.transform import from_bounds\n\n            # Create a simple affine transform (identity transform for pixel coordinates)\n            transform = from_bounds(0, 0, width, height, width, height)\n\n            # Prepare probability output metadata\n            prob_meta = {\n                \"driver\": \"GTiff\",\n                \"height\": height,\n                \"width\": width,\n                \"count\": num_classes,\n                \"dtype\": \"float32\",\n                \"transform\": transform,\n            }\n\n            # Save normalized probabilities as multi-band raster\n            with rasterio.open(probability_path, \"w\", **prob_meta) as dst:\n                for class_idx in range(num_classes):\n                    # Normalize probabilities\n                    prob_band = np.zeros((height, width), dtype=np.float32)\n                    prob_band[valid_pixels] = normalized_probs[class_idx, valid_pixels]\n                    dst.write(prob_band, class_idx + 1)\n\n            if not quiet:\n                print(f\"Saved probability map to {probability_path}\")\n\n            # Save individual class probabilities if requested\n            if save_class_probabilities:\n                # Prepare single-band metadata\n                single_band_meta = {\n                    \"driver\": \"GTiff\",\n                    \"height\": height,\n                    \"width\": width,\n                    \"count\": 1,\n                    \"dtype\": \"float32\",\n                    \"transform\": transform,\n                }\n\n                # Get base filename and extension\n                prob_base = os.path.splitext(probability_path)[0]\n                prob_ext = os.path.splitext(probability_path)[1]\n\n                for class_idx in range(num_classes):\n                    # Create filename for this class\n                    class_prob_path = f\"{prob_base}_class_{class_idx}{prob_ext}\"\n\n                    # Normalize probabilities\n                    prob_band = np.zeros((height, width), dtype=np.float32)\n                    prob_band[valid_pixels] = normalized_probs[class_idx, valid_pixels]\n\n                    # Save single-band file\n                    with rasterio.open(class_prob_path, \"w\", **single_band_meta) as dst:\n                        dst.write(prob_band, 1)\n\n                    if not quiet:\n                        print(\n                            f\"Saved class {class_idx} probability to {class_prob_path}\"\n                        )\n\n        return output_path, inference_time\n</code></pre>"},{"location":"train/#geoai.train.semantic_segmentation","title":"<code>semantic_segmentation(input_path, output_path, model_path, architecture='unet', encoder_name='resnet34', num_channels=3, num_classes=2, window_size=512, overlap=256, batch_size=4, device=None, probability_path=None, probability_threshold=None, save_class_probabilities=False, quiet=False, **kwargs)</code>","text":"<p>Perform semantic segmentation on an image file using a trained model.</p> <p>This function automatically detects the input format and uses the appropriate inference method for either GeoTIFF files or regular image formats (JPG, PNG, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to input image file (GeoTIFF, JPG, PNG, etc.).</p> required <code>output_path</code> <code>str</code> <p>Path to save output mask file.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>architecture</code> <code>str</code> <p>Model architecture used for training.</p> <code>'unet'</code> <code>encoder_name</code> <code>str</code> <p>Encoder backbone name used for training.</p> <code>'resnet34'</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the model.</p> <code>2</code> <code>window_size</code> <code>int</code> <p>Size of sliding window for inference.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference.</p> <code>4</code> <code>device</code> <code>device</code> <p>Device to run inference on.</p> <code>None</code> <code>probability_path</code> <code>str</code> <p>Path to save probability map. If provided, the normalized class probabilities will be saved as a multi-band raster where each band contains probabilities for each class.</p> <code>None</code> <code>probability_threshold</code> <code>float</code> <p>Probability threshold for binary classification. Only used when num_classes=2. If provided, pixels with class 1 probability &gt;= threshold are classified as class 1, otherwise class 0. If None (default), uses argmax. Must be between 0 and 1.</p> <code>None</code> <code>save_class_probabilities</code> <code>bool</code> <p>If True and probability_path is provided, saves each class probability as a separate single-band file. Files will be named like \"probability_class_0.tif\", \"probability_class_1.tif\", etc. in the same directory as probability_path. Defaults to False.</p> <code>False</code> <code>quiet</code> <code>bool</code> <p>If True, suppress progress bar. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Output mask is saved to output_path.</p> Source code in <code>geoai/train.py</code> <pre><code>def semantic_segmentation(\n    input_path: str,\n    output_path: str,\n    model_path: str,\n    architecture: str = \"unet\",\n    encoder_name: str = \"resnet34\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    window_size: int = 512,\n    overlap: int = 256,\n    batch_size: int = 4,\n    device: Optional[torch.device] = None,\n    probability_path: Optional[str] = None,\n    probability_threshold: Optional[float] = None,\n    save_class_probabilities: bool = False,\n    quiet: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform semantic segmentation on an image file using a trained model.\n\n    This function automatically detects the input format and uses the appropriate\n    inference method for either GeoTIFF files or regular image formats (JPG, PNG, etc.).\n\n    Args:\n        input_path (str): Path to input image file (GeoTIFF, JPG, PNG, etc.).\n        output_path (str): Path to save output mask file.\n        model_path (str): Path to trained model weights.\n        architecture (str): Model architecture used for training.\n        encoder_name (str): Encoder backbone name used for training.\n        num_channels (int): Number of channels in the input image and model.\n        num_classes (int): Number of classes in the model.\n        window_size (int): Size of sliding window for inference.\n        overlap (int): Overlap between adjacent windows.\n        batch_size (int): Batch size for inference.\n        device (torch.device, optional): Device to run inference on.\n        probability_path (str, optional): Path to save probability map. If provided,\n            the normalized class probabilities will be saved as a multi-band raster\n            where each band contains probabilities for each class.\n        probability_threshold (float, optional): Probability threshold for binary classification.\n            Only used when num_classes=2. If provided, pixels with class 1 probability &gt;= threshold\n            are classified as class 1, otherwise class 0. If None (default), uses argmax.\n            Must be between 0 and 1.\n        save_class_probabilities (bool): If True and probability_path is provided, saves each\n            class probability as a separate single-band file. Files will be named like\n            \"probability_class_0.tif\", \"probability_class_1.tif\", etc. in the same directory\n            as probability_path. Defaults to False.\n        quiet (bool): If True, suppress progress bar. Defaults to False.\n        **kwargs: Additional arguments.\n\n    Returns:\n        None: Output mask is saved to output_path.\n    \"\"\"\n    if device is None:\n        device = get_device()\n\n    # Detect file format based on extension\n    input_ext = os.path.splitext(input_path)[1].lower()\n    is_geotiff = input_ext in [\".tif\", \".tiff\", \".jp2\", \".img\"]\n    formats = {\n        \".tif\": \"GeoTIFF\",\n        \".tiff\": \"GeoTIFF\",\n        \".jp2\": \"JP2OpenJPEG\",\n        \".img\": \"IMG\",\n    }\n\n    if not quiet:\n        print(\n            f\"Input file format: {formats[input_ext] if is_geotiff else 'Regular image'} ({input_ext})\"\n        )\n\n    # Load model\n    model = get_smp_model(\n        architecture=architecture,\n        encoder_name=encoder_name,\n        encoder_weights=None,  # We're loading trained weights\n        in_channels=num_channels,\n        classes=num_classes,\n        activation=None,\n    )\n\n    if not os.path.exists(model_path):\n        try:\n            model_path = download_model_from_hf(model_path)\n        except Exception as e:\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    # Load state dict and handle DataParallel module prefix\n    state_dict = torch.load(model_path, map_location=device)\n\n    # Remove 'module.' prefix if present (from DataParallel training)\n    if any(key.startswith(\"module.\") for key in state_dict.keys()):\n        state_dict = {\n            key.replace(\"module.\", \"\"): value for key, value in state_dict.items()\n        }\n\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n\n    # Validate probability_threshold\n    if probability_threshold is not None:\n        if not (0 &lt;= probability_threshold &lt;= 1):\n            raise ValueError(\"probability_threshold must be between 0 and 1\")\n        if num_classes != 2:\n            raise ValueError(\n                \"probability_threshold is only supported for binary classification (num_classes=2)\"\n            )\n\n    # Use appropriate inference function based on file format\n    if is_geotiff:\n        semantic_inference_on_geotiff(\n            model=model,\n            geotiff_path=input_path,\n            output_path=output_path,\n            window_size=window_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            num_channels=num_channels,\n            num_classes=num_classes,\n            device=device,\n            probability_path=probability_path,\n            probability_threshold=probability_threshold,\n            save_class_probabilities=save_class_probabilities,\n            quiet=quiet,\n            **kwargs,\n        )\n    else:\n        # Create output directory if it doesn't exist\n        os.makedirs(os.path.abspath(os.path.dirname(output_path)), exist_ok=True)\n\n        semantic_inference_on_image(\n            model=model,\n            image_path=input_path,\n            output_path=output_path,\n            window_size=window_size,\n            overlap=overlap,\n            batch_size=batch_size,\n            num_channels=num_channels,\n            num_classes=num_classes,\n            device=device,\n            binary_output=True,  # Convert to binary output for better visualization\n            probability_path=probability_path,\n            probability_threshold=probability_threshold,\n            save_class_probabilities=save_class_probabilities,\n            quiet=quiet,\n            **kwargs,\n        )\n</code></pre>"},{"location":"train/#geoai.train.semantic_segmentation_batch","title":"<code>semantic_segmentation_batch(input_dir, output_dir, model_path, architecture='unet', encoder_name='resnet34', num_channels=3, num_classes=2, window_size=512, overlap=256, batch_size=4, device=None, filenames=None, quiet=False, **kwargs)</code>","text":"<p>Perform semantic segmentation on a batch of images from an input directory.</p> <p>This function processes all images in a directory and saves the results to an output directory. It automatically detects the input format and uses the appropriate inference method for either GeoTIFF files or regular image formats (JPG, PNG, etc.). For GeoTIFF inputs, outputs are saved as GeoTIFF. For other formats, outputs are saved as PNG to preserve exact values.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input image files to process.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output mask files.</p> required <code>model_path</code> <code>str</code> <p>Path to trained model weights.</p> required <code>architecture</code> <code>str</code> <p>Model architecture used for training. Defaults to \"unet\".</p> <code>'unet'</code> <code>encoder_name</code> <code>str</code> <p>Encoder backbone name used for training. Defaults to \"resnet34\".</p> <code>'resnet34'</code> <code>num_channels</code> <code>int</code> <p>Number of channels in the input image and model. Defaults to 3.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of classes in the model. Defaults to 2.</p> <code>2</code> <code>window_size</code> <code>int</code> <p>Size of sliding window for inference. Defaults to 512.</p> <code>512</code> <code>overlap</code> <code>int</code> <p>Overlap between adjacent windows. Defaults to 256.</p> <code>256</code> <code>batch_size</code> <code>int</code> <p>Batch size for inference. Defaults to 4.</p> <code>4</code> <code>device</code> <code>device</code> <p>Device to run inference on. If None, uses CUDA if available.</p> <code>None</code> <code>filenames</code> <code>list</code> <p>List of output filenames. If None, defaults to \"_mask.\" for each input file where  is \"tif\" for GeoTIFF inputs and \"png\" for other formats. If provided, must match the number of input files. <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress progress bar. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the inference functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>None</code> <p>Output masks are saved to output_dir.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If input_dir doesn't exist or contains no supported image files.</p> <code>ValueError</code> <p>If filenames is provided but doesn't match the number of input files.</p> Source code in <code>geoai/train.py</code> <pre><code>def semantic_segmentation_batch(\n    input_dir: str,\n    output_dir: str,\n    model_path: str,\n    architecture: str = \"unet\",\n    encoder_name: str = \"resnet34\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    window_size: int = 512,\n    overlap: int = 256,\n    batch_size: int = 4,\n    device: Optional[torch.device] = None,\n    filenames: Optional[List[str]] = None,\n    quiet: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Perform semantic segmentation on a batch of images from an input directory.\n\n    This function processes all images in a directory and saves the results to an output directory.\n    It automatically detects the input format and uses the appropriate inference method for either\n    GeoTIFF files or regular image formats (JPG, PNG, etc.). For GeoTIFF inputs, outputs are saved\n    as GeoTIFF. For other formats, outputs are saved as PNG to preserve exact values.\n\n    Args:\n        input_dir (str): Directory containing input image files to process.\n        output_dir (str): Directory to save output mask files.\n        model_path (str): Path to trained model weights.\n        architecture (str): Model architecture used for training. Defaults to \"unet\".\n        encoder_name (str): Encoder backbone name used for training. Defaults to \"resnet34\".\n        num_channels (int): Number of channels in the input image and model. Defaults to 3.\n        num_classes (int): Number of classes in the model. Defaults to 2.\n        window_size (int): Size of sliding window for inference. Defaults to 512.\n        overlap (int): Overlap between adjacent windows. Defaults to 256.\n        batch_size (int): Batch size for inference. Defaults to 4.\n        device (torch.device, optional): Device to run inference on. If None, uses CUDA if available.\n        filenames (list, optional): List of output filenames. If None, defaults to\n            \"&lt;input_filename&gt;_mask.&lt;ext&gt;\" for each input file where &lt;ext&gt; is \"tif\" for GeoTIFF\n            inputs and \"png\" for other formats. If provided, must match the number of input files.\n        quiet (bool): If True, suppress progress bar. Defaults to False.\n        **kwargs: Additional arguments passed to the inference functions.\n\n    Returns:\n        None: Output masks are saved to output_dir.\n\n    Raises:\n        FileNotFoundError: If input_dir doesn't exist or contains no supported image files.\n        ValueError: If filenames is provided but doesn't match the number of input files.\n    \"\"\"\n    if device is None:\n        device = get_device()\n\n    # Check if input directory exists\n    if not os.path.exists(input_dir):\n        raise FileNotFoundError(f\"Input directory does not exist: {input_dir}\")\n\n    # Create output directory if it doesn't exist\n    os.makedirs(os.path.abspath(output_dir), exist_ok=True)\n\n    # Get all supported image files\n    image_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n    image_files = sorted(\n        [\n            os.path.join(input_dir, f)\n            for f in os.listdir(input_dir)\n            if f.lower().endswith(image_extensions)\n        ]\n    )\n\n    if len(image_files) == 0:\n        raise FileNotFoundError(f\"No supported image files found in {input_dir}\")\n\n    print(f\"Found {len(image_files)} image files to process\")\n\n    # Load model once for all images\n    model = get_smp_model(\n        architecture=architecture,\n        encoder_name=encoder_name,\n        encoder_weights=None,  # We're loading trained weights\n        in_channels=num_channels,\n        classes=num_classes,\n        activation=None,\n    )\n\n    if not os.path.exists(model_path):\n        try:\n            model_path = download_model_from_hf(model_path)\n        except Exception as e:\n            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n\n    # Load state dict and handle DataParallel module prefix\n    state_dict = torch.load(model_path, map_location=device)\n\n    # Remove 'module.' prefix if present (from DataParallel training)\n    if any(key.startswith(\"module.\") for key in state_dict.keys()):\n        state_dict = {\n            key.replace(\"module.\", \"\"): value for key, value in state_dict.items()\n        }\n\n    model.load_state_dict(state_dict)\n    model.to(device)\n    model.eval()\n\n    # Generate output filenames if not provided\n    if filenames is None:\n        filenames = []\n        for image_file in image_files:\n            base_name = os.path.splitext(os.path.basename(image_file))[0]\n            input_ext = os.path.splitext(image_file)[1].lower()\n\n            # Use GeoTIFF extension for GeoTIFF inputs, PNG for others\n            if input_ext in [\".tif\", \".tiff\"]:\n                output_ext = \".tif\"\n            else:\n                output_ext = \".png\"\n\n            output_filename = f\"{base_name}_mask{output_ext}\"\n            filenames.append(os.path.join(output_dir, output_filename))\n    else:\n        # Validate filenames list\n        if len(filenames) != len(image_files):\n            raise ValueError(\n                f\"Number of filenames ({len(filenames)}) must match number of input files ({len(image_files)})\"\n            )\n\n    # Process each image\n    for i, (input_path, output_path) in enumerate(zip(image_files, filenames)):\n        print(\n            f\"Processing file {i + 1}/{len(image_files)}: {os.path.basename(input_path)}\"\n        )\n\n        # Detect file format based on extension\n        input_ext = os.path.splitext(input_path)[1].lower()\n        is_geotiff = input_ext in [\".tif\", \".tiff\"]\n\n        try:\n            # Use appropriate inference function based on file format\n            if is_geotiff:\n                semantic_inference_on_geotiff(\n                    model=model,\n                    geotiff_path=input_path,\n                    output_path=output_path,\n                    window_size=window_size,\n                    overlap=overlap,\n                    batch_size=batch_size,\n                    num_channels=num_channels,\n                    num_classes=num_classes,\n                    device=device,\n                    quiet=quiet,\n                    **kwargs,\n                )\n            else:\n                semantic_inference_on_image(\n                    model=model,\n                    image_path=input_path,\n                    output_path=output_path,\n                    window_size=window_size,\n                    overlap=overlap,\n                    batch_size=batch_size,\n                    num_channels=num_channels,\n                    num_classes=num_classes,\n                    device=device,\n                    binary_output=True,  # Convert to binary output for better visualization\n                    quiet=quiet,\n                    **kwargs,\n                )\n        except Exception as e:\n            print(f\"Error processing {input_path}: {str(e)}\")\n            continue\n\n    print(f\"Batch processing completed. Results saved to {output_dir}\")\n</code></pre>"},{"location":"train/#geoai.train.train_MaskRCNN_model","title":"<code>train_MaskRCNN_model(images_dir, labels_dir, output_dir, input_format='directory', num_channels=3, model=None, pretrained=True, pretrained_model_path=None, batch_size=4, num_epochs=10, learning_rate=0.005, seed=42, val_split=0.2, visualize=False, resume_training=False, print_freq=10, device=None, num_workers=None, verbose=True)</code>","text":"<p>Train and evaluate Mask R-CNN model for instance segmentation.</p> <p>This function trains a Mask R-CNN model for instance segmentation using the provided dataset. It supports loading a pretrained model to either initialize the backbone or to continue training from a specific checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing image GeoTIFF files (for 'directory' format), or root directory containing images/ subdirectory (for 'yolo' format), or directory containing images (for 'coco' format).</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing label GeoTIFF files (for 'directory' format), or path to COCO annotations JSON file (for 'coco' format), or not used (for 'yolo' format - labels are in images_dir/labels/).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model checkpoints and results.</p> required <code>input_format</code> <code>str</code> <p>Input data format - 'directory' (default), 'coco', or 'yolo'. - 'directory': Standard directory structure with separate images_dir and labels_dir - 'coco': COCO JSON format (labels_dir should be path to instances.json) - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)</p> <code>'directory'</code> <code>num_channels</code> <code>int</code> <p>Number of input channels. If None, auto-detected. Defaults to 3.</p> <code>3</code> <code>model</code> <code>Module</code> <p>Predefined model. If None, a new model is created.</p> <code>None</code> <code>pretrained</code> <code>bool</code> <p>Whether to use pretrained backbone. This is ignored if pretrained_model_path is provided. Defaults to True.</p> <code>True</code> <code>pretrained_model_path</code> <code>str</code> <p>Path to a .pth file to load as a pretrained model for continued training. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 4.</p> <code>4</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 10.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate. Defaults to 0.005.</p> <code>0.005</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>val_split</code> <code>float</code> <p>Fraction of data to use for validation (0-1). Defaults to 0.2.</p> <code>0.2</code> <code>visualize</code> <code>bool</code> <p>Whether to generate visualizations of model predictions. Defaults to False.</p> <code>False</code> <code>resume_training</code> <code>bool</code> <p>If True and pretrained_model_path is provided, will try to load optimizer and scheduler states as well. Defaults to False.</p> <code>False</code> <code>print_freq</code> <code>int</code> <p>Frequency of printing training progress. Defaults to 10.</p> <code>10</code> <code>device</code> <code>device</code> <p>Device to train on. If None, uses CUDA if available.</p> <code>None</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. If None, uses 0 on macOS and Windows, 8 otherwise.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, prints detailed training progress. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If pretrained_model_path is provided but file doesn't exist.</p> <code>RuntimeError</code> <p>If there's an issue loading the pretrained model.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_MaskRCNN_model(\n    images_dir: str,\n    labels_dir: str,\n    output_dir: str,\n    input_format: str = \"directory\",\n    num_channels: int = 3,\n    model: Optional[torch.nn.Module] = None,\n    pretrained: bool = True,\n    pretrained_model_path: Optional[str] = None,\n    batch_size: int = 4,\n    num_epochs: int = 10,\n    learning_rate: float = 0.005,\n    seed: int = 42,\n    val_split: float = 0.2,\n    visualize: bool = False,\n    resume_training: bool = False,\n    print_freq: int = 10,\n    device: Optional[torch.device] = None,\n    num_workers: Optional[int] = None,\n    verbose: bool = True,\n) -&gt; torch.nn.Module:\n    \"\"\"Train and evaluate Mask R-CNN model for instance segmentation.\n\n    This function trains a Mask R-CNN model for instance segmentation using the\n    provided dataset. It supports loading a pretrained model to either initialize\n    the backbone or to continue training from a specific checkpoint.\n\n    Args:\n        images_dir (str): Directory containing image GeoTIFF files (for 'directory' format),\n            or root directory containing images/ subdirectory (for 'yolo' format),\n            or directory containing images (for 'coco' format).\n        labels_dir (str): Directory containing label GeoTIFF files (for 'directory' format),\n            or path to COCO annotations JSON file (for 'coco' format),\n            or not used (for 'yolo' format - labels are in images_dir/labels/).\n        output_dir (str): Directory to save model checkpoints and results.\n        input_format (str): Input data format - 'directory' (default), 'coco', or 'yolo'.\n            - 'directory': Standard directory structure with separate images_dir and labels_dir\n            - 'coco': COCO JSON format (labels_dir should be path to instances.json)\n            - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)\n        num_channels (int, optional): Number of input channels. If None, auto-detected.\n            Defaults to 3.\n        model (torch.nn.Module, optional): Predefined model. If None, a new model is created.\n        pretrained (bool): Whether to use pretrained backbone. This is ignored if\n            pretrained_model_path is provided. Defaults to True.\n        pretrained_model_path (str, optional): Path to a .pth file to load as a\n            pretrained model for continued training. Defaults to None.\n        batch_size (int): Batch size for training. Defaults to 4.\n        num_epochs (int): Number of training epochs. Defaults to 10.\n        learning_rate (float): Initial learning rate. Defaults to 0.005.\n        seed (int): Random seed for reproducibility. Defaults to 42.\n        val_split (float): Fraction of data to use for validation (0-1). Defaults to 0.2.\n        visualize (bool): Whether to generate visualizations of model predictions.\n            Defaults to False.\n        resume_training (bool): If True and pretrained_model_path is provided,\n            will try to load optimizer and scheduler states as well. Defaults to False.\n        print_freq (int): Frequency of printing training progress. Defaults to 10.\n        device (torch.device): Device to train on. If None, uses CUDA if available.\n        num_workers (int): Number of workers for data loading. If None, uses 0 on macOS and Windows, 8 otherwise.\n        verbose (bool): If True, prints detailed training progress. Defaults to True.\n    Returns:\n        None: Model weights are saved to output_dir.\n\n    Raises:\n        FileNotFoundError: If pretrained_model_path is provided but file doesn't exist.\n        RuntimeError: If there's an issue loading the pretrained model.\n    \"\"\"\n\n    import datetime\n\n    # Set random seeds for reproducibility\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Create output directory\n    os.makedirs(os.path.abspath(output_dir), exist_ok=True)\n\n    # Get device\n    if device is None:\n        device = get_device()\n    print(f\"Using device: {device}\")\n\n    # Get all image and label files based on input format\n    if input_format.lower() == \"coco\":\n        # Parse COCO format annotations\n        if verbose:\n            print(f\"Loading COCO format annotations from {labels_dir}\")\n        # For COCO format, labels_dir is path to instances.json\n        # Labels are typically in a \"labels\" directory parallel to \"annotations\"\n        coco_root = os.path.dirname(os.path.dirname(labels_dir))  # Go up two levels\n        labels_directory = os.path.join(coco_root, \"labels\")\n        image_files, label_files = parse_coco_annotations(\n            labels_dir, images_dir, labels_directory\n        )\n    elif input_format.lower() == \"yolo\":\n        # Parse YOLO format annotations\n        if verbose:\n            print(f\"Loading YOLO format data from {images_dir}\")\n        image_files, label_files = parse_yolo_annotations(images_dir)\n    else:\n        # Default: directory format\n        # Support multiple image formats: GeoTIFF, PNG, JPG, JPEG, TIF, TIFF\n        image_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n        label_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n\n        image_files = sorted(\n            [\n                os.path.join(images_dir, f)\n                for f in os.listdir(images_dir)\n                if f.lower().endswith(image_extensions)\n            ]\n        )\n        label_files = sorted(\n            [\n                os.path.join(labels_dir, f)\n                for f in os.listdir(labels_dir)\n                if f.lower().endswith(label_extensions)\n            ]\n        )\n\n        # Ensure matching files\n        if len(image_files) != len(label_files):\n            print(\"Warning: Number of image files and label files don't match!\")\n            # Find matching files by basename\n            basenames = [os.path.basename(f) for f in image_files]\n            label_files = [\n                os.path.join(labels_dir, os.path.basename(f))\n                for f in image_files\n                if os.path.exists(os.path.join(labels_dir, os.path.basename(f)))\n            ]\n            image_files = [\n                f\n                for f, b in zip(image_files, basenames)\n                if os.path.exists(os.path.join(labels_dir, b))\n            ]\n            print(f\"Using {len(image_files)} matching files\")\n\n    print(f\"Found {len(image_files)} image files and {len(label_files)} label files\")\n\n    # Split data into train and validation sets\n    train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n        image_files, label_files, test_size=val_split, random_state=seed\n    )\n\n    print(f\"Training on {len(train_imgs)} images, validating on {len(val_imgs)} images\")\n\n    # Create datasets\n    train_dataset = ObjectDetectionDataset(\n        train_imgs, train_labels, transforms=get_transform(train=True)\n    )\n    val_dataset = ObjectDetectionDataset(\n        val_imgs, val_labels, transforms=get_transform(train=False)\n    )\n\n    # Create data loaders\n    # Use num_workers=0 on macOS and Windows to avoid multiprocessing issues\n    # Windows often has issues with multiprocessing in Jupyter notebooks\n    # Increase num_workers for better data loading performance\n    if num_workers is None:\n        num_workers = 0 if platform.system() in [\"Darwin\", \"Windows\"] else 8\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        collate_fn=collate_fn,\n        num_workers=num_workers,\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=collate_fn,\n        num_workers=num_workers,\n    )\n\n    # Initialize model (2 classes: background and building)\n    if model is None:\n        model = get_instance_segmentation_model(\n            num_classes=2, num_channels=num_channels, pretrained=pretrained\n        )\n    model.to(device)\n\n    # Set up optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(\n        params, lr=learning_rate, momentum=0.9, weight_decay=0.0005\n    )\n\n    # Set up learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n\n    # Initialize training variables\n    start_epoch = 0\n    best_iou = 0\n\n    # Initialize training history\n    training_history = {\n        \"train_loss\": [],\n        \"val_loss\": [],\n        \"val_iou\": [],\n        \"epochs\": [],\n        \"lr\": [],\n    }\n\n    # Load pretrained model if provided\n    if pretrained_model_path:\n        if not os.path.exists(pretrained_model_path):\n            raise FileNotFoundError(\n                f\"Pretrained model file not found: {pretrained_model_path}\"\n            )\n\n        print(f\"Loading pretrained model from: {pretrained_model_path}\")\n        try:\n            # Check if it's a full checkpoint or just model weights\n            checkpoint = torch.load(pretrained_model_path, map_location=device)\n\n            if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n                # It's a checkpoint with extra information\n                model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n                if resume_training:\n                    # Resume from checkpoint\n                    start_epoch = checkpoint.get(\"epoch\", 0) + 1\n                    best_iou = checkpoint.get(\"best_iou\", 0)\n\n                    if \"optimizer_state_dict\" in checkpoint:\n                        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n                    if \"scheduler_state_dict\" in checkpoint:\n                        lr_scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n\n                    print(f\"Resuming training from epoch {start_epoch}\")\n                    print(f\"Previous best IoU: {best_iou:.4f}\")\n            else:\n                # Assume it's just the model weights\n                model.load_state_dict(checkpoint)\n\n            print(\"Pretrained model loaded successfully\")\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load pretrained model: {str(e)}\")\n\n    # Training loop\n    for epoch in range(start_epoch, num_epochs):\n        # Train one epoch\n        train_loss = train_one_epoch(\n            model, optimizer, train_loader, device, epoch, print_freq, verbose\n        )\n\n        # Update learning rate\n        lr_scheduler.step()\n\n        # Evaluate\n        eval_metrics = evaluate(model, val_loader, device)\n\n        # Record training history\n        training_history[\"train_loss\"].append(train_loss)\n        training_history[\"val_loss\"].append(eval_metrics[\"loss\"])\n        training_history[\"val_iou\"].append(eval_metrics[\"IoU\"])\n        training_history[\"epochs\"].append(epoch + 1)\n        training_history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n\n        # Print metrics\n        print(\n            f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {eval_metrics['loss']:.4f}, Val IoU: {eval_metrics['IoU']:.4f}\"\n        )\n\n        # Save best model\n        if eval_metrics[\"IoU\"] &gt; best_iou:\n            best_iou = eval_metrics[\"IoU\"]\n            print(f\"Saving best model with IoU: {best_iou:.4f}\")\n            torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n\n    # Save final model\n    torch.save(model.state_dict(), os.path.join(output_dir, \"final_model.pth\"))\n\n    # Save training history\n    torch.save(training_history, os.path.join(output_dir, \"training_history.pth\"))\n\n    # Load best model for evaluation and visualization\n    model.load_state_dict(torch.load(os.path.join(output_dir, \"best_model.pth\")))\n\n    # Final evaluation\n    final_metrics = evaluate(model, val_loader, device)\n    print(\n        f\"Final Evaluation - Loss: {final_metrics['loss']:.4f}, IoU: {final_metrics['IoU']:.4f}\"\n    )\n\n    # Visualize results\n    if visualize:\n        print(\"Generating visualizations...\")\n        visualize_predictions(\n            model,\n            val_dataset,\n            device,\n            num_samples=5,\n            output_dir=os.path.join(output_dir, \"visualizations\"),\n        )\n\n    # Save training summary\n    with open(os.path.join(output_dir, \"training_summary.txt\"), \"w\") as f:\n        f.write(\n            f\"Training completed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        )\n        f.write(f\"Total epochs: {num_epochs}\\n\")\n        f.write(f\"Best validation IoU: {best_iou:.4f}\\n\")\n        f.write(f\"Final validation IoU: {final_metrics['IoU']:.4f}\\n\")\n        f.write(f\"Final validation loss: {final_metrics['loss']:.4f}\\n\")\n\n        if pretrained_model_path:\n            f.write(f\"Started from pretrained model: {pretrained_model_path}\\n\")\n            if resume_training:\n                f.write(f\"Resumed training from epoch {start_epoch}\\n\")\n\n    print(f\"Training complete! Trained model saved to {output_dir}\")\n</code></pre>"},{"location":"train/#geoai.train.train_instance_segmentation_model","title":"<code>train_instance_segmentation_model(images_dir, labels_dir, output_dir, input_format='directory', num_classes=2, num_channels=3, batch_size=4, num_epochs=10, learning_rate=0.005, seed=42, val_split=0.2, visualize=False, device=None, verbose=True, **kwargs)</code>","text":"<p>Train an instance segmentation model using Mask R-CNN.</p> <p>This is a wrapper function for train_MaskRCNN_model with clearer naming.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing image GeoTIFF files (for 'directory' format), or root directory containing images/ subdirectory (for 'yolo' format), or directory containing images (for 'coco' format).</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing label GeoTIFF files (for 'directory' format), or path to COCO annotations JSON file (for 'coco' format), or not used (for 'yolo' format - labels are in images_dir/labels/).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model checkpoints and results.</p> required <code>input_format</code> <code>str</code> <p>Input data format - 'directory' (default), 'coco', or 'yolo'. - 'directory': Standard directory structure with separate images_dir and labels_dir - 'coco': COCO JSON format (labels_dir should be path to instances.json) - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)</p> <code>'directory'</code> <code>num_classes</code> <code>int</code> <p>Number of classes (including background). Defaults to 2.</p> <code>2</code> <code>num_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 4.</p> <code>4</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 10.</p> <code>10</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate. Defaults to 0.005.</p> <code>0.005</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>val_split</code> <code>float</code> <p>Fraction of data to use for validation (0-1). Defaults to 0.2.</p> <code>0.2</code> <code>visualize</code> <code>bool</code> <p>Whether to generate visualizations. Defaults to False.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to train on. If None, uses CUDA if available.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, prints detailed training progress. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to train_MaskRCNN_model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>None</code> <code>Module</code> <p>Model weights are saved to output_dir.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_instance_segmentation_model(\n    images_dir: str,\n    labels_dir: str,\n    output_dir: str,\n    input_format: str = \"directory\",\n    num_classes: int = 2,\n    num_channels: int = 3,\n    batch_size: int = 4,\n    num_epochs: int = 10,\n    learning_rate: float = 0.005,\n    seed: int = 42,\n    val_split: float = 0.2,\n    visualize: bool = False,\n    device: Optional[torch.device] = None,\n    verbose: bool = True,\n    **kwargs: Any,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Train an instance segmentation model using Mask R-CNN.\n\n    This is a wrapper function for train_MaskRCNN_model with clearer naming.\n\n    Args:\n        images_dir (str): Directory containing image GeoTIFF files (for 'directory' format),\n            or root directory containing images/ subdirectory (for 'yolo' format),\n            or directory containing images (for 'coco' format).\n        labels_dir (str): Directory containing label GeoTIFF files (for 'directory' format),\n            or path to COCO annotations JSON file (for 'coco' format),\n            or not used (for 'yolo' format - labels are in images_dir/labels/).\n        output_dir (str): Directory to save model checkpoints and results.\n        input_format (str): Input data format - 'directory' (default), 'coco', or 'yolo'.\n            - 'directory': Standard directory structure with separate images_dir and labels_dir\n            - 'coco': COCO JSON format (labels_dir should be path to instances.json)\n            - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)\n        num_classes (int): Number of classes (including background). Defaults to 2.\n        num_channels (int): Number of input channels. Defaults to 3.\n        batch_size (int): Batch size for training. Defaults to 4.\n        num_epochs (int): Number of training epochs. Defaults to 10.\n        learning_rate (float): Initial learning rate. Defaults to 0.005.\n        seed (int): Random seed for reproducibility. Defaults to 42.\n        val_split (float): Fraction of data to use for validation (0-1). Defaults to 0.2.\n        visualize (bool): Whether to generate visualizations. Defaults to False.\n        device (torch.device): Device to train on. If None, uses CUDA if available.\n        verbose (bool): If True, prints detailed training progress. Defaults to True.\n        **kwargs: Additional arguments passed to train_MaskRCNN_model.\n\n    Returns:\n        None: Model weights are saved to output_dir.\n    \"\"\"\n    # Create model with the specified number of classes\n    model = get_instance_segmentation_model(\n        num_classes=num_classes, num_channels=num_channels, pretrained=True\n    )\n\n    return train_MaskRCNN_model(\n        images_dir=images_dir,\n        labels_dir=labels_dir,\n        output_dir=output_dir,\n        input_format=input_format,\n        num_channels=num_channels,\n        model=model,\n        batch_size=batch_size,\n        num_epochs=num_epochs,\n        learning_rate=learning_rate,\n        seed=seed,\n        val_split=val_split,\n        visualize=visualize,\n        device=device,\n        verbose=verbose,\n        **kwargs,\n    )\n</code></pre>"},{"location":"train/#geoai.train.train_one_epoch","title":"<code>train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10, verbose=True)</code>","text":"<p>Train the model for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to use.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>device</code> <code>device</code> <p>Device to train on.</p> required <code>epoch</code> <code>int</code> <p>Current epoch number.</p> required <code>print_freq</code> <code>int</code> <p>How often to print progress.</p> <code>10</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed progress.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average loss for the epoch.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_one_epoch(\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    data_loader: DataLoader,\n    device: torch.device,\n    epoch: int,\n    print_freq: int = 10,\n    verbose: bool = True,\n) -&gt; float:\n    \"\"\"\n    Train the model for one epoch.\n\n    Args:\n        model (torch.nn.Module): The model to train.\n        optimizer (torch.optim.Optimizer): The optimizer to use.\n        data_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        device (torch.device): Device to train on.\n        epoch (int): Current epoch number.\n        print_freq (int): How often to print progress.\n        verbose (bool): Whether to print detailed progress.\n\n    Returns:\n        float: Average loss for the epoch.\n    \"\"\"\n    model.train()\n    total_loss = 0\n\n    start_time = time.time()\n\n    for i, (images, targets) in enumerate(data_loader):\n        # Move images and targets to device\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        # Forward pass\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        # Backward pass\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        # Track loss\n        total_loss += losses.item()\n\n        # Print progress\n        if i % print_freq == 0:\n            elapsed_time = time.time() - start_time\n            if verbose:\n                print(\n                    f\"Epoch: {epoch + 1}, Batch: {i + 1}/{len(data_loader)}, Loss: {losses.item():.4f}, Time: {elapsed_time:.2f}s\"\n                )\n            start_time = time.time()\n\n    # Calculate average loss\n    avg_loss = total_loss / len(data_loader)\n    return avg_loss\n</code></pre>"},{"location":"train/#geoai.train.train_segmentation_model","title":"<code>train_segmentation_model(images_dir, labels_dir, output_dir, input_format='directory', architecture='unet', encoder_name='resnet34', encoder_weights='imagenet', num_channels=3, num_classes=2, batch_size=8, num_epochs=50, learning_rate=0.001, weight_decay=0.0001, seed=42, val_split=0.2, print_freq=10, verbose=True, save_best_only=True, plot_curves=False, device=None, checkpoint_path=None, resume_training=False, target_size=None, resize_mode='resize', num_workers=None, early_stopping_patience=None, train_transforms=None, val_transforms=None, **kwargs)</code>","text":"<p>Train a semantic segmentation model for object detection using segmentation-models-pytorch.</p> <p>This function trains a semantic segmentation model for object detection (e.g., building detection) using models from the segmentation-models-pytorch library. Unlike instance segmentation (Mask R-CNN), this approach treats the task as pixel-level binary classification.</p> <p>Parameters:</p> Name Type Description Default <code>images_dir</code> <code>str</code> <p>Directory containing image GeoTIFF files (for 'directory' format), or root directory containing images/ subdirectory (for 'yolo' format), or directory containing images (for 'coco' format).</p> required <code>labels_dir</code> <code>str</code> <p>Directory containing label GeoTIFF files (for 'directory' format), or path to COCO annotations JSON file (for 'coco' format), or not used (for 'yolo' format - labels are in images_dir/labels/).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save model checkpoints and results.</p> required <code>input_format</code> <code>str</code> <p>Input data format - 'directory' (default), 'coco', or 'yolo'. - 'directory': Standard directory structure with separate images_dir and labels_dir - 'coco': COCO JSON format (labels_dir should be path to instances.json) - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)</p> <code>'directory'</code> <code>architecture</code> <code>str</code> <p>Model architecture ('unet', 'deeplabv3', 'deeplabv3plus', 'fpn', 'pspnet', 'linknet', 'manet'). Defaults to 'unet'.</p> <code>'unet'</code> <code>encoder_name</code> <code>str</code> <p>Encoder backbone name (e.g., 'resnet34', 'resnet50', 'efficientnet-b0'). Defaults to 'resnet34'.</p> <code>'resnet34'</code> <code>encoder_weights</code> <code>str</code> <p>Encoder pretrained weights ('imagenet' or None). Defaults to 'imagenet'.</p> <code>'imagenet'</code> <code>num_channels</code> <code>int</code> <p>Number of input channels. Defaults to 3.</p> <code>3</code> <code>num_classes</code> <code>int</code> <p>Number of output classes (typically 2 for binary segmentation). Defaults to 2.</p> <code>2</code> <code>batch_size</code> <code>int</code> <p>Batch size for training. Defaults to 8.</p> <code>8</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs. Defaults to 50.</p> <code>50</code> <code>learning_rate</code> <code>float</code> <p>Initial learning rate. Defaults to 0.001.</p> <code>0.001</code> <code>weight_decay</code> <code>float</code> <p>Weight decay for optimizer. Defaults to 1e-4.</p> <code>0.0001</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Defaults to 42.</p> <code>42</code> <code>val_split</code> <code>float</code> <p>Fraction of data to use for validation (0-1). Defaults to 0.2.</p> <code>0.2</code> <code>print_freq</code> <code>int</code> <p>Frequency of printing training progress. Defaults to 10.</p> <code>10</code> <code>verbose</code> <code>bool</code> <p>If True, prints detailed training progress. Defaults to True.</p> <code>True</code> <code>save_best_only</code> <code>bool</code> <p>If True, only saves the best model. Otherwise saves all checkpoints. Defaults to True.</p> <code>True</code> <code>plot_curves</code> <code>bool</code> <p>If True, plots training curves. Defaults to False.</p> <code>False</code> <code>device</code> <code>device</code> <p>Device to train on. If None, uses CUDA if available.</p> <code>None</code> <code>checkpoint_path</code> <code>str</code> <p>Path to a checkpoint file to load for resuming training. If provided, will load model weights and optionally optimizer/scheduler state.</p> <code>None</code> <code>resume_training</code> <code>bool</code> <p>If True and checkpoint_path is provided, will resume training from the checkpoint including optimizer and scheduler state. Defaults to False.</p> <code>False</code> <code>target_size</code> <code>tuple</code> <p>Target size (height, width) for standardizing images. If None, the function will automatically detect if images have varying sizes and set a default target_size of (512, 512) to prevent batching errors. To disable automatic resizing, set this parameter explicitly. Example: (512, 512). Defaults to None.</p> <code>None</code> <code>resize_mode</code> <code>str</code> <p>How to handle size standardization when target_size is specified. 'resize' - Resize images to target_size (may change aspect ratio) 'pad' - Pad images to target_size (preserves aspect ratio). Defaults to 'resize'.</p> <code>'resize'</code> <code>num_workers</code> <code>int</code> <p>Number of workers for data loading. If None, uses 0 on macOS and Windows, 8 otherwise. Both image and mask should be torch.Tensor objects. The image tensor is expected to be in CHW format (channels, height, width), and the mask tensor in HW format (height, width). If None, uses default transforms (horizontal flip with 0.5 probability). Defaults to None.</p> <code>None</code> <code>val_transforms</code> <code>callable</code> <p>Custom transforms for validation data. Should be a callable that accepts (image, mask) tensors and returns transformed (image, mask). The image tensor is expected to be in CHW format (channels, height, width), and the mask tensor in HW format (height, width). Both image and mask should be torch.Tensor objects. If None, uses default transforms (horizontal flip with 0.5 probability). Defaults to None.</p> <code>None</code> <code>val_transforms</code> <code>callable</code> <p>Custom transforms for validation data. Should be a callable that accepts (image, mask) tensors and returns transformed (image, mask). If None, uses default transforms (no augmentation). Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to smp.create_model().</p> <code>{}</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If segmentation-models-pytorch is not installed.</p> <code>FileNotFoundError</code> <p>If input directories don't exist or contain no matching files.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_segmentation_model(\n    images_dir: str,\n    labels_dir: str,\n    output_dir: str,\n    input_format: str = \"directory\",\n    architecture: str = \"unet\",\n    encoder_name: str = \"resnet34\",\n    encoder_weights: Optional[str] = \"imagenet\",\n    num_channels: int = 3,\n    num_classes: int = 2,\n    batch_size: int = 8,\n    num_epochs: int = 50,\n    learning_rate: float = 0.001,\n    weight_decay: float = 1e-4,\n    seed: int = 42,\n    val_split: float = 0.2,\n    print_freq: int = 10,\n    verbose: bool = True,\n    save_best_only: bool = True,\n    plot_curves: bool = False,\n    device: Optional[torch.device] = None,\n    checkpoint_path: Optional[str] = None,\n    resume_training: bool = False,\n    target_size: Optional[Tuple[int, int]] = None,\n    resize_mode: str = \"resize\",\n    num_workers: Optional[int] = None,\n    early_stopping_patience: Optional[int] = None,\n    train_transforms: Optional[Callable] = None,\n    val_transforms: Optional[Callable] = None,\n    **kwargs: Any,\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Train a semantic segmentation model for object detection using segmentation-models-pytorch.\n\n    This function trains a semantic segmentation model for object detection (e.g., building detection)\n    using models from the segmentation-models-pytorch library. Unlike instance segmentation (Mask R-CNN),\n    this approach treats the task as pixel-level binary classification.\n\n    Args:\n        images_dir (str): Directory containing image GeoTIFF files (for 'directory' format),\n            or root directory containing images/ subdirectory (for 'yolo' format),\n            or directory containing images (for 'coco' format).\n        labels_dir (str): Directory containing label GeoTIFF files (for 'directory' format),\n            or path to COCO annotations JSON file (for 'coco' format),\n            or not used (for 'yolo' format - labels are in images_dir/labels/).\n        output_dir (str): Directory to save model checkpoints and results.\n        input_format (str): Input data format - 'directory' (default), 'coco', or 'yolo'.\n            - 'directory': Standard directory structure with separate images_dir and labels_dir\n            - 'coco': COCO JSON format (labels_dir should be path to instances.json)\n            - 'yolo': YOLO format (images_dir is root with images/ and labels/ subdirectories)\n        architecture (str): Model architecture ('unet', 'deeplabv3', 'deeplabv3plus', 'fpn',\n            'pspnet', 'linknet', 'manet'). Defaults to 'unet'.\n        encoder_name (str): Encoder backbone name (e.g., 'resnet34', 'resnet50', 'efficientnet-b0').\n            Defaults to 'resnet34'.\n        encoder_weights (str): Encoder pretrained weights ('imagenet' or None). Defaults to 'imagenet'.\n        num_channels (int): Number of input channels. Defaults to 3.\n        num_classes (int): Number of output classes (typically 2 for binary segmentation). Defaults to 2.\n        batch_size (int): Batch size for training. Defaults to 8.\n        num_epochs (int): Number of training epochs. Defaults to 50.\n        learning_rate (float): Initial learning rate. Defaults to 0.001.\n        weight_decay (float): Weight decay for optimizer. Defaults to 1e-4.\n        seed (int): Random seed for reproducibility. Defaults to 42.\n        val_split (float): Fraction of data to use for validation (0-1). Defaults to 0.2.\n        print_freq (int): Frequency of printing training progress. Defaults to 10.\n        verbose (bool): If True, prints detailed training progress. Defaults to True.\n        save_best_only (bool): If True, only saves the best model. Otherwise saves all checkpoints.\n            Defaults to True.\n        plot_curves (bool): If True, plots training curves. Defaults to False.\n        device (torch.device): Device to train on. If None, uses CUDA if available.\n        checkpoint_path (str, optional): Path to a checkpoint file to load for resuming training.\n            If provided, will load model weights and optionally optimizer/scheduler state.\n        resume_training (bool): If True and checkpoint_path is provided, will resume training\n            from the checkpoint including optimizer and scheduler state. Defaults to False.\n        target_size (tuple, optional): Target size (height, width) for standardizing images.\n            If None, the function will automatically detect if images have varying sizes and set\n            a default target_size of (512, 512) to prevent batching errors. To disable automatic\n            resizing, set this parameter explicitly. Example: (512, 512). Defaults to None.\n        resize_mode (str): How to handle size standardization when target_size is specified.\n            'resize' - Resize images to target_size (may change aspect ratio)\n            'pad' - Pad images to target_size (preserves aspect ratio). Defaults to 'resize'.\n        num_workers (int): Number of workers for data loading. If None, uses 0 on macOS and Windows, 8 otherwise.\n            Both image and mask should be torch.Tensor objects. The image tensor is expected to be in\n            CHW format (channels, height, width), and the mask tensor in HW format (height, width).\n            If None, uses default transforms (horizontal flip with 0.5 probability). Defaults to None.\n        val_transforms (callable, optional): Custom transforms for validation data.\n            Should be a callable that accepts (image, mask) tensors and returns transformed (image, mask).\n            The image tensor is expected to be in CHW format (channels, height, width), and the mask tensor in HW format (height, width).\n            Both image and mask should be torch.Tensor objects. If None, uses default transforms\n            (horizontal flip with 0.5 probability). Defaults to None.\n        val_transforms (callable, optional): Custom transforms for validation data.\n            Should be a callable that accepts (image, mask) tensors and returns transformed (image, mask).\n            If None, uses default transforms (no augmentation). Defaults to None.\n        **kwargs: Additional arguments passed to smp.create_model().\n    Returns:\n        None: Model weights are saved to output_dir.\n\n    Raises:\n        ImportError: If segmentation-models-pytorch is not installed.\n        FileNotFoundError: If input directories don't exist or contain no matching files.\n    \"\"\"\n    import datetime\n\n    if not SMP_AVAILABLE:\n        raise ImportError(\n            \"segmentation-models-pytorch is not installed. \"\n            \"Please install it with: pip install segmentation-models-pytorch\"\n        )\n\n    # Set random seeds for reproducibility\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n    # Create output directory\n    os.makedirs(os.path.abspath(output_dir), exist_ok=True)\n\n    # Get device\n    if device is None:\n        device = get_device()\n    print(f\"Using device: {device}\")\n\n    # Get all image and label files based on input format\n    if input_format.lower() == \"coco\":\n        # Parse COCO format annotations\n        if verbose:\n            print(f\"Loading COCO format annotations from {labels_dir}\")\n        # For COCO format, labels_dir is path to instances.json\n        # Labels are typically in a \"labels\" directory parallel to \"annotations\"\n        coco_root = os.path.dirname(os.path.dirname(labels_dir))  # Go up two levels\n        labels_directory = os.path.join(coco_root, \"labels\")\n        image_files, label_files = parse_coco_annotations(\n            labels_dir, images_dir, labels_directory\n        )\n    elif input_format.lower() == \"yolo\":\n        # Parse YOLO format annotations\n        if verbose:\n            print(f\"Loading YOLO format data from {images_dir}\")\n        image_files, label_files = parse_yolo_annotations(images_dir)\n    else:\n        # Default: directory format\n        # Support multiple image formats: GeoTIFF, PNG, JPG, JPEG, TIF, TIFF\n        image_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n        label_extensions = (\".tif\", \".tiff\", \".png\", \".jpg\", \".jpeg\")\n\n        image_files = sorted(\n            [\n                os.path.join(images_dir, f)\n                for f in os.listdir(images_dir)\n                if f.lower().endswith(image_extensions)\n            ]\n        )\n        label_files = sorted(\n            [\n                os.path.join(labels_dir, f)\n                for f in os.listdir(labels_dir)\n                if f.lower().endswith(label_extensions)\n            ]\n        )\n\n        # Ensure matching files\n        if len(image_files) != len(label_files):\n            print(\"Warning: Number of image files and label files don't match!\")\n            # Find matching files by basename\n            basenames = [os.path.basename(f) for f in image_files]\n            label_files = [\n                os.path.join(labels_dir, os.path.basename(f))\n                for f in image_files\n                if os.path.exists(os.path.join(labels_dir, os.path.basename(f)))\n            ]\n            image_files = [\n                f\n                for f, b in zip(image_files, basenames)\n                if os.path.exists(os.path.join(labels_dir, b))\n            ]\n            print(f\"Using {len(image_files)} matching files\")\n\n    print(f\"Found {len(image_files)} image files and {len(label_files)} label files\")\n\n    if len(image_files) == 0:\n        raise FileNotFoundError(\"No matching image and label files found\")\n\n    # Split data into train and validation sets\n    train_imgs, val_imgs, train_labels, val_labels = train_test_split(\n        image_files, label_files, test_size=val_split, random_state=seed\n    )\n\n    print(f\"Training on {len(train_imgs)} images, validating on {len(val_imgs)} images\")\n\n    # Auto-detect image sizes and set target_size if needed\n    if target_size is None:\n        print(\"Checking image sizes for compatibility...\")\n\n        # Sample a few images to check size consistency\n        sample_images = train_imgs[: min(5, len(train_imgs))]\n        image_sizes = []\n\n        for img_path in sample_images:\n            try:\n                if img_path.lower().endswith((\".tif\", \".tiff\")):\n                    with rasterio.open(img_path) as src:\n                        height, width = src.height, src.width\n                else:\n                    with Image.open(img_path) as img:\n                        width, height = img.size\n                image_sizes.append((height, width))\n            except Exception as e:\n                print(f\"Warning: Could not read image {img_path}: {e}\")\n                continue\n\n        # Check if all images have the same size\n        if len(image_sizes) == 0:\n            print(\n                \"Warning: Could not read any sample images. Setting target_size to (512, 512) as a safe default.\"\n            )\n            target_size = (512, 512)\n        else:\n            unique_sizes = set(image_sizes)\n            if len(unique_sizes) &gt; 1:\n                print(\n                    f\"Warning: Found images with different sizes: {list(unique_sizes)}\"\n                )\n                print(\n                    \"Setting target_size to (512, 512) to standardize image dimensions.\"\n                )\n                print(\"This will resize all images to 512x512 pixels.\")\n                print(\"To use a different size, set target_size parameter explicitly.\")\n                target_size = (512, 512)\n            else:\n                print(f\"All sampled images have the same size: {image_sizes[0]}\")\n                print(\"No resizing needed.\")\n\n    # Create datasets\n    # Use custom transforms if provided, otherwise use default transforms\n    train_transform = (\n        train_transforms\n        if train_transforms is not None\n        else get_semantic_transform(train=True)\n    )\n    val_transform = (\n        val_transforms\n        if val_transforms is not None\n        else get_semantic_transform(train=False)\n    )\n\n    train_dataset = SemanticSegmentationDataset(\n        train_imgs,\n        train_labels,\n        transforms=train_transform,\n        num_channels=num_channels,\n        target_size=target_size,\n        resize_mode=resize_mode,\n        num_classes=num_classes,\n    )\n    val_dataset = SemanticSegmentationDataset(\n        val_imgs,\n        val_labels,\n        transforms=val_transform,\n        num_channels=num_channels,\n        target_size=target_size,\n        resize_mode=resize_mode,\n        num_classes=num_classes,\n    )\n\n    # Create data loaders\n    # Use num_workers=0 on macOS and Windows to avoid multiprocessing issues\n    # Windows often has issues with multiprocessing in Jupyter notebooks\n    # Increase num_workers for better data loading performance\n    if num_workers is None:\n        num_workers = 0 if platform.system() in [\"Darwin\", \"Windows\"] else 8\n\n    try:\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=batch_size,\n            shuffle=True,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=True,\n        )\n\n        # Test the data loader by loading one batch to catch size mismatch errors early\n        print(\"Testing data loader...\")\n        try:\n            next(iter(train_loader))\n            print(\"Data loader test passed.\")\n        except RuntimeError as e:\n            if \"stack expects each tensor to be equal size\" in str(e):\n                raise RuntimeError(\n                    \"Images have different sizes and cannot be batched together. \"\n                    \"Please set target_size parameter to standardize image dimensions. \"\n                    \"Example: target_size=(512, 512). \"\n                    f\"Original error: {str(e)}\"\n                ) from e\n            else:\n                raise\n\n    except Exception as e:\n        if \"stack expects each tensor to be equal size\" in str(e):\n            raise RuntimeError(\n                \"Images have different sizes and cannot be batched together. \"\n                \"Please set target_size parameter to standardize image dimensions. \"\n                \"Example: target_size=(512, 512). \"\n                f\"Original error: {str(e)}\"\n            ) from e\n        else:\n            raise\n\n    # Initialize model\n    model = get_smp_model(\n        architecture=architecture,\n        encoder_name=encoder_name,\n        encoder_weights=encoder_weights,\n        in_channels=num_channels,\n        classes=num_classes,\n        activation=None,  # We'll apply softmax later\n        **kwargs,\n    )\n    model.to(device)\n\n    # Enable multi-GPU training if multiple GPUs are available\n    if torch.cuda.device_count() &gt; 1:\n        print(f\"Using {torch.cuda.device_count()} GPUs for training\")\n        model = torch.nn.DataParallel(model)\n\n    # Set up loss function (CrossEntropyLoss for multi-class, can also use F1Loss)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    # Set up optimizer\n    optimizer = torch.optim.Adam(\n        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n    )\n\n    # Set up learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode=\"min\", factor=0.5, patience=5\n    )\n\n    # Initialize tracking variables\n    best_iou = 0\n    train_losses = []\n    val_losses = []\n    val_ious = []\n    val_f1s = []\n    val_precisions = []\n    val_recalls = []\n    start_epoch = 0\n    epochs_without_improvement = 0\n\n    # Load checkpoint if provided\n    if checkpoint_path is not None:\n        if not os.path.exists(checkpoint_path):\n            raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n\n        print(f\"Loading checkpoint from: {checkpoint_path}\")\n        try:\n            checkpoint = torch.load(checkpoint_path, map_location=device)\n\n            if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n                # Load model state\n                model.load_state_dict(checkpoint[\"model_state_dict\"])\n\n                if resume_training:\n                    # Resume training from checkpoint\n                    start_epoch = checkpoint.get(\"epoch\", 0) + 1\n                    best_iou = checkpoint.get(\"best_iou\", 0)\n\n                    # Load optimizer state if available\n                    if \"optimizer_state_dict\" in checkpoint:\n                        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n                    # Load scheduler state if available\n                    if \"scheduler_state_dict\" in checkpoint:\n                        lr_scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n\n                    # Load training history if available\n                    if \"train_losses\" in checkpoint:\n                        train_losses = checkpoint[\"train_losses\"]\n                    if \"val_losses\" in checkpoint:\n                        val_losses = checkpoint[\"val_losses\"]\n                    if \"val_ious\" in checkpoint:\n                        val_ious = checkpoint[\"val_ious\"]\n                    if \"val_f1s\" in checkpoint:\n                        val_f1s = checkpoint[\"val_f1s\"]\n                    # Also check for old val_dices format for backward compatibility\n                    elif \"val_dices\" in checkpoint:\n                        val_f1s = checkpoint[\"val_dices\"]\n                    if \"val_precisions\" in checkpoint:\n                        val_precisions = checkpoint[\"val_precisions\"]\n                    if \"val_recalls\" in checkpoint:\n                        val_recalls = checkpoint[\"val_recalls\"]\n\n                    print(f\"Resuming training from epoch {start_epoch}\")\n                    print(f\"Previous best IoU: {best_iou:.4f}\")\n                else:\n                    print(\"Loaded model weights only (not resuming training state)\")\n            else:\n                # Assume it's just model weights\n                model.load_state_dict(checkpoint)\n                print(\"Loaded model weights only\")\n\n        except Exception as e:\n            raise RuntimeError(f\"Failed to load checkpoint: {str(e)}\")\n\n    print(f\"Starting training with {architecture} + {encoder_name}\")\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    if start_epoch &gt; 0:\n        print(f\"Resuming from epoch {start_epoch}/{num_epochs}\")\n\n    # Training loop\n    for epoch in range(start_epoch, num_epochs):\n        # Train one epoch\n        train_loss = train_semantic_one_epoch(\n            model,\n            optimizer,\n            train_loader,\n            device,\n            epoch,\n            criterion,\n            print_freq,\n            verbose,\n        )\n        train_losses.append(train_loss)\n\n        # Evaluate on validation set\n        eval_metrics = evaluate_semantic(\n            model, val_loader, device, criterion, num_classes=num_classes\n        )\n        val_losses.append(eval_metrics[\"loss\"])\n        val_ious.append(eval_metrics[\"IoU\"])\n        val_f1s.append(eval_metrics[\"F1\"])\n        val_precisions.append(eval_metrics[\"Precision\"])\n        val_recalls.append(eval_metrics[\"Recall\"])\n\n        # Update learning rate\n        lr_scheduler.step(eval_metrics[\"loss\"])\n\n        # Print metrics\n        print(\n            f\"Epoch {epoch+1}/{num_epochs}: \"\n            f\"Train Loss: {train_loss:.4f}, \"\n            f\"Val Loss: {eval_metrics['loss']:.4f}, \"\n            f\"Val IoU: {eval_metrics['IoU']:.4f}, \"\n            f\"Val F1: {eval_metrics['F1']:.4f}, \"\n            f\"Val Precision: {eval_metrics['Precision']:.4f}, \"\n            f\"Val Recall: {eval_metrics['Recall']:.4f}\"\n        )\n\n        # Save best model and check for early stopping\n        if eval_metrics[\"IoU\"] &gt; best_iou:\n            best_iou = eval_metrics[\"IoU\"]\n            epochs_without_improvement = 0\n            print(f\"Saving best model with IoU: {best_iou:.4f}\")\n            torch.save(model.state_dict(), os.path.join(output_dir, \"best_model.pth\"))\n        else:\n            epochs_without_improvement += 1\n            if (\n                early_stopping_patience is not None\n                and epochs_without_improvement &gt;= early_stopping_patience\n            ):\n                print(\n                    f\"\\nEarly stopping triggered after {epochs_without_improvement} epochs without improvement\"\n                )\n                print(f\"Best validation IoU: {best_iou:.4f}\")\n                break\n\n        # Save checkpoint every 10 epochs (if not save_best_only)\n        if not save_best_only and ((epoch + 1) % 10 == 0 or epoch == num_epochs - 1):\n            torch.save(\n                {\n                    \"epoch\": epoch,\n                    \"model_state_dict\": model.state_dict(),\n                    \"optimizer_state_dict\": optimizer.state_dict(),\n                    \"scheduler_state_dict\": lr_scheduler.state_dict(),\n                    \"best_iou\": best_iou,\n                    \"architecture\": architecture,\n                    \"encoder_name\": encoder_name,\n                    \"num_channels\": num_channels,\n                    \"num_classes\": num_classes,\n                    \"train_losses\": train_losses,\n                    \"val_losses\": val_losses,\n                    \"val_ious\": val_ious,\n                    \"val_f1s\": val_f1s,\n                    \"val_precisions\": val_precisions,\n                    \"val_recalls\": val_recalls,\n                },\n                os.path.join(output_dir, f\"checkpoint_epoch_{epoch+1}.pth\"),\n            )\n\n    # Save final model\n    torch.save(model.state_dict(), os.path.join(output_dir, \"final_model.pth\"))\n\n    # Save training history\n    history = {\n        \"train_losses\": train_losses,\n        \"val_losses\": val_losses,\n        \"val_ious\": val_ious,\n        \"val_f1s\": val_f1s,\n        \"val_precisions\": val_precisions,\n        \"val_recalls\": val_recalls,\n    }\n    torch.save(history, os.path.join(output_dir, \"training_history.pth\"))\n\n    # Save training summary\n    with open(\n        os.path.join(output_dir, \"training_summary.txt\"), \"w\", encoding=\"utf-8\"\n    ) as f:\n        f.write(\n            f\"Training completed on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        )\n        f.write(f\"Architecture: {architecture}\\n\")\n        f.write(f\"Encoder: {encoder_name}\\n\")\n        f.write(f\"Total epochs: {num_epochs}\\n\")\n        f.write(f\"Best validation IoU: {best_iou:.4f}\\n\")\n        f.write(f\"Final validation IoU: {val_ious[-1]:.4f}\\n\")\n        f.write(f\"Final validation F1: {val_f1s[-1]:.4f}\\n\")\n        f.write(f\"Final validation Precision: {val_precisions[-1]:.4f}\\n\")\n        f.write(f\"Final validation Recall: {val_recalls[-1]:.4f}\\n\")\n        f.write(f\"Final validation loss: {val_losses[-1]:.4f}\\n\")\n\n    print(f\"Training complete! Best IoU: {best_iou:.4f}\")\n    print(f\"Models saved to {output_dir}\")\n\n    # Plot training curves\n    if plot_curves:\n        try:\n            plt.figure(figsize=(15, 5))\n\n            plt.subplot(1, 3, 1)\n            plt.plot(train_losses, label=\"Train Loss\")\n            plt.plot(val_losses, label=\"Val Loss\")\n            plt.title(\"Loss\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.legend()\n            plt.grid(True)\n\n            plt.subplot(1, 3, 2)\n            plt.plot(val_ious, label=\"Val IoU\")\n            plt.title(\"IoU Score\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"IoU\")\n            plt.legend()\n            plt.grid(True)\n\n            plt.subplot(1, 3, 3)\n            plt.plot(val_f1s, label=\"Val F1\")\n            plt.title(\"F1 Score\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"F1\")\n            plt.legend()\n            plt.grid(True)\n\n            plt.tight_layout()\n            plt.savefig(\n                os.path.join(output_dir, \"training_curves.png\"),\n                dpi=150,\n                bbox_inches=\"tight\",\n            )\n            print(\n                f\"Training curves saved to {os.path.join(output_dir, 'training_curves.png')}\"\n            )\n            plt.close()\n        except Exception as e:\n            print(f\"Could not save training curves: {e}\")\n</code></pre>"},{"location":"train/#geoai.train.train_semantic_one_epoch","title":"<code>train_semantic_one_epoch(model, optimizer, data_loader, device, epoch, criterion, print_freq=10, verbose=True)</code>","text":"<p>Train the semantic segmentation model for one epoch.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to train.</p> required <code>optimizer</code> <code>Optimizer</code> <p>The optimizer to use.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader for training data.</p> required <code>device</code> <code>device</code> <p>Device to train on.</p> required <code>epoch</code> <code>int</code> <p>Current epoch number.</p> required <code>criterion</code> <code>Any</code> <p>Loss function.</p> required <code>print_freq</code> <code>int</code> <p>How often to print progress.</p> <code>10</code> <code>verbose</code> <code>bool</code> <p>Whether to print detailed progress.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Average loss for the epoch.</p> Source code in <code>geoai/train.py</code> <pre><code>def train_semantic_one_epoch(\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    data_loader: DataLoader,\n    device: torch.device,\n    epoch: int,\n    criterion: Any,\n    print_freq: int = 10,\n    verbose: bool = True,\n) -&gt; float:\n    \"\"\"\n    Train the semantic segmentation model for one epoch.\n\n    Args:\n        model (torch.nn.Module): The model to train.\n        optimizer (torch.optim.Optimizer): The optimizer to use.\n        data_loader (torch.utils.data.DataLoader): DataLoader for training data.\n        device (torch.device): Device to train on.\n        epoch (int): Current epoch number.\n        criterion: Loss function.\n        print_freq (int): How often to print progress.\n        verbose (bool): Whether to print detailed progress.\n\n    Returns:\n        float: Average loss for the epoch.\n    \"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = len(data_loader)\n\n    start_time = time.time()\n\n    for i, (images, targets) in enumerate(data_loader):\n        # Move images and targets to device\n        images = images.to(device)\n        targets = targets.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track loss\n        total_loss += loss.item()\n\n        # Print progress\n        if i % print_freq == 0:\n            elapsed_time = time.time() - start_time\n            if verbose:\n                print(\n                    f\"Epoch: {epoch + 1}, Batch: {i + 1}/{num_batches}, Loss: {loss.item():.4f}, Time: {elapsed_time:.2f}s\"\n                )\n            start_time = time.time()\n\n    # Calculate average loss\n    avg_loss = total_loss / num_batches\n    return avg_loss\n</code></pre>"},{"location":"train/#geoai.train.visualize_predictions","title":"<code>visualize_predictions(model, dataset, device, num_samples=5, output_dir=None)</code>","text":"<p>Visualize model predictions.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>Trained model.</p> required <code>dataset</code> <code>Dataset</code> <p>Dataset to visualize.</p> required <code>device</code> <code>device</code> <p>Device to run inference on.</p> required <code>num_samples</code> <code>int</code> <p>Number of samples to visualize.</p> <code>5</code> <code>output_dir</code> <code>str</code> <p>Directory to save visualizations. If None, visualizations are displayed but not saved.</p> <code>None</code> Source code in <code>geoai/train.py</code> <pre><code>def visualize_predictions(\n    model: torch.nn.Module,\n    dataset: Dataset,\n    device: torch.device,\n    num_samples: int = 5,\n    output_dir: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Visualize model predictions.\n\n    Args:\n        model (torch.nn.Module): Trained model.\n        dataset (torch.utils.data.Dataset): Dataset to visualize.\n        device (torch.device): Device to run inference on.\n        num_samples (int): Number of samples to visualize.\n        output_dir (str, optional): Directory to save visualizations. If None,\n            visualizations are displayed but not saved.\n    \"\"\"\n    model.eval()\n\n    # Create output directory if needed\n    if output_dir:\n        os.makedirs(os.path.abspath(output_dir), exist_ok=True)\n\n    # Select random samples\n    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n\n    for idx in indices:\n        # Get image and target\n        image, target = dataset[idx]\n\n        # Convert to device and add batch dimension\n        image = image.to(device)\n        image_batch = [image]\n\n        # Get prediction\n        with torch.no_grad():\n            output = model(image_batch)[0]\n\n        # Convert image from CHW to HWC for display (first 3 bands as RGB)\n        rgb_image = image[:3].cpu().numpy()\n        rgb_image = np.transpose(rgb_image, (1, 2, 0))\n        rgb_image = np.clip(rgb_image, 0, 1)  # Ensure values are in [0,1]\n\n        # Create binary ground truth mask (combine all instances)\n        gt_masks = target[\"masks\"].cpu().numpy()\n        gt_combined = (\n            np.max(gt_masks, axis=0)\n            if len(gt_masks) &gt; 0\n            else np.zeros((image.shape[1], image.shape[2]), dtype=np.uint8)\n        )\n\n        # Create binary prediction mask (combine all instances with score &gt; 0.5)\n        pred_masks = output[\"masks\"].cpu().numpy()\n        pred_scores = output[\"scores\"].cpu().numpy()\n        high_conf_indices = pred_scores &gt; 0.5\n\n        pred_combined = np.zeros((image.shape[1], image.shape[2]), dtype=np.float32)\n        if np.any(high_conf_indices):\n            for mask in pred_masks[high_conf_indices]:\n                # Apply threshold to each predicted mask\n                binary_mask = (mask[0] &gt; 0.5).astype(np.float32)\n                # Combine with existing masks\n                pred_combined = np.maximum(pred_combined, binary_mask)\n\n        # Create figure\n        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\n        # Show RGB image\n        axs[0].imshow(rgb_image)\n        axs[0].set_title(\"RGB Image\")\n        axs[0].axis(\"off\")\n\n        # Show prediction\n        axs[1].imshow(pred_combined, cmap=\"viridis\")\n        axs[1].set_title(f\"Predicted Buildings: {np.sum(high_conf_indices)} instances\")\n        axs[1].axis(\"off\")\n\n        # Show ground truth\n        axs[2].imshow(gt_combined, cmap=\"viridis\")\n        axs[2].set_title(f\"Ground Truth: {len(gt_masks)} instances\")\n        axs[2].axis(\"off\")\n\n        plt.tight_layout()\n\n        # Save or show\n        if output_dir:\n            plt.savefig(os.path.join(output_dir, f\"prediction_{idx}.png\"))\n            plt.close()\n        else:\n            plt.show()\n</code></pre>"},{"location":"utils/","title":"utils module","text":"<p>The utils module contains common functions and classes used by the other modules.</p>"},{"location":"utils/#geoai.utils.adaptive_regularization","title":"<code>adaptive_regularization(building_polygons, simplify_tolerance=0.5, area_threshold=0.9, preserve_shape=True)</code>","text":"<p>Adaptively regularizes building footprints based on their characteristics.</p> <p>This approach determines the best regularization method for each building.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons</p> required <code>simplify_tolerance</code> <code>float</code> <p>Distance tolerance for simplification</p> <code>0.5</code> <code>area_threshold</code> <code>float</code> <p>Minimum acceptable area ratio</p> <code>0.9</code> <code>preserve_shape</code> <code>bool</code> <p>Whether to preserve overall shape for complex buildings</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def adaptive_regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n    simplify_tolerance: float = 0.5,\n    area_threshold: float = 0.9,\n    preserve_shape: bool = True,\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    Adaptively regularizes building footprints based on their characteristics.\n\n    This approach determines the best regularization method for each building.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons\n        simplify_tolerance: Distance tolerance for simplification\n        area_threshold: Minimum acceptable area ratio\n        preserve_shape: Whether to preserve overall shape for complex buildings\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Analyze the overall dataset to set appropriate parameters\n    if is_gdf := isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # Skip invalid geometries\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Measure building complexity\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Determine if the building has a clear principal direction\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Normalize angles to 0-180 range and get histogram\n        norm_angles = angles % 180\n        hist, bins = np.histogram(\n            norm_angles, bins=18, range=(0, 180), weights=segment_lengths\n        )\n\n        # Calculate direction clarity (ratio of longest direction to total)\n        direction_clarity = np.max(hist) / np.sum(hist) if np.sum(hist) &gt; 0 else 0\n\n        # Choose regularization method based on building characteristics\n        if complexity &lt; 1.2 and direction_clarity &gt; 0.5:\n            # Simple building with clear direction: use rotated rectangle\n            bin_max = np.argmax(hist)\n            bin_centers = (bins[:-1] + bins[1:]) / 2\n            dominant_angle = bin_centers[bin_max]\n\n            # Rotate to align with coordinate system\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create bounding box in rotated space\n            bounds = rotated.bounds\n            rect = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            # Rotate back\n            result = rotate(rect, dominant_angle, origin=\"centroid\")\n\n            # Quality check\n            if (\n                result.area / building.area &lt; area_threshold\n                or result.area / building.area &gt; (1.0 / area_threshold)\n            ):\n                # Too much area change, use simplified original\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n\n        else:\n            # Complex building or no clear direction: preserve shape\n            if preserve_shape:\n                # Simplify with topology preservation\n                result = building.simplify(simplify_tolerance, preserve_topology=True)\n            else:\n                # Fall back to convex hull for very complex shapes\n                result = building.convex_hull\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"utils/#geoai.utils.add_geometric_properties","title":"<code>add_geometric_properties(data, properties=None, area_unit='m2', length_unit='m')</code>","text":"<p>Calculates geometric properties and adds them to the GeoDataFrame.</p> <p>This function calculates various geometric properties of features in a GeoDataFrame and adds them as new columns without modifying existing attributes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>GeoDataFrame</code> <p>GeoDataFrame containing vector features.</p> required <code>properties</code> <code>Optional[List[str]]</code> <p>List of geometric properties to calculate. Options include: 'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds', 'convex_hull_area', 'orientation', 'complexity', 'area_bbox', 'area_convex', 'area_filled', 'major_length', 'minor_length', 'eccentricity', 'diameter_areagth', 'extent', 'solidity', 'elongation'. Defaults to ['area', 'length'] if None.</p> <code>None</code> <code>area_unit</code> <code>str</code> <p>String specifying the unit for area calculation ('m2', 'km2', 'ha'). Defaults to 'm2'.</p> <code>'m2'</code> <code>length_unit</code> <code>str</code> <p>String specifying the unit for length calculation ('m', 'km'). Defaults to 'm'.</p> <code>'m'</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added</p> <code>GeoDataFrame</code> <p>geometric property columns.</p> Source code in <code>geoai/utils.py</code> <pre><code>def add_geometric_properties(\n    data: gpd.GeoDataFrame,\n    properties: Optional[List[str]] = None,\n    area_unit: str = \"m2\",\n    length_unit: str = \"m\",\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Calculates geometric properties and adds them to the GeoDataFrame.\n\n    This function calculates various geometric properties of features in a\n    GeoDataFrame and adds them as new columns without modifying existing attributes.\n\n    Args:\n        data: GeoDataFrame containing vector features.\n        properties: List of geometric properties to calculate. Options include:\n            'area', 'length', 'perimeter', 'centroid_x', 'centroid_y', 'bounds',\n            'convex_hull_area', 'orientation', 'complexity', 'area_bbox',\n            'area_convex', 'area_filled', 'major_length', 'minor_length',\n            'eccentricity', 'diameter_areagth', 'extent', 'solidity',\n            'elongation'.\n            Defaults to ['area', 'length'] if None.\n        area_unit: String specifying the unit for area calculation ('m2', 'km2',\n            'ha'). Defaults to 'm2'.\n        length_unit: String specifying the unit for length calculation ('m', 'km').\n            Defaults to 'm'.\n\n    Returns:\n        geopandas.GeoDataFrame: A copy of the input GeoDataFrame with added\n        geometric property columns.\n    \"\"\"\n    from shapely.ops import unary_union\n\n    if isinstance(data, str):\n        data = read_vector(data)\n\n    # Make a copy to avoid modifying the original\n    result = data.copy()\n\n    # Default properties to calculate\n    if properties is None:\n        properties = [\n            \"area\",\n            \"length\",\n            \"perimeter\",\n            \"convex_hull_area\",\n            \"orientation\",\n            \"complexity\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"diameter_area\",\n            \"extent\",\n            \"solidity\",\n            \"elongation\",\n        ]\n\n    # Make sure we're working with a GeoDataFrame with a valid CRS\n\n    if not isinstance(result, gpd.GeoDataFrame):\n        raise ValueError(\"Input must be a GeoDataFrame\")\n\n    if result.crs is None:\n        raise ValueError(\n            \"GeoDataFrame must have a defined coordinate reference system (CRS)\"\n        )\n\n    # Ensure we're working with a projected CRS for accurate measurements\n    if result.crs.is_geographic:\n        # Reproject to a suitable projected CRS for accurate measurements\n        result = result.to_crs(result.estimate_utm_crs())\n\n    # Basic area calculation with unit conversion\n    if \"area\" in properties:\n        # Calculate area (only for polygons)\n        result[\"area\"] = result.geometry.apply(\n            lambda geom: geom.area if isinstance(geom, (Polygon, MultiPolygon)) else 0\n        )\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area\"] = result[\"area\"] / 1_000_000  # m\u00b2 to km\u00b2\n            result.rename(columns={\"area\": \"area_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area\"] = result[\"area\"] / 10_000  # m\u00b2 to hectares\n            result.rename(columns={\"area\": \"area_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area\": \"area_m2\"}, inplace=True)\n\n    # Length calculation with unit conversion\n    if \"length\" in properties:\n        # Calculate length (works for lines and polygon boundaries)\n        result[\"length\"] = result.geometry.length\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"length\"] = result[\"length\"] / 1_000  # m to km\n            result.rename(columns={\"length\": \"length_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"length\": \"length_m\"}, inplace=True)\n\n    # Perimeter calculation (for polygons)\n    if \"perimeter\" in properties:\n        result[\"perimeter\"] = result.geometry.apply(\n            lambda geom: (\n                geom.boundary.length if isinstance(geom, (Polygon, MultiPolygon)) else 0\n            )\n        )\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"perimeter\"] = result[\"perimeter\"] / 1_000  # m to km\n            result.rename(columns={\"perimeter\": \"perimeter_km\"}, inplace=True)\n        else:  # Default is m\n            result.rename(columns={\"perimeter\": \"perimeter_m\"}, inplace=True)\n\n    # Centroid coordinates\n    if \"centroid_x\" in properties or \"centroid_y\" in properties:\n        centroids = result.geometry.centroid\n\n        if \"centroid_x\" in properties:\n            result[\"centroid_x\"] = centroids.x\n\n        if \"centroid_y\" in properties:\n            result[\"centroid_y\"] = centroids.y\n\n    # Bounding box properties\n    if \"bounds\" in properties:\n        bounds = result.geometry.bounds\n        result[\"minx\"] = bounds.minx\n        result[\"miny\"] = bounds.miny\n        result[\"maxx\"] = bounds.maxx\n        result[\"maxy\"] = bounds.maxy\n\n    # Area of bounding box\n    if \"area_bbox\" in properties:\n        bounds = result.geometry.bounds\n        result[\"area_bbox\"] = (bounds.maxx - bounds.minx) * (bounds.maxy - bounds.miny)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 1_000_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_bbox\"] = result[\"area_bbox\"] / 10_000\n            result.rename(columns={\"area_bbox\": \"area_bbox_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_bbox\": \"area_bbox_m2\"}, inplace=True)\n\n    # Area of convex hull\n    if \"area_convex\" in properties or \"convex_hull_area\" in properties:\n        result[\"area_convex\"] = result.geometry.convex_hull.area\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 1_000_000\n            result.rename(columns={\"area_convex\": \"area_convex_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_convex\"] = result[\"area_convex\"] / 10_000\n            result.rename(columns={\"area_convex\": \"area_convex_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_convex\": \"area_convex_m2\"}, inplace=True)\n\n        # For backward compatibility\n        if \"convex_hull_area\" in properties and \"area_convex\" not in properties:\n            result[\"convex_hull_area\"] = result[\"area_convex\"]\n            if area_unit == \"km2\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_km2\"}, inplace=True\n                )\n            elif area_unit == \"ha\":\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_ha\"}, inplace=True\n                )\n            else:\n                result.rename(\n                    columns={\"convex_hull_area\": \"convex_hull_area_m2\"}, inplace=True\n                )\n\n    # Area of filled geometry (no holes)\n    if \"area_filled\" in properties:\n\n        def get_filled_area(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return 0\n\n            if isinstance(geom, MultiPolygon):\n                # For MultiPolygon, fill all constituent polygons\n                filled_polys = [Polygon(p.exterior) for p in geom.geoms]\n                return unary_union(filled_polys).area\n            else:\n                # For single Polygon, create a new one with just the exterior ring\n                return Polygon(geom.exterior).area\n\n        result[\"area_filled\"] = result.geometry.apply(get_filled_area)\n\n        # Convert to requested units\n        if area_unit == \"km2\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 1_000_000\n            result.rename(columns={\"area_filled\": \"area_filled_km2\"}, inplace=True)\n        elif area_unit == \"ha\":\n            result[\"area_filled\"] = result[\"area_filled\"] / 10_000\n            result.rename(columns={\"area_filled\": \"area_filled_ha\"}, inplace=True)\n        else:  # Default is m\u00b2\n            result.rename(columns={\"area_filled\": \"area_filled_m2\"}, inplace=True)\n\n    # Axes lengths, eccentricity, orientation, and elongation\n    if any(\n        p in properties\n        for p in [\n            \"major_length\",\n            \"minor_length\",\n            \"eccentricity\",\n            \"orientation\",\n            \"elongation\",\n        ]\n    ):\n\n        def get_axes_properties(geom):\n            # Skip non-polygons\n            if not isinstance(geom, (Polygon, MultiPolygon)):\n                return None, None, None, None, None\n\n            # Handle multipolygons by using the largest polygon\n            if isinstance(geom, MultiPolygon):\n                # Get the polygon with the largest area\n                geom = sorted(list(geom.geoms), key=lambda p: p.area, reverse=True)[0]\n\n            try:\n                # Get the minimum rotated rectangle\n                rect = geom.minimum_rotated_rectangle\n\n                # Extract coordinates\n                coords = list(rect.exterior.coords)[\n                    :-1\n                ]  # Remove the duplicated last point\n\n                if len(coords) &lt; 4:\n                    return None, None, None, None, None\n\n                # Calculate lengths of all four sides\n                sides = []\n                for i in range(len(coords)):\n                    p1 = coords[i]\n                    p2 = coords[(i + 1) % len(coords)]\n                    dx = p2[0] - p1[0]\n                    dy = p2[1] - p1[1]\n                    length = np.sqrt(dx**2 + dy**2)\n                    angle = np.degrees(np.arctan2(dy, dx)) % 180\n                    sides.append((length, angle, p1, p2))\n\n                # Group sides by length (allowing for small differences due to floating point precision)\n                # This ensures we correctly identify the rectangle's dimensions\n                sides_grouped = {}\n                tolerance = 1e-6  # Tolerance for length comparison\n\n                for s in sides:\n                    length, angle = s[0], s[1]\n                    matched = False\n\n                    for key in sides_grouped:\n                        if abs(length - key) &lt; tolerance:\n                            sides_grouped[key].append(s)\n                            matched = True\n                            break\n\n                    if not matched:\n                        sides_grouped[length] = [s]\n\n                # Get unique lengths (should be 2 for a rectangle, parallel sides have equal length)\n                unique_lengths = sorted(sides_grouped.keys(), reverse=True)\n\n                if len(unique_lengths) != 2:\n                    # If we don't get exactly 2 unique lengths, something is wrong with the rectangle\n                    # Fall back to simpler method using bounds\n                    bounds = rect.bounds\n                    width = bounds[2] - bounds[0]\n                    height = bounds[3] - bounds[1]\n                    major_length = max(width, height)\n                    minor_length = min(width, height)\n                    orientation = 0 if width &gt; height else 90\n                else:\n                    major_length = unique_lengths[0]\n                    minor_length = unique_lengths[1]\n                    # Get orientation from the major axis\n                    orientation = sides_grouped[major_length][0][1]\n\n                # Calculate eccentricity\n                if major_length &gt; 0:\n                    # Eccentricity for an ellipse: e = sqrt(1 - (b\u00b2/a\u00b2))\n                    # where a is the semi-major axis and b is the semi-minor axis\n                    eccentricity = np.sqrt(\n                        1 - ((minor_length / 2) ** 2 / (major_length / 2) ** 2)\n                    )\n                else:\n                    eccentricity = 0\n\n                # Calculate elongation (ratio of minor to major axis)\n                elongation = major_length / minor_length if major_length &gt; 0 else 1\n\n                return major_length, minor_length, eccentricity, orientation, elongation\n\n            except Exception as e:\n                # For debugging\n                # print(f\"Error calculating axes: {e}\")\n                return None, None, None, None, None\n\n        # Apply the function and split the results\n        axes_data = result.geometry.apply(get_axes_properties)\n\n        if \"major_length\" in properties:\n            result[\"major_length\"] = axes_data.apply(lambda x: x[0] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"major_length\"] = result[\"major_length\"] / 1_000\n                result.rename(columns={\"major_length\": \"major_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"major_length\": \"major_length_m\"}, inplace=True)\n\n        if \"minor_length\" in properties:\n            result[\"minor_length\"] = axes_data.apply(lambda x: x[1] if x else None)\n            # Convert to requested units\n            if length_unit == \"km\":\n                result[\"minor_length\"] = result[\"minor_length\"] / 1_000\n                result.rename(columns={\"minor_length\": \"minor_length_km\"}, inplace=True)\n            else:\n                result.rename(columns={\"minor_length\": \"minor_length_m\"}, inplace=True)\n\n        if \"eccentricity\" in properties:\n            result[\"eccentricity\"] = axes_data.apply(lambda x: x[2] if x else None)\n\n        if \"orientation\" in properties:\n            result[\"orientation\"] = axes_data.apply(lambda x: x[3] if x else None)\n\n        if \"elongation\" in properties:\n            result[\"elongation\"] = axes_data.apply(lambda x: x[4] if x else None)\n\n    # Equivalent diameter based on area\n    if \"diameter_areagth\" in properties:\n\n        def get_equivalent_diameter(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n            # Diameter of a circle with the same area: d = 2 * sqrt(A / \u03c0)\n            return 2 * np.sqrt(geom.area / np.pi)\n\n        result[\"diameter_areagth\"] = result.geometry.apply(get_equivalent_diameter)\n\n        # Convert to requested units\n        if length_unit == \"km\":\n            result[\"diameter_areagth\"] = result[\"diameter_areagth\"] / 1_000\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_km\"},\n                inplace=True,\n            )\n        else:\n            result.rename(\n                columns={\"diameter_areagth\": \"equivalent_diameter_area_m\"},\n                inplace=True,\n            )\n\n    # Extent (ratio of shape area to bounding box area)\n    if \"extent\" in properties:\n\n        def get_extent(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            bounds = geom.bounds\n            bbox_area = (bounds[2] - bounds[0]) * (bounds[3] - bounds[1])\n\n            if bbox_area &gt; 0:\n                return geom.area / bbox_area\n            return None\n\n        result[\"extent\"] = result.geometry.apply(get_extent)\n\n    # Solidity (ratio of shape area to convex hull area)\n    if \"solidity\" in properties:\n\n        def get_solidity(geom):\n            if not isinstance(geom, (Polygon, MultiPolygon)) or geom.area &lt;= 0:\n                return None\n\n            convex_hull_area = geom.convex_hull.area\n\n            if convex_hull_area &gt; 0:\n                return geom.area / convex_hull_area\n            return None\n\n        result[\"solidity\"] = result.geometry.apply(get_solidity)\n\n    # Complexity (ratio of perimeter to area)\n    if \"complexity\" in properties:\n\n        def calc_complexity(geom):\n            if isinstance(geom, (Polygon, MultiPolygon)) and geom.area &gt; 0:\n                # Shape index: P / (2 * sqrt(\u03c0 * A))\n                # Normalized to 1 for a circle, higher for more complex shapes\n                return geom.boundary.length / (2 * np.sqrt(np.pi * geom.area))\n            return None\n\n        result[\"complexity\"] = result.geometry.apply(calc_complexity)\n\n    return result\n</code></pre>"},{"location":"utils/#geoai.utils.analyze_vector_attributes","title":"<code>analyze_vector_attributes(vector_path, attribute_name)</code>","text":"<p>Analyze a specific attribute in a vector dataset and create a histogram.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to analyze</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing analysis results for the attribute</p> Source code in <code>geoai/utils.py</code> <pre><code>def analyze_vector_attributes(\n    vector_path: str, attribute_name: str\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Analyze a specific attribute in a vector dataset and create a histogram.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to analyze\n\n    Returns:\n        dict: Dictionary containing analysis results for the attribute\n    \"\"\"\n    try:\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return None\n\n        # Get the attribute series\n        attr = gdf[attribute_name]\n\n        # Perform different analyses based on data type\n        if pd.api.types.is_numeric_dtype(attr):\n            # Numeric attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"numeric\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"min\": attr.min(),\n                \"max\": attr.max(),\n                \"mean\": attr.mean(),\n                \"median\": attr.median(),\n                \"std\": attr.std(),\n                \"unique_values\": attr.nunique(),\n            }\n\n            # Create histogram\n            plt.figure(figsize=(10, 6))\n            plt.hist(attr.dropna(), bins=20, alpha=0.7, color=\"blue\")\n            plt.title(f\"Histogram of {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Frequency\")\n            plt.grid(True, alpha=0.3)\n            plt.show()\n\n        else:\n            # Categorical attribute\n            analysis = {\n                \"attribute\": attribute_name,\n                \"type\": \"categorical\",\n                \"count\": attr.count(),\n                \"null_count\": attr.isna().sum(),\n                \"unique_values\": attr.nunique(),\n                \"value_counts\": attr.value_counts().to_dict(),\n            }\n\n            # Create bar plot for top categories\n            top_n = min(10, attr.nunique())\n            plt.figure(figsize=(10, 6))\n            attr.value_counts().head(top_n).plot(kind=\"bar\", color=\"skyblue\")\n            plt.title(f\"Top {top_n} values for {attribute_name}\")\n            plt.xlabel(attribute_name)\n            plt.ylabel(\"Count\")\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n\n        return analysis\n\n    except Exception as e:\n        print(f\"Error analyzing attribute: {str(e)}\")\n        return None\n</code></pre>"},{"location":"utils/#geoai.utils.batch_vector_to_raster","title":"<code>batch_vector_to_raster(vector_path, output_dir, attribute_field=None, reference_rasters=None, bounds_list=None, output_filename_pattern='{vector_name}_{index}', pixel_size=1.0, all_touched=False, fill_value=0, dtype=np.uint8, nodata=None)</code>","text":"<p>Batch convert vector data to multiple rasters based on different extents or reference rasters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output raster files.</p> required <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values.</p> <code>None</code> <code>reference_rasters</code> <code>list</code> <p>List of paths to reference rasters for dimensions, transform and CRS.</p> <code>None</code> <code>bounds_list</code> <code>list</code> <p>List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.</p> <code>None</code> <code>output_filename_pattern</code> <code>str</code> <p>Pattern for output filenames. Can include {vector_name} and {index} placeholders.</p> <code>'{vector_name}_{index}'</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size to use if reference_rasters not provided.</p> <code>1.0</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type of the output raster.</p> <code>uint8</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of paths to the created raster files.</p> Source code in <code>geoai/utils.py</code> <pre><code>def batch_vector_to_raster(\n    vector_path,\n    output_dir,\n    attribute_field=None,\n    reference_rasters=None,\n    bounds_list=None,\n    output_filename_pattern=\"{vector_name}_{index}\",\n    pixel_size=1.0,\n    all_touched=False,\n    fill_value=0,\n    dtype=np.uint8,\n    nodata=None,\n) -&gt; List[str]:\n    \"\"\"\n    Batch convert vector data to multiple rasters based on different extents or reference rasters.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_dir (str): Directory to save output raster files.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n        reference_rasters (list): List of paths to reference rasters for dimensions, transform and CRS.\n        bounds_list (list): List of bounds tuples (left, bottom, right, top) to use if reference_rasters not provided.\n        output_filename_pattern (str): Pattern for output filenames.\n            Can include {vector_name} and {index} placeholders.\n        pixel_size (float or tuple): Pixel size to use if reference_rasters not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n\n    Returns:\n        List[str]: List of paths to the created raster files.\n    \"\"\"\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Load vector data if it's a path\n    if isinstance(vector_path, str):\n        gdf = gpd.read_file(vector_path)\n        vector_name = os.path.splitext(os.path.basename(vector_path))[0]\n    else:\n        gdf = vector_path\n        vector_name = \"vector\"\n\n    # Check input parameters\n    if reference_rasters is None and bounds_list is None:\n        raise ValueError(\"Either reference_rasters or bounds_list must be provided.\")\n\n    # Use reference_rasters if provided, otherwise use bounds_list\n    if reference_rasters is not None:\n        sources = reference_rasters\n        is_raster_reference = True\n    else:\n        sources = bounds_list\n        is_raster_reference = False\n\n    # Create output filenames\n    output_files = []\n\n    # Process each source (reference raster or bounds)\n    for i, source in enumerate(tqdm(sources, desc=\"Processing\")):\n        # Generate output filename\n        output_filename = output_filename_pattern.format(\n            vector_name=vector_name, index=i\n        )\n        if not output_filename.endswith(\".tif\"):\n            output_filename += \".tif\"\n        output_path = os.path.join(output_dir, output_filename)\n\n        if is_raster_reference:\n            # Use reference raster\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                reference_raster=source,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n        else:\n            # Use bounds\n            vector_to_raster(\n                vector_path=gdf,\n                output_path=output_path,\n                bounds=source,\n                pixel_size=pixel_size,\n                attribute_field=attribute_field,\n                all_touched=all_touched,\n                fill_value=fill_value,\n                dtype=dtype,\n                nodata=nodata,\n            )\n\n        output_files.append(output_path)\n\n    return output_files\n</code></pre>"},{"location":"utils/#geoai.utils.bbox_to_xy","title":"<code>bbox_to_xy(src_fp, coords, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.     Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright     While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[float]</code> <p>A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.</p> Source code in <code>geoai/utils.py</code> <pre><code>def bbox_to_xy(\n    src_fp: str, coords: List[float], coord_crs: str = \"epsg:4326\", **kwargs: Any\n) -&gt; List[float]:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n        Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n        While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n\n    Args:\n        src_fp (str): The source raster file path.\n        coords (list): A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]\n        coord_crs (str, optional): The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.\n    \"\"\"\n\n    if isinstance(coords, str):\n        gdf = gpd.read_file(coords)\n        coords = gdf.geometry.bounds.values.tolist()\n        if gdf.crs is not None:\n            coord_crs = f\"epsg:{gdf.crs.to_epsg()}\"\n    elif isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n    if isinstance(coords, dict):\n        import json\n\n        geojson = json.dumps(coords)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        coords = gdf.geometry.bounds.values.tolist()\n\n    elif not isinstance(coords, list):\n        raise ValueError(\"coords must be a list of coordinates.\")\n\n    if not isinstance(coords[0], list):\n        coords = [coords]\n\n    new_coords = []\n\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n\n        for coord in coords:\n            minx, miny, maxx, maxy = coord\n\n            if coord_crs != src.crs:\n                minx, miny = transform_coords(minx, miny, coord_crs, src.crs, **kwargs)\n                maxx, maxy = transform_coords(maxx, maxy, coord_crs, src.crs, **kwargs)\n\n                rows1, cols1 = rasterio.transform.rowcol(\n                    src.transform, minx, miny, **kwargs\n                )\n                rows2, cols2 = rasterio.transform.rowcol(\n                    src.transform, maxx, maxy, **kwargs\n                )\n\n                new_coords.append([cols1, rows1, cols2, rows2])\n\n            else:\n                new_coords.append([minx, miny, maxx, maxy])\n\n    result = []\n\n    for coord in new_coords:\n        minx, miny, maxx, maxy = coord\n\n        if (\n            minx &gt;= 0\n            and miny &gt;= 0\n            and maxx &gt;= 0\n            and maxy &gt;= 0\n            and minx &lt; width\n            and miny &lt; height\n            and maxx &lt; width\n            and maxy &lt; height\n        ):\n            # Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n            # While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n            result.append([minx, maxy, maxx, miny])\n\n    if len(result) == 0:\n        print(\"No valid pixel coordinates found.\")\n        return None\n    elif len(result) == 1:\n        return result[0]\n    elif len(result) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    return result\n</code></pre>"},{"location":"utils/#geoai.utils.boxes_to_vector","title":"<code>boxes_to_vector(coords, src_crs, dst_crs='EPSG:4326', output=None, **kwargs)</code>","text":"<p>Convert a list of bounding box coordinates to vector data.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].</p> required <code>src_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.</p> required <code>dst_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").</p> <code>'EPSG:4326'</code> <code>output</code> <code>str or None</code> <p>The full file path (including the directory and filename without the extension) where the vector data should be saved.                            If None (default), the function returns the GeoDataFrame without saving it to a file.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def boxes_to_vector(\n    coords: Union[List[List[float]], np.ndarray],\n    src_crs: str,\n    dst_crs: str = \"EPSG:4326\",\n    output: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a list of bounding box coordinates to vector data.\n\n    Args:\n        coords (list): A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].\n        src_crs (int or str): The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.\n        dst_crs (int or str, optional): The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").\n        output (str or None, optional): The full file path (including the directory and filename without the extension) where the vector data should be saved.\n                                       If None (default), the function returns the GeoDataFrame without saving it to a file.\n        **kwargs: Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.\n\n    Returns:\n        geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.\n    \"\"\"\n\n    from shapely.geometry import box\n\n    # Create a list of Shapely Polygon objects based on the provided coordinates\n    polygons = [box(*coord) for coord in coords]\n\n    # Create a GeoDataFrame with the Shapely Polygon objects\n    gdf = gpd.GeoDataFrame({\"geometry\": polygons}, crs=src_crs)\n\n    # Reproject the GeoDataFrame to the specified EPSG code\n    gdf_reprojected = gdf.to_crs(dst_crs)\n\n    if output is not None:\n        gdf_reprojected.to_file(output, **kwargs)\n    else:\n        return gdf_reprojected\n</code></pre>"},{"location":"utils/#geoai.utils.calc_stats","title":"<code>calc_stats(dataset, divide_by=1.0)</code>","text":"<p>Calculate the statistics (mean and std) for the entire dataset.</p> <p>This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html. Credit to the torchgeo developers for the original implementation.</p> <p>Warning: This is an approximation. The correct value should take into account the mean for the whole dataset for computing individual stds.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>RasterDataset</code> <p>The dataset to calculate statistics for.</p> required <code>divide_by</code> <code>float</code> <p>The value to divide the image data by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.</p> Source code in <code>geoai/utils.py</code> <pre><code>def calc_stats(dataset, divide_by: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Calculate the statistics (mean and std) for the entire dataset.\n\n    This function is adapted from the plot_batch() function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html.\n    Credit to the torchgeo developers for the original implementation.\n\n    Warning: This is an approximation. The correct value should take into account the\n    mean for the whole dataset for computing individual stds.\n\n    Args:\n        dataset (RasterDataset): The dataset to calculate statistics for.\n        divide_by (float, optional): The value to divide the image data by. Defaults to 1.0.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]: The mean and standard deviation for each band.\n    \"\"\"\n\n    # To avoid loading the entire dataset in memory, we will loop through each img\n    # The filenames will be retrieved from the dataset's rtree index\n    files = [\n        item.object\n        for item in dataset.index.intersection(dataset.index.bounds, objects=True)\n    ]\n\n    # Resetting statistics\n    accum_mean = 0\n    accum_std = 0\n\n    for file in files:\n        img = rasterio.open(file).read() / divide_by  # type: ignore\n        accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n        accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n\n    # at the end, we shall have 2 vectors with length n=chnls\n    # we will average them considering the number of images\n    return accum_mean / len(files), accum_std / len(files)\n</code></pre>"},{"location":"utils/#geoai.utils.clip_raster_by_bbox","title":"<code>clip_raster_by_bbox(input_raster, output_raster, bbox, bands=None, bbox_type='geo', bbox_crs=None)</code>","text":"<p>Clip a raster dataset using a bounding box and optionally select specific bands.</p> <p>Parameters:</p> Name Type Description Default <code>input_raster</code> <code>str</code> <p>Path to the input raster file.</p> required <code>output_raster</code> <code>str</code> <p>Path where the clipped raster will be saved.</p> required <code>bbox</code> <code>tuple</code> <p>Bounding box coordinates either as:          - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"          - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"</p> required <code>bands</code> <code>list</code> <p>List of band indices to keep (1-based indexing).                    If None, all bands will be kept.</p> <code>None</code> <code>bbox_type</code> <code>str</code> <p>Type of bounding box coordinates. Either \"geo\" for                       geographic coordinates or \"pixel\" for row/column indices.                       Default is \"geo\".</p> <code>'geo'</code> <code>bbox_crs</code> <code>str or dict</code> <p>CRS of the bbox if different from the raster CRS.                              Can be provided as EPSG code (e.g., \"EPSG:4326\") or                              as a proj4 string. Only applies when bbox_type=\"geo\".                              If None, assumes bbox is in the same CRS as the raster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the clipped output raster.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If required dependencies are not installed.</p> <code>ValueError</code> <p>If the bbox is invalid, bands are out of range, or bbox_type is invalid.</p> <code>RuntimeError</code> <p>If the clipping operation fails.</p> <p>Examples:</p> <p>Clip using geographic coordinates in the same CRS as the raster</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n'clipped_geo.tif'\n</code></pre> <p>Clip using WGS84 coordinates when the raster is in a different CRS</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n...                     bbox_crs=\"EPSG:4326\")\n'clipped_wgs84.tif'\n</code></pre> <p>Clip using row/column indices</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n...                     bbox_type=\"pixel\")\n'clipped_pixel.tif'\n</code></pre> <p>Clip with band selection</p> <pre><code>&gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n...                     bands=[1, 3])\n'clipped_bands.tif'\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def clip_raster_by_bbox(\n    input_raster: str,\n    output_raster: str,\n    bbox: List[float],\n    bands: Optional[List[int]] = None,\n    bbox_type: str = \"geo\",\n    bbox_crs: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Clip a raster dataset using a bounding box and optionally select specific bands.\n\n    Args:\n        input_raster (str): Path to the input raster file.\n        output_raster (str): Path where the clipped raster will be saved.\n        bbox (tuple): Bounding box coordinates either as:\n                     - Geographic coordinates (minx, miny, maxx, maxy) if bbox_type=\"geo\"\n                     - Pixel indices (min_row, min_col, max_row, max_col) if bbox_type=\"pixel\"\n        bands (list, optional): List of band indices to keep (1-based indexing).\n                               If None, all bands will be kept.\n        bbox_type (str, optional): Type of bounding box coordinates. Either \"geo\" for\n                                  geographic coordinates or \"pixel\" for row/column indices.\n                                  Default is \"geo\".\n        bbox_crs (str or dict, optional): CRS of the bbox if different from the raster CRS.\n                                         Can be provided as EPSG code (e.g., \"EPSG:4326\") or\n                                         as a proj4 string. Only applies when bbox_type=\"geo\".\n                                         If None, assumes bbox is in the same CRS as the raster.\n\n    Returns:\n        str: Path to the clipped output raster.\n\n    Raises:\n        ImportError: If required dependencies are not installed.\n        ValueError: If the bbox is invalid, bands are out of range, or bbox_type is invalid.\n        RuntimeError: If the clipping operation fails.\n\n    Examples:\n        Clip using geographic coordinates in the same CRS as the raster\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_geo.tif', (100, 200, 300, 400))\n        'clipped_geo.tif'\n\n        Clip using WGS84 coordinates when the raster is in a different CRS\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_wgs84.tif', (-122.5, 37.7, -122.4, 37.8),\n        ...                     bbox_crs=\"EPSG:4326\")\n        'clipped_wgs84.tif'\n\n        Clip using row/column indices\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_pixel.tif', (50, 100, 150, 200),\n        ...                     bbox_type=\"pixel\")\n        'clipped_pixel.tif'\n\n        Clip with band selection\n        &gt;&gt;&gt; clip_raster_by_bbox('input.tif', 'clipped_bands.tif', (100, 200, 300, 400),\n        ...                     bands=[1, 3])\n        'clipped_bands.tif'\n    \"\"\"\n    from rasterio.transform import from_bounds\n    from rasterio.warp import transform_bounds\n\n    # Validate bbox_type\n    if bbox_type not in [\"geo\", \"pixel\"]:\n        raise ValueError(\"bbox_type must be either 'geo' or 'pixel'\")\n\n    # Validate bbox\n    if len(bbox) != 4:\n        raise ValueError(\"bbox must contain exactly 4 values\")\n\n    # Open the source raster\n    with rasterio.open(input_raster) as src:\n        # Get the source CRS\n        src_crs = src.crs\n\n        # Handle different bbox types\n        if bbox_type == \"geo\":\n            minx, miny, maxx, maxy = bbox\n\n            # Validate geographic bbox\n            if minx &gt;= maxx or miny &gt;= maxy:\n                raise ValueError(\n                    \"Invalid geographic bbox. Expected (minx, miny, maxx, maxy) where minx &lt; maxx and miny &lt; maxy\"\n                )\n\n            # If bbox_crs is provided and different from the source CRS, transform the bbox\n            if bbox_crs is not None and bbox_crs != src_crs:\n                try:\n                    # Transform bbox coordinates from bbox_crs to src_crs\n                    minx, miny, maxx, maxy = transform_bounds(\n                        bbox_crs, src_crs, minx, miny, maxx, maxy\n                    )\n                except Exception as e:\n                    raise ValueError(\n                        f\"Failed to transform bbox from {bbox_crs} to {src_crs}: {str(e)}\"\n                    )\n\n            # Calculate the pixel window from geographic coordinates\n            window = src.window(minx, miny, maxx, maxy)\n\n            # Use the same bounds for the output transform\n            output_bounds = (minx, miny, maxx, maxy)\n\n        else:  # bbox_type == \"pixel\"\n            min_row, min_col, max_row, max_col = bbox\n\n            # Validate pixel bbox\n            if min_row &gt;= max_row or min_col &gt;= max_col:\n                raise ValueError(\n                    \"Invalid pixel bbox. Expected (min_row, min_col, max_row, max_col) where min_row &lt; max_row and min_col &lt; max_col\"\n                )\n\n            if (\n                min_row &lt; 0\n                or min_col &lt; 0\n                or max_row &gt; src.height\n                or max_col &gt; src.width\n            ):\n                raise ValueError(\n                    f\"Pixel indices out of bounds. Raster dimensions are {src.height} rows x {src.width} columns\"\n                )\n\n            # Create a window from pixel coordinates\n            window = Window(min_col, min_row, max_col - min_col, max_row - min_row)\n\n            # Calculate the geographic bounds for this window\n            window_transform = src.window_transform(window)\n            output_bounds = rasterio.transform.array_bounds(\n                window.height, window.width, window_transform\n            )\n            # Reorder to (minx, miny, maxx, maxy)\n            output_bounds = (\n                output_bounds[0],\n                output_bounds[1],\n                output_bounds[2],\n                output_bounds[3],\n            )\n\n        # Get window dimensions\n        window_width = int(window.width)\n        window_height = int(window.height)\n\n        # Check if the window is valid\n        if window_width &lt;= 0 or window_height &lt;= 0:\n            raise ValueError(\"Bounding box results in an empty window\")\n\n        # Handle band selection\n        if bands is None:\n            # Use all bands\n            bands_to_read = list(range(1, src.count + 1))\n        else:\n            # Validate band indices\n            if not all(1 &lt;= b &lt;= src.count for b in bands):\n                raise ValueError(f\"Band indices must be between 1 and {src.count}\")\n            bands_to_read = bands\n\n        # Calculate new transform for the clipped raster\n        new_transform = from_bounds(\n            output_bounds[0],\n            output_bounds[1],\n            output_bounds[2],\n            output_bounds[3],\n            window_width,\n            window_height,\n        )\n\n        # Create a metadata dictionary for the output\n        out_meta = src.meta.copy()\n        out_meta.update(\n            {\n                \"height\": window_height,\n                \"width\": window_width,\n                \"transform\": new_transform,\n                \"count\": len(bands_to_read),\n            }\n        )\n\n        # Read the data for the selected bands\n        data = []\n        for band_idx in bands_to_read:\n            band_data = src.read(band_idx, window=window)\n            data.append(band_data)\n\n        # Stack the bands into a single array\n        if len(data) &gt; 1:\n            clipped_data = np.stack(data)\n        else:\n            clipped_data = data[0][np.newaxis, :, :]\n\n        # Write the output raster\n        with rasterio.open(output_raster, \"w\", **out_meta) as dst:\n            dst.write(clipped_data)\n\n    return output_raster\n</code></pre>"},{"location":"utils/#geoai.utils.coords_to_xy","title":"<code>coords_to_xy(src_fp, coords, coord_crs='epsg:4326', return_out_of_bounds=False, **kwargs)</code>","text":"<p>Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>ndarray</code> <p>A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]     or [[[x1, y1]], [[x2, y2]], ...].</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>return_out_of_bounds</code> <code>bool</code> <p>Whether to return out-of-bounds coordinates. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D or 3D array of pixel coordinates in the same format as the input.</p> Source code in <code>geoai/utils.py</code> <pre><code>def coords_to_xy(\n    src_fp: str,\n    coords: np.ndarray,\n    coord_crs: str = \"epsg:4326\",\n    return_out_of_bounds: bool = False,\n    **kwargs: Any,\n) -&gt; np.ndarray:\n    \"\"\"Converts a list or array of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        coords: A 2D or 3D array of coordinates. Can be of shape [[x1, y1], [x2, y2], ...]\n                or [[[x1, y1]], [[x2, y2]], ...].\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        return_out_of_bounds: Whether to return out-of-bounds coordinates. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A 2D or 3D array of pixel coordinates in the same format as the input.\n    \"\"\"\n    from rasterio.warp import transform as transform_coords\n\n    out_of_bounds = []\n    if isinstance(coords, np.ndarray):\n        input_is_3d = coords.ndim == 3  # Check if the input is a 3D array\n    else:\n        input_is_3d = False\n\n    # Flatten the 3D array to 2D if necessary\n    if input_is_3d:\n        original_shape = coords.shape  # Store the original shape\n        coords = coords.reshape(-1, 2)  # Flatten to 2D\n\n    # Convert ndarray to a list if necessary\n    if isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n\n    xs, ys = zip(*coords)\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n        if coord_crs != src.crs:\n            xs, ys = transform_coords(coord_crs, src.crs, xs, ys, **kwargs)\n        rows, cols = rasterio.transform.rowcol(src.transform, xs, ys, **kwargs)\n\n    result = [[col, row] for col, row in zip(cols, rows)]\n\n    output = []\n\n    for i, (x, y) in enumerate(result):\n        if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height:\n            output.append([x, y])\n        else:\n            out_of_bounds.append(i)\n\n    # Convert the output back to the original shape if input was 3D\n    output = np.array(output)\n    if input_is_3d:\n        output = output.reshape(original_shape)\n\n    # Handle cases where no valid pixel coordinates are found\n    if len(output) == 0:\n        print(\"No valid pixel coordinates found.\")\n    elif len(output) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    if return_out_of_bounds:\n        return output, out_of_bounds\n    else:\n        return output\n</code></pre>"},{"location":"utils/#geoai.utils.create_overview_image","title":"<code>create_overview_image(src, tile_coordinates, output_path, tile_size, stride, geojson_path=None)</code>","text":"<p>Create an overview image showing all tiles and their status, with optional GeoJSON export.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>DatasetReader</code> <p>The source raster dataset.</p> required <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>output_path</code> <code>str</code> <p>The path where the overview image will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels.</p> required <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Controls overlap between adjacent tiles.</p> required <code>geojson_path</code> <code>str</code> <p>If provided, exports the tile rectangles as GeoJSON to this path.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved overview image.</p> Source code in <code>geoai/utils.py</code> <pre><code>def create_overview_image(\n    src, tile_coordinates, output_path, tile_size, stride, geojson_path=None\n) -&gt; str:\n    \"\"\"Create an overview image showing all tiles and their status, with optional GeoJSON export.\n\n    Args:\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        output_path (str): The path where the overview image will be saved.\n        tile_size (int): The size of each tile in pixels.\n        stride (int): The stride between tiles in pixels. Controls overlap between adjacent tiles.\n        geojson_path (str, optional): If provided, exports the tile rectangles as GeoJSON to this path.\n\n    Returns:\n        str: Path to the saved overview image.\n    \"\"\"\n    # Read a reduced version of the source image\n    overview_scale = max(\n        1, int(max(src.width, src.height) / 2000)\n    )  # Scale to max ~2000px\n    overview_width = src.width // overview_scale\n    overview_height = src.height // overview_scale\n\n    # Read downsampled image\n    overview_data = src.read(\n        out_shape=(src.count, overview_height, overview_width),\n        resampling=rasterio.enums.Resampling.average,\n    )\n\n    # Create RGB image for display\n    if overview_data.shape[0] &gt;= 3:\n        rgb = np.moveaxis(overview_data[:3], 0, -1)\n    else:\n        # For single band, create grayscale RGB\n        rgb = np.stack([overview_data[0], overview_data[0], overview_data[0]], axis=-1)\n\n    # Normalize for display\n    for i in range(rgb.shape[-1]):\n        band = rgb[..., i]\n        non_zero = band[band &gt; 0]\n        if len(non_zero) &gt; 0:\n            p2, p98 = np.percentile(non_zero, (2, 98))\n            rgb[..., i] = np.clip((band - p2) / (p98 - p2), 0, 1)\n\n    # Create figure\n    plt.figure(figsize=(12, 12))\n    plt.imshow(rgb)\n\n    # If GeoJSON export is requested, prepare GeoJSON structures\n    if geojson_path:\n        features = []\n\n    # Draw tile boundaries\n    for tile in tile_coordinates:\n        # Convert bounds to pixel coordinates in overview\n        bounds = tile[\"bounds\"]\n        # Calculate scaled pixel coordinates\n        x_min = int((tile[\"x\"]) / overview_scale)\n        y_min = int((tile[\"y\"]) / overview_scale)\n        width = int(tile_size / overview_scale)\n        height = int(tile_size / overview_scale)\n\n        # Draw rectangle\n        color = \"lime\" if tile[\"has_features\"] else \"red\"\n        rect = plt.Rectangle(\n            (x_min, y_min), width, height, fill=False, edgecolor=color, linewidth=0.5\n        )\n        plt.gca().add_patch(rect)\n\n        # Add tile number if not too crowded\n        if width &gt; 20 and height &gt; 20:\n            plt.text(\n                x_min + width / 2,\n                y_min + height / 2,\n                str(tile[\"index\"]),\n                color=\"white\",\n                ha=\"center\",\n                va=\"center\",\n                fontsize=8,\n            )\n\n        # Add to GeoJSON features if exporting\n        if geojson_path:\n            # Create a polygon from the bounds (already in geo-coordinates)\n            minx, miny, maxx, maxy = bounds\n            polygon = box(minx, miny, maxx, maxy)\n\n            # Calculate overlap with neighboring tiles\n            overlap = 0\n            if stride &lt; tile_size:\n                overlap = tile_size - stride\n\n            # Create a GeoJSON feature\n            feature = {\n                \"type\": \"Feature\",\n                \"geometry\": mapping(polygon),\n                \"properties\": {\n                    \"index\": tile[\"index\"],\n                    \"has_features\": tile[\"has_features\"],\n                    \"bounds_pixel\": [\n                        tile[\"x\"],\n                        tile[\"y\"],\n                        tile[\"x\"] + tile_size,\n                        tile[\"y\"] + tile_size,\n                    ],\n                    \"tile_size_px\": tile_size,\n                    \"stride_px\": stride,\n                    \"overlap_px\": overlap,\n                },\n            }\n\n            # Add any additional properties from the tile\n            for key, value in tile.items():\n                if key not in [\"x\", \"y\", \"index\", \"has_features\", \"bounds\"]:\n                    feature[\"properties\"][key] = value\n\n            features.append(feature)\n\n    plt.title(\"Tile Overview (Green = Contains Features, Red = Empty)\")\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n    plt.close()\n\n    print(f\"Overview image saved to {output_path}\")\n\n    # Export GeoJSON if requested\n    if geojson_path:\n        geojson_collection = {\n            \"type\": \"FeatureCollection\",\n            \"features\": features,\n            \"properties\": {\n                \"crs\": (\n                    src.crs.to_string()\n                    if hasattr(src.crs, \"to_string\")\n                    else str(src.crs)\n                ),\n                \"total_tiles\": len(features),\n                \"source_raster_dimensions\": [src.width, src.height],\n            },\n        }\n\n        # Save to file\n        with open(geojson_path, \"w\") as f:\n            json.dump(geojson_collection, f)\n\n        print(f\"GeoJSON saved to {geojson_path}\")\n\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.create_split_map","title":"<code>create_split_map(left_layer='TERRAIN', right_layer='OpenTopoMap', left_args=None, right_args=None, left_array_args=None, right_array_args=None, zoom_control=True, fullscreen_control=True, layer_control=True, add_close_button=False, left_label=None, right_label=None, left_position='bottomleft', right_position='bottomright', widget_layout=None, draggable=True, center=[20, 0], zoom=2, height='600px', basemap=None, basemap_args=None, m=None, **kwargs)</code>","text":"<p>Adds split map.</p> <p>Parameters:</p> Name Type Description Default <code>left_layer</code> <code>str</code> <p>The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.</p> <code>'TERRAIN'</code> <code>right_layer</code> <code>str</code> <p>The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.</p> <code>'OpenTopoMap'</code> <code>left_args</code> <code>dict</code> <p>The arguments for the left tile layer. Defaults to {}.</p> <code>None</code> <code>right_args</code> <code>dict</code> <p>The arguments for the right tile layer. Defaults to {}.</p> <code>None</code> <code>left_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the left layer. Defaults to {}.</p> <code>None</code> <code>right_array_args</code> <code>dict</code> <p>The arguments for array_to_image for the right layer. Defaults to {}.</p> <code>None</code> <code>zoom_control</code> <code>bool</code> <p>Whether to add zoom control. Defaults to True.</p> <code>True</code> <code>fullscreen_control</code> <code>bool</code> <p>Whether to add fullscreen control. Defaults to True.</p> <code>True</code> <code>layer_control</code> <code>bool</code> <p>Whether to add layer control. Defaults to True.</p> <code>True</code> <code>add_close_button</code> <code>bool</code> <p>Whether to add a close button. Defaults to False.</p> <code>False</code> <code>left_label</code> <code>str</code> <p>The label for the left layer. Defaults to None.</p> <code>None</code> <code>right_label</code> <code>str</code> <p>The label for the right layer. Defaults to None.</p> <code>None</code> <code>left_position</code> <code>str</code> <p>The position for the left label. Defaults to \"bottomleft\".</p> <code>'bottomleft'</code> <code>right_position</code> <code>str</code> <p>The position for the right label. Defaults to \"bottomright\".</p> <code>'bottomright'</code> <code>widget_layout</code> <code>dict</code> <p>The layout for the widget. Defaults to None.</p> <code>None</code> <code>draggable</code> <code>bool</code> <p>Whether the split map is draggable. Defaults to True.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def create_split_map(\n    left_layer: Optional[str] = \"TERRAIN\",\n    right_layer: Optional[str] = \"OpenTopoMap\",\n    left_args: Optional[dict] = None,\n    right_args: Optional[dict] = None,\n    left_array_args: Optional[dict] = None,\n    right_array_args: Optional[dict] = None,\n    zoom_control: Optional[bool] = True,\n    fullscreen_control: Optional[bool] = True,\n    layer_control: Optional[bool] = True,\n    add_close_button: Optional[bool] = False,\n    left_label: Optional[str] = None,\n    right_label: Optional[str] = None,\n    left_position: Optional[str] = \"bottomleft\",\n    right_position: Optional[str] = \"bottomright\",\n    widget_layout: Optional[dict] = None,\n    draggable: Optional[bool] = True,\n    center: Optional[List[float]] = [20, 0],\n    zoom: Optional[int] = 2,\n    height: Optional[int] = \"600px\",\n    basemap: Optional[str] = None,\n    basemap_args: Optional[Dict] = None,\n    m: Optional[Any] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Adds split map.\n\n    Args:\n        left_layer (str, optional): The left tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'TERRAIN'.\n        right_layer (str, optional): The right tile layer. Can be a local file path, HTTP URL, or a basemap name. Defaults to 'OpenTopoMap'.\n        left_args (dict, optional): The arguments for the left tile layer. Defaults to {}.\n        right_args (dict, optional): The arguments for the right tile layer. Defaults to {}.\n        left_array_args (dict, optional): The arguments for array_to_image for the left layer. Defaults to {}.\n        right_array_args (dict, optional): The arguments for array_to_image for the right layer. Defaults to {}.\n        zoom_control (bool, optional): Whether to add zoom control. Defaults to True.\n        fullscreen_control (bool, optional): Whether to add fullscreen control. Defaults to True.\n        layer_control (bool, optional): Whether to add layer control. Defaults to True.\n        add_close_button (bool, optional): Whether to add a close button. Defaults to False.\n        left_label (str, optional): The label for the left layer. Defaults to None.\n        right_label (str, optional): The label for the right layer. Defaults to None.\n        left_position (str, optional): The position for the left label. Defaults to \"bottomleft\".\n        right_position (str, optional): The position for the right label. Defaults to \"bottomright\".\n        widget_layout (dict, optional): The layout for the widget. Defaults to None.\n        draggable (bool, optional): Whether the split map is draggable. Defaults to True.\n    \"\"\"\n\n    if left_args is None:\n        left_args = {}\n\n    if right_args is None:\n        right_args = {}\n\n    if left_array_args is None:\n        left_array_args = {}\n\n    if right_array_args is None:\n        right_array_args = {}\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if m is None:\n        m = leafmap.Map(center=center, zoom=zoom, height=height, **kwargs)\n        m.clear_layers()\n    if isinstance(basemap, str):\n        if basemap.endswith(\".tif\"):\n            if basemap.startswith(\"http\"):\n                m.add_cog_layer(basemap, name=\"Basemap\", **basemap_args)\n            else:\n                m.add_raster(basemap, layer_name=\"Basemap\", **basemap_args)\n        else:\n            m.add_basemap(basemap)\n    m.split_map(\n        left_layer=left_layer,\n        right_layer=right_layer,\n        left_args=left_args,\n        right_args=right_args,\n        left_array_args=left_array_args,\n        right_array_args=right_array_args,\n        zoom_control=zoom_control,\n        fullscreen_control=fullscreen_control,\n        layer_control=layer_control,\n        add_close_button=add_close_button,\n        left_label=left_label,\n        right_label=right_label,\n        left_position=left_position,\n        right_position=right_position,\n        widget_layout=widget_layout,\n        draggable=draggable,\n    )\n\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.dict_to_image","title":"<code>dict_to_image(data_dict, output=None, **kwargs)</code>","text":"<p>Convert a dictionary containing spatial data to a rasterio dataset or save it to a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>This function transforms a dictionary with CRS, bounding box, and image data into a rasterio DatasetReader using leafmap's array_to_image utility after first converting to a rioxarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict[str, Any]</code> <p>A dictionary containing: - 'crs': A pyproj CRS object - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes   and optionally mint, maxt for temporal bounds - 'image': A tensor or array-like object with image data</p> required <code>output</code> <code>Optional[str]</code> <p>Optional path to save the image to a file. If not provided, the image will be returned as a rasterio DatasetReader object.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to leafmap.array_to_image. Common options include: - colormap: str, name of the colormap (e.g., 'viridis', 'terrain') - vmin: float, minimum value for colormap scaling - vmax: float, maximum value for colormap scaling</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Any]</code> <p>A rasterio DatasetReader object that can be used for visualization or</p> <code>Union[str, Any]</code> <p>further processing.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; image = dict_to_image(\n...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n...     colormap='terrain'\n... )\n&gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n&gt;&gt;&gt; show(image, ax=ax)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_image(\n    data_dict: Dict[str, Any], output: Optional[str] = None, **kwargs: Any\n) -&gt; Union[str, Any]:\n    \"\"\"Convert a dictionary containing spatial data to a rasterio dataset or save it to\n    a file. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\".\n    It can be generated from a TorchGeo dataset sampler.\n\n    This function transforms a dictionary with CRS, bounding box, and image data\n    into a rasterio DatasetReader using leafmap's array_to_image utility after\n    first converting to a rioxarray DataArray.\n\n    Args:\n        data_dict: A dictionary containing:\n            - 'crs': A pyproj CRS object\n            - 'bounds': A BoundingBox object with minx, maxx, miny, maxy attributes\n              and optionally mint, maxt for temporal bounds\n            - 'image': A tensor or array-like object with image data\n        output: Optional path to save the image to a file. If not provided, the image\n            will be returned as a rasterio DatasetReader object.\n        **kwargs: Additional keyword arguments to pass to leafmap.array_to_image.\n            Common options include:\n            - colormap: str, name of the colormap (e.g., 'viridis', 'terrain')\n            - vmin: float, minimum value for colormap scaling\n            - vmax: float, maximum value for colormap scaling\n\n    Returns:\n        A rasterio DatasetReader object that can be used for visualization or\n        further processing.\n\n    Examples:\n        &gt;&gt;&gt; image = dict_to_image(\n        ...     {'crs': CRS.from_epsg(26911), 'bounds': bbox, 'image': tensor},\n        ...     colormap='terrain'\n        ... )\n        &gt;&gt;&gt; fig, ax = plt.subplots(figsize=(10, 10))\n        &gt;&gt;&gt; show(image, ax=ax)\n    \"\"\"\n    da = dict_to_rioxarray(data_dict)\n\n    if output is not None:\n        out_dir = os.path.abspath(os.path.dirname(output))\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        da.rio.to_raster(output)\n        return output\n    else:\n        image = leafmap.array_to_image(da, **kwargs)\n        return image\n</code></pre>"},{"location":"utils/#geoai.utils.dict_to_rioxarray","title":"<code>dict_to_rioxarray(data_dict)</code>","text":"<p>Convert a dictionary to a xarray DataArray. The dictionary should contain the following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo dataset sampler.</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>Dict</code> <p>The dictionary containing the data.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>xr.DataArray: The xarray DataArray.</p> Source code in <code>geoai/utils.py</code> <pre><code>def dict_to_rioxarray(data_dict: Dict) -&gt; xr.DataArray:\n    \"\"\"Convert a dictionary to a xarray DataArray. The dictionary should contain the\n    following keys: \"crs\", \"bounds\", and \"image\". It can be generated from a TorchGeo\n    dataset sampler.\n\n    Args:\n        data_dict (Dict): The dictionary containing the data.\n\n    Returns:\n        xr.DataArray: The xarray DataArray.\n    \"\"\"\n\n    from collections import namedtuple\n\n    from affine import Affine\n\n    BoundingBox = namedtuple(\"BoundingBox\", [\"minx\", \"maxx\", \"miny\", \"maxy\"])\n\n    # Extract components from the dictionary\n    crs = data_dict[\"crs\"]\n    bounds = data_dict[\"bounds\"]\n    image_tensor = data_dict[\"image\"]\n\n    if hasattr(bounds, \"left\"):\n        bounds = BoundingBox(bounds.left, bounds.right, bounds.bottom, bounds.top)\n\n    # Convert tensor to numpy array if needed\n    if hasattr(image_tensor, \"numpy\"):\n        # For PyTorch tensors\n        image_array = image_tensor.numpy()\n    else:\n        # If it's already a numpy array or similar\n        image_array = np.array(image_tensor)\n\n    # Calculate pixel resolution\n    width = image_array.shape[2]  # Width is the size of the last dimension\n    height = image_array.shape[1]  # Height is the size of the middle dimension\n\n    res_x = (bounds.maxx - bounds.minx) / width\n    res_y = (bounds.maxy - bounds.miny) / height\n\n    # Create the transform matrix\n    transform = Affine(res_x, 0.0, bounds.minx, 0.0, -res_y, bounds.maxy)\n\n    # Create dimensions\n    x_coords = np.linspace(bounds.minx + res_x / 2, bounds.maxx - res_x / 2, width)\n    y_coords = np.linspace(bounds.maxy - res_y / 2, bounds.miny + res_y / 2, height)\n\n    # If time dimension exists in the bounds\n    if hasattr(bounds, \"mint\") and hasattr(bounds, \"maxt\"):\n        # Create a single time value or range if needed\n        t_coords = [\n            bounds.mint\n        ]  # Or np.linspace(bounds.mint, bounds.maxt, num_time_steps)\n\n        # Create DataArray with time dimension\n        dims = (\n            (\"band\", \"y\", \"x\")\n            if image_array.shape[0] &lt;= 10\n            else (\"time\", \"band\", \"y\", \"x\")\n        )\n\n        if dims[0] == \"band\":\n            # For multi-band single time\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"band\": np.arange(1, image_array.shape[0] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n        else:\n            # For multi-time multi-band\n            da = xr.DataArray(\n                image_array,\n                dims=dims,\n                coords={\n                    \"time\": t_coords,\n                    \"band\": np.arange(1, image_array.shape[1] + 1),\n                    \"y\": y_coords,\n                    \"x\": x_coords,\n                },\n            )\n    else:\n        # Create DataArray without time dimension\n        da = xr.DataArray(\n            image_array,\n            dims=(\"band\", \"y\", \"x\"),\n            coords={\n                \"band\": np.arange(1, image_array.shape[0] + 1),\n                \"y\": y_coords,\n                \"x\": x_coords,\n            },\n        )\n\n    # Set spatial attributes\n    da.rio.write_crs(crs, inplace=True)\n    da.rio.write_transform(transform, inplace=True)\n\n    return da\n</code></pre>"},{"location":"utils/#geoai.utils.download_file","title":"<code>download_file(url, output_path=None, overwrite=False, unzip=True)</code>","text":"<p>Download a file from a given URL with a progress bar. Optionally unzip the file if it's a ZIP archive.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to download.</p> required <code>output_path</code> <code>str</code> <p>The path where the downloaded file will be saved. If not provided, the filename from the URL will be used.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Whether to unzip the file if it is a ZIP archive.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the downloaded file or the extracted directory.</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_file(\n    url: str,\n    output_path: Optional[str] = None,\n    overwrite: bool = False,\n    unzip: bool = True,\n) -&gt; str:\n    \"\"\"\n    Download a file from a given URL with a progress bar.\n    Optionally unzip the file if it's a ZIP archive.\n\n    Args:\n        url (str): The URL of the file to download.\n        output_path (str, optional): The path where the downloaded file will be saved.\n            If not provided, the filename from the URL will be used.\n        overwrite (bool, optional): Whether to overwrite the file if it already exists.\n        unzip (bool, optional): Whether to unzip the file if it is a ZIP archive.\n\n    Returns:\n        str: The path to the downloaded file or the extracted directory.\n    \"\"\"\n\n    import zipfile\n\n    from tqdm import tqdm\n\n    if output_path is None:\n        output_path = os.path.basename(url)\n\n    if os.path.exists(output_path) and not overwrite:\n        print(f\"File already exists: {output_path}\")\n    else:\n        # Download the file with a progress bar\n        response = requests.get(url, stream=True, timeout=50)\n        response.raise_for_status()\n        total_size = int(response.headers.get(\"content-length\", 0))\n\n        with (\n            open(output_path, \"wb\") as file,\n            tqdm(\n                desc=f\"Downloading {os.path.basename(output_path)}\",\n                total=total_size,\n                unit=\"B\",\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as progress_bar,\n        ):\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    file.write(chunk)\n                    progress_bar.update(len(chunk))\n\n    # If the file is a ZIP archive and unzip is True\n    if unzip and zipfile.is_zipfile(output_path):\n        extract_dir = os.path.splitext(output_path)[0]\n        if not os.path.exists(extract_dir) or overwrite:\n            with zipfile.ZipFile(output_path, \"r\") as zip_ref:\n                zip_ref.extractall(extract_dir)\n            print(f\"Extracted to: {extract_dir}\")\n        return extract_dir\n\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.download_model_from_hf","title":"<code>download_model_from_hf(model_path, repo_id=None)</code>","text":"<p>Download the object detection model from Hugging Face.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the model file.</p> required <code>repo_id</code> <code>Optional[str]</code> <p>Hugging Face repository ID.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to the downloaded model file</p> Source code in <code>geoai/utils.py</code> <pre><code>def download_model_from_hf(model_path: str, repo_id: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Download the object detection model from Hugging Face.\n\n    Args:\n        model_path: Path to the model file.\n        repo_id: Hugging Face repository ID.\n\n    Returns:\n        Path to the downloaded model file\n    \"\"\"\n    from huggingface_hub import hf_hub_download\n\n    try:\n\n        # Define the repository ID and model filename\n        if repo_id is None:\n            print(\n                \"Repo is not specified, using default Hugging Face repo_id: giswqs/geoai\"\n            )\n            repo_id = \"giswqs/geoai\"\n\n        # Download the model\n        model_path = hf_hub_download(repo_id=repo_id, filename=model_path)\n        print(f\"Model downloaded to: {model_path}\")\n\n        return model_path\n\n    except Exception as e:\n        print(f\"Error downloading model from Hugging Face: {e}\")\n        print(\"Please specify a local model path or ensure internet connectivity.\")\n        raise\n</code></pre>"},{"location":"utils/#geoai.utils.empty_cache","title":"<code>empty_cache()</code>","text":"<p>Empty the cache of the current device.</p> Source code in <code>geoai/utils.py</code> <pre><code>def empty_cache() -&gt; None:\n    \"\"\"Empty the cache of the current device.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n</code></pre>"},{"location":"utils/#geoai.utils.export_geotiff_tiles","title":"<code>export_geotiff_tiles(in_raster, out_folder, in_class_data, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False)</code>","text":"<p>Export georeferenced GeoTIFF tiles and labels from raster and classification data.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image</p> required <code>out_folder</code> <code>str</code> <p>Path to output folder</p> required <code>in_class_data</code> <code>str</code> <p>Path to classification data - can be vector file or raster</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles(\n    in_raster,\n    out_folder,\n    in_class_data,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n):\n    \"\"\"\n    Export georeferenced GeoTIFF tiles and labels from raster and classification data.\n\n    Args:\n        in_raster (str): Path to input raster image\n        out_folder (str): Path to output folder\n        in_class_data (str): Path to classification data - can be vector file or raster\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n    \"\"\"\n\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Create output directories\n    os.makedirs(out_folder, exist_ok=True)\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n    ann_dir = os.path.join(out_folder, \"annotations\")\n    os.makedirs(ann_dir, exist_ok=True)\n\n    # Determine if class data is raster or vector\n    is_class_data_raster = False\n    if isinstance(in_class_data, str):\n        file_ext = Path(in_class_data).suffix.lower()\n        # Common raster extensions\n        if file_ext in [\".tif\", \".tiff\", \".img\", \".jp2\", \".png\", \".bmp\", \".gif\"]:\n            try:\n                with rasterio.open(in_class_data) as src:\n                    is_class_data_raster = True\n                    if not quiet:\n                        print(f\"Detected in_class_data as raster: {in_class_data}\")\n                        print(f\"Raster CRS: {src.crs}\")\n                        print(f\"Raster dimensions: {src.width} x {src.height}\")\n            except Exception:\n                is_class_data_raster = False\n                if not quiet:\n                    print(f\"Unable to open {in_class_data} as raster, trying as vector\")\n\n    # Open the input raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Resolution: {src.res}\")\n            print(f\"  Bands: {src.count}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Calculate number of tiles\n        num_tiles_x = math.ceil((src.width - tile_size) / stride) + 1\n        num_tiles_y = math.ceil((src.height - tile_size) / stride) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        if max_tiles is None:\n            max_tiles = total_tiles\n\n        # Process classification data\n        class_to_id = {}\n\n        if is_class_data_raster:\n            # Load raster class data\n            with rasterio.open(in_class_data) as class_src:\n                # Check if raster CRS matches\n                if class_src.crs != src.crs:\n                    warnings.warn(\n                        f\"CRS mismatch: Class raster ({class_src.crs}) doesn't match input raster ({src.crs}). \"\n                        f\"Results may be misaligned.\"\n                    )\n\n                # Get unique values from raster\n                # Sample to avoid loading huge rasters\n                sample_data = class_src.read(\n                    1,\n                    out_shape=(\n                        1,\n                        min(class_src.height, 1000),\n                        min(class_src.width, 1000),\n                    ),\n                )\n\n                unique_classes = np.unique(sample_data)\n                unique_classes = unique_classes[\n                    unique_classes &gt; 0\n                ]  # Remove 0 as it's typically background\n\n                if not quiet:\n                    print(\n                        f\"Found {len(unique_classes)} unique classes in raster: {unique_classes}\"\n                    )\n\n                # Create class mapping\n                class_to_id = {int(cls): i + 1 for i, cls in enumerate(unique_classes)}\n        else:\n            # Load vector class data\n            try:\n                gdf = gpd.read_file(in_class_data)\n                if not quiet:\n                    print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n                    print(f\"Vector CRS: {gdf.crs}\")\n\n                # Always reproject to match raster CRS\n                if gdf.crs != src.crs:\n                    if not quiet:\n                        print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n                    gdf = gdf.to_crs(src.crs)\n\n                # Apply buffer if specified\n                if buffer_radius &gt; 0:\n                    gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n                    if not quiet:\n                        print(f\"Applied buffer of {buffer_radius} units\")\n\n                # Check if class_value_field exists\n                if class_value_field in gdf.columns:\n                    unique_classes = gdf[class_value_field].unique()\n                    if not quiet:\n                        print(\n                            f\"Found {len(unique_classes)} unique classes: {unique_classes}\"\n                        )\n                    # Create class mapping\n                    class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n                else:\n                    if not quiet:\n                        print(\n                            f\"WARNING: '{class_value_field}' not found in vector data. Using default class ID 1.\"\n                        )\n                    class_to_id = {1: 1}  # Default mapping\n            except Exception as e:\n                raise ValueError(f\"Error processing vector data: {e}\")\n\n        # Create progress bar\n        pbar = tqdm(\n            total=min(total_tiles, max_tiles),\n            desc=\"Generating tiles\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Track statistics for summary\n        stats = {\n            \"total_tiles\": 0,\n            \"tiles_with_features\": 0,\n            \"feature_pixels\": 0,\n            \"errors\": 0,\n            \"tile_coordinates\": [],  # For overview image\n        }\n\n        # Process tiles\n        tile_index = 0\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                if tile_index &gt;= max_tiles:\n                    break\n\n                # Calculate window coordinates\n                window_x = x * stride\n                window_y = y * stride\n\n                # Adjust for edge cases\n                if window_x + tile_size &gt; src.width:\n                    window_x = src.width - tile_size\n                if window_y + tile_size &gt; src.height:\n                    window_y = src.height - tile_size\n\n                # Define window\n                window = Window(window_x, window_y, tile_size, tile_size)\n\n                # Get window transform and bounds\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size * window_transform[0]  # Add width\n                miny = maxy + tile_size * window_transform[4]  # Add height\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Store tile coordinates for overview\n                if create_overview:\n                    stats[\"tile_coordinates\"].append(\n                        {\n                            \"index\": tile_index,\n                            \"x\": window_x,\n                            \"y\": window_y,\n                            \"bounds\": [minx, miny, maxx, maxy],\n                            \"has_features\": False,\n                        }\n                    )\n\n                # Create label mask\n                label_mask = np.zeros((tile_size, tile_size), dtype=np.uint8)\n                has_features = False\n\n                # Process classification data to create labels\n                if is_class_data_raster:\n                    # For raster class data\n                    with rasterio.open(in_class_data) as class_src:\n                        # Calculate window in class raster\n                        src_bounds = src.bounds\n                        class_bounds = class_src.bounds\n\n                        # Check if windows overlap\n                        if (\n                            src_bounds.left &gt; class_bounds.right\n                            or src_bounds.right &lt; class_bounds.left\n                            or src_bounds.bottom &gt; class_bounds.top\n                            or src_bounds.top &lt; class_bounds.bottom\n                        ):\n                            warnings.warn(\n                                \"Class raster and input raster do not overlap.\"\n                            )\n                        else:\n                            # Get corresponding window in class raster\n                            window_class = rasterio.windows.from_bounds(\n                                minx, miny, maxx, maxy, class_src.transform\n                            )\n\n                            # Read label data\n                            try:\n                                label_data = class_src.read(\n                                    1,\n                                    window=window_class,\n                                    boundless=True,\n                                    out_shape=(tile_size, tile_size),\n                                )\n\n                                # Remap class values if needed\n                                if class_to_id:\n                                    remapped_data = np.zeros_like(label_data)\n                                    for orig_val, new_val in class_to_id.items():\n                                        remapped_data[label_data == orig_val] = new_val\n                                    label_mask = remapped_data\n                                else:\n                                    label_mask = label_data\n\n                                # Check if we have any features\n                                if np.any(label_mask &gt; 0):\n                                    has_features = True\n                                    stats[\"feature_pixels\"] += np.count_nonzero(\n                                        label_mask\n                                    )\n                            except Exception as e:\n                                pbar.write(f\"Error reading class raster window: {e}\")\n                                stats[\"errors\"] += 1\n                else:\n                    # For vector class data\n                    # Find features that intersect with window\n                    window_features = gdf[gdf.intersects(window_bounds)]\n\n                    if len(window_features) &gt; 0:\n                        for idx, feature in window_features.iterrows():\n                            # Get class value\n                            if class_value_field in feature:\n                                class_val = feature[class_value_field]\n                                class_id = class_to_id.get(class_val, 1)\n                            else:\n                                class_id = 1\n\n                            # Get geometry in window coordinates\n                            geom = feature.geometry.intersection(window_bounds)\n                            if not geom.is_empty:\n                                try:\n                                    # Rasterize feature\n                                    feature_mask = features.rasterize(\n                                        [(geom, class_id)],\n                                        out_shape=(tile_size, tile_size),\n                                        transform=window_transform,\n                                        fill=0,\n                                        all_touched=all_touched,\n                                    )\n\n                                    # Add to label mask\n                                    label_mask = np.maximum(label_mask, feature_mask)\n\n                                    # Check if the feature was actually rasterized\n                                    if np.any(feature_mask):\n                                        has_features = True\n                                        if create_overview and tile_index &lt; len(\n                                            stats[\"tile_coordinates\"]\n                                        ):\n                                            stats[\"tile_coordinates\"][tile_index][\n                                                \"has_features\"\n                                            ] = True\n                                except Exception as e:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                    stats[\"errors\"] += 1\n\n                # Skip tile if no features and skip_empty_tiles is True\n                if skip_empty_tiles and not has_features:\n                    pbar.update(1)\n                    tile_index += 1\n                    continue\n\n                # Read image data\n                image_data = src.read(window=window)\n\n                # Export image as GeoTIFF\n                image_path = os.path.join(image_dir, f\"tile_{tile_index:06d}.tif\")\n\n                # Create profile for image GeoTIFF\n                image_profile = src.profile.copy()\n                image_profile.update(\n                    {\n                        \"height\": tile_size,\n                        \"width\": tile_size,\n                        \"count\": image_data.shape[0],\n                        \"transform\": window_transform,\n                    }\n                )\n\n                # Save image as GeoTIFF\n                try:\n                    with rasterio.open(image_path, \"w\", **image_profile) as dst:\n                        dst.write(image_data)\n                    stats[\"total_tiles\"] += 1\n                except Exception as e:\n                    pbar.write(f\"ERROR saving image GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size,\n                    \"width\": tile_size,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Export label as GeoTIFF\n                label_path = os.path.join(label_dir, f\"tile_{tile_index:06d}.tif\")\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask.astype(np.uint8), 1)\n\n                    if has_features:\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += np.count_nonzero(label_mask)\n                except Exception as e:\n                    pbar.write(f\"ERROR saving label GeoTIFF: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Create XML annotation for object detection if using vector class data\n                if (\n                    not is_class_data_raster\n                    and \"gdf\" in locals()\n                    and len(window_features) &gt; 0\n                ):\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = f\"tile_{tile_index:06d}.tif\"\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size)\n                    ET.SubElement(size, \"height\").text = str(tile_size)\n                    ET.SubElement(size, \"depth\").text = str(image_data.shape[0])\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    # Add objects\n                    for idx, feature in window_features.iterrows():\n                        # Get feature class\n                        if class_value_field in feature:\n                            class_val = feature[class_value_field]\n                        else:\n                            class_val = \"object\"\n\n                        # Get geometry bounds in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            # Get bounds in world coordinates\n                            minx_f, miny_f, maxx_f, maxy_f = geom.bounds\n\n                            # Convert to pixel coordinates\n                            col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                            col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                            # Ensure coordinates are within tile bounds\n                            xmin = max(0, min(tile_size, int(col_min)))\n                            ymin = max(0, min(tile_size, int(row_min)))\n                            xmax = max(0, min(tile_size, int(col_max)))\n                            ymax = max(0, min(tile_size, int(row_max)))\n\n                            # Only add if the box has non-zero area\n                            if xmax &gt; xmin and ymax &gt; ymin:\n                                obj = ET.SubElement(root, \"object\")\n                                ET.SubElement(obj, \"name\").text = str(class_val)\n                                ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                                bbox = ET.SubElement(obj, \"bndbox\")\n                                ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                                ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                                ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                                ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    tree = ET.ElementTree(root)\n                    xml_path = os.path.join(ann_dir, f\"tile_{tile_index:06d}.xml\")\n                    tree.write(xml_path)\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                tile_index += 1\n                if tile_index &gt;= max_tiles:\n                    break\n\n            if tile_index &gt;= max_tiles:\n                break\n\n        # Close progress bar\n        pbar.close()\n\n        # Create overview image if requested\n        if create_overview and stats[\"tile_coordinates\"]:\n            try:\n                create_overview_image(\n                    src,\n                    stats[\"tile_coordinates\"],\n                    os.path.join(out_folder, \"overview.png\"),\n                    tile_size,\n                    stride,\n                )\n            except Exception as e:\n                print(f\"Failed to create overview image: {e}\")\n\n        # Report results\n        if not quiet:\n            print(\"\\n------- Export Summary -------\")\n            print(f\"Total tiles exported: {stats['total_tiles']}\")\n            print(\n                f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n            )\n            if stats[\"tiles_with_features\"] &gt; 0:\n                print(\n                    f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n                )\n            if stats[\"errors\"] &gt; 0:\n                print(f\"Errors encountered: {stats['errors']}\")\n            print(f\"Output saved to: {out_folder}\")\n\n            # Verify georeference in a sample image and label\n            if stats[\"total_tiles\"] &gt; 0:\n                print(\"\\n------- Georeference Verification -------\")\n                sample_image = os.path.join(image_dir, f\"tile_0.tif\")\n                sample_label = os.path.join(label_dir, f\"tile_0.tif\")\n\n                if os.path.exists(sample_image):\n                    try:\n                        with rasterio.open(sample_image) as img:\n                            print(f\"Image CRS: {img.crs}\")\n                            print(f\"Image transform: {img.transform}\")\n                            print(\n                                f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                            )\n                            print(\n                                f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying image georeference: {e}\")\n\n                if os.path.exists(sample_label):\n                    try:\n                        with rasterio.open(sample_label) as lbl:\n                            print(f\"Label CRS: {lbl.crs}\")\n                            print(f\"Label transform: {lbl.transform}\")\n                            print(\n                                f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                            )\n                            print(\n                                f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                            )\n                    except Exception as e:\n                        print(f\"Error verifying label georeference: {e}\")\n\n        # Return statistics dictionary for further processing if needed\n        return stats\n</code></pre>"},{"location":"utils/#geoai.utils.export_geotiff_tiles_batch","title":"<code>export_geotiff_tiles_batch(images_folder, masks_folder, output_folder, tile_size=256, stride=128, class_value_field='class', buffer_radius=0, max_tiles=None, quiet=False, all_touched=True, create_overview=False, skip_empty_tiles=False, image_extensions=None, mask_extensions=None)</code>","text":"<p>Export georeferenced GeoTIFF tiles from folders of images and masks.</p> <p>This function processes multiple image-mask pairs from input folders, generating tiles for each pair. All image tiles are saved to a single 'images' folder and all mask tiles to a single 'masks' folder.</p> <p>Images and masks are paired by their sorted order (alphabetically), not by filename matching. The number of images and masks must be equal.</p> <p>Parameters:</p> Name Type Description Default <code>images_folder</code> <code>str</code> <p>Path to folder containing raster images</p> required <code>masks_folder</code> <code>str</code> <p>Path to folder containing classification masks/vectors</p> required <code>output_folder</code> <code>str</code> <p>Path to output folder</p> required <code>tile_size</code> <code>int</code> <p>Size of tiles in pixels (square)</p> <code>256</code> <code>stride</code> <code>int</code> <p>Step size between tiles</p> <code>128</code> <code>class_value_field</code> <code>str</code> <p>Field containing class values (for vector data)</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer to add around features (in units of the CRS)</p> <code>0</code> <code>max_tiles</code> <code>int</code> <p>Maximum number of tiles to process per image (None for all)</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>If True, suppress non-essential output</p> <code>False</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization (for vector data)</p> <code>True</code> <code>create_overview</code> <code>bool</code> <p>Whether to create an overview image of all tiles</p> <code>False</code> <code>skip_empty_tiles</code> <code>bool</code> <p>If True, skip tiles with no features</p> <code>False</code> <code>image_extensions</code> <code>list</code> <p>List of image file extensions to process (default: common raster formats)</p> <code>None</code> <code>mask_extensions</code> <code>list</code> <p>List of mask file extensions to process (default: common raster/vector formats)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Dictionary containing batch processing statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no images or masks found, or if counts don't match</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_geotiff_tiles_batch(\n    images_folder,\n    masks_folder,\n    output_folder,\n    tile_size=256,\n    stride=128,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    max_tiles=None,\n    quiet=False,\n    all_touched=True,\n    create_overview=False,\n    skip_empty_tiles=False,\n    image_extensions=None,\n    mask_extensions=None,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Export georeferenced GeoTIFF tiles from folders of images and masks.\n\n    This function processes multiple image-mask pairs from input folders,\n    generating tiles for each pair. All image tiles are saved to a single\n    'images' folder and all mask tiles to a single 'masks' folder.\n\n    Images and masks are paired by their sorted order (alphabetically), not by\n    filename matching. The number of images and masks must be equal.\n\n    Args:\n        images_folder (str): Path to folder containing raster images\n        masks_folder (str): Path to folder containing classification masks/vectors\n        output_folder (str): Path to output folder\n        tile_size (int): Size of tiles in pixels (square)\n        stride (int): Step size between tiles\n        class_value_field (str): Field containing class values (for vector data)\n        buffer_radius (float): Buffer to add around features (in units of the CRS)\n        max_tiles (int): Maximum number of tiles to process per image (None for all)\n        quiet (bool): If True, suppress non-essential output\n        all_touched (bool): Whether to use all_touched=True in rasterization (for vector data)\n        create_overview (bool): Whether to create an overview image of all tiles\n        skip_empty_tiles (bool): If True, skip tiles with no features\n        image_extensions (list): List of image file extensions to process (default: common raster formats)\n        mask_extensions (list): List of mask file extensions to process (default: common raster/vector formats)\n\n    Returns:\n        Dict[str, Any]: Dictionary containing batch processing statistics\n\n    Raises:\n        ValueError: If no images or masks found, or if counts don't match\n    \"\"\"\n\n    import logging\n\n    logging.getLogger(\"rasterio\").setLevel(logging.ERROR)\n\n    # Default extensions if not provided\n    if image_extensions is None:\n        image_extensions = [\".tif\", \".tiff\", \".jpg\", \".jpeg\", \".png\", \".jp2\", \".img\"]\n    if mask_extensions is None:\n        mask_extensions = [\n            \".tif\",\n            \".tiff\",\n            \".jpg\",\n            \".jpeg\",\n            \".png\",\n            \".jp2\",\n            \".img\",\n            \".shp\",\n            \".geojson\",\n            \".gpkg\",\n            \".geoparquet\",\n            \".json\",\n        ]\n\n    # Convert extensions to lowercase for comparison\n    image_extensions = [ext.lower() for ext in image_extensions]\n    mask_extensions = [ext.lower() for ext in mask_extensions]\n\n    # Create output folder structure\n    os.makedirs(output_folder, exist_ok=True)\n    output_images_dir = os.path.join(output_folder, \"images\")\n    output_masks_dir = os.path.join(output_folder, \"masks\")\n    os.makedirs(output_images_dir, exist_ok=True)\n    os.makedirs(output_masks_dir, exist_ok=True)\n\n    # Get list of image files\n    image_files = []\n    for ext in image_extensions:\n        pattern = os.path.join(images_folder, f\"*{ext}\")\n        image_files.extend(glob.glob(pattern))\n\n    # Get list of mask files\n    mask_files = []\n    for ext in mask_extensions:\n        pattern = os.path.join(masks_folder, f\"*{ext}\")\n        mask_files.extend(glob.glob(pattern))\n\n    # Sort files for consistent processing\n    image_files.sort()\n    mask_files.sort()\n\n    if not image_files:\n        raise ValueError(\n            f\"No image files found in {images_folder} with extensions {image_extensions}\"\n        )\n\n    if not mask_files:\n        raise ValueError(\n            f\"No mask files found in {masks_folder} with extensions {mask_extensions}\"\n        )\n\n    if len(image_files) != len(mask_files):\n        raise ValueError(\n            f\"Number of image files ({len(image_files)}) does not match number of mask files ({len(mask_files)})\"\n        )\n\n    # Initialize batch statistics\n    batch_stats = {\n        \"total_image_pairs\": 0,\n        \"processed_pairs\": 0,\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"errors\": 0,\n        \"processed_files\": [],\n        \"failed_files\": [],\n    }\n\n    if not quiet:\n        print(\n            f\"Found {len(image_files)} image files and {len(mask_files)} mask files to process\"\n        )\n        print(f\"Processing batch from {images_folder} and {masks_folder}\")\n        print(f\"Output folder: {output_folder}\")\n        print(\"-\" * 60)\n\n    # Global tile counter for unique naming\n    global_tile_counter = 0\n\n    # Process each image-mask pair by sorted order\n    for idx, (image_file, mask_file) in enumerate(\n        tqdm(\n            zip(image_files, mask_files),\n            desc=\"Processing image pairs\",\n            disable=quiet,\n            total=len(image_files),\n        )\n    ):\n        batch_stats[\"total_image_pairs\"] += 1\n\n        # Get base filename without extension for naming (use image filename)\n        base_name = os.path.splitext(os.path.basename(image_file))[0]\n\n        try:\n            if not quiet:\n                print(f\"\\nProcessing: {base_name}\")\n                print(f\"  Image: {os.path.basename(image_file)}\")\n                print(f\"  Mask: {os.path.basename(mask_file)}\")\n\n            # Process the image-mask pair manually to get direct control over tile saving\n            tiles_generated = _process_image_mask_pair(\n                image_file=image_file,\n                mask_file=mask_file,\n                base_name=base_name,\n                output_images_dir=output_images_dir,\n                output_masks_dir=output_masks_dir,\n                global_tile_counter=global_tile_counter,\n                tile_size=tile_size,\n                stride=stride,\n                class_value_field=class_value_field,\n                buffer_radius=buffer_radius,\n                max_tiles=max_tiles,\n                all_touched=all_touched,\n                skip_empty_tiles=skip_empty_tiles,\n                quiet=quiet,\n            )\n\n            # Update counters\n            global_tile_counter += tiles_generated[\"total_tiles\"]\n\n            # Update batch statistics\n            batch_stats[\"processed_pairs\"] += 1\n            batch_stats[\"total_tiles\"] += tiles_generated[\"total_tiles\"]\n            batch_stats[\"tiles_with_features\"] += tiles_generated[\"tiles_with_features\"]\n            batch_stats[\"errors\"] += tiles_generated[\"errors\"]\n\n            batch_stats[\"processed_files\"].append(\n                {\n                    \"image\": image_file,\n                    \"mask\": mask_file,\n                    \"base_name\": base_name,\n                    \"tiles_generated\": tiles_generated[\"total_tiles\"],\n                    \"tiles_with_features\": tiles_generated[\"tiles_with_features\"],\n                }\n            )\n\n        except Exception as e:\n            if not quiet:\n                print(f\"ERROR processing {base_name}: {e}\")\n            batch_stats[\"failed_files\"].append(\n                {\"image\": image_file, \"mask\": mask_file, \"error\": str(e)}\n            )\n            batch_stats[\"errors\"] += 1\n\n    # Print batch summary\n    if not quiet:\n        print(\"\\n\" + \"=\" * 60)\n        print(\"BATCH PROCESSING SUMMARY\")\n        print(\"=\" * 60)\n        print(f\"Total image pairs found: {batch_stats['total_image_pairs']}\")\n        print(f\"Successfully processed: {batch_stats['processed_pairs']}\")\n        print(f\"Failed to process: {len(batch_stats['failed_files'])}\")\n        print(f\"Total tiles generated: {batch_stats['total_tiles']}\")\n        print(f\"Tiles with features: {batch_stats['tiles_with_features']}\")\n\n        if batch_stats[\"total_tiles\"] &gt; 0:\n            feature_percentage = (\n                batch_stats[\"tiles_with_features\"] / batch_stats[\"total_tiles\"]\n            ) * 100\n            print(f\"Feature percentage: {feature_percentage:.1f}%\")\n\n        if batch_stats[\"errors\"] &gt; 0:\n            print(f\"Total errors: {batch_stats['errors']}\")\n\n        print(f\"Output saved to: {output_folder}\")\n        print(f\"  Images: {output_images_dir}\")\n        print(f\"  Masks: {output_masks_dir}\")\n\n        # List failed files if any\n        if batch_stats[\"failed_files\"]:\n            print(f\"\\nFailed files:\")\n            for failed in batch_stats[\"failed_files\"]:\n                print(f\"  - {os.path.basename(failed['image'])}: {failed['error']}\")\n\n    return batch_stats\n</code></pre>"},{"location":"utils/#geoai.utils.export_tiles_to_geojson","title":"<code>export_tiles_to_geojson(tile_coordinates, src, output_path, tile_size=None, stride=None)</code>","text":"<p>Export tile rectangles directly to GeoJSON without creating an overview image.</p> <p>Parameters:</p> Name Type Description Default <code>tile_coordinates</code> <code>list</code> <p>A list of dictionaries containing tile information.</p> required <code>src</code> <code>DatasetReader</code> <p>The source raster dataset.</p> required <code>output_path</code> <code>str</code> <p>The path where the GeoJSON will be saved.</p> required <code>tile_size</code> <code>int</code> <p>The size of each tile in pixels. Only needed if not in tile_coordinates.</p> <code>None</code> <code>stride</code> <code>int</code> <p>The stride between tiles in pixels. Used to calculate overlaps between tiles.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the saved GeoJSON file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def export_tiles_to_geojson(\n    tile_coordinates, src, output_path, tile_size=None, stride=None\n) -&gt; str:\n    \"\"\"\n    Export tile rectangles directly to GeoJSON without creating an overview image.\n\n    Args:\n        tile_coordinates (list): A list of dictionaries containing tile information.\n        src (rasterio.io.DatasetReader): The source raster dataset.\n        output_path (str): The path where the GeoJSON will be saved.\n        tile_size (int, optional): The size of each tile in pixels. Only needed if not in tile_coordinates.\n        stride (int, optional): The stride between tiles in pixels. Used to calculate overlaps between tiles.\n\n    Returns:\n        str: Path to the saved GeoJSON file.\n    \"\"\"\n    features = []\n\n    for tile in tile_coordinates:\n        # Get the size from the tile or use the provided parameter\n        tile_width = tile.get(\"width\", tile.get(\"size\", tile_size))\n        tile_height = tile.get(\"height\", tile.get(\"size\", tile_size))\n\n        if tile_width is None or tile_height is None:\n            raise ValueError(\n                \"Tile size not found in tile data and no tile_size parameter provided\"\n            )\n\n        # Get bounds from the tile\n        if \"bounds\" in tile:\n            # If bounds are already in geo coordinates\n            minx, miny, maxx, maxy = tile[\"bounds\"]\n        else:\n            # Try to calculate bounds from transform if available\n            if hasattr(src, \"transform\"):\n                # Convert pixel coordinates to geo coordinates\n                window_transform = src.transform\n                x, y = tile[\"x\"], tile[\"y\"]\n                minx = window_transform[2] + x * window_transform[0]\n                maxy = window_transform[5] + y * window_transform[4]\n                maxx = minx + tile_width * window_transform[0]\n                miny = maxy + tile_height * window_transform[4]\n            else:\n                raise ValueError(\n                    \"Cannot determine bounds. Neither 'bounds' in tile nor transform in src.\"\n                )\n\n        # Calculate overlap with neighboring tiles if stride is provided\n        overlap = 0\n        if stride is not None and stride &lt; tile_width:\n            overlap = tile_width - stride\n\n        # Create a polygon from the bounds\n        polygon = box(minx, miny, maxx, maxy)\n\n        # Create a GeoJSON feature\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": mapping(polygon),\n            \"properties\": {\n                \"index\": tile[\"index\"],\n                \"has_features\": tile.get(\"has_features\", False),\n                \"tile_width_px\": tile_width,\n                \"tile_height_px\": tile_height,\n            },\n        }\n\n        # Add overlap information if stride is provided\n        if stride is not None:\n            feature[\"properties\"][\"stride_px\"] = stride\n            feature[\"properties\"][\"overlap_px\"] = overlap\n\n        # Add additional properties from the tile\n        for key, value in tile.items():\n            if key not in [\"bounds\", \"geometry\"]:\n                feature[\"properties\"][key] = value\n\n        features.append(feature)\n\n    # Create the GeoJSON collection\n    geojson_collection = {\n        \"type\": \"FeatureCollection\",\n        \"features\": features,\n        \"properties\": {\n            \"crs\": (\n                src.crs.to_string() if hasattr(src.crs, \"to_string\") else str(src.crs)\n            ),\n            \"total_tiles\": len(features),\n            \"source_raster_dimensions\": (\n                [src.width, src.height] if hasattr(src, \"width\") else None\n            ),\n        },\n    }\n\n    # Create directory if it doesn't exist\n    os.makedirs(os.path.dirname(os.path.abspath(output_path)) or \".\", exist_ok=True)\n\n    # Save to file\n    with open(output_path, \"w\") as f:\n        json.dump(geojson_collection, f)\n\n    print(f\"GeoJSON saved to {output_path}\")\n    return output_path\n</code></pre>"},{"location":"utils/#geoai.utils.export_training_data","title":"<code>export_training_data(in_raster, out_folder, in_class_data, image_chip_format='GEOTIFF', tile_size_x=256, tile_size_y=256, stride_x=None, stride_y=None, output_nofeature_tiles=True, metadata_format='PASCAL_VOC', start_index=0, class_value_field='class', buffer_radius=0, in_mask_polygons=None, rotation_angle=0, reference_system=None, blacken_around_feature=False, crop_mode='FIXED_SIZE', in_raster2=None, in_instance_data=None, instance_class_value_field=None, min_polygon_overlap_ratio=0.0, all_touched=True, save_geotiff=True, quiet=False)</code>","text":"<p>Export training data for deep learning using TorchGeo with progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>in_raster</code> <code>str</code> <p>Path to input raster image.</p> required <code>out_folder</code> <code>str</code> <p>Output folder path where chips and labels will be saved.</p> required <code>in_class_data</code> <code>str</code> <p>Path to vector file containing class polygons.</p> required <code>image_chip_format</code> <code>str</code> <p>Output image format (PNG, JPEG, TIFF, GEOTIFF).</p> <code>'GEOTIFF'</code> <code>tile_size_x</code> <code>int</code> <p>Width of image chips in pixels.</p> <code>256</code> <code>tile_size_y</code> <code>int</code> <p>Height of image chips in pixels.</p> <code>256</code> <code>stride_x</code> <code>int</code> <p>Horizontal stride between chips. If None, uses tile_size_x.</p> <code>None</code> <code>stride_y</code> <code>int</code> <p>Vertical stride between chips. If None, uses tile_size_y.</p> <code>None</code> <code>output_nofeature_tiles</code> <code>bool</code> <p>Whether to export chips without features.</p> <code>True</code> <code>metadata_format</code> <code>str</code> <p>Output metadata format (PASCAL_VOC, KITTI, COCO).</p> <code>'PASCAL_VOC'</code> <code>start_index</code> <code>int</code> <p>Starting index for chip filenames.</p> <code>0</code> <code>class_value_field</code> <code>str</code> <p>Field name in in_class_data containing class values.</p> <code>'class'</code> <code>buffer_radius</code> <code>float</code> <p>Buffer radius around features (in CRS units).</p> <code>0</code> <code>in_mask_polygons</code> <code>str</code> <p>Path to vector file containing mask polygons.</p> <code>None</code> <code>rotation_angle</code> <code>float</code> <p>Rotation angle in degrees.</p> <code>0</code> <code>reference_system</code> <code>str</code> <p>Reference system code.</p> <code>None</code> <code>blacken_around_feature</code> <code>bool</code> <p>Whether to mask areas outside of features.</p> <code>False</code> <code>crop_mode</code> <code>str</code> <p>Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).</p> <code>'FIXED_SIZE'</code> <code>in_raster2</code> <code>str</code> <p>Path to secondary raster image.</p> <code>None</code> <code>in_instance_data</code> <code>str</code> <p>Path to vector file containing instance polygons.</p> <code>None</code> <code>instance_class_value_field</code> <code>str</code> <p>Field name in in_instance_data for instance classes.</p> <code>None</code> <code>min_polygon_overlap_ratio</code> <code>float</code> <p>Minimum overlap ratio for polygons.</p> <code>0.0</code> <code>all_touched</code> <code>bool</code> <p>Whether to use all_touched=True in rasterization.</p> <code>True</code> <code>save_geotiff</code> <code>bool</code> <p>Whether to save as GeoTIFF with georeferencing.</p> <code>True</code> <code>quiet</code> <code>bool</code> <p>If True, suppress most output messages.</p> <code>False</code> Source code in <code>geoai/utils.py</code> <pre><code>def export_training_data(\n    in_raster,\n    out_folder,\n    in_class_data,\n    image_chip_format=\"GEOTIFF\",\n    tile_size_x=256,\n    tile_size_y=256,\n    stride_x=None,\n    stride_y=None,\n    output_nofeature_tiles=True,\n    metadata_format=\"PASCAL_VOC\",\n    start_index=0,\n    class_value_field=\"class\",\n    buffer_radius=0,\n    in_mask_polygons=None,\n    rotation_angle=0,\n    reference_system=None,\n    blacken_around_feature=False,\n    crop_mode=\"FIXED_SIZE\",  # Implemented but not fully used yet\n    in_raster2=None,\n    in_instance_data=None,\n    instance_class_value_field=None,  # Implemented but not fully used yet\n    min_polygon_overlap_ratio=0.0,\n    all_touched=True,\n    save_geotiff=True,\n    quiet=False,\n):\n    \"\"\"\n    Export training data for deep learning using TorchGeo with progress bar.\n\n    Args:\n        in_raster (str): Path to input raster image.\n        out_folder (str): Output folder path where chips and labels will be saved.\n        in_class_data (str): Path to vector file containing class polygons.\n        image_chip_format (str): Output image format (PNG, JPEG, TIFF, GEOTIFF).\n        tile_size_x (int): Width of image chips in pixels.\n        tile_size_y (int): Height of image chips in pixels.\n        stride_x (int): Horizontal stride between chips. If None, uses tile_size_x.\n        stride_y (int): Vertical stride between chips. If None, uses tile_size_y.\n        output_nofeature_tiles (bool): Whether to export chips without features.\n        metadata_format (str): Output metadata format (PASCAL_VOC, KITTI, COCO).\n        start_index (int): Starting index for chip filenames.\n        class_value_field (str): Field name in in_class_data containing class values.\n        buffer_radius (float): Buffer radius around features (in CRS units).\n        in_mask_polygons (str): Path to vector file containing mask polygons.\n        rotation_angle (float): Rotation angle in degrees.\n        reference_system (str): Reference system code.\n        blacken_around_feature (bool): Whether to mask areas outside of features.\n        crop_mode (str): Crop mode (FIXED_SIZE, CENTERED_ON_FEATURE).\n        in_raster2 (str): Path to secondary raster image.\n        in_instance_data (str): Path to vector file containing instance polygons.\n        instance_class_value_field (str): Field name in in_instance_data for instance classes.\n        min_polygon_overlap_ratio (float): Minimum overlap ratio for polygons.\n        all_touched (bool): Whether to use all_touched=True in rasterization.\n        save_geotiff (bool): Whether to save as GeoTIFF with georeferencing.\n        quiet (bool): If True, suppress most output messages.\n    \"\"\"\n    # Create output directories\n    image_dir = os.path.join(out_folder, \"images\")\n    os.makedirs(image_dir, exist_ok=True)\n\n    label_dir = os.path.join(out_folder, \"labels\")\n    os.makedirs(label_dir, exist_ok=True)\n\n    # Define annotation directories based on metadata format\n    if metadata_format == \"PASCAL_VOC\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n    elif metadata_format == \"COCO\":\n        ann_dir = os.path.join(out_folder, \"annotations\")\n        os.makedirs(ann_dir, exist_ok=True)\n        # Initialize COCO annotations dictionary\n        coco_annotations = {\"images\": [], \"annotations\": [], \"categories\": []}\n\n    # Initialize statistics dictionary\n    stats = {\n        \"total_tiles\": 0,\n        \"tiles_with_features\": 0,\n        \"feature_pixels\": 0,\n        \"errors\": 0,\n    }\n\n    # Open raster\n    with rasterio.open(in_raster) as src:\n        if not quiet:\n            print(f\"\\nRaster info for {in_raster}:\")\n            print(f\"  CRS: {src.crs}\")\n            print(f\"  Dimensions: {src.width} x {src.height}\")\n            print(f\"  Bounds: {src.bounds}\")\n\n        # Set defaults for stride if not provided\n        if stride_x is None:\n            stride_x = tile_size_x\n        if stride_y is None:\n            stride_y = tile_size_y\n\n        # Calculate number of tiles in x and y directions\n        num_tiles_x = math.ceil((src.width - tile_size_x) / stride_x) + 1\n        num_tiles_y = math.ceil((src.height - tile_size_y) / stride_y) + 1\n        total_tiles = num_tiles_x * num_tiles_y\n\n        # Read class data\n        gdf = gpd.read_file(in_class_data)\n        if not quiet:\n            print(f\"Loaded {len(gdf)} features from {in_class_data}\")\n            print(f\"Available columns: {gdf.columns.tolist()}\")\n            print(f\"GeoJSON CRS: {gdf.crs}\")\n\n        # Check if class_value_field exists\n        if class_value_field not in gdf.columns:\n            if not quiet:\n                print(\n                    f\"WARNING: '{class_value_field}' field not found in the input data. Using default class value 1.\"\n                )\n            # Add a default class column\n            gdf[class_value_field] = 1\n            unique_classes = [1]\n        else:\n            # Print unique classes for debugging\n            unique_classes = gdf[class_value_field].unique()\n            if not quiet:\n                print(f\"Found {len(unique_classes)} unique classes: {unique_classes}\")\n\n        # CRITICAL: Always reproject to match raster CRS to ensure proper alignment\n        if gdf.crs != src.crs:\n            if not quiet:\n                print(f\"Reprojecting features from {gdf.crs} to {src.crs}\")\n            gdf = gdf.to_crs(src.crs)\n        elif reference_system and gdf.crs != reference_system:\n            if not quiet:\n                print(\n                    f\"Reprojecting features to specified reference system {reference_system}\"\n                )\n            gdf = gdf.to_crs(reference_system)\n\n        # Check overlap between raster and vector data\n        raster_bounds = box(*src.bounds)\n        vector_bounds = box(*gdf.total_bounds)\n        if not raster_bounds.intersects(vector_bounds):\n            if not quiet:\n                print(\n                    \"WARNING: The vector data doesn't intersect with the raster extent!\"\n                )\n                print(f\"Raster bounds: {src.bounds}\")\n                print(f\"Vector bounds: {gdf.total_bounds}\")\n        else:\n            overlap = (\n                raster_bounds.intersection(vector_bounds).area / vector_bounds.area\n            )\n            if not quiet:\n                print(f\"Overlap between raster and vector: {overlap:.2%}\")\n\n        # Apply buffer if specified\n        if buffer_radius &gt; 0:\n            gdf[\"geometry\"] = gdf.buffer(buffer_radius)\n\n        # Initialize class mapping (ensure all classes are mapped to non-zero values)\n        class_to_id = {cls: i + 1 for i, cls in enumerate(unique_classes)}\n\n        # Store category info for COCO format\n        if metadata_format == \"COCO\":\n            for cls_val in unique_classes:\n                coco_annotations[\"categories\"].append(\n                    {\n                        \"id\": class_to_id[cls_val],\n                        \"name\": str(cls_val),\n                        \"supercategory\": \"object\",\n                    }\n                )\n\n        # Load mask polygons if provided\n        mask_gdf = None\n        if in_mask_polygons:\n            mask_gdf = gpd.read_file(in_mask_polygons)\n            if reference_system:\n                mask_gdf = mask_gdf.to_crs(reference_system)\n            elif mask_gdf.crs != src.crs:\n                mask_gdf = mask_gdf.to_crs(src.crs)\n\n        # Process instance data if provided\n        instance_gdf = None\n        if in_instance_data:\n            instance_gdf = gpd.read_file(in_instance_data)\n            if reference_system:\n                instance_gdf = instance_gdf.to_crs(reference_system)\n            elif instance_gdf.crs != src.crs:\n                instance_gdf = instance_gdf.to_crs(src.crs)\n\n        # Load secondary raster if provided\n        src2 = None\n        if in_raster2:\n            src2 = rasterio.open(in_raster2)\n\n        # Set up augmentation if rotation is specified\n        augmentation = None\n        if rotation_angle != 0:\n            # Fixed: Added data_keys parameter to AugmentationSequential\n            augmentation = torchgeo.transforms.AugmentationSequential(\n                torch.nn.ModuleList([RandomRotation(rotation_angle)]),\n                data_keys=[\"image\"],  # Add data_keys parameter\n            )\n\n        # Initialize annotation ID for COCO format\n        ann_id = 0\n\n        # Create progress bar\n        pbar = tqdm(\n            total=total_tiles,\n            desc=f\"Generating tiles (with features: 0)\",\n            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}&lt;{remaining}, {rate_fmt}]\",\n        )\n\n        # Generate tiles\n        chip_index = start_index\n        for y in range(num_tiles_y):\n            for x in range(num_tiles_x):\n                # Calculate window coordinates\n                window_x = x * stride_x\n                window_y = y * stride_y\n\n                # Adjust for edge cases\n                if window_x + tile_size_x &gt; src.width:\n                    window_x = src.width - tile_size_x\n                if window_y + tile_size_y &gt; src.height:\n                    window_y = src.height - tile_size_y\n\n                # Adjust window based on crop_mode\n                if crop_mode == \"CENTERED_ON_FEATURE\" and len(gdf) &gt; 0:\n                    # Find the nearest feature to the center of this window\n                    window_center_x = window_x + tile_size_x // 2\n                    window_center_y = window_y + tile_size_y // 2\n\n                    # Convert center to world coordinates\n                    center_x, center_y = src.xy(window_center_y, window_center_x)\n                    center_point = gpd.points_from_xy([center_x], [center_y])[0]\n\n                    # Find nearest feature\n                    distances = gdf.geometry.distance(center_point)\n                    nearest_idx = distances.idxmin()\n                    nearest_feature = gdf.iloc[nearest_idx]\n\n                    # Get centroid of nearest feature\n                    feature_centroid = nearest_feature.geometry.centroid\n\n                    # Convert feature centroid to pixel coordinates\n                    feature_row, feature_col = src.index(\n                        feature_centroid.x, feature_centroid.y\n                    )\n\n                    # Adjust window to center on feature\n                    window_x = max(\n                        0, min(src.width - tile_size_x, feature_col - tile_size_x // 2)\n                    )\n                    window_y = max(\n                        0, min(src.height - tile_size_y, feature_row - tile_size_y // 2)\n                    )\n\n                # Define window\n                window = Window(window_x, window_y, tile_size_x, tile_size_y)\n\n                # Get window transform and bounds in source CRS\n                window_transform = src.window_transform(window)\n\n                # Calculate window bounds more explicitly and accurately\n                minx = window_transform[2]  # Upper left x\n                maxy = window_transform[5]  # Upper left y\n                maxx = minx + tile_size_x * window_transform[0]  # Add width\n                miny = (\n                    maxy + tile_size_y * window_transform[4]\n                )  # Add height (note: transform[4] is typically negative)\n\n                window_bounds = box(minx, miny, maxx, maxy)\n\n                # Apply rotation if specified\n                if rotation_angle != 0:\n                    window_bounds = rotate(\n                        window_bounds, rotation_angle, origin=\"center\"\n                    )\n\n                # Find features that intersect with window\n                window_features = gdf[gdf.intersects(window_bounds)]\n\n                # Process instance data if provided\n                window_instances = None\n                if instance_gdf is not None and instance_class_value_field is not None:\n                    window_instances = instance_gdf[\n                        instance_gdf.intersects(window_bounds)\n                    ]\n                    if len(window_instances) &gt; 0:\n                        if not quiet:\n                            pbar.write(\n                                f\"Found {len(window_instances)} instances in tile {chip_index}\"\n                            )\n\n                # Skip if no features and output_nofeature_tiles is False\n                if not output_nofeature_tiles and len(window_features) == 0:\n                    pbar.update(1)  # Still update progress bar\n                    continue\n\n                # Check polygon overlap ratio if specified\n                if min_polygon_overlap_ratio &gt; 0 and len(window_features) &gt; 0:\n                    valid_features = []\n                    for _, feature in window_features.iterrows():\n                        overlap_ratio = (\n                            feature.geometry.intersection(window_bounds).area\n                            / feature.geometry.area\n                        )\n                        if overlap_ratio &gt;= min_polygon_overlap_ratio:\n                            valid_features.append(feature)\n\n                    if len(valid_features) &gt; 0:\n                        window_features = gpd.GeoDataFrame(valid_features)\n                    elif not output_nofeature_tiles:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Apply mask if provided\n                if mask_gdf is not None:\n                    mask_features = mask_gdf[mask_gdf.intersects(window_bounds)]\n                    if len(mask_features) == 0:\n                        pbar.update(1)  # Still update progress bar\n                        continue\n\n                # Read image data - keep original for GeoTIFF export\n                orig_image_data = src.read(window=window)\n\n                # Create a copy for processing\n                image_data = orig_image_data.copy().astype(np.float32)\n\n                # Normalize image data for processing\n                for band in range(image_data.shape[0]):\n                    band_min, band_max = np.percentile(image_data[band], (1, 99))\n                    if band_max &gt; band_min:\n                        image_data[band] = np.clip(\n                            (image_data[band] - band_min) / (band_max - band_min), 0, 1\n                        )\n\n                # Read secondary image data if provided\n                if src2:\n                    image_data2 = src2.read(window=window)\n                    # Stack the two images\n                    image_data = np.vstack((image_data, image_data2))\n\n                # Apply blacken_around_feature if needed\n                if blacken_around_feature and len(window_features) &gt; 0:\n                    mask = np.zeros((tile_size_y, tile_size_x), dtype=bool)\n                    for _, feature in window_features.iterrows():\n                        # Project feature to pixel coordinates\n                        feature_pixels = features.rasterize(\n                            [(feature.geometry, 1)],\n                            out_shape=(tile_size_y, tile_size_x),\n                            transform=window_transform,\n                        )\n                        mask = np.logical_or(mask, feature_pixels.astype(bool))\n\n                    # Apply mask to image\n                    for band in range(image_data.shape[0]):\n                        temp = image_data[band, :, :]\n                        temp[~mask] = 0\n                        image_data[band, :, :] = temp\n\n                # Apply rotation if specified\n                if augmentation:\n                    # Convert to torch tensor for augmentation\n                    image_tensor = torch.from_numpy(image_data).unsqueeze(\n                        0\n                    )  # Add batch dimension\n                    # Apply augmentation with proper data format\n                    augmented = augmentation({\"image\": image_tensor})\n                    image_data = (\n                        augmented[\"image\"].squeeze(0).numpy()\n                    )  # Remove batch dimension\n\n                # Create a processed version for regular image formats\n                processed_image = (image_data * 255).astype(np.uint8)\n\n                # Create label mask\n                label_mask = np.zeros((tile_size_y, tile_size_x), dtype=np.uint8)\n                has_features = False\n\n                if len(window_features) &gt; 0:\n                    for idx, feature in window_features.iterrows():\n                        # Get class value\n                        class_val = (\n                            feature[class_value_field]\n                            if class_value_field in feature\n                            else 1\n                        )\n                        if isinstance(class_val, str):\n                            # If class is a string, use its position in the unique classes list\n                            class_id = class_to_id.get(class_val, 1)\n                        else:\n                            # If class is already a number, use it directly\n                            class_id = int(class_val) if class_val &gt; 0 else 1\n\n                        # Get the geometry in pixel coordinates\n                        geom = feature.geometry.intersection(window_bounds)\n                        if not geom.is_empty:\n                            try:\n                                # Rasterize the feature\n                                feature_mask = features.rasterize(\n                                    [(geom, class_id)],\n                                    out_shape=(tile_size_y, tile_size_x),\n                                    transform=window_transform,\n                                    fill=0,\n                                    all_touched=all_touched,\n                                )\n\n                                # Update mask with higher class values taking precedence\n                                label_mask = np.maximum(label_mask, feature_mask)\n\n                                # Check if any pixels were added\n                                if np.any(feature_mask):\n                                    has_features = True\n                            except Exception as e:\n                                if not quiet:\n                                    pbar.write(f\"Error rasterizing feature {idx}: {e}\")\n                                stats[\"errors\"] += 1\n\n                # Save as GeoTIFF if requested\n                if save_geotiff or image_chip_format.upper() in [\n                    \"TIFF\",\n                    \"TIF\",\n                    \"GEOTIFF\",\n                ]:\n                    # Standardize extension to .tif for GeoTIFF files\n                    image_filename = f\"tile_{chip_index:06d}.tif\"\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create profile for the GeoTIFF\n                    profile = src.profile.copy()\n                    profile.update(\n                        {\n                            \"height\": tile_size_y,\n                            \"width\": tile_size_x,\n                            \"count\": orig_image_data.shape[0],\n                            \"transform\": window_transform,\n                        }\n                    )\n\n                    # Save the GeoTIFF with original data\n                    try:\n                        with rasterio.open(image_path, \"w\", **profile) as dst:\n                            dst.write(orig_image_data)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving image GeoTIFF for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n                else:\n                    # For non-GeoTIFF formats, use PIL to save the image\n                    image_filename = (\n                        f\"tile_{chip_index:06d}.{image_chip_format.lower()}\"\n                    )\n                    image_path = os.path.join(image_dir, image_filename)\n\n                    # Create PIL image for saving\n                    if processed_image.shape[0] == 1:\n                        img = Image.fromarray(processed_image[0])\n                    elif processed_image.shape[0] == 3:\n                        # For RGB, need to transpose and make sure it's the right data type\n                        rgb_data = np.transpose(processed_image, (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n                    else:\n                        # For multiband images, save only RGB or first three bands\n                        rgb_data = np.transpose(processed_image[:3], (1, 2, 0))\n                        img = Image.fromarray(rgb_data)\n\n                    # Save image\n                    try:\n                        img.save(image_path)\n                        stats[\"total_tiles\"] += 1\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(f\"ERROR saving image for tile {chip_index}: {e}\")\n                        stats[\"errors\"] += 1\n\n                # Save label as GeoTIFF\n                label_filename = f\"tile_{chip_index:06d}.tif\"\n                label_path = os.path.join(label_dir, label_filename)\n\n                # Create profile for label GeoTIFF\n                label_profile = {\n                    \"driver\": \"GTiff\",\n                    \"height\": tile_size_y,\n                    \"width\": tile_size_x,\n                    \"count\": 1,\n                    \"dtype\": \"uint8\",\n                    \"crs\": src.crs,\n                    \"transform\": window_transform,\n                }\n\n                # Save label GeoTIFF\n                try:\n                    with rasterio.open(label_path, \"w\", **label_profile) as dst:\n                        dst.write(label_mask, 1)\n\n                    if has_features:\n                        pixel_count = np.count_nonzero(label_mask)\n                        stats[\"tiles_with_features\"] += 1\n                        stats[\"feature_pixels\"] += pixel_count\n                except Exception as e:\n                    if not quiet:\n                        pbar.write(f\"ERROR saving label for tile {chip_index}: {e}\")\n                    stats[\"errors\"] += 1\n\n                # Also save a PNG version for easy visualization if requested\n                if metadata_format == \"PASCAL_VOC\":\n                    try:\n                        # Ensure correct data type for PIL\n                        png_label = label_mask.astype(np.uint8)\n                        label_img = Image.fromarray(png_label)\n                        label_png_path = os.path.join(\n                            label_dir, f\"tile_{chip_index:06d}.png\"\n                        )\n                        label_img.save(label_png_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving PNG label for tile {chip_index}: {e}\"\n                            )\n                            pbar.write(\n                                f\"  Label mask shape: {label_mask.shape}, dtype: {label_mask.dtype}\"\n                            )\n                            # Try again with explicit conversion\n                            try:\n                                # Alternative approach for problematic arrays\n                                png_data = np.zeros(\n                                    (tile_size_y, tile_size_x), dtype=np.uint8\n                                )\n                                np.copyto(png_data, label_mask, casting=\"unsafe\")\n                                label_img = Image.fromarray(png_data)\n                                label_img.save(label_png_path)\n                                pbar.write(\n                                    f\"  Succeeded using alternative conversion method\"\n                                )\n                            except Exception as e2:\n                                pbar.write(f\"  Second attempt also failed: {e2}\")\n                                stats[\"errors\"] += 1\n\n                # Generate annotations\n                if metadata_format == \"PASCAL_VOC\" and len(window_features) &gt; 0:\n                    # Create XML annotation\n                    root = ET.Element(\"annotation\")\n                    ET.SubElement(root, \"folder\").text = \"images\"\n                    ET.SubElement(root, \"filename\").text = image_filename\n\n                    size = ET.SubElement(root, \"size\")\n                    ET.SubElement(size, \"width\").text = str(tile_size_x)\n                    ET.SubElement(size, \"height\").text = str(tile_size_y)\n                    ET.SubElement(size, \"depth\").text = str(min(image_data.shape[0], 3))\n\n                    # Add georeference information\n                    geo = ET.SubElement(root, \"georeference\")\n                    ET.SubElement(geo, \"crs\").text = str(src.crs)\n                    ET.SubElement(geo, \"transform\").text = str(\n                        window_transform\n                    ).replace(\"\\n\", \"\")\n                    ET.SubElement(geo, \"bounds\").text = (\n                        f\"{minx}, {miny}, {maxx}, {maxy}\"\n                    )\n\n                    for _, feature in window_features.iterrows():\n                        # Convert feature geometry to pixel coordinates\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        obj = ET.SubElement(root, \"object\")\n                        ET.SubElement(obj, \"name\").text = str(\n                            feature[class_value_field]\n                        )\n                        ET.SubElement(obj, \"difficult\").text = \"0\"\n\n                        bbox = ET.SubElement(obj, \"bndbox\")\n                        ET.SubElement(bbox, \"xmin\").text = str(xmin)\n                        ET.SubElement(bbox, \"ymin\").text = str(ymin)\n                        ET.SubElement(bbox, \"xmax\").text = str(xmax)\n                        ET.SubElement(bbox, \"ymax\").text = str(ymax)\n\n                    # Save XML\n                    try:\n                        tree = ET.ElementTree(root)\n                        xml_path = os.path.join(ann_dir, f\"tile_{chip_index:06d}.xml\")\n                        tree.write(xml_path)\n                    except Exception as e:\n                        if not quiet:\n                            pbar.write(\n                                f\"ERROR saving XML annotation for tile {chip_index}: {e}\"\n                            )\n                        stats[\"errors\"] += 1\n\n                elif metadata_format == \"COCO\" and len(window_features) &gt; 0:\n                    # Add image info\n                    image_id = chip_index\n                    coco_annotations[\"images\"].append(\n                        {\n                            \"id\": image_id,\n                            \"file_name\": image_filename,\n                            \"width\": tile_size_x,\n                            \"height\": tile_size_y,\n                            \"crs\": str(src.crs),\n                            \"transform\": str(window_transform),\n                        }\n                    )\n\n                    # Add annotations for each feature\n                    for _, feature in window_features.iterrows():\n                        feature_bounds = feature.geometry.intersection(window_bounds)\n                        if feature_bounds.is_empty:\n                            continue\n\n                        # Get pixel coordinates of bounds\n                        minx_f, miny_f, maxx_f, maxy_f = feature_bounds.bounds\n\n                        # Convert to pixel coordinates\n                        col_min, row_min = ~window_transform * (minx_f, maxy_f)\n                        col_max, row_max = ~window_transform * (maxx_f, miny_f)\n\n                        # Ensure coordinates are within bounds\n                        xmin = max(0, min(tile_size_x, int(col_min)))\n                        ymin = max(0, min(tile_size_y, int(row_min)))\n                        xmax = max(0, min(tile_size_x, int(col_max)))\n                        ymax = max(0, min(tile_size_y, int(row_max)))\n\n                        # Skip if box is too small\n                        if xmax - xmin &lt; 1 or ymax - ymin &lt; 1:\n                            continue\n\n                        width = xmax - xmin\n                        height = ymax - ymin\n\n                        # Add annotation\n                        ann_id += 1\n                        category_id = class_to_id[feature[class_value_field]]\n\n                        coco_annotations[\"annotations\"].append(\n                            {\n                                \"id\": ann_id,\n                                \"image_id\": image_id,\n                                \"category_id\": category_id,\n                                \"bbox\": [xmin, ymin, width, height],\n                                \"area\": width * height,\n                                \"iscrowd\": 0,\n                            }\n                        )\n\n                # Update progress bar\n                pbar.update(1)\n                pbar.set_description(\n                    f\"Generated: {stats['total_tiles']}, With features: {stats['tiles_with_features']}\"\n                )\n\n                chip_index += 1\n\n        # Close progress bar\n        pbar.close()\n\n        # Save COCO annotations if applicable\n        if metadata_format == \"COCO\":\n            try:\n                with open(os.path.join(ann_dir, \"instances.json\"), \"w\") as f:\n                    json.dump(coco_annotations, f)\n            except Exception as e:\n                if not quiet:\n                    print(f\"ERROR saving COCO annotations: {e}\")\n                stats[\"errors\"] += 1\n\n        # Close secondary raster if opened\n        if src2:\n            src2.close()\n\n    # Print summary\n    if not quiet:\n        print(\"\\n------- Export Summary -------\")\n        print(f\"Total tiles exported: {stats['total_tiles']}\")\n        print(\n            f\"Tiles with features: {stats['tiles_with_features']} ({stats['tiles_with_features']/max(1, stats['total_tiles'])*100:.1f}%)\"\n        )\n        if stats[\"tiles_with_features\"] &gt; 0:\n            print(\n                f\"Average feature pixels per tile: {stats['feature_pixels']/stats['tiles_with_features']:.1f}\"\n            )\n        if stats[\"errors\"] &gt; 0:\n            print(f\"Errors encountered: {stats['errors']}\")\n        print(f\"Output saved to: {out_folder}\")\n\n        # Verify georeference in a sample image and label\n        if stats[\"total_tiles\"] &gt; 0:\n            print(\"\\n------- Georeference Verification -------\")\n            sample_image = os.path.join(image_dir, f\"tile_{start_index}.tif\")\n            sample_label = os.path.join(label_dir, f\"tile_{start_index}.tif\")\n\n            if os.path.exists(sample_image):\n                try:\n                    with rasterio.open(sample_image) as img:\n                        print(f\"Image CRS: {img.crs}\")\n                        print(f\"Image transform: {img.transform}\")\n                        print(\n                            f\"Image has georeference: {img.crs is not None and img.transform is not None}\"\n                        )\n                        print(\n                            f\"Image dimensions: {img.width}x{img.height}, {img.count} bands, {img.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying image georeference: {e}\")\n\n            if os.path.exists(sample_label):\n                try:\n                    with rasterio.open(sample_label) as lbl:\n                        print(f\"Label CRS: {lbl.crs}\")\n                        print(f\"Label transform: {lbl.transform}\")\n                        print(\n                            f\"Label has georeference: {lbl.crs is not None and lbl.transform is not None}\"\n                        )\n                        print(\n                            f\"Label dimensions: {lbl.width}x{lbl.height}, {lbl.count} bands, {lbl.dtypes[0]} type\"\n                        )\n                except Exception as e:\n                    print(f\"Error verifying label georeference: {e}\")\n\n    # Return statistics\n    return stats, out_folder\n</code></pre>"},{"location":"utils/#geoai.utils.geojson_to_coords","title":"<code>geojson_to_coords(geojson, src_crs='epsg:4326', dst_crs='epsg:4326')</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>str | dict</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>geoai/utils.py</code> <pre><code>def geojson_to_coords(\n    geojson: str, src_crs: str = \"epsg:4326\", dst_crs: str = \"epsg:4326\"\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.\n\n    Args:\n        geojson (str | dict): The geojson file path or a dictionary of feature collection.\n        src_crs (str, optional): The source CRS. Defaults to \"epsg:4326\".\n        dst_crs (str, optional): The destination CRS. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n\n    import json\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    if isinstance(geojson, dict):\n        geojson = json.dumps(geojson)\n    gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n    centroids = gdf.geometry.centroid\n    centroid_list = [[point.x, point.y] for point in centroids]\n    if src_crs != dst_crs:\n        centroid_list = transform_coords(\n            [x[0] for x in centroid_list],\n            [x[1] for x in centroid_list],\n            src_crs,\n            dst_crs,\n        )\n        centroid_list = [[x, y] for x, y in zip(centroid_list[0], centroid_list[1])]\n    return centroid_list\n</code></pre>"},{"location":"utils/#geoai.utils.geojson_to_xy","title":"<code>geojson_to_xy(src_fp, geojson, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>geojson</code> <code>str</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[float]]</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>geoai/utils.py</code> <pre><code>def geojson_to_xy(\n    src_fp: str, geojson: str, coord_crs: str = \"epsg:4326\", **kwargs: Any\n) -&gt; List[List[float]]:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        geojson: The geojson file path or a dictionary of feature collection.\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    with rasterio.open(src_fp) as src:\n        src_crs = src.crs\n    coords = geojson_to_coords(geojson, coord_crs, src_crs)\n    return coords_to_xy(src_fp, coords, src_crs, **kwargs)\n</code></pre>"},{"location":"utils/#geoai.utils.get_device","title":"<code>get_device()</code>","text":"<p>Returns the best available device for deep learning in the order: CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_device() -&gt; torch.device:\n    \"\"\"\n    Returns the best available device for deep learning in the order:\n    CUDA (NVIDIA GPU) &gt; MPS (Apple Silicon GPU) &gt; CPU\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n        return torch.device(\"mps\")\n    else:\n        return torch.device(\"cpu\")\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_info","title":"<code>get_raster_info(raster_path)</code>","text":"<p>Display basic information about a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing the basic information about the raster</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info(raster_path: str) -&gt; Dict[str, Any]:\n    \"\"\"Display basic information about a raster dataset.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster\n    \"\"\"\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Get basic metadata\n        info = {\n            \"driver\": src.driver,\n            \"width\": src.width,\n            \"height\": src.height,\n            \"count\": src.count,\n            \"dtype\": src.dtypes[0],\n            \"crs\": src.crs.to_string() if src.crs else \"No CRS defined\",\n            \"transform\": src.transform,\n            \"bounds\": src.bounds,\n            \"resolution\": (src.transform[0], -src.transform[4]),\n            \"nodata\": src.nodata,\n        }\n\n        # Calculate statistics for each band\n        stats = []\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n            band_stats = {\n                \"band\": i,\n                \"min\": float(band.min()),\n                \"max\": float(band.max()),\n                \"mean\": float(band.mean()),\n                \"std\": float(band.std()),\n            }\n            stats.append(band_stats)\n\n        info[\"band_stats\"] = stats\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_info_gdal","title":"<code>get_raster_info_gdal(raster_path)</code>","text":"<p>Get basic information about a raster dataset using GDAL.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the raster, or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_info_gdal(raster_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get basic information about a raster dataset using GDAL.\n\n    Args:\n        raster_path (str): Path to the raster file\n\n    Returns:\n        dict: Dictionary containing the basic information about the raster,\n            or None if the file cannot be opened\n    \"\"\"\n\n    from osgeo import gdal\n\n    # Open the dataset\n    ds = gdal.Open(raster_path)\n    if ds is None:\n        print(f\"Error: Could not open {raster_path}\")\n        return None\n\n    # Get basic information\n    info = {\n        \"driver\": ds.GetDriver().ShortName,\n        \"width\": ds.RasterXSize,\n        \"height\": ds.RasterYSize,\n        \"count\": ds.RasterCount,\n        \"projection\": ds.GetProjection(),\n        \"geotransform\": ds.GetGeoTransform(),\n    }\n\n    # Calculate resolution\n    gt = ds.GetGeoTransform()\n    if gt:\n        info[\"resolution\"] = (abs(gt[1]), abs(gt[5]))\n        info[\"origin\"] = (gt[0], gt[3])\n\n    # Get band information\n    bands_info = []\n    for i in range(1, ds.RasterCount + 1):\n        band = ds.GetRasterBand(i)\n        stats = band.GetStatistics(True, True)\n        band_info = {\n            \"band\": i,\n            \"datatype\": gdal.GetDataTypeName(band.DataType),\n            \"min\": stats[0],\n            \"max\": stats[1],\n            \"mean\": stats[2],\n            \"std\": stats[3],\n            \"nodata\": band.GetNoDataValue(),\n        }\n        bands_info.append(band_info)\n\n    info[\"bands\"] = bands_info\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_resolution","title":"<code>get_raster_resolution(image_path)</code>","text":"<p>Get pixel resolution from the raster using rasterio.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>The path to the raster image.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>A tuple of (x resolution, y resolution).</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_resolution(image_path: str) -&gt; Tuple[float, float]:\n    \"\"\"Get pixel resolution from the raster using rasterio.\n\n    Args:\n        image_path: The path to the raster image.\n\n    Returns:\n        A tuple of (x resolution, y resolution).\n    \"\"\"\n    with rasterio.open(image_path) as src:\n        res = src.res\n    return res\n</code></pre>"},{"location":"utils/#geoai.utils.get_raster_stats","title":"<code>get_raster_stats(raster_path, divide_by=1.0)</code>","text":"<p>Calculate statistics for each band in a raster dataset.</p> <p>This function computes min, max, mean, and standard deviation values for each band in the provided raster, returning results in a dictionary with lists for each statistic type.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>divide_by</code> <code>float</code> <p>Value to divide pixel values by. Defaults to 1.0, which keeps the original pixel</p> <code>1.0</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing lists of statistics with keys: - 'min': List of minimum values for each band - 'max': List of maximum values for each band - 'mean': List of mean values for each band - 'std': List of standard deviation values for each band</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_raster_stats(raster_path: str, divide_by: float = 1.0) -&gt; Dict[str, Any]:\n    \"\"\"Calculate statistics for each band in a raster dataset.\n\n    This function computes min, max, mean, and standard deviation values\n    for each band in the provided raster, returning results in a dictionary\n    with lists for each statistic type.\n\n    Args:\n        raster_path (str): Path to the raster file\n        divide_by (float, optional): Value to divide pixel values by.\n            Defaults to 1.0, which keeps the original pixel\n\n    Returns:\n        dict: Dictionary containing lists of statistics with keys:\n            - 'min': List of minimum values for each band\n            - 'max': List of maximum values for each band\n            - 'mean': List of mean values for each band\n            - 'std': List of standard deviation values for each band\n    \"\"\"\n    # Initialize the results dictionary with empty lists\n    stats = {\"min\": [], \"max\": [], \"mean\": [], \"std\": []}\n\n    # Open the raster dataset\n    with rasterio.open(raster_path) as src:\n        # Calculate statistics for each band\n        for i in range(1, src.count + 1):\n            band = src.read(i, masked=True)\n\n            # Append statistics for this band to each list\n            stats[\"min\"].append(float(band.min()) / divide_by)\n            stats[\"max\"].append(float(band.max()) / divide_by)\n            stats[\"mean\"].append(float(band.mean()) / divide_by)\n            stats[\"std\"].append(float(band.std()) / divide_by)\n\n    return stats\n</code></pre>"},{"location":"utils/#geoai.utils.get_vector_info","title":"<code>get_vector_info(vector_path)</code>","text":"<p>Display basic information about a vector dataset using GeoPandas.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the vector dataset</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info(vector_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Display basic information about a vector dataset using GeoPandas.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset\n    \"\"\"\n    # Open the vector dataset\n    gdf = (\n        gpd.read_parquet(vector_path)\n        if vector_path.endswith(\".parquet\")\n        else gpd.read_file(vector_path)\n    )\n\n    # Get basic metadata\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": os.path.splitext(vector_path)[1][1:].upper(),  # Format from extension\n        \"feature_count\": len(gdf),\n        \"crs\": str(gdf.crs),\n        \"geometry_type\": str(gdf.geom_type.value_counts().to_dict()),\n        \"attribute_count\": len(gdf.columns) - 1,  # Subtract the geometry column\n        \"attribute_names\": list(gdf.columns[gdf.columns != \"geometry\"]),\n        \"bounds\": gdf.total_bounds.tolist(),\n    }\n\n    # Add statistics about numeric attributes\n    numeric_columns = gdf.select_dtypes(include=[\"number\"]).columns\n    attribute_stats = {}\n    for col in numeric_columns:\n        if col != \"geometry\":\n            attribute_stats[col] = {\n                \"min\": gdf[col].min(),\n                \"max\": gdf[col].max(),\n                \"mean\": gdf[col].mean(),\n                \"std\": gdf[col].std(),\n                \"null_count\": gdf[col].isna().sum(),\n            }\n\n    info[\"attribute_stats\"] = attribute_stats\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.get_vector_info_ogr","title":"<code>get_vector_info_ogr(vector_path)</code>","text":"<p>Get basic information about a vector dataset using OGR.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing the basic information about the vector dataset, or None if the file cannot be opened</p> Source code in <code>geoai/utils.py</code> <pre><code>def get_vector_info_ogr(vector_path: str) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get basic information about a vector dataset using OGR.\n\n    Args:\n        vector_path (str): Path to the vector file\n\n    Returns:\n        dict: Dictionary containing the basic information about the vector dataset,\n            or None if the file cannot be opened\n    \"\"\"\n    from osgeo import ogr\n\n    # Register all OGR drivers\n    ogr.RegisterAll()\n\n    # Open the dataset\n    ds = ogr.Open(vector_path)\n    if ds is None:\n        print(f\"Error: Could not open {vector_path}\")\n        return None\n\n    # Basic dataset information\n    info = {\n        \"file_path\": vector_path,\n        \"driver\": ds.GetDriver().GetName(),\n        \"layer_count\": ds.GetLayerCount(),\n        \"layers\": [],\n    }\n\n    # Extract information for each layer\n    for i in range(ds.GetLayerCount()):\n        layer = ds.GetLayer(i)\n        layer_info = {\n            \"name\": layer.GetName(),\n            \"feature_count\": layer.GetFeatureCount(),\n            \"geometry_type\": ogr.GeometryTypeToName(layer.GetGeomType()),\n            \"spatial_ref\": (\n                layer.GetSpatialRef().ExportToWkt() if layer.GetSpatialRef() else \"None\"\n            ),\n            \"extent\": layer.GetExtent(),\n            \"fields\": [],\n        }\n\n        # Get field information\n        defn = layer.GetLayerDefn()\n        for j in range(defn.GetFieldCount()):\n            field_defn = defn.GetFieldDefn(j)\n            field_info = {\n                \"name\": field_defn.GetName(),\n                \"type\": field_defn.GetTypeName(),\n                \"width\": field_defn.GetWidth(),\n                \"precision\": field_defn.GetPrecision(),\n            }\n            layer_info[\"fields\"].append(field_info)\n\n        info[\"layers\"].append(layer_info)\n\n    # Close the dataset\n    ds = None\n\n    return info\n</code></pre>"},{"location":"utils/#geoai.utils.hybrid_regularization","title":"<code>hybrid_regularization(building_polygons)</code>","text":"<p>A comprehensive hybrid approach to building footprint regularization.</p> <p>Applies different strategies based on building characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def hybrid_regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    A comprehensive hybrid approach to building footprint regularization.\n\n    Applies different strategies based on building characteristics.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely.affinity import rotate\n    from shapely.geometry import Polygon\n\n    # Use minimum_rotated_rectangle instead of oriented_envelope\n    try:\n        from shapely.minimum_rotated_rectangle import minimum_rotated_rectangle\n    except ImportError:\n        # For older Shapely versions\n        def minimum_rotated_rectangle(geom):\n            \"\"\"Calculate the minimum rotated rectangle for a geometry\"\"\"\n            # For older Shapely versions, implement a simple version\n            return geom.minimum_rotated_rectangle\n\n    # Determine input type for correct return\n    is_gdf = isinstance(building_polygons, gpd.GeoDataFrame)\n\n    # Extract geometries if GeoDataFrame\n    if is_gdf:\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    results = []\n\n    for building in geom_objects:\n        # 1. Analyze building characteristics\n        if not hasattr(building, \"exterior\") or building.is_empty:\n            results.append(building)\n            continue\n\n        # Calculate shape complexity metrics\n        complexity = building.length / (4 * np.sqrt(building.area))\n\n        # Calculate dominant angle\n        coords = np.array(building.exterior.coords)[:-1]\n        segments = np.diff(np.vstack([coords, coords[0]]), axis=0)\n        segment_lengths = np.sqrt(segments[:, 0] ** 2 + segments[:, 1] ** 2)\n        segment_angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n        # Weight angles by segment length\n        hist, bins = np.histogram(\n            segment_angles % 180, bins=36, range=(0, 180), weights=segment_lengths\n        )\n        bin_centers = (bins[:-1] + bins[1:]) / 2\n        dominant_angle = bin_centers[np.argmax(hist)]\n\n        # Check if building is close to orthogonal\n        is_orthogonal = min(dominant_angle % 45, 45 - (dominant_angle % 45)) &lt; 5\n\n        # 2. Apply appropriate regularization strategy\n        if complexity &gt; 1.5:\n            # Complex buildings: use minimum rotated rectangle\n            result = minimum_rotated_rectangle(building)\n        elif is_orthogonal:\n            # Near-orthogonal buildings: orthogonalize in place\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Create orthogonal hull in rotated space\n            bounds = rotated.bounds\n            ortho_hull = Polygon(\n                [\n                    (bounds[0], bounds[1]),\n                    (bounds[2], bounds[1]),\n                    (bounds[2], bounds[3]),\n                    (bounds[0], bounds[3]),\n                ]\n            )\n\n            result = rotate(ortho_hull, dominant_angle, origin=\"centroid\")\n        else:\n            # Diagonal buildings: use custom approach for diagonal buildings\n            # Rotate to align with axes\n            rotated = rotate(building, -dominant_angle, origin=\"centroid\")\n\n            # Simplify in rotated space\n            simplified = rotated.simplify(0.3, preserve_topology=True)\n\n            # Get the bounds in rotated space\n            bounds = simplified.bounds\n            min_x, min_y, max_x, max_y = bounds\n\n            # Create a rectangular hull in rotated space\n            rect_poly = Polygon(\n                [(min_x, min_y), (max_x, min_y), (max_x, max_y), (min_x, max_y)]\n            )\n\n            # Rotate back to original orientation\n            result = rotate(rect_poly, dominant_angle, origin=\"centroid\")\n\n        results.append(result)\n\n    # Return in same format as input\n    if is_gdf:\n        return gpd.GeoDataFrame(geometry=results, crs=building_polygons.crs)\n    else:\n        return results\n</code></pre>"},{"location":"utils/#geoai.utils.inspect_pth_file","title":"<code>inspect_pth_file(pth_path)</code>","text":"<p>Inspect a PyTorch .pth model file to determine its architecture.</p> <p>Parameters:</p> Name Type Description Default <code>pth_path</code> <code>str</code> <p>Path to the .pth file to inspect</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Information about the model architecture</p> Source code in <code>geoai/utils.py</code> <pre><code>def inspect_pth_file(pth_path: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Inspect a PyTorch .pth model file to determine its architecture.\n\n    Args:\n        pth_path: Path to the .pth file to inspect\n\n    Returns:\n        Information about the model architecture\n    \"\"\"\n    # Check if file exists\n    if not os.path.exists(pth_path):\n        print(f\"Error: File {pth_path} not found\")\n        return\n\n    # Load the checkpoint\n    try:\n        checkpoint = torch.load(pth_path, map_location=torch.device(\"cpu\"))\n        print(f\"\\n{'='*50}\")\n        print(f\"Inspecting model file: {pth_path}\")\n        print(f\"{'='*50}\\n\")\n\n        # Check if it's a state_dict or a complete model\n        if isinstance(checkpoint, OrderedDict) or isinstance(checkpoint, dict):\n            if \"state_dict\" in checkpoint:\n                print(\"Found 'state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"state_dict\"]\n            elif \"model_state_dict\" in checkpoint:\n                print(\"Found 'model_state_dict' key in the checkpoint.\")\n                state_dict = checkpoint[\"model_state_dict\"]\n            else:\n                print(\"Assuming file contains a direct state_dict.\")\n                state_dict = checkpoint\n\n            # Print the keys in the checkpoint\n            print(\"\\nCheckpoint contains the following keys:\")\n            for key in checkpoint.keys():\n                if isinstance(checkpoint[key], dict):\n                    print(f\"- {key} (dictionary with {len(checkpoint[key])} items)\")\n                elif isinstance(checkpoint[key], (torch.Tensor, list, tuple)):\n                    print(\n                        f\"- {key} (shape/size: {len(checkpoint[key]) if isinstance(checkpoint[key], (list, tuple)) else checkpoint[key].shape})\"\n                    )\n                else:\n                    print(f\"- {key} ({type(checkpoint[key]).__name__})\")\n\n            # Try to infer the model architecture from the state_dict keys\n            print(\"\\nAnalyzing model architecture from state_dict...\")\n\n            # Extract layer keys for analysis\n            layer_keys = list(state_dict.keys())\n\n            # Print the first few layer keys to understand naming pattern\n            print(\"\\nFirst 10 layer names in state_dict:\")\n            for i, key in enumerate(layer_keys[:10]):\n                shape = state_dict[key].shape\n                print(f\"- {key} (shape: {shape})\")\n\n            # Look for architecture indicators in the keys\n            architecture_indicators = {\n                \"conv\": 0,\n                \"bn\": 0,\n                \"layer\": 0,\n                \"fc\": 0,\n                \"backbone\": 0,\n                \"encoder\": 0,\n                \"decoder\": 0,\n                \"unet\": 0,\n                \"resnet\": 0,\n                \"classifier\": 0,\n                \"deeplab\": 0,\n                \"fcn\": 0,\n            }\n\n            for key in layer_keys:\n                for indicator in architecture_indicators:\n                    if indicator in key.lower():\n                        architecture_indicators[indicator] += 1\n\n            print(\"\\nArchitecture indicators found in layer names:\")\n            for indicator, count in architecture_indicators.items():\n                if count &gt; 0:\n                    print(f\"- '{indicator}' appears {count} times\")\n\n            # Count total parameters\n            total_params = sum(p.numel() for p in state_dict.values())\n            print(f\"\\nTotal parameters: {total_params:,}\")\n\n            # Try to load the model with different architectures\n            print(\"\\nAttempting to match with common architectures...\")\n\n            # Try to identify if it's a segmentation model\n            if any(\"out\" in k or \"classifier\" in k for k in layer_keys):\n                print(\"Model appears to be a segmentation model.\")\n\n                # Check if it might be a UNet\n                if (\n                    architecture_indicators[\"encoder\"] &gt; 0\n                    and architecture_indicators[\"decoder\"] &gt; 0\n                ):\n                    print(\n                        \"Architecture seems to be a UNet-based model with encoder-decoder structure.\"\n                    )\n                # Check for FCN or DeepLab indicators\n                elif architecture_indicators[\"fcn\"] &gt; 0:\n                    print(\n                        \"Architecture seems to be FCN-based (Fully Convolutional Network).\"\n                    )\n                elif architecture_indicators[\"deeplab\"] &gt; 0:\n                    print(\"Architecture seems to be DeepLab-based.\")\n                elif architecture_indicators[\"backbone\"] &gt; 0:\n                    print(\n                        \"Model has a backbone architecture, likely a modern segmentation model.\"\n                    )\n\n            # Try to infer output classes from the final layer\n            output_layer_keys = [\n                k for k in layer_keys if \"classifier\" in k or k.endswith(\".out.weight\")\n            ]\n            if output_layer_keys:\n                output_shape = state_dict[output_layer_keys[0]].shape\n                if len(output_shape) &gt;= 2:\n                    num_classes = output_shape[0]\n                    print(f\"\\nModel likely has {num_classes} output classes.\")\n\n            print(\"\\nSUMMARY:\")\n            print(\"The model appears to be\", end=\" \")\n            if architecture_indicators[\"unet\"] &gt; 0:\n                print(\"a UNet architecture.\", end=\" \")\n            elif architecture_indicators[\"fcn\"] &gt; 0:\n                print(\"an FCN architecture.\", end=\" \")\n            elif architecture_indicators[\"deeplab\"] &gt; 0:\n                print(\"a DeepLab architecture.\", end=\" \")\n            elif architecture_indicators[\"resnet\"] &gt; 0:\n                print(\"ResNet-based.\", end=\" \")\n            else:\n                print(\"a custom architecture.\", end=\" \")\n\n            # Try to load with common models\n            try_common_architectures(state_dict)\n\n        else:\n            print(\n                \"The file contains an entire model object rather than just a state dictionary.\"\n            )\n            # If it's a complete model, we can directly examine its architecture\n            print(checkpoint)\n\n    except Exception as e:\n        print(f\"Error loading the model file: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>geoai/utils.py</code> <pre><code>def install_package(package: Union[str, List[str]]) -&gt; None:\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n    elif isinstance(package, list):\n        packages = package\n    else:\n        raise ValueError(\"The package argument must be a string or a list of strings.\")\n\n    for package in packages:\n        if package.startswith(\"https\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"utils/#geoai.utils.masks_to_vector","title":"<code>masks_to_vector(mask_path, output_path=None, simplify_tolerance=1.0, mask_threshold=0.5, min_object_area=100, max_object_area=None, nms_iou_threshold=0.5)</code>","text":"<p>Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.</p> <p>Parameters:</p> Name Type Description Default <code>mask_path</code> <code>str</code> <p>Path to the building masks GeoTIFF</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoJSON (default: mask_path with .geojson extension)</p> <code>None</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for polygon simplification (default: self.simplify_tolerance)</p> <code>1.0</code> <code>mask_threshold</code> <code>float</code> <p>Threshold for mask binarization (default: self.mask_threshold)</p> <code>0.5</code> <code>min_object_area</code> <code>int</code> <p>Minimum area in pixels to keep a building (default: self.min_object_area)</p> <code>100</code> <code>max_object_area</code> <code>Optional[int]</code> <p>Maximum area in pixels to keep a building (default: self.max_object_area)</p> <code>None</code> <code>nms_iou_threshold</code> <code>float</code> <p>IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)</p> <code>0.5</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>GeoDataFrame with building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def masks_to_vector(\n    mask_path: str,\n    output_path: Optional[str] = None,\n    simplify_tolerance: float = 1.0,\n    mask_threshold: float = 0.5,\n    min_object_area: int = 100,\n    max_object_area: Optional[int] = None,\n    nms_iou_threshold: float = 0.5,\n) -&gt; Any:\n    \"\"\"\n    Convert a building mask GeoTIFF to vector polygons and save as a vector dataset.\n\n    Args:\n        mask_path: Path to the building masks GeoTIFF\n        output_path: Path to save the output GeoJSON (default: mask_path with .geojson extension)\n        simplify_tolerance: Tolerance for polygon simplification (default: self.simplify_tolerance)\n        mask_threshold: Threshold for mask binarization (default: self.mask_threshold)\n        min_object_area: Minimum area in pixels to keep a building (default: self.min_object_area)\n        max_object_area: Maximum area in pixels to keep a building (default: self.max_object_area)\n        nms_iou_threshold: IoU threshold for non-maximum suppression (default: self.nms_iou_threshold)\n\n    Returns:\n        Any: GeoDataFrame with building footprints\n    \"\"\"\n    # Set default output path if not provided\n    # if output_path is None:\n    #     output_path = os.path.splitext(mask_path)[0] + \".geojson\"\n\n    print(f\"Converting mask to GeoJSON with parameters:\")\n    print(f\"- Mask threshold: {mask_threshold}\")\n    print(f\"- Min building area: {min_object_area}\")\n    print(f\"- Simplify tolerance: {simplify_tolerance}\")\n    print(f\"- NMS IoU threshold: {nms_iou_threshold}\")\n\n    # Open the mask raster\n    with rasterio.open(mask_path) as src:\n        # Read the mask data\n        mask_data = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Print mask statistics\n        print(f\"Mask dimensions: {mask_data.shape}\")\n        print(f\"Mask value range: {mask_data.min()} to {mask_data.max()}\")\n\n        # Prepare for connected component analysis\n        # Binarize the mask based on threshold\n        binary_mask = (mask_data &gt; (mask_threshold * 255)).astype(np.uint8)\n\n        # Apply morphological operations for better results (optional)\n        kernel = np.ones((3, 3), np.uint8)\n        binary_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find connected components\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        print(f\"Found {num_labels-1} potential buildings\")  # Subtract 1 for background\n\n        # Create list to store polygons and confidence values\n        all_polygons = []\n        all_confidences = []\n\n        # Process each component (skip the first one which is background)\n        for i in tqdm(range(1, num_labels)):\n            # Extract this building\n            area = stats[i, cv2.CC_STAT_AREA]\n\n            # Skip if too small\n            if area &lt; min_object_area:\n                continue\n\n            # Skip if too large\n            if max_object_area is not None and area &gt; max_object_area:\n                continue\n\n            # Create a mask for this building\n            building_mask = (labels == i).astype(np.uint8)\n\n            # Find contours\n            contours, _ = cv2.findContours(\n                building_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n\n            # Process each contour\n            for contour in contours:\n                # Skip if too few points\n                if contour.shape[0] &lt; 3:\n                    continue\n\n                # Simplify contour if it has many points\n                if contour.shape[0] &gt; 50 and simplify_tolerance &gt; 0:\n                    epsilon = simplify_tolerance * cv2.arcLength(contour, True)\n                    contour = cv2.approxPolyDP(contour, epsilon, True)\n\n                # Convert to list of (x, y) coordinates\n                polygon_points = contour.reshape(-1, 2)\n\n                # Convert pixel coordinates to geographic coordinates\n                geo_points = []\n                for x, y in polygon_points:\n                    gx, gy = transform * (x, y)\n                    geo_points.append((gx, gy))\n\n                # Create Shapely polygon\n                if len(geo_points) &gt;= 3:\n                    try:\n                        shapely_poly = Polygon(geo_points)\n                        if shapely_poly.is_valid and shapely_poly.area &gt; 0:\n                            all_polygons.append(shapely_poly)\n\n                            # Calculate \"confidence\" as normalized size\n                            # This is a proxy since we don't have model confidence scores\n                            normalized_size = min(1.0, area / 1000)  # Cap at 1.0\n                            all_confidences.append(normalized_size)\n                    except Exception as e:\n                        print(f\"Error creating polygon: {e}\")\n\n        print(f\"Created {len(all_polygons)} valid polygons\")\n\n        # Create GeoDataFrame\n        if not all_polygons:\n            print(\"No valid polygons found\")\n            return None\n\n        gdf = gpd.GeoDataFrame(\n            {\n                \"geometry\": all_polygons,\n                \"confidence\": all_confidences,\n                \"class\": 1,  # Building class\n            },\n            crs=crs,\n        )\n\n        def filter_overlapping_polygons(gdf, **kwargs):\n            \"\"\"\n            Filter overlapping polygons using non-maximum suppression.\n\n            Args:\n                gdf: GeoDataFrame with polygons\n                **kwargs: Optional parameters:\n                    nms_iou_threshold: IoU threshold for filtering\n\n            Returns:\n                Filtered GeoDataFrame\n            \"\"\"\n            if len(gdf) &lt;= 1:\n                return gdf\n\n            # Get parameters from kwargs or use instance defaults\n            iou_threshold = kwargs.get(\"nms_iou_threshold\", nms_iou_threshold)\n\n            # Sort by confidence\n            gdf = gdf.sort_values(\"confidence\", ascending=False)\n\n            # Fix any invalid geometries\n            gdf[\"geometry\"] = gdf[\"geometry\"].apply(\n                lambda geom: geom.buffer(0) if not geom.is_valid else geom\n            )\n\n            keep_indices = []\n            polygons = gdf.geometry.values\n\n            for i in range(len(polygons)):\n                if i in keep_indices:\n                    continue\n\n                keep = True\n                for j in keep_indices:\n                    # Skip invalid geometries\n                    if not polygons[i].is_valid or not polygons[j].is_valid:\n                        continue\n\n                    # Calculate IoU\n                    try:\n                        intersection = polygons[i].intersection(polygons[j]).area\n                        union = polygons[i].area + polygons[j].area - intersection\n                        iou = intersection / union if union &gt; 0 else 0\n\n                        if iou &gt; iou_threshold:\n                            keep = False\n                            break\n                    except Exception:\n                        # Skip on topology exceptions\n                        continue\n\n                if keep:\n                    keep_indices.append(i)\n\n            return gdf.iloc[keep_indices]\n\n        # Apply non-maximum suppression to remove overlapping polygons\n        gdf = filter_overlapping_polygons(gdf, nms_iou_threshold=nms_iou_threshold)\n\n        print(f\"Final building count after filtering: {len(gdf)}\")\n\n        # Save to file\n        if output_path is not None:\n            gdf.to_file(output_path)\n            print(f\"Saved {len(gdf)} building footprints to {output_path}\")\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.mosaic_geotiffs","title":"<code>mosaic_geotiffs(input_dir, output_file, mask_file=None)</code>","text":"<p>Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).</p> <p>This function identifies all GeoTIFF files in the specified directory, creates a seamless mosaic with proper handling of nodata values, and saves as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output will be clipped to the extent of the mask.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to the directory containing GeoTIFF files.</p> required <code>output_file</code> <code>str</code> <p>Path to the output Cloud Optimized GeoTIFF file.</p> required <code>mask_file</code> <code>str</code> <p>Path to a mask file to clip the output. If provided, the output will be clipped to the extent of this mask. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>None</code> <p>True if the mosaic was created successfully, False otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\nTrue\n&gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\nTrue\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def mosaic_geotiffs(\n    input_dir: str, output_file: str, mask_file: Optional[str] = None\n) -&gt; None:\n    \"\"\"Create a mosaic from all GeoTIFF files as a Cloud Optimized GeoTIFF (COG).\n\n    This function identifies all GeoTIFF files in the specified directory,\n    creates a seamless mosaic with proper handling of nodata values, and saves\n    as a Cloud Optimized GeoTIFF format. If a mask file is provided, the output\n    will be clipped to the extent of the mask.\n\n    Args:\n        input_dir (str): Path to the directory containing GeoTIFF files.\n        output_file (str): Path to the output Cloud Optimized GeoTIFF file.\n        mask_file (str, optional): Path to a mask file to clip the output.\n            If provided, the output will be clipped to the extent of this mask.\n            Defaults to None.\n\n    Returns:\n        bool: True if the mosaic was created successfully, False otherwise.\n\n    Examples:\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif')\n        True\n        &gt;&gt;&gt; mosaic_geotiffs('naip', 'merged_naip.tif', 'boundary.tif')\n        True\n    \"\"\"\n    import glob\n\n    from osgeo import gdal\n\n    gdal.UseExceptions()\n    # Get all tif files in the directory\n    tif_files = glob.glob(os.path.join(input_dir, \"*.tif\"))\n\n    if not tif_files:\n        print(\"No GeoTIFF files found in the specified directory.\")\n        return False\n\n    # Analyze the first input file to determine compression and nodata settings\n    ds = gdal.Open(tif_files[0])\n    if ds is None:\n        print(f\"Unable to open {tif_files[0]}\")\n        return False\n\n    # Get driver metadata from the first file\n    driver = ds.GetDriver()\n    creation_options = []\n\n    # Check compression type\n    metadata = ds.GetMetadata(\"IMAGE_STRUCTURE\")\n    if \"COMPRESSION\" in metadata:\n        compression = metadata[\"COMPRESSION\"]\n        creation_options.append(f\"COMPRESS={compression}\")\n    else:\n        # Default compression if none detected\n        creation_options.append(\"COMPRESS=LZW\")\n\n    # Add COG-specific creation options\n    creation_options.extend([\"TILED=YES\", \"BLOCKXSIZE=512\", \"BLOCKYSIZE=512\"])\n\n    # Check for nodata value in the first band of the first file\n    band = ds.GetRasterBand(1)\n    has_nodata = band.GetNoDataValue() is not None\n    nodata_value = band.GetNoDataValue() if has_nodata else None\n\n    # Close the dataset\n    ds = None\n\n    # Create a temporary VRT (Virtual Dataset)\n    vrt_path = os.path.join(input_dir, \"temp_mosaic.vrt\")\n\n    # Build VRT from input files with proper nodata handling\n    vrt_options = gdal.BuildVRTOptions(\n        resampleAlg=\"nearest\",\n        srcNodata=nodata_value if has_nodata else None,\n        VRTNodata=nodata_value if has_nodata else None,\n    )\n    vrt_dataset = gdal.BuildVRT(vrt_path, tif_files, options=vrt_options)\n\n    # Close the VRT dataset to flush it to disk\n    vrt_dataset = None\n\n    # Create temp mosaic\n    temp_mosaic = output_file + \".temp.tif\"\n\n    # Convert VRT to GeoTIFF with the same compression as input\n    translate_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=creation_options,\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(temp_mosaic, vrt_path, options=translate_options)\n\n    # Apply mask if provided\n    if mask_file and os.path.exists(mask_file):\n        print(f\"Clipping mosaic to mask: {mask_file}\")\n\n        # Create a temporary clipped file\n        clipped_mosaic = output_file + \".clipped.tif\"\n\n        # Open mask file\n        mask_ds = gdal.Open(mask_file)\n        if mask_ds is None:\n            print(f\"Unable to open mask file: {mask_file}\")\n            # Continue without clipping\n        else:\n            # Get mask extent\n            mask_geotransform = mask_ds.GetGeoTransform()\n            mask_projection = mask_ds.GetProjection()\n            mask_ulx = mask_geotransform[0]\n            mask_uly = mask_geotransform[3]\n            mask_lrx = mask_ulx + (mask_geotransform[1] * mask_ds.RasterXSize)\n            mask_lry = mask_uly + (mask_geotransform[5] * mask_ds.RasterYSize)\n\n            # Close mask dataset\n            mask_ds = None\n\n            # Use warp options to clip\n            warp_options = gdal.WarpOptions(\n                format=\"GTiff\",\n                outputBounds=[mask_ulx, mask_lry, mask_lrx, mask_uly],\n                dstSRS=mask_projection,\n                creationOptions=creation_options,\n                srcNodata=nodata_value if has_nodata else None,\n                dstNodata=nodata_value if has_nodata else None,\n            )\n\n            # Apply clipping\n            gdal.Warp(clipped_mosaic, temp_mosaic, options=warp_options)\n\n            # Remove the unclipped temp mosaic and use the clipped one\n            os.remove(temp_mosaic)\n            temp_mosaic = clipped_mosaic\n\n    # Create internal overviews for the temp mosaic\n    ds = gdal.Open(temp_mosaic, gdal.GA_Update)\n    overview_list = [2, 4, 8, 16, 32]\n    ds.BuildOverviews(\"NEAREST\", overview_list)\n    ds = None  # Close the dataset to ensure overviews are written\n\n    # Convert the temp mosaic to a proper COG\n    cog_options = gdal.TranslateOptions(\n        format=\"GTiff\",\n        creationOptions=[\n            \"TILED=YES\",\n            \"COPY_SRC_OVERVIEWS=YES\",\n            \"COMPRESS=DEFLATE\",\n            \"PREDICTOR=2\",\n            \"BLOCKXSIZE=512\",\n            \"BLOCKYSIZE=512\",\n        ],\n        noData=nodata_value if has_nodata else None,\n    )\n    gdal.Translate(output_file, temp_mosaic, options=cog_options)\n\n    # Clean up temporary files\n    if os.path.exists(vrt_path):\n        os.remove(vrt_path)\n    if os.path.exists(temp_mosaic):\n        os.remove(temp_mosaic)\n\n    print(f\"Cloud Optimized GeoTIFF mosaic created successfully: {output_file}\")\n    return True\n</code></pre>"},{"location":"utils/#geoai.utils.orthogonalize","title":"<code>orthogonalize(input_path, output_path=None, epsilon=0.2, min_area=10, min_segments=4, area_tolerance=0.7, detect_triangles=True)</code>","text":"<p>Orthogonalizes object masks in a GeoTIFF file.</p> <p>This function reads a GeoTIFF containing object masks (binary or labeled regions), converts the raster masks to vector polygons, applies orthogonalization to each polygon, and optionally writes the result to a GeoJSON file. The source code is adapted from the Solar Panel Detection algorithm by Esri. See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284. Credits to Esri for the original code.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input GeoTIFF file.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output GeoJSON file. If None, no file is saved.</p> <code>None</code> <code>epsilon</code> <code>float</code> <p>Simplification tolerance for the Douglas-Peucker algorithm. Higher values result in more simplification. Default is 0.2.</p> <code>0.2</code> <code>min_area</code> <code>float</code> <p>Minimum area of polygons to process (smaller ones are kept as-is).</p> <code>10</code> <code>min_segments</code> <code>int</code> <p>Minimum number of segments to keep after simplification. Default is 4 (for rectangular shapes).</p> <code>4</code> <code>area_tolerance</code> <code>float</code> <p>Allowed ratio of area change. Values less than 1.0 restrict area change. Default is 0.7 (allows reduction to 70% of original area).</p> <code>0.7</code> <code>detect_triangles</code> <code>bool</code> <p>If True, performs additional check to avoid creating triangular shapes.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>A GeoDataFrame containing the orthogonalized features.</p> Source code in <code>geoai/utils.py</code> <pre><code>def orthogonalize(\n    input_path,\n    output_path=None,\n    epsilon=0.2,\n    min_area=10,\n    min_segments=4,\n    area_tolerance=0.7,\n    detect_triangles=True,\n) -&gt; Any:\n    \"\"\"\n    Orthogonalizes object masks in a GeoTIFF file.\n\n    This function reads a GeoTIFF containing object masks (binary or labeled regions),\n    converts the raster masks to vector polygons, applies orthogonalization to each polygon,\n    and optionally writes the result to a GeoJSON file.\n    The source code is adapted from the Solar Panel Detection algorithm by Esri.\n    See https://www.arcgis.com/home/item.html?id=c2508d72f2614104bfcfd5ccf1429284.\n    Credits to Esri for the original code.\n\n    Args:\n        input_path (str): Path to the input GeoTIFF file.\n        output_path (str, optional): Path to save the output GeoJSON file. If None, no file is saved.\n        epsilon (float, optional): Simplification tolerance for the Douglas-Peucker algorithm.\n            Higher values result in more simplification. Default is 0.2.\n        min_area (float, optional): Minimum area of polygons to process (smaller ones are kept as-is).\n        min_segments (int, optional): Minimum number of segments to keep after simplification.\n            Default is 4 (for rectangular shapes).\n        area_tolerance (float, optional): Allowed ratio of area change. Values less than 1.0 restrict\n            area change. Default is 0.7 (allows reduction to 70% of original area).\n        detect_triangles (bool, optional): If True, performs additional check to avoid creating triangular shapes.\n\n    Returns:\n        Any: A GeoDataFrame containing the orthogonalized features.\n    \"\"\"\n\n    from functools import partial\n\n    def orthogonalize_ring(ring, epsilon=0.2, min_segments=4):\n        \"\"\"\n        Orthogonalizes a ring (list of coordinates).\n\n        Args:\n            ring (list): List of [x, y] coordinates forming a ring\n            epsilon (float, optional): Simplification tolerance\n            min_segments (int, optional): Minimum number of segments to keep\n\n        Returns:\n            list: Orthogonalized list of coordinates\n        \"\"\"\n        if len(ring) &lt;= 3:\n            return ring\n\n        # Convert to numpy array\n        ring_arr = np.array(ring)\n\n        # Get orientation\n        angle = math.degrees(get_orientation(ring_arr))\n\n        # Simplify using Ramer-Douglas-Peucker algorithm\n        ring_arr = simplify(ring_arr, eps=epsilon)\n\n        # If simplified too much, adjust epsilon to maintain minimum segments\n        if len(ring_arr) &lt; min_segments:\n            # Try with smaller epsilon until we get at least min_segments points\n            for adjust_factor in [0.75, 0.5, 0.25, 0.1]:\n                test_arr = simplify(np.array(ring), eps=epsilon * adjust_factor)\n                if len(test_arr) &gt;= min_segments:\n                    ring_arr = test_arr\n                    break\n\n        # Convert to dataframe for processing\n        df = to_dataframe(ring_arr)\n\n        # Add orientation information\n        add_orientation(df, angle)\n\n        # Align segments to orthogonal directions\n        df = align(df)\n\n        # Merge collinear line segments\n        df = merge_lines(df)\n\n        if len(df) == 0:\n            return ring\n\n        # If we have a triangle-like result (3 segments or less), return the original shape\n        if len(df) &lt;= 3:\n            return ring\n\n        # Join the orthogonalized segments back into a ring\n        joined_ring = join_ring(df)\n\n        # If the join operation didn't produce a valid ring, return the original\n        if len(joined_ring) == 0 or len(joined_ring[0]) &lt; 3:\n            return ring\n\n        # Enhanced validation: check for triangular result and geometric validity\n        result_coords = joined_ring[0]\n\n        # If result has 3 or fewer points (triangle), use original\n        if len(result_coords) &lt;= 3:  # 2 points + closing point (degenerate)\n            return ring\n\n        # Additional validation: check for degenerate geometry\n        # Calculate area ratio to detect if the shape got severely distorted\n        def calculate_polygon_area(coords):\n            if len(coords) &lt; 3:\n                return 0\n            area = 0\n            n = len(coords)\n            for i in range(n):\n                j = (i + 1) % n\n                area += coords[i][0] * coords[j][1]\n                area -= coords[j][0] * coords[i][1]\n            return abs(area) / 2\n\n        original_area = calculate_polygon_area(ring)\n        result_area = calculate_polygon_area(result_coords)\n\n        # If the area changed dramatically (more than 30% shrinkage or 300% growth), use original\n        if original_area &gt; 0 and result_area &gt; 0:\n            area_ratio = result_area / original_area\n            if area_ratio &lt; 0.3 or area_ratio &gt; 3.0:\n                return ring\n\n        # Check for triangular spikes and problematic artifacts\n        very_acute_angle_count = 0\n        triangular_spike_detected = False\n\n        for i in range(len(result_coords) - 1):  # -1 to exclude closing point\n            p1 = result_coords[i - 1]\n            p2 = result_coords[i]\n            p3 = result_coords[(i + 1) % (len(result_coords) - 1)]\n\n            # Calculate angle at p2\n            v1 = np.array([p1[0] - p2[0], p1[1] - p2[1]])\n            v2 = np.array([p3[0] - p2[0], p3[1] - p2[1]])\n\n            v1_norm = np.linalg.norm(v1)\n            v2_norm = np.linalg.norm(v2)\n\n            if v1_norm &gt; 0 and v2_norm &gt; 0:\n                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                cos_angle = np.clip(cos_angle, -1, 1)\n                angle = np.arccos(cos_angle)\n\n                # Count very acute angles (&lt; 20 degrees) - these are likely spikes\n                if angle &lt; np.pi / 9:  # 20 degrees\n                    very_acute_angle_count += 1\n                    # If it's very acute with short sides, it's definitely a spike\n                    if v1_norm &lt; 5 or v2_norm &lt; 5:\n                        triangular_spike_detected = True\n\n        # Check for excessively long edges that might be artifacts\n        edge_lengths = []\n        for i in range(len(result_coords) - 1):\n            edge_len = np.sqrt(\n                (result_coords[i + 1][0] - result_coords[i][0]) ** 2\n                + (result_coords[i + 1][1] - result_coords[i][1]) ** 2\n            )\n            edge_lengths.append(edge_len)\n\n        excessive_edge_detected = False\n        if len(edge_lengths) &gt; 0:\n            avg_edge_length = np.mean(edge_lengths)\n            max_edge_length = np.max(edge_lengths)\n            # Only reject if edge is extremely disproportionate (8x average)\n            if max_edge_length &gt; avg_edge_length * 8:\n                excessive_edge_detected = True\n\n        # Check for triangular artifacts by detecting spikes that extend beyond bounds\n        # Calculate original bounds\n        orig_xs = [p[0] for p in ring]\n        orig_ys = [p[1] for p in ring]\n        orig_min_x, orig_max_x = min(orig_xs), max(orig_xs)\n        orig_min_y, orig_max_y = min(orig_ys), max(orig_ys)\n        orig_width = orig_max_x - orig_min_x\n        orig_height = orig_max_y - orig_min_y\n\n        # Calculate result bounds\n        result_xs = [p[0] for p in result_coords]\n        result_ys = [p[1] for p in result_coords]\n        result_min_x, result_max_x = min(result_xs), max(result_xs)\n        result_min_y, result_max_y = min(result_ys), max(result_ys)\n\n        # Stricter bounds checking to catch triangular artifacts\n        bounds_extension_detected = False\n        # More conservative: only allow 10% extension\n        tolerance_x = max(orig_width * 0.1, 1.0)  # 10% tolerance, at least 1 unit\n        tolerance_y = max(orig_height * 0.1, 1.0)  # 10% tolerance, at least 1 unit\n\n        if (\n            result_min_x &lt; orig_min_x - tolerance_x\n            or result_max_x &gt; orig_max_x + tolerance_x\n            or result_min_y &lt; orig_min_y - tolerance_y\n            or result_max_y &gt; orig_max_y + tolerance_y\n        ):\n            bounds_extension_detected = True\n\n        # Reject if we detect triangular spikes, excessive edges, or bounds violations\n        if (\n            triangular_spike_detected\n            or very_acute_angle_count &gt; 2  # Multiple very acute angles\n            or excessive_edge_detected\n            or bounds_extension_detected\n        ):  # Any significant bounds extension\n            return ring\n\n        # Convert back to a list and ensure it's closed\n        result = joined_ring[0].tolist()\n        if len(result) &gt; 0 and (result[0] != result[-1]):\n            result.append(result[0])\n\n        return result\n\n    def vectorize_mask(mask, transform):\n        \"\"\"\n        Converts a binary mask to vector polygons.\n\n        Args:\n            mask (numpy.ndarray): Binary mask where non-zero values represent objects\n            transform (rasterio.transform.Affine): Affine transformation matrix\n\n        Returns:\n            list: List of GeoJSON features\n        \"\"\"\n        shapes = features.shapes(mask, transform=transform)\n        features_list = []\n\n        for shape, value in shapes:\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                features_list.append(\n                    {\n                        \"type\": \"Feature\",\n                        \"properties\": {\"value\": int(value)},\n                        \"geometry\": shape,\n                    }\n                )\n\n        return features_list\n\n    def rasterize_features(features, shape, transform, dtype=np.uint8):\n        \"\"\"\n        Converts vector features back to a raster mask.\n\n        Args:\n            features (list): List of GeoJSON features\n            shape (tuple): Shape of the output raster (height, width)\n            transform (rasterio.transform.Affine): Affine transformation matrix\n            dtype (numpy.dtype, optional): Data type of the output raster\n\n        Returns:\n            numpy.ndarray: Rasterized mask\n        \"\"\"\n        mask = features.rasterize(\n            [\n                (feature[\"geometry\"], feature[\"properties\"][\"value\"])\n                for feature in features\n            ],\n            out_shape=shape,\n            transform=transform,\n            fill=0,\n            dtype=dtype,\n        )\n\n        return mask\n\n    # The following helper functions are from the original code\n    def get_orientation(contour):\n        \"\"\"\n        Calculate the orientation angle of a contour.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            float: Orientation angle in radians\n        \"\"\"\n        box = cv2.minAreaRect(contour.astype(int))\n        (cx, cy), (w, h), angle = box\n        return math.radians(angle)\n\n    def simplify(contour, eps=0.2):\n        \"\"\"\n        Simplify a contour using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            contour (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n            eps (float, optional): Epsilon value for simplification\n\n        Returns:\n            numpy.ndarray: Simplified contour\n        \"\"\"\n        return rdp(contour, epsilon=eps)\n\n    def to_dataframe(ring):\n        \"\"\"\n        Convert a ring to a pandas DataFrame with line segment information.\n\n        Args:\n            ring (numpy.ndarray): Array of shape (n, 2) containing point coordinates\n\n        Returns:\n            pandas.DataFrame: DataFrame with line segment information\n        \"\"\"\n        df = pd.DataFrame(ring, columns=[\"x1\", \"y1\"])\n        df[\"x2\"] = df[\"x1\"].shift(-1)\n        df[\"y2\"] = df[\"y1\"].shift(-1)\n        df.dropna(inplace=True)\n        df[\"angle_atan\"] = np.arctan2((df[\"y2\"] - df[\"y1\"]), (df[\"x2\"] - df[\"x1\"]))\n        df[\"angle_atan_deg\"] = df[\"angle_atan\"] * 57.2958\n        df[\"len\"] = np.sqrt((df[\"y2\"] - df[\"y1\"]) ** 2 + (df[\"x2\"] - df[\"x1\"]) ** 2)\n        df[\"cx\"] = (df[\"x2\"] + df[\"x1\"]) / 2.0\n        df[\"cy\"] = (df[\"y2\"] + df[\"y1\"]) / 2.0\n        return df\n\n    def add_orientation(df, angle):\n        \"\"\"\n        Add orientation information to the DataFrame.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n            angle (float): Orientation angle in degrees\n\n        Returns:\n            None: Modifies the DataFrame in-place\n        \"\"\"\n        rtangle = angle + 90\n        is_parallel = (\n            (df[\"angle_atan_deg\"] &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] &lt; (angle + 45))\n        ) | (\n            (df[\"angle_atan_deg\"] + 180 &gt; (angle - 45))\n            &amp; (df[\"angle_atan_deg\"] + 180 &lt; (angle + 45))\n        )\n        df[\"angle\"] = math.radians(angle)\n        df[\"angle\"] = df[\"angle\"].where(is_parallel, math.radians(rtangle))\n\n    def align(df):\n        \"\"\"\n        Align line segments to their nearest orthogonal direction.\n\n        Args:\n            df (pandas.DataFrame): DataFrame with line segment information\n\n        Returns:\n            pandas.DataFrame: DataFrame with aligned line segments\n        \"\"\"\n        # Handle edge case with empty dataframe\n        if len(df) == 0:\n            return df.copy()\n\n        df_clone = df.copy()\n\n        # Ensure angle column exists and has valid values\n        if \"angle\" not in df_clone.columns or df_clone[\"angle\"].isna().any():\n            # If angle data is missing, add default angles based on atan2\n            df_clone[\"angle\"] = df_clone[\"angle_atan\"]\n\n        # Ensure length and center point data is valid\n        if \"len\" not in df_clone.columns or df_clone[\"len\"].isna().any():\n            # Recalculate lengths if missing\n            df_clone[\"len\"] = np.sqrt(\n                (df_clone[\"x2\"] - df_clone[\"x1\"]) ** 2\n                + (df_clone[\"y2\"] - df_clone[\"y1\"]) ** 2\n            )\n\n        if \"cx\" not in df_clone.columns or df_clone[\"cx\"].isna().any():\n            df_clone[\"cx\"] = (df_clone[\"x1\"] + df_clone[\"x2\"]) / 2.0\n\n        if \"cy\" not in df_clone.columns or df_clone[\"cy\"].isna().any():\n            df_clone[\"cy\"] = (df_clone[\"y1\"] + df_clone[\"y2\"]) / 2.0\n\n        # Apply orthogonal alignment\n        df_clone[\"x1\"] = df_clone[\"cx\"] - ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"x2\"] = df_clone[\"cx\"] + ((df_clone[\"len\"] / 2) * np.cos(df[\"angle\"]))\n        df_clone[\"y1\"] = df_clone[\"cy\"] - ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n        df_clone[\"y2\"] = df_clone[\"cy\"] + ((df_clone[\"len\"] / 2) * np.sin(df[\"angle\"]))\n\n        return df_clone\n\n    def merge_lines(df_aligned):\n        \"\"\"\n        Merge collinear line segments.\n\n        Args:\n            df_aligned (pandas.DataFrame): DataFrame with aligned line segments\n\n        Returns:\n            pandas.DataFrame: DataFrame with merged line segments\n        \"\"\"\n        ortho_lines = []\n        groups = df_aligned.groupby(\n            (df_aligned[\"angle\"].shift() != df_aligned[\"angle\"]).cumsum()\n        )\n        for x, y in groups:\n            group_cx = (y[\"cx\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            group_cy = (y[\"cy\"] * y[\"len\"]).sum() / y[\"len\"].sum()\n            cumlen = y[\"len\"].sum()\n\n            ortho_lines.append((group_cx, group_cy, cumlen, y[\"angle\"].iloc[0]))\n\n        ortho_list = []\n        for cx, cy, length, rot_angle in ortho_lines:\n            X1 = cx - (length / 2) * math.cos(rot_angle)\n            X2 = cx + (length / 2) * math.cos(rot_angle)\n            Y1 = cy - (length / 2) * math.sin(rot_angle)\n            Y2 = cy + (length / 2) * math.sin(rot_angle)\n\n            ortho_list.append(\n                {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": length,\n                    \"cx\": cx,\n                    \"cy\": cy,\n                    \"angle\": rot_angle,\n                }\n            )\n\n        # Improved fix: Prevent merging that would create triangular or problematic shapes\n        if (\n            len(ortho_list) &gt; 3 and ortho_list[0][\"angle\"] == ortho_list[-1][\"angle\"]\n        ):  # join first and last segment if they're in same direction\n            # Check if merging would result in 3 or 4 segments (potentially triangular)\n            resulting_segments = len(ortho_list) - 1\n            if resulting_segments &lt;= 4:\n                # For very small polygons, be extra cautious about merging\n                # Calculate the spatial relationship between first and last segments\n                first_center = np.array([ortho_list[0][\"cx\"], ortho_list[0][\"cy\"]])\n                last_center = np.array([ortho_list[-1][\"cx\"], ortho_list[-1][\"cy\"]])\n                center_distance = np.linalg.norm(first_center - last_center)\n\n                # Get average segment length for comparison\n                avg_length = sum(seg[\"len\"] for seg in ortho_list) / len(ortho_list)\n\n                # Only merge if segments are close enough and it won't create degenerate shapes\n                if center_distance &gt; avg_length * 1.5:\n                    # Skip merging - segments are too far apart\n                    pass\n                else:\n                    # Proceed with merging only for well-connected segments\n                    totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n                    merge_cx = (\n                        (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                        + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n                    ) / totlen\n\n                    merge_cy = (\n                        (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                        + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n                    ) / totlen\n\n                    rot_angle = ortho_list[0][\"angle\"]\n                    X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n                    X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n                    Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n                    Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n                    ortho_list[-1] = {\n                        \"x1\": X1,\n                        \"y1\": Y1,\n                        \"x2\": X2,\n                        \"y2\": Y2,\n                        \"len\": totlen,\n                        \"cx\": merge_cx,\n                        \"cy\": merge_cy,\n                        \"angle\": rot_angle,\n                    }\n                    ortho_list = ortho_list[1:]\n            else:\n                # For larger polygons, proceed with standard merging\n                totlen = ortho_list[0][\"len\"] + ortho_list[-1][\"len\"]\n                merge_cx = (\n                    (ortho_list[0][\"cx\"] * ortho_list[0][\"len\"])\n                    + (ortho_list[-1][\"cx\"] * ortho_list[-1][\"len\"])\n                ) / totlen\n\n                merge_cy = (\n                    (ortho_list[0][\"cy\"] * ortho_list[0][\"len\"])\n                    + (ortho_list[-1][\"cy\"] * ortho_list[-1][\"len\"])\n                ) / totlen\n\n                rot_angle = ortho_list[0][\"angle\"]\n                X1 = merge_cx - (totlen / 2) * math.cos(rot_angle)\n                X2 = merge_cx + (totlen / 2) * math.cos(rot_angle)\n                Y1 = merge_cy - (totlen / 2) * math.sin(rot_angle)\n                Y2 = merge_cy + (totlen / 2) * math.sin(rot_angle)\n\n                ortho_list[-1] = {\n                    \"x1\": X1,\n                    \"y1\": Y1,\n                    \"x2\": X2,\n                    \"y2\": Y2,\n                    \"len\": totlen,\n                    \"cx\": merge_cx,\n                    \"cy\": merge_cy,\n                    \"angle\": rot_angle,\n                }\n                ortho_list = ortho_list[1:]\n        ortho_df = pd.DataFrame(ortho_list)\n        return ortho_df\n\n    def find_intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n        \"\"\"\n        Find the intersection point of two line segments.\n\n        Args:\n            x1, y1, x2, y2: Coordinates of the first line segment\n            x3, y3, x4, y4: Coordinates of the second line segment\n\n        Returns:\n            list: [x, y] coordinates of the intersection point\n\n        Raises:\n            ZeroDivisionError: If the lines are parallel or collinear\n        \"\"\"\n        # Calculate the denominator of the intersection formula\n        denominator = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)\n\n        # Check if lines are parallel or collinear (denominator close to zero)\n        if abs(denominator) &lt; 1e-10:\n            raise ZeroDivisionError(\"Lines are parallel or collinear\")\n\n        px = (\n            (x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n        py = (\n            (x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)\n        ) / denominator\n\n        # Check if the intersection point is within a reasonable distance\n        # from both line segments to avoid extreme extrapolation\n        def point_on_segment(x, y, x1, y1, x2, y2, tolerance=2.0):\n            # Check if point (x,y) is near the line segment from (x1,y1) to (x2,y2)\n            # First check if it's near the infinite line\n            line_len = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n            if line_len &lt; 1e-10:\n                return np.sqrt((x - x1) ** 2 + (y - y1) ** 2) &lt;= tolerance\n\n            t = ((x - x1) * (x2 - x1) + (y - y1) * (y2 - y1)) / (line_len**2)\n\n            # Check distance to the infinite line\n            proj_x = x1 + t * (x2 - x1)\n            proj_y = y1 + t * (y2 - y1)\n            dist_to_line = np.sqrt((x - proj_x) ** 2 + (y - proj_y) ** 2)\n\n            # Check if the projection is near the segment, not just the infinite line\n            if t &lt; -tolerance or t &gt; 1 + tolerance:\n                # If far from the segment, compute distance to the nearest endpoint\n                dist_to_start = np.sqrt((x - x1) ** 2 + (y - y1) ** 2)\n                dist_to_end = np.sqrt((x - x2) ** 2 + (y - y2) ** 2)\n                return min(dist_to_start, dist_to_end) &lt;= tolerance * 2\n\n            return dist_to_line &lt;= tolerance\n\n        # Check if intersection is reasonably close to both line segments\n        if not (\n            point_on_segment(px, py, x1, y1, x2, y2)\n            and point_on_segment(px, py, x3, y3, x4, y4)\n        ):\n            # If intersection is far from segments, it's probably extrapolating too much\n            raise ValueError(\"Intersection point too far from line segments\")\n\n        return [px, py]\n\n    def join_ring(merged_df):\n        \"\"\"\n        Join line segments to form a closed ring.\n\n        Args:\n            merged_df (pandas.DataFrame): DataFrame with merged line segments\n\n        Returns:\n            numpy.ndarray: Array of shape (1, n, 2) containing the ring coordinates\n        \"\"\"\n        # Handle edge cases\n        if len(merged_df) &lt; 3:\n            # Not enough segments to form a valid polygon\n            return np.array([[]])\n\n        ring = []\n\n        # Find intersections between adjacent line segments\n        for i in range(len(merged_df) - 1):\n            x1, y1, x2, y2, *_ = merged_df.iloc[i]\n            x3, y3, x4, y4, *_ = merged_df.iloc[i + 1]\n\n            try:\n                intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n                # Check if the intersection point is too far from either line segment\n                # This helps prevent extending edges beyond reasonable bounds\n                dist_to_seg1 = min(\n                    np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                    np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n                )\n                dist_to_seg2 = min(\n                    np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                    np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n                )\n\n                # Use the maximum of line segment lengths as a reference\n                max_len = max(\n                    np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                    np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n                )\n\n                # Improved intersection validation\n                # Calculate angle between segments to detect sharp corners\n                v1 = np.array([x2 - x1, y2 - y1])\n                v2 = np.array([x4 - x3, y4 - y3])\n                v1_norm = np.linalg.norm(v1)\n                v2_norm = np.linalg.norm(v2)\n\n                if v1_norm &gt; 0 and v2_norm &gt; 0:\n                    cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                    cos_angle = np.clip(cos_angle, -1, 1)\n                    angle = np.arccos(cos_angle)\n\n                    # Check for very sharp angles that could create triangular artifacts\n                    is_sharp_angle = (\n                        angle &lt; np.pi / 6 or angle &gt; 5 * np.pi / 6\n                    )  # &lt;30\u00b0 or &gt;150\u00b0\n                else:\n                    is_sharp_angle = False\n\n                # Determine whether to use intersection or segment endpoint\n                if (\n                    dist_to_seg1 &gt; max_len * 0.5\n                    or dist_to_seg2 &gt; max_len * 0.5\n                    or is_sharp_angle\n                ):\n                    # Use a more conservative approach for problematic intersections\n                    # Use the closer endpoint between segments\n                    dist_x2_to_seg2 = min(\n                        np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2),\n                        np.sqrt((x2 - x4) ** 2 + (y2 - y4) ** 2),\n                    )\n                    dist_x3_to_seg1 = min(\n                        np.sqrt((x3 - x1) ** 2 + (y3 - y1) ** 2),\n                        np.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2),\n                    )\n\n                    if dist_x2_to_seg2 &lt;= dist_x3_to_seg1:\n                        ring.append([x2, y2])\n                    else:\n                        ring.append([x3, y3])\n                else:\n                    ring.append(intersection)\n            except Exception:\n                # If intersection calculation fails, use the endpoint of the first segment\n                ring.append([x2, y2])\n\n        # Connect last segment with first segment\n        x1, y1, x2, y2, *_ = merged_df.iloc[-1]\n        x3, y3, x4, y4, *_ = merged_df.iloc[0]\n\n        try:\n            intersection = find_intersection(x1, y1, x2, y2, x3, y3, x4, y4)\n\n            # Check if the intersection point is too far from either line segment\n            dist_to_seg1 = min(\n                np.sqrt((intersection[0] - x1) ** 2 + (intersection[1] - y1) ** 2),\n                np.sqrt((intersection[0] - x2) ** 2 + (intersection[1] - y2) ** 2),\n            )\n            dist_to_seg2 = min(\n                np.sqrt((intersection[0] - x3) ** 2 + (intersection[1] - y3) ** 2),\n                np.sqrt((intersection[0] - x4) ** 2 + (intersection[1] - y4) ** 2),\n            )\n\n            max_len = max(\n                np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2),\n                np.sqrt((x4 - x3) ** 2 + (y4 - y3) ** 2),\n            )\n\n            # Apply same sharp angle detection for closing segment\n            v1 = np.array([x2 - x1, y2 - y1])\n            v2 = np.array([x4 - x3, y4 - y3])\n            v1_norm = np.linalg.norm(v1)\n            v2_norm = np.linalg.norm(v2)\n\n            if v1_norm &gt; 0 and v2_norm &gt; 0:\n                cos_angle = np.dot(v1, v2) / (v1_norm * v2_norm)\n                cos_angle = np.clip(cos_angle, -1, 1)\n                angle = np.arccos(cos_angle)\n                is_sharp_angle = angle &lt; np.pi / 6 or angle &gt; 5 * np.pi / 6\n            else:\n                is_sharp_angle = False\n\n            if (\n                dist_to_seg1 &gt; max_len * 0.5\n                or dist_to_seg2 &gt; max_len * 0.5\n                or is_sharp_angle\n            ):\n                # Use conservative approach for closing segment\n                dist_x2_to_seg2 = min(\n                    np.sqrt((x2 - x3) ** 2 + (y2 - y3) ** 2),\n                    np.sqrt((x2 - x4) ** 2 + (y2 - y4) ** 2),\n                )\n                dist_x3_to_seg1 = min(\n                    np.sqrt((x3 - x1) ** 2 + (y3 - y1) ** 2),\n                    np.sqrt((x3 - x2) ** 2 + (y3 - y2) ** 2),\n                )\n\n                if dist_x2_to_seg2 &lt;= dist_x3_to_seg1:\n                    ring.append([x2, y2])\n                else:\n                    ring.append([x3, y3])\n            else:\n                ring.append(intersection)\n        except Exception:\n            # If intersection calculation fails, use the endpoint of the last segment\n            ring.append([x2, y2])\n\n        # Ensure the ring is closed\n        if len(ring) &gt; 0 and (ring[0][0] != ring[-1][0] or ring[0][1] != ring[-1][1]):\n            ring.append(ring[0])\n\n        return np.array([ring])\n\n    def rdp(M, epsilon=0, dist=None, algo=\"iter\", return_mask=False):\n        \"\"\"\n        Simplifies a given array of points using the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float, optional): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            algo (str, optional): Algorithm to use ('iter' or 'rec')\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray or list: Simplified points or mask\n        \"\"\"\n        if dist is None:\n            dist = pldist\n\n        if algo == \"iter\":\n            algo = partial(rdp_iter, return_mask=return_mask)\n        elif algo == \"rec\":\n            if return_mask:\n                raise NotImplementedError(\n                    'return_mask=True not supported with algo=\"rec\"'\n                )\n            algo = rdp_rec\n\n        if \"numpy\" in str(type(M)):\n            return algo(M, epsilon, dist)\n\n        return algo(np.array(M), epsilon, dist).tolist()\n\n    def pldist(point, start, end):\n        \"\"\"\n        Calculates the distance from 'point' to the line given by 'start' and 'end'.\n\n        Args:\n            point (numpy.ndarray): Point coordinates\n            start (numpy.ndarray): Start point of the line\n            end (numpy.ndarray): End point of the line\n\n        Returns:\n            float: Distance from point to line\n        \"\"\"\n        if np.all(np.equal(start, end)):\n            return np.linalg.norm(point - start)\n\n        # Fix for NumPy 2.0 deprecation warning - handle 2D vectors properly\n        # Instead of using cross product directly, calculate the area of the\n        # parallelogram formed by the vectors and divide by the length of the line\n        line_vec = end - start\n        point_vec = point - start\n\n        # Area of parallelogram = |a|*|b|*sin(\u03b8)\n        # For 2D vectors: |a\u00d7b| = |a|*|b|*sin(\u03b8) = determinant([ax, ay], [bx, by])\n        area = abs(line_vec[0] * point_vec[1] - line_vec[1] * point_vec[0])\n\n        # Distance = Area / |line_vec|\n        return area / np.linalg.norm(line_vec)\n\n    def rdp_rec(M, epsilon, dist=pldist):\n        \"\"\"\n        Recursive implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Simplified points\n        \"\"\"\n        dmax = 0.0\n        index = -1\n\n        for i in range(1, M.shape[0]):\n            d = dist(M[i], M[0], M[-1])\n\n            if d &gt; dmax:\n                index = i\n                dmax = d\n\n        if dmax &gt; epsilon:\n            r1 = rdp_rec(M[: index + 1], epsilon, dist)\n            r2 = rdp_rec(M[index:], epsilon, dist)\n\n            return np.vstack((r1[:-1], r2))\n        else:\n            return np.vstack((M[0], M[-1]))\n\n    def _rdp_iter(M, start_index, last_index, epsilon, dist=pldist):\n        \"\"\"\n        Internal iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            start_index (int): Start index\n            last_index (int): Last index\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n\n        Returns:\n            numpy.ndarray: Boolean mask of points to keep\n        \"\"\"\n        stk = []\n        stk.append([start_index, last_index])\n        global_start_index = start_index\n        indices = np.ones(last_index - start_index + 1, dtype=bool)\n\n        while stk:\n            start_index, last_index = stk.pop()\n\n            dmax = 0.0\n            index = start_index\n\n            for i in range(index + 1, last_index):\n                if indices[i - global_start_index]:\n                    d = dist(M[i], M[start_index], M[last_index])\n                    if d &gt; dmax:\n                        index = i\n                        dmax = d\n\n            if dmax &gt; epsilon:\n                stk.append([start_index, index])\n                stk.append([index, last_index])\n            else:\n                for i in range(start_index + 1, last_index):\n                    indices[i - global_start_index] = False\n\n        return indices\n\n    def rdp_iter(M, epsilon, dist=pldist, return_mask=False):\n        \"\"\"\n        Iterative implementation of the Ramer-Douglas-Peucker algorithm.\n\n        Args:\n            M (numpy.ndarray): Array of shape (n, d) containing point coordinates\n            epsilon (float): Epsilon value for simplification\n            dist (callable, optional): Distance function\n            return_mask (bool, optional): Whether to return a mask instead of the simplified array\n\n        Returns:\n            numpy.ndarray: Simplified points or boolean mask\n        \"\"\"\n        mask = _rdp_iter(M, 0, len(M) - 1, epsilon, dist)\n\n        if return_mask:\n            return mask\n\n        return M[mask]\n\n    # Read the raster data\n    with rasterio.open(input_path) as src:\n        # Read the first band (assuming it contains the mask)\n        mask = src.read(1)\n        transform = src.transform\n        crs = src.crs\n\n        # Extract shapes from the raster mask\n        shapes = list(features.shapes(mask, transform=transform))\n\n        # Initialize progress bar\n        print(f\"Processing {len(shapes)} features...\")\n\n        # Convert shapes to GeoJSON features\n        features_list = []\n        for shape, value in tqdm(shapes, desc=\"Converting features\", unit=\"shape\"):\n            if value &gt; 0:  # Only process non-zero values (actual objects)\n                # Convert GeoJSON geometry to Shapely polygon\n                polygon = Polygon(shape[\"coordinates\"][0])\n\n                # Skip tiny polygons\n                if polygon.area &lt; min_area:\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n                    continue\n\n                # Check if shape is triangular and if we want to avoid triangular shapes\n                if detect_triangles:\n                    # Create a simplified version to check number of vertices\n                    simple_polygon = polygon.simplify(epsilon)\n                    if (\n                        len(simple_polygon.exterior.coords) &lt;= 4\n                    ):  # 3 points + closing point\n                        # Likely a triangular shape - skip orthogonalization\n                        features_list.append(\n                            {\n                                \"type\": \"Feature\",\n                                \"properties\": {\"value\": int(value)},\n                                \"geometry\": shape,\n                            }\n                        )\n                        continue\n\n                # Process larger, non-triangular polygons\n                try:\n                    # Convert shapely polygon to a ring format for orthogonalization\n                    exterior_ring = list(polygon.exterior.coords)\n                    interior_rings = [\n                        list(interior.coords) for interior in polygon.interiors\n                    ]\n\n                    # Calculate bounding box aspect ratio to help with parameter tuning\n                    minx, miny, maxx, maxy = polygon.bounds\n                    width = maxx - minx\n                    height = maxy - miny\n                    aspect_ratio = max(width, height) / max(1.0, min(width, height))\n\n                    # Determine if this shape is likely to be a building/rectangular object\n                    # Long thin objects might require different treatment\n                    is_rectangular = aspect_ratio &lt; 3.0\n\n                    # Rectangular objects usually need more careful orthogonalization\n                    epsilon_adjusted = epsilon\n                    min_segments_adjusted = min_segments\n\n                    if is_rectangular:\n                        # For rectangular objects, use more conservative epsilon\n                        epsilon_adjusted = epsilon * 0.75\n                        # Ensure we get at least 4 points for a proper rectangle\n                        min_segments_adjusted = max(4, min_segments)\n\n                    # Orthogonalize the exterior and interior rings\n                    orthogonalized_exterior = orthogonalize_ring(\n                        exterior_ring,\n                        epsilon=epsilon_adjusted,\n                        min_segments=min_segments_adjusted,\n                    )\n\n                    orthogonalized_interiors = [\n                        orthogonalize_ring(\n                            ring,\n                            epsilon=epsilon_adjusted,\n                            min_segments=min_segments_adjusted,\n                        )\n                        for ring in interior_rings\n                    ]\n\n                    # Validate the result - calculate area change\n                    original_area = polygon.area\n                    orthogonalized_poly = Polygon(orthogonalized_exterior)\n\n                    if orthogonalized_poly.is_valid:\n                        area_ratio = (\n                            orthogonalized_poly.area / original_area\n                            if original_area &gt; 0\n                            else 0\n                        )\n\n                        # If area changed too much, revert to original\n                        if area_ratio &lt; area_tolerance or area_ratio &gt; (\n                            1.0 / area_tolerance\n                        ):\n                            # Use original polygon instead\n                            geometry = shape\n                        else:\n                            # Create a new geometry with orthogonalized rings\n                            geometry = {\n                                \"type\": \"Polygon\",\n                                \"coordinates\": [orthogonalized_exterior],\n                            }\n\n                            # Add interior rings if they exist\n                            if orthogonalized_interiors:\n                                geometry[\"coordinates\"].extend(\n                                    [ring for ring in orthogonalized_interiors]\n                                )\n                    else:\n                        # If resulting polygon is invalid, use original\n                        geometry = shape\n\n                    # Add the feature to the list\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": geometry,\n                        }\n                    )\n                except Exception as e:\n                    # Keep the original shape if orthogonalization fails\n                    features_list.append(\n                        {\n                            \"type\": \"Feature\",\n                            \"properties\": {\"value\": int(value)},\n                            \"geometry\": shape,\n                        }\n                    )\n\n        # Create the final GeoJSON structure\n        geojson = {\n            \"type\": \"FeatureCollection\",\n            \"crs\": {\"type\": \"name\", \"properties\": {\"name\": str(crs)}},\n            \"features\": features_list,\n        }\n\n        # Convert to GeoDataFrame and set the CRS\n        gdf = gpd.GeoDataFrame.from_features(geojson[\"features\"], crs=crs)\n\n        # Save to file if output_path is provided\n        if output_path:\n            print(f\"Saving to {output_path}...\")\n            gdf.to_file(output_path)\n            print(\"Done!\")\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.plot_batch","title":"<code>plot_batch(batch, bright=1.0, cols=4, width=5, chnls=[2, 1, 0], cmap='Blues')</code>","text":"<p>Plot a batch of images and masks. This function is adapted from the plot_batch() function in the torchgeo library at https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html Credit to the torchgeo developers for the original implementation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Dict[str, Any]</code> <p>The batch containing images and masks.</p> required <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <code>cols</code> <code>int</code> <p>The number of columns in the plot grid. Defaults to 4.</p> <code>4</code> <code>width</code> <code>int</code> <p>The width of each plot. Defaults to 5.</p> <code>5</code> <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>cmap</code> <code>str</code> <p>The colormap to use for masks. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_batch(\n    batch: Dict[str, Any],\n    bright: float = 1.0,\n    cols: int = 4,\n    width: int = 5,\n    chnls: List[int] = [2, 1, 0],\n    cmap: str = \"Blues\",\n) -&gt; None:\n    \"\"\"\n    Plot a batch of images and masks. This function is adapted from the plot_batch()\n    function in the torchgeo library at\n    https://torchgeo.readthedocs.io/en/stable/tutorials/earth_surface_water.html\n    Credit to the torchgeo developers for the original implementation.\n\n    Args:\n        batch (Dict[str, Any]): The batch containing images and masks.\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n        cols (int, optional): The number of columns in the plot grid. Defaults to 4.\n        width (int, optional): The width of each plot. Defaults to 5.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        cmap (str, optional): The colormap to use for masks. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n\n    try:\n        from torchgeo.datasets import unbind_samples\n    except ImportError as e:\n        raise ImportError(\n            \"Your torchgeo version is too old. Please upgrade to the latest version using 'pip install -U torchgeo'.\"\n        )\n\n    # Get the samples and the number of items in the batch\n    samples = unbind_samples(batch.copy())\n\n    # if batch contains images and masks, the number of images will be doubled\n    n = 2 * len(samples) if (\"image\" in batch) and (\"mask\" in batch) else len(samples)\n\n    # calculate the number of rows in the grid\n    rows = n // cols + (1 if n % cols != 0 else 0)\n\n    # create a grid\n    _, axs = plt.subplots(rows, cols, figsize=(cols * width, rows * width))\n\n    if (\"image\" in batch) and (\"mask\" in batch):\n        # plot the images on the even axis\n        plot_images(\n            images=map(lambda x: x[\"image\"], samples),\n            axs=axs.reshape(-1)[::2],\n            chnls=chnls,\n            bright=bright,\n        )\n\n        # plot the masks on the odd axis\n        plot_masks(masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1)[1::2])\n\n    else:\n        if \"image\" in batch:\n            plot_images(\n                images=map(lambda x: x[\"image\"], samples),\n                axs=axs.reshape(-1),\n                chnls=chnls,\n                bright=bright,\n            )\n\n        elif \"mask\" in batch:\n            plot_masks(\n                masks=map(lambda x: x[\"mask\"], samples), axs=axs.reshape(-1), cmap=cmap\n            )\n</code></pre>"},{"location":"utils/#geoai.utils.plot_images","title":"<code>plot_images(images, axs, chnls=[2, 1, 0], bright=1.0)</code>","text":"<p>Plot a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Iterable[Tensor]</code> <p>The images to plot.</p> required <code>axs</code> <code>Iterable[Axes]</code> <p>The axes to plot the images on.</p> required <code>chnls</code> <code>List[int]</code> <p>The channels to use for RGB. Defaults to [2, 1, 0].</p> <code>[2, 1, 0]</code> <code>bright</code> <code>float</code> <p>The brightness factor. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_images(\n    images: Iterable[torch.Tensor],\n    axs: Iterable[plt.Axes],\n    chnls: List[int] = [2, 1, 0],\n    bright: float = 1.0,\n) -&gt; None:\n    \"\"\"\n    Plot a list of images.\n\n    Args:\n        images (Iterable[torch.Tensor]): The images to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the images on.\n        chnls (List[int], optional): The channels to use for RGB. Defaults to [2, 1, 0].\n        bright (float, optional): The brightness factor. Defaults to 1.0.\n\n    Returns:\n        None\n    \"\"\"\n    for img, ax in zip(images, axs):\n        arr = torch.clamp(bright * img, min=0, max=1).numpy()\n        rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n        ax.imshow(rgb)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"utils/#geoai.utils.plot_masks","title":"<code>plot_masks(masks, axs, cmap='Blues')</code>","text":"<p>Plot a list of masks.</p> <p>Parameters:</p> Name Type Description Default <code>masks</code> <code>Iterable[Tensor]</code> <p>The masks to plot.</p> required <code>axs</code> <code>Iterable[Axes]</code> <p>The axes to plot the masks on.</p> required <code>cmap</code> <code>str</code> <p>The colormap to use. Defaults to \"Blues\".</p> <code>'Blues'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_masks(\n    masks: Iterable[torch.Tensor], axs: Iterable[plt.Axes], cmap: str = \"Blues\"\n) -&gt; None:\n    \"\"\"\n    Plot a list of masks.\n\n    Args:\n        masks (Iterable[torch.Tensor]): The masks to plot.\n        axs (Iterable[plt.Axes]): The axes to plot the masks on.\n        cmap (str, optional): The colormap to use. Defaults to \"Blues\".\n\n    Returns:\n        None\n    \"\"\"\n    for mask, ax in zip(masks, axs):\n        ax.imshow(mask.squeeze().numpy(), cmap=cmap)\n        ax.axis(\"off\")\n</code></pre>"},{"location":"utils/#geoai.utils.plot_performance_metrics","title":"<code>plot_performance_metrics(history_path, figsize=(15, 5), verbose=True, save_path=None, kwargs=None)</code>","text":"<p>Plot performance metrics from a history object.</p> <p>Parameters:</p> Name Type Description Default <code>history_path</code> <code>str</code> <p>The history object to plot.</p> required <code>figsize</code> <code>Tuple[int, int]</code> <p>The figure size.</p> <code>(15, 5)</code> <code>verbose</code> <code>bool</code> <p>Whether to print the best and final metrics.</p> <code>True</code> Source code in <code>geoai/utils.py</code> <pre><code>def plot_performance_metrics(\n    history_path: str,\n    figsize: Tuple[int, int] = (15, 5),\n    verbose: bool = True,\n    save_path: Optional[str] = None,\n    kwargs: Optional[Dict] = None,\n) -&gt; None:\n    \"\"\"Plot performance metrics from a history object.\n\n    Args:\n        history_path: The history object to plot.\n        figsize: The figure size.\n        verbose: Whether to print the best and final metrics.\n    \"\"\"\n    if kwargs is None:\n        kwargs = {}\n    history = torch.load(history_path)\n\n    # Handle different key naming conventions\n    train_loss_key = \"train_losses\" if \"train_losses\" in history else \"train_loss\"\n    val_loss_key = \"val_losses\" if \"val_losses\" in history else \"val_loss\"\n    val_iou_key = \"val_ious\" if \"val_ious\" in history else \"val_iou\"\n    val_dice_key = \"val_dices\" if \"val_dices\" in history else \"val_dice\"\n\n    # Determine number of subplots based on available metrics\n    has_dice = val_dice_key in history\n    n_plots = 3 if has_dice else 2\n    figsize = (15, 5) if has_dice else (10, 5)\n\n    plt.figure(figsize=figsize)\n\n    # Plot loss\n    plt.subplot(1, n_plots, 1)\n    if train_loss_key in history:\n        plt.plot(history[train_loss_key], label=\"Train Loss\")\n    if val_loss_key in history:\n        plt.plot(history[val_loss_key], label=\"Val Loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot IoU\n    plt.subplot(1, n_plots, 2)\n    if val_iou_key in history:\n        plt.plot(history[val_iou_key], label=\"Val IoU\")\n    plt.title(\"IoU Score\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"IoU\")\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Dice if available\n    if has_dice:\n        plt.subplot(1, n_plots, 3)\n        plt.plot(history[val_dice_key], label=\"Val Dice\")\n        plt.title(\"Dice Score\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Dice\")\n        plt.legend()\n        plt.grid(True)\n\n    plt.tight_layout()\n\n    if save_path:\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 150\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n        plt.savefig(save_path, **kwargs)\n\n    plt.show()\n\n    if verbose:\n        if val_iou_key in history:\n            print(f\"Best IoU: {max(history[val_iou_key]):.4f}\")\n            print(f\"Final IoU: {history[val_iou_key][-1]:.4f}\")\n        if val_dice_key in history:\n            print(f\"Best Dice: {max(history[val_dice_key]):.4f}\")\n            print(f\"Final Dice: {history[val_dice_key][-1]:.4f}\")\n</code></pre>"},{"location":"utils/#geoai.utils.plot_prediction_comparison","title":"<code>plot_prediction_comparison(original_image, prediction_image, ground_truth_image=None, titles=None, figsize=(15, 5), save_path=None, show_plot=True, prediction_colormap='gray', ground_truth_colormap='gray', original_colormap=None, indexes=None, divider=None)</code>","text":"<p>Plot original image, prediction, and optional ground truth side by side.</p> <p>Supports input as file paths, NumPy arrays, or PIL Images. For multi-band images, selected channels can be specified via <code>indexes</code>. If the image data is not normalized (e.g., Sentinel-2 [0, 10000]), the <code>divider</code> can be used to scale values for visualization.</p> <p>Parameters:</p> Name Type Description Default <code>original_image</code> <code>Union[str, ndarray, Image]</code> <p>Original input image as a file path, NumPy array, or PIL Image.</p> required <code>prediction_image</code> <code>Union[str, ndarray, Image]</code> <p>Predicted segmentation mask image.</p> required <code>ground_truth_image</code> <code>Optional[Union[str, ndarray, Image]]</code> <p>Ground truth mask image. Defaults to None.</p> <code>None</code> <code>titles</code> <code>Optional[List[str]]</code> <p>List of titles for the subplots. If not provided, default titles are used.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>Size of the entire figure in inches. Defaults to (15, 5).</p> <code>(15, 5)</code> <code>save_path</code> <code>Optional[str]</code> <p>If specified, saves the figure to this path. Defaults to None.</p> <code>None</code> <code>show_plot</code> <code>bool</code> <p>Whether to display the figure using plt.show(). Defaults to True.</p> <code>True</code> <code>prediction_colormap</code> <code>str</code> <p>Colormap to use for the prediction mask. Defaults to \"gray\".</p> <code>'gray'</code> <code>ground_truth_colormap</code> <code>str</code> <p>Colormap to use for the ground truth mask. Defaults to \"gray\".</p> <code>'gray'</code> <code>original_colormap</code> <code>Optional[str]</code> <p>Colormap to use for the original image if it's grayscale. Defaults to None.</p> <code>None</code> <code>indexes</code> <code>Optional[List[int]]</code> <p>List of band/channel indexes (0-based for NumPy, 1-based for rasterio) to extract from the original image. Useful for multi-band imagery like Sentinel-2. Defaults to None.</p> <code>None</code> <code>divider</code> <code>Optional[float]</code> <p>Value to divide the original image by for normalization (e.g., 10000 for reflectance). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>matplotlib.figure.Figure: The generated matplotlib figure object.</p> Source code in <code>geoai/utils.py</code> <pre><code>def plot_prediction_comparison(\n    original_image: Union[str, np.ndarray, Image.Image],\n    prediction_image: Union[str, np.ndarray, Image.Image],\n    ground_truth_image: Optional[Union[str, np.ndarray, Image.Image]] = None,\n    titles: Optional[List[str]] = None,\n    figsize: Tuple[int, int] = (15, 5),\n    save_path: Optional[str] = None,\n    show_plot: bool = True,\n    prediction_colormap: str = \"gray\",\n    ground_truth_colormap: str = \"gray\",\n    original_colormap: Optional[str] = None,\n    indexes: Optional[List[int]] = None,\n    divider: Optional[float] = None,\n) -&gt; None:\n    \"\"\"Plot original image, prediction, and optional ground truth side by side.\n\n    Supports input as file paths, NumPy arrays, or PIL Images. For multi-band\n    images, selected channels can be specified via `indexes`. If the image data\n    is not normalized (e.g., Sentinel-2 [0, 10000]), the `divider` can be used\n    to scale values for visualization.\n\n    Args:\n        original_image (Union[str, np.ndarray, Image.Image]):\n            Original input image as a file path, NumPy array, or PIL Image.\n        prediction_image (Union[str, np.ndarray, Image.Image]):\n            Predicted segmentation mask image.\n        ground_truth_image (Optional[Union[str, np.ndarray, Image.Image]], optional):\n            Ground truth mask image. Defaults to None.\n        titles (Optional[List[str]], optional):\n            List of titles for the subplots. If not provided, default titles are used.\n        figsize (Tuple[int, int], optional):\n            Size of the entire figure in inches. Defaults to (15, 5).\n        save_path (Optional[str], optional):\n            If specified, saves the figure to this path. Defaults to None.\n        show_plot (bool, optional):\n            Whether to display the figure using plt.show(). Defaults to True.\n        prediction_colormap (str, optional):\n            Colormap to use for the prediction mask. Defaults to \"gray\".\n        ground_truth_colormap (str, optional):\n            Colormap to use for the ground truth mask. Defaults to \"gray\".\n        original_colormap (Optional[str], optional):\n            Colormap to use for the original image if it's grayscale. Defaults to None.\n        indexes (Optional[List[int]], optional):\n            List of band/channel indexes (0-based for NumPy, 1-based for rasterio) to extract from the original image.\n            Useful for multi-band imagery like Sentinel-2. Defaults to None.\n        divider (Optional[float], optional):\n            Value to divide the original image by for normalization (e.g., 10000 for reflectance). Defaults to None.\n\n    Returns:\n        matplotlib.figure.Figure:\n            The generated matplotlib figure object.\n    \"\"\"\n\n    def _load_image(img_input, indexes=None):\n        \"\"\"Helper function to load image from various input types.\"\"\"\n        if isinstance(img_input, str):\n            if img_input.lower().endswith((\".tif\", \".tiff\")):\n                with rasterio.open(img_input) as src:\n                    if indexes:\n                        img = src.read(indexes)  # 1-based\n                        img = (\n                            np.transpose(img, (1, 2, 0)) if len(indexes) &gt; 1 else img[0]\n                        )\n                    else:\n                        img = src.read()\n                        if img.shape[0] == 1:\n                            img = img[0]\n                        else:\n                            img = np.transpose(img, (1, 2, 0))\n            else:\n                img = np.array(Image.open(img_input))\n        elif isinstance(img_input, Image.Image):\n            img = np.array(img_input)\n        elif isinstance(img_input, np.ndarray):\n            img = img_input\n            if indexes is not None and img.ndim == 3:\n                img = img[:, :, indexes]\n        else:\n            raise ValueError(f\"Unsupported image type: {type(img_input)}\")\n        return img\n\n    # Load images\n    original = _load_image(original_image, indexes=indexes)\n    prediction = _load_image(prediction_image)\n    ground_truth = (\n        _load_image(ground_truth_image) if ground_truth_image is not None else None\n    )\n\n    # Apply divider normalization if requested\n    if divider is not None and isinstance(original, np.ndarray) and original.ndim == 3:\n        original = np.clip(original.astype(np.float32) / divider, 0, 1)\n\n    # Determine layout\n    num_plots = 3 if ground_truth is not None else 2\n    fig, axes = plt.subplots(1, num_plots, figsize=figsize)\n    if num_plots == 2:\n        axes = [axes[0], axes[1]]\n\n    if titles is None:\n        titles = [\"Original Image\", \"Prediction\"]\n        if ground_truth is not None:\n            titles.append(\"Ground Truth\")\n\n    # Plot original\n    if original.ndim == 3 and original.shape[2] in [3, 4]:\n        axes[0].imshow(original)\n    else:\n        axes[0].imshow(original, cmap=original_colormap)\n    axes[0].set_title(titles[0])\n    axes[0].axis(\"off\")\n\n    # Prediction\n    axes[1].imshow(prediction, cmap=prediction_colormap)\n    axes[1].set_title(titles[1])\n    axes[1].axis(\"off\")\n\n    # Ground truth\n    if ground_truth is not None:\n        axes[2].imshow(ground_truth, cmap=ground_truth_colormap)\n        axes[2].set_title(titles[2])\n        axes[2].axis(\"off\")\n\n    plt.tight_layout()\n\n    if save_path:\n        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n        print(f\"Plot saved to: {save_path}\")\n\n    if show_plot:\n        plt.show()\n\n    return fig\n</code></pre>"},{"location":"utils/#geoai.utils.print_raster_info","title":"<code>print_raster_info(raster_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a raster dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the raster file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the raster. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing raster information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_raster_info(\n    raster_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a raster dataset and optionally show a preview.\n\n    Args:\n        raster_path (str): Path to the raster file\n        show_preview (bool, optional): Whether to display a visual preview of the raster.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing raster information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_raster_info(raster_path)\n\n        # Print basic information\n        print(f\"===== RASTER INFORMATION: {raster_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Dimensions: {info['width']} x {info['height']} pixels\")\n        print(f\"Number of bands: {info['count']}\")\n        print(f\"Data type: {info['dtype']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Georeferenced Bounds: {info['bounds']}\")\n        print(f\"Pixel Resolution: {info['resolution'][0]}, {info['resolution'][1]}\")\n        print(f\"NoData Value: {info['nodata']}\")\n\n        # Print band statistics\n        print(\"\\n----- Band Statistics -----\")\n        for band_stat in info[\"band_stats\"]:\n            print(f\"Band {band_stat['band']}:\")\n            print(f\"  Min: {band_stat['min']:.2f}\")\n            print(f\"  Max: {band_stat['max']:.2f}\")\n            print(f\"  Mean: {band_stat['mean']:.2f}\")\n            print(f\"  Std Dev: {band_stat['std']:.2f}\")\n\n        # Show a preview if requested\n        if show_preview:\n            with rasterio.open(raster_path) as src:\n                # For multi-band images, show RGB composite or first band\n                if src.count &gt;= 3:\n                    # Try to show RGB composite\n                    rgb = np.dstack([src.read(i) for i in range(1, 4)])\n                    plt.figure(figsize=figsize)\n                    plt.imshow(rgb)\n                    plt.title(f\"RGB Preview: {raster_path}\")\n                else:\n                    # Show first band for single-band images\n                    plt.figure(figsize=figsize)\n                    show(\n                        src.read(1),\n                        cmap=\"viridis\",\n                        title=f\"Band 1 Preview: {raster_path}\",\n                    )\n                    plt.colorbar(label=\"Pixel Value\")\n                plt.show()\n\n    except Exception as e:\n        print(f\"Error reading raster: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.print_vector_info","title":"<code>print_vector_info(vector_path, show_preview=True, figsize=(10, 8))</code>","text":"<p>Print formatted information about a vector dataset and optionally show a preview.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>show_preview</code> <code>bool</code> <p>Whether to display a visual preview of the vector data. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing vector information if successful, None otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def print_vector_info(\n    vector_path: str, show_preview: bool = True, figsize: Tuple[int, int] = (10, 8)\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Print formatted information about a vector dataset and optionally show a preview.\n\n    Args:\n        vector_path (str): Path to the vector file\n        show_preview (bool, optional): Whether to display a visual preview of the vector data.\n            Defaults to True.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        dict: Dictionary containing vector information if successful, None otherwise\n    \"\"\"\n    try:\n        info = get_vector_info(vector_path)\n\n        # Print basic information\n        print(f\"===== VECTOR INFORMATION: {vector_path} =====\")\n        print(f\"Driver: {info['driver']}\")\n        print(f\"Feature count: {info['feature_count']}\")\n        print(f\"Geometry types: {info['geometry_type']}\")\n        print(f\"Coordinate Reference System: {info['crs']}\")\n        print(f\"Bounds: {info['bounds']}\")\n        print(f\"Number of attributes: {info['attribute_count']}\")\n        print(f\"Attribute names: {', '.join(info['attribute_names'])}\")\n\n        # Print attribute statistics\n        if info[\"attribute_stats\"]:\n            print(\"\\n----- Attribute Statistics -----\")\n            for attr, stats in info[\"attribute_stats\"].items():\n                print(f\"Attribute: {attr}\")\n                for stat_name, stat_value in stats.items():\n                    print(\n                        f\"  {stat_name}: {stat_value:.4f}\"\n                        if isinstance(stat_value, float)\n                        else f\"  {stat_name}: {stat_value}\"\n                    )\n\n        # Show a preview if requested\n        if show_preview:\n            gdf = (\n                gpd.read_parquet(vector_path)\n                if vector_path.endswith(\".parquet\")\n                else gpd.read_file(vector_path)\n            )\n            fig, ax = plt.subplots(figsize=figsize)\n            gdf.plot(ax=ax, cmap=\"viridis\")\n            ax.set_title(f\"Preview: {vector_path}\")\n            plt.tight_layout()\n            plt.show()\n\n            # # Show a sample of the attribute table\n            # if not gdf.empty:\n            #     print(\"\\n----- Sample of attribute table (first 5 rows) -----\")\n            #     print(gdf.head().to_string())\n\n    except Exception as e:\n        print(f\"Error reading vector data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.raster_to_vector","title":"<code>raster_to_vector(raster_path, output_path=None, threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', unique_attribute_value=False, output_format='geojson', plot_result=False)</code>","text":"<p>Convert a raster label mask to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>raster_path</code> <code>str</code> <p>Path to the input raster file (e.g., GeoTIFF).</p> required <code>output_path</code> <code>str</code> <p>Path to save the output vector file. If None, returns GeoDataFrame without saving.</p> <code>None</code> <code>threshold</code> <code>int / float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>unique_attribute_value</code> <code>bool</code> <p>Whether to generate unique values for each shape within a class.</p> <code>False</code> <code>output_format</code> <code>str</code> <p>Format for output file - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting polygons overlaid on the raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector(\n    raster_path: str,\n    output_path: Optional[str] = None,\n    threshold: float = 0,\n    min_area: float = 10,\n    simplify_tolerance: Optional[float] = None,\n    class_values: Optional[List[int]] = None,\n    attribute_name: str = \"class\",\n    unique_attribute_value: bool = False,\n    output_format: str = \"geojson\",\n    plot_result: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"\n    Convert a raster label mask to vector polygons.\n\n    Args:\n        raster_path (str): Path to the input raster file (e.g., GeoTIFF).\n        output_path (str): Path to save the output vector file. If None, returns GeoDataFrame without saving.\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        unique_attribute_value (bool): Whether to generate unique values for each shape within a class.\n        output_format (str): Format for output file - 'geojson', 'shapefile', 'gpkg'.\n        plot_result (bool): Whether to plot the resulting polygons overlaid on the raster.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vectorized polygons.\n    \"\"\"\n    # Open the raster file\n    with rasterio.open(raster_path) as src:\n        # Read the data\n        data = src.read(1)\n\n        # Get metadata\n        transform = src.transform\n        crs = src.crs\n\n        # Create mask based on threshold and class values\n        if class_values is not None:\n            # Create a mask for each specified class value\n            masks = {val: (data == val) for val in class_values}\n        else:\n            # Create a mask for values above threshold\n            masks = {1: (data &gt; threshold)}\n            class_values = [1]  # Default class\n\n        # Initialize list to store features\n        all_features = []\n\n        # Process each class value\n        for class_val in class_values:\n            mask = masks[class_val]\n            shape_count = 1\n            # Vectorize the mask\n            for geom, value in features.shapes(\n                mask.astype(np.uint8), mask=mask, transform=transform\n            ):\n                # Convert to shapely geometry\n                geom = shape(geom)\n\n                # Skip small polygons\n                if geom.area &lt; min_area:\n                    continue\n\n                # Simplify geometry if requested\n                if simplify_tolerance is not None:\n                    geom = geom.simplify(simplify_tolerance)\n\n                # Add to features list with class value\n                if unique_attribute_value:\n                    all_features.append(\n                        {\"geometry\": geom, attribute_name: class_val * shape_count}\n                    )\n                else:\n                    all_features.append({\"geometry\": geom, attribute_name: class_val})\n\n                shape_count += 1\n\n        # Create GeoDataFrame\n        if all_features:\n            gdf = gpd.GeoDataFrame(all_features, crs=crs)\n        else:\n            print(\"Warning: No features were extracted from the raster.\")\n            # Return empty GeoDataFrame with correct CRS\n            gdf = gpd.GeoDataFrame([], geometry=[], crs=crs)\n\n        # Save to file if requested\n        if output_path is not None:\n            # Create directory if it doesn't exist\n            os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n            # Save to file based on format\n            if output_format.lower() == \"geojson\":\n                gdf.to_file(output_path, driver=\"GeoJSON\")\n            elif output_format.lower() == \"shapefile\":\n                gdf.to_file(output_path)\n            elif output_format.lower() == \"gpkg\":\n                gdf.to_file(output_path, driver=\"GPKG\")\n            else:\n                raise ValueError(f\"Unsupported output format: {output_format}\")\n\n            print(f\"Vectorized data saved to {output_path}\")\n\n        # Plot result if requested\n        if plot_result:\n            fig, ax = plt.subplots(figsize=(12, 12))\n\n            # Plot raster\n            raster_img = src.read()\n            if raster_img.shape[0] == 1:\n                plt.imshow(raster_img[0], cmap=\"viridis\", alpha=0.7)\n            else:\n                # Use first 3 bands for RGB display\n                rgb = raster_img[:3].transpose(1, 2, 0)\n                # Normalize for display\n                rgb = np.clip(rgb / rgb.max(), 0, 1)\n                plt.imshow(rgb)\n\n            # Plot vector boundaries\n            if not gdf.empty:\n                gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\", linewidth=2)\n\n            plt.title(\"Raster with Vectorized Boundaries\")\n            plt.axis(\"off\")\n            plt.tight_layout()\n            plt.show()\n\n        return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.raster_to_vector_batch","title":"<code>raster_to_vector_batch(input_dir, output_dir, pattern='*.tif', threshold=0, min_area=10, simplify_tolerance=None, class_values=None, attribute_name='class', output_format='geojson', merge_output=False, merge_filename='merged_vectors')</code>","text":"<p>Batch convert multiple raster files to vector polygons.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Directory containing input raster files.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output vector files.</p> required <code>pattern</code> <code>str</code> <p>Pattern to match raster files (e.g., '*.tif').</p> <code>'*.tif'</code> <code>threshold</code> <code>int / float</code> <p>Pixel values greater than this threshold will be vectorized.</p> <code>0</code> <code>min_area</code> <code>float</code> <p>Minimum polygon area in square map units to keep.</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for geometry simplification. None for no simplification.</p> <code>None</code> <code>class_values</code> <code>list</code> <p>Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.</p> <code>None</code> <code>attribute_name</code> <code>str</code> <p>Name of the attribute field for the class values.</p> <code>'class'</code> <code>output_format</code> <code>str</code> <p>Format for output files - 'geojson', 'shapefile', 'gpkg'.</p> <code>'geojson'</code> <code>merge_output</code> <code>bool</code> <p>Whether to merge all output vectors into a single file.</p> <code>False</code> <code>merge_filename</code> <code>str</code> <p>Filename for the merged output (without extension).</p> <code>'merged_vectors'</code> <p>Returns:</p> Type Description <code>Optional[GeoDataFrame]</code> <p>geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def raster_to_vector_batch(\n    input_dir: str,\n    output_dir: str,\n    pattern: str = \"*.tif\",\n    threshold: float = 0,\n    min_area: float = 10,\n    simplify_tolerance: Optional[float] = None,\n    class_values: Optional[List[int]] = None,\n    attribute_name: str = \"class\",\n    output_format: str = \"geojson\",\n    merge_output: bool = False,\n    merge_filename: str = \"merged_vectors\",\n) -&gt; Optional[gpd.GeoDataFrame]:\n    \"\"\"\n    Batch convert multiple raster files to vector polygons.\n\n    Args:\n        input_dir (str): Directory containing input raster files.\n        output_dir (str): Directory to save output vector files.\n        pattern (str): Pattern to match raster files (e.g., '*.tif').\n        threshold (int/float): Pixel values greater than this threshold will be vectorized.\n        min_area (float): Minimum polygon area in square map units to keep.\n        simplify_tolerance (float): Tolerance for geometry simplification. None for no simplification.\n        class_values (list): Specific pixel values to vectorize. If None, all values &gt; threshold are vectorized.\n        attribute_name (str): Name of the attribute field for the class values.\n        output_format (str): Format for output files - 'geojson', 'shapefile', 'gpkg'.\n        merge_output (bool): Whether to merge all output vectors into a single file.\n        merge_filename (str): Filename for the merged output (without extension).\n\n    Returns:\n        geopandas.GeoDataFrame or None: If merge_output is True, returns the merged GeoDataFrame.\n    \"\"\"\n    import glob\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Get list of raster files\n    raster_files = glob.glob(os.path.join(input_dir, pattern))\n\n    if not raster_files:\n        print(f\"No files matching pattern '{pattern}' found in {input_dir}\")\n        return None\n\n    print(f\"Found {len(raster_files)} raster files to process\")\n\n    # Process each raster file\n    gdfs = []\n    for raster_file in tqdm(raster_files, desc=\"Processing rasters\"):\n        # Get output filename\n        base_name = os.path.splitext(os.path.basename(raster_file))[0]\n        if output_format.lower() == \"geojson\":\n            out_file = os.path.join(output_dir, f\"{base_name}.geojson\")\n        elif output_format.lower() == \"shapefile\":\n            out_file = os.path.join(output_dir, f\"{base_name}.shp\")\n        elif output_format.lower() == \"gpkg\":\n            out_file = os.path.join(output_dir, f\"{base_name}.gpkg\")\n        else:\n            raise ValueError(f\"Unsupported output format: {output_format}\")\n\n        # Convert raster to vector\n        if merge_output:\n            # Don't save individual files if merging\n            gdf = raster_to_vector(\n                raster_file,\n                output_path=None,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n            )\n\n            # Add filename as attribute\n            if not gdf.empty:\n                gdf[\"source_file\"] = base_name\n                gdfs.append(gdf)\n        else:\n            # Save individual files\n            raster_to_vector(\n                raster_file,\n                output_path=out_file,\n                threshold=threshold,\n                min_area=min_area,\n                simplify_tolerance=simplify_tolerance,\n                class_values=class_values,\n                attribute_name=attribute_name,\n                output_format=output_format,\n            )\n\n    # Merge output if requested\n    if merge_output and gdfs:\n        merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n\n        # Set CRS to the CRS of the first GeoDataFrame\n        if merged_gdf.crs is None and gdfs:\n            merged_gdf.crs = gdfs[0].crs\n\n        # Save merged output\n        if output_format.lower() == \"geojson\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.geojson\")\n            merged_gdf.to_file(merged_file, driver=\"GeoJSON\")\n        elif output_format.lower() == \"shapefile\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.shp\")\n            merged_gdf.to_file(merged_file)\n        elif output_format.lower() == \"gpkg\":\n            merged_file = os.path.join(output_dir, f\"{merge_filename}.gpkg\")\n            merged_gdf.to_file(merged_file, driver=\"GPKG\")\n\n        print(f\"Merged vector data saved to {merged_file}\")\n        return merged_gdf\n\n    return None\n</code></pre>"},{"location":"utils/#geoai.utils.read_raster","title":"<code>read_raster(source, band=None, masked=True, **kwargs)</code>","text":"<p>Reads raster data from various formats using rioxarray.</p> <p>This function reads raster data from local files or URLs into a rioxarray data structure with preserved geospatial metadata.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>String path to the raster file or URL.</p> required <code>band</code> <code>Optional[Union[int, List[int]]]</code> <p>Integer or list of integers specifying which band(s) to read. Defaults to None (all bands).</p> <code>None</code> <code>masked</code> <code>bool</code> <p>Boolean indicating whether to mask nodata values. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rioxarray.open_rasterio.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>xarray.DataArray: A DataArray containing the raster data with geospatial metadata preserved.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local GeoTIFF</p> <pre><code>&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n&gt;&gt;&gt;\nRead only band 1 from a remote GeoTIFF\n&gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n&gt;&gt;&gt;\nRead a raster without masking nodata values\n&gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_raster(\n    source: str,\n    band: Optional[Union[int, List[int]]] = None,\n    masked: bool = True,\n    **kwargs: Any,\n) -&gt; xr.DataArray:\n    \"\"\"Reads raster data from various formats using rioxarray.\n\n    This function reads raster data from local files or URLs into a rioxarray\n    data structure with preserved geospatial metadata.\n\n    Args:\n        source: String path to the raster file or URL.\n        band: Integer or list of integers specifying which band(s) to read.\n            Defaults to None (all bands).\n        masked: Boolean indicating whether to mask nodata values.\n            Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rioxarray.open_rasterio.\n\n    Returns:\n        xarray.DataArray: A DataArray containing the raster data with geospatial\n            metadata preserved.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\")\n        &gt;&gt;&gt;\n        Read only band 1 from a remote GeoTIFF\n        &gt;&gt;&gt; raster = read_raster(\"https://example.com/data.tif\", band=1)\n        &gt;&gt;&gt;\n        Read a raster without masking nodata values\n        &gt;&gt;&gt; raster = read_raster(\"path/to/data.tif\", masked=False)\n    \"\"\"\n    import urllib.parse\n\n    from rasterio.errors import RasterioIOError\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"Raster file does not exist: {source}\")\n\n    try:\n        # Open the raster with rioxarray\n        raster = rxr.open_rasterio(source, masked=masked, **kwargs)\n\n        # Handle band selection if specified\n        if band is not None:\n            if isinstance(band, (list, tuple)):\n                # Convert from 1-based indexing to 0-based indexing\n                band_indices = [b - 1 for b in band]\n                raster = raster.isel(band=band_indices)\n            else:\n                # Single band selection (convert from 1-based to 0-based indexing)\n                raster = raster.isel(band=band - 1)\n\n        return raster\n\n    except RasterioIOError as e:\n        raise ValueError(f\"Could not read raster from source '{source}': {str(e)}\")\n    except Exception as e:\n        raise ValueError(f\"Error reading raster data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.read_vector","title":"<code>read_vector(source, layer=None, **kwargs)</code>","text":"<p>Reads vector data from various formats including GeoParquet.</p> <p>This function dynamically determines the file type based on extension and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>String path to the vector file or URL.</p> required <code>layer</code> <code>Optional[str]</code> <p>String or integer specifying which layer to read from multi-layer files (only applicable for formats like GPKG, GeoJSON, etc.). Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the underlying reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file format is not supported or source cannot be accessed.</p> <p>Examples:</p> <p>Read a local shapefile</p> <pre><code>&gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n&gt;&gt;&gt;\nRead a GeoParquet file from URL\n&gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n&gt;&gt;&gt;\nRead a specific layer from a GeoPackage\n&gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def read_vector(\n    source: str, layer: Optional[str] = None, **kwargs: Any\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Reads vector data from various formats including GeoParquet.\n\n    This function dynamically determines the file type based on extension\n    and reads it into a GeoDataFrame. It supports both local files and HTTP/HTTPS URLs.\n\n    Args:\n        source: String path to the vector file or URL.\n        layer: String or integer specifying which layer to read from multi-layer\n            files (only applicable for formats like GPKG, GeoJSON, etc.).\n            Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the underlying reader.\n\n    Returns:\n        geopandas.GeoDataFrame: A GeoDataFrame containing the vector data.\n\n    Raises:\n        ValueError: If the file format is not supported or source cannot be accessed.\n\n    Examples:\n        Read a local shapefile\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.shp\")\n        &gt;&gt;&gt;\n        Read a GeoParquet file from URL\n        &gt;&gt;&gt; gdf = read_vector(\"https://example.com/data.parquet\")\n        &gt;&gt;&gt;\n        Read a specific layer from a GeoPackage\n        &gt;&gt;&gt; gdf = read_vector(\"path/to/data.gpkg\", layer=\"layer_name\")\n    \"\"\"\n\n    import urllib.parse\n\n    import fiona\n\n    # Determine if source is a URL or local file\n    parsed_url = urllib.parse.urlparse(source)\n    is_url = parsed_url.scheme in [\"http\", \"https\"]\n\n    # If it's a local file, check if it exists\n    if not is_url and not os.path.exists(source):\n        raise ValueError(f\"File does not exist: {source}\")\n\n    # Get file extension\n    _, ext = os.path.splitext(source)\n    ext = ext.lower()\n\n    # Handle GeoParquet files\n    if ext in [\".parquet\", \".pq\", \".geoparquet\"]:\n        return gpd.read_parquet(source, **kwargs)\n\n    # Handle common vector formats\n    if ext in [\".shp\", \".geojson\", \".json\", \".gpkg\", \".gml\", \".kml\", \".gpx\"]:\n        # For formats that might have multiple layers\n        if ext in [\".gpkg\", \".gml\"] and layer is not None:\n            return gpd.read_file(source, layer=layer, **kwargs)\n        return gpd.read_file(source, **kwargs)\n\n    # Try to use fiona to identify valid layers for formats that might have them\n    # Only attempt this for local files as fiona.listlayers might not work with URLs\n    if layer is None and ext in [\".gpkg\", \".gml\"] and not is_url:\n        try:\n            layers = fiona.listlayers(source)\n            if layers:\n                return gpd.read_file(source, layer=layers[0], **kwargs)\n        except Exception:\n            # If listing layers fails, we'll fall through to the generic read attempt\n            pass\n\n    # For other formats or when layer listing fails, attempt to read using GeoPandas\n    try:\n        return gpd.read_file(source, **kwargs)\n    except Exception as e:\n        raise ValueError(f\"Could not read from source '{source}': {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.region_groups","title":"<code>region_groups(image, connectivity=1, min_size=10, max_size=None, threshold=None, properties=None, intensity_image=None, out_csv=None, out_vector=None, out_image=None, **kwargs)</code>","text":"<p>Segment regions in an image and filter them based on size.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, DataArray, ndarray]</code> <p>Input image, can be a file path, xarray DataArray, or numpy array.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling. Defaults to 1 for 4-connectivity. Use 2 for 8-connectivity.</p> <code>1</code> <code>min_size</code> <code>int</code> <p>Minimum size of regions to keep. Defaults to 10.</p> <code>10</code> <code>max_size</code> <code>Optional[int]</code> <p>Maximum size of regions to keep. Defaults to None.</p> <code>None</code> <code>threshold</code> <code>Optional[int]</code> <p>Threshold for filling holes. Defaults to None, which is equal to min_size.</p> <code>None</code> <code>properties</code> <code>Optional[List[str]]</code> <p>List of properties to measure. See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops Defaults to None.</p> <code>None</code> <code>intensity_image</code> <code>Optional[Union[str, DataArray, ndarray]]</code> <p>Intensity image to measure properties. Defaults to None.</p> <code>None</code> <code>out_csv</code> <code>Optional[str]</code> <p>Path to save the properties as a CSV file. Defaults to None.</p> <code>None</code> <code>out_vector</code> <code>Optional[str]</code> <p>Path to save the vector file. Defaults to None.</p> <code>None</code> <code>out_image</code> <code>Optional[str]</code> <p>Path to save the output image. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, DataFrame], Tuple[DataArray, DataFrame]]</code> <p>Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.</p> Source code in <code>geoai/utils.py</code> <pre><code>def region_groups(\n    image: Union[str, \"xr.DataArray\", np.ndarray],\n    connectivity: int = 1,\n    min_size: int = 10,\n    max_size: Optional[int] = None,\n    threshold: Optional[int] = None,\n    properties: Optional[List[str]] = None,\n    intensity_image: Optional[Union[str, \"xr.DataArray\", np.ndarray]] = None,\n    out_csv: Optional[str] = None,\n    out_vector: Optional[str] = None,\n    out_image: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Union[Tuple[np.ndarray, \"pd.DataFrame\"], Tuple[\"xr.DataArray\", \"pd.DataFrame\"]]:\n    \"\"\"\n    Segment regions in an image and filter them based on size.\n\n    Args:\n        image (Union[str, xr.DataArray, np.ndarray]): Input image, can be a file\n            path, xarray DataArray, or numpy array.\n        connectivity (int, optional): Connectivity for labeling. Defaults to 1\n            for 4-connectivity. Use 2 for 8-connectivity.\n        min_size (int, optional): Minimum size of regions to keep. Defaults to 10.\n        max_size (Optional[int], optional): Maximum size of regions to keep.\n            Defaults to None.\n        threshold (Optional[int], optional): Threshold for filling holes.\n            Defaults to None, which is equal to min_size.\n        properties (Optional[List[str]], optional): List of properties to measure.\n            See https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops\n            Defaults to None.\n        intensity_image (Optional[Union[str, xr.DataArray, np.ndarray]], optional):\n            Intensity image to measure properties. Defaults to None.\n        out_csv (Optional[str], optional): Path to save the properties as a CSV file.\n            Defaults to None.\n        out_vector (Optional[str], optional): Path to save the vector file.\n            Defaults to None.\n        out_image (Optional[str], optional): Path to save the output image.\n            Defaults to None.\n\n    Returns:\n        Union[Tuple[np.ndarray, pd.DataFrame], Tuple[xr.DataArray, pd.DataFrame]]: Labeled image and properties DataFrame.\n    \"\"\"\n    import scipy.ndimage as ndi\n    from skimage import measure\n\n    if isinstance(image, str):\n        ds = rxr.open_rasterio(image)\n        da = ds.sel(band=1)\n        array = da.values.squeeze()\n    elif isinstance(image, xr.DataArray):\n        da = image\n        array = image.values.squeeze()\n    elif isinstance(image, np.ndarray):\n        array = image\n    else:\n        raise ValueError(\n            \"The input image must be a file path, xarray DataArray, or numpy array.\"\n        )\n\n    if threshold is None:\n        threshold = min_size\n\n    # Define a custom function to calculate median intensity\n    def intensity_median(region, intensity_image):\n        # Extract the intensity values for the region\n        return np.median(intensity_image[region])\n\n    # Add your custom function to the list of extra properties\n    if intensity_image is not None:\n        extra_props = (intensity_median,)\n    else:\n        extra_props = None\n\n    if properties is None:\n        properties = [\n            \"label\",\n            \"area\",\n            \"area_bbox\",\n            \"area_convex\",\n            \"area_filled\",\n            \"axis_major_length\",\n            \"axis_minor_length\",\n            \"eccentricity\",\n            \"diameter_areagth\",\n            \"extent\",\n            \"orientation\",\n            \"perimeter\",\n            \"solidity\",\n        ]\n\n        if intensity_image is not None:\n\n            properties += [\n                \"intensity_max\",\n                \"intensity_mean\",\n                \"intensity_min\",\n                \"intensity_std\",\n            ]\n\n    if intensity_image is not None:\n        if isinstance(intensity_image, str):\n            ds = rxr.open_rasterio(intensity_image)\n            intensity_da = ds.sel(band=1)\n            intensity_image = intensity_da.values.squeeze()\n        elif isinstance(intensity_image, xr.DataArray):\n            intensity_image = intensity_image.values.squeeze()\n        elif isinstance(intensity_image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\n                \"The intensity_image must be a file path, xarray DataArray, or numpy array.\"\n            )\n\n    label_image = measure.label(array, connectivity=connectivity)\n    props = measure.regionprops_table(\n        label_image, properties=properties, intensity_image=intensity_image, **kwargs\n    )\n\n    df = pd.DataFrame(props)\n\n    # Get the labels of regions with area smaller than the threshold\n    small_regions = df[df[\"area\"] &lt; min_size][\"label\"].values\n    # Set the corresponding labels in the label_image to zero\n    for region_label in small_regions:\n        label_image[label_image == region_label] = 0\n\n    if max_size is not None:\n        large_regions = df[df[\"area\"] &gt; max_size][\"label\"].values\n        for region_label in large_regions:\n            label_image[label_image == region_label] = 0\n\n    # Find the background (holes) which are zeros\n    holes = label_image == 0\n\n    # Label the holes (connected components in the background)\n    labeled_holes, _ = ndi.label(holes)\n\n    # Measure properties of the labeled holes, including area and bounding box\n    hole_props = measure.regionprops(labeled_holes)\n\n    # Loop through each hole and fill it if it is smaller than the threshold\n    for prop in hole_props:\n        if prop.area &lt; threshold:\n            # Get the coordinates of the small hole\n            coords = prop.coords\n\n            # Find the surrounding region's ID (non-zero value near the hole)\n            surrounding_region_values = []\n            for coord in coords:\n                x, y = coord\n                # Get a 3x3 neighborhood around the hole pixel\n                neighbors = label_image[max(0, x - 1) : x + 2, max(0, y - 1) : y + 2]\n                # Exclude the hole pixels (zeros) and get region values\n                region_values = neighbors[neighbors != 0]\n                if region_values.size &gt; 0:\n                    surrounding_region_values.append(\n                        region_values[0]\n                    )  # Take the first non-zero value\n\n            if surrounding_region_values:\n                # Fill the hole with the mode (most frequent) of the surrounding region values\n                fill_value = max(\n                    set(surrounding_region_values), key=surrounding_region_values.count\n                )\n                label_image[coords[:, 0], coords[:, 1]] = fill_value\n\n    label_image, num_labels = measure.label(\n        label_image, connectivity=connectivity, return_num=True\n    )\n    props = measure.regionprops_table(\n        label_image,\n        properties=properties,\n        intensity_image=intensity_image,\n        extra_properties=extra_props,\n        **kwargs,\n    )\n\n    df = pd.DataFrame(props)\n    df[\"elongation\"] = df[\"axis_major_length\"] / df[\"axis_minor_length\"]\n\n    dtype = \"uint8\"\n    if num_labels &gt; 255 and num_labels &lt;= 65535:\n        dtype = \"uint16\"\n    elif num_labels &gt; 65535:\n        dtype = \"uint32\"\n\n    if out_csv is not None:\n        df.to_csv(out_csv, index=False)\n\n    if isinstance(image, np.ndarray):\n        return label_image, df\n    else:\n        da.values = label_image\n        if out_image is not None:\n            da.rio.to_raster(out_image, dtype=dtype)\n\n        if out_vector is not None:\n            tmp_raster = None\n            tmp_vector = None\n            try:\n                if out_image is None:\n                    tmp_raster = temp_file_path(\".tif\")\n                    da.rio.to_raster(tmp_raster, dtype=dtype)\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        tmp_raster,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                else:\n                    tmp_vector = temp_file_path(\".gpkg\")\n                    raster_to_vector(\n                        out_image,\n                        tmp_vector,\n                        attribute_name=\"value\",\n                        unique_attribute_value=True,\n                    )\n                gdf = gpd.read_file(tmp_vector)\n                gdf[\"label\"] = gdf[\"value\"].astype(int)\n                gdf.drop(columns=[\"value\"], inplace=True)\n                gdf2 = pd.merge(gdf, df, on=\"label\", how=\"left\")\n                gdf2.to_file(out_vector)\n                gdf2.sort_values(\"label\", inplace=True)\n                df = gdf2\n            finally:\n                try:\n                    if tmp_raster is not None and os.path.exists(tmp_raster):\n                        os.remove(tmp_raster)\n                    if tmp_vector is not None and os.path.exists(tmp_vector):\n                        os.remove(tmp_vector)\n                except Exception as e:\n                    print(f\"Warning: Failed to delete temporary files: {str(e)}\")\n\n        return da, df\n</code></pre>"},{"location":"utils/#geoai.utils.regularization","title":"<code>regularization(building_polygons, angle_tolerance=10, simplify_tolerance=0.5, orthogonalize=True, preserve_topology=True)</code>","text":"<p>Regularizes building footprint polygons with multiple techniques beyond minimum rotated rectangles.</p> <p>Parameters:</p> Name Type Description Default <code>building_polygons</code> <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons containing building footprints</p> required <code>angle_tolerance</code> <code>float</code> <p>Degrees within which angles will be regularized to 90/180 degrees</p> <code>10</code> <code>simplify_tolerance</code> <code>float</code> <p>Distance tolerance for Douglas-Peucker simplification</p> <code>0.5</code> <code>orthogonalize</code> <code>bool</code> <p>Whether to enforce orthogonal angles in the final polygons</p> <code>True</code> <code>preserve_topology</code> <code>bool</code> <p>Whether to preserve topology during simplification</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[GeoDataFrame, List[Polygon]]</code> <p>GeoDataFrame or list of shapely Polygons with regularized building footprints</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularization(\n    building_polygons: Union[gpd.GeoDataFrame, List[Polygon]],\n    angle_tolerance: float = 10,\n    simplify_tolerance: float = 0.5,\n    orthogonalize: bool = True,\n    preserve_topology: bool = True,\n) -&gt; Union[gpd.GeoDataFrame, List[Polygon]]:\n    \"\"\"\n    Regularizes building footprint polygons with multiple techniques beyond minimum\n    rotated rectangles.\n\n    Args:\n        building_polygons: GeoDataFrame or list of shapely Polygons containing building footprints\n        angle_tolerance: Degrees within which angles will be regularized to 90/180 degrees\n        simplify_tolerance: Distance tolerance for Douglas-Peucker simplification\n        orthogonalize: Whether to enforce orthogonal angles in the final polygons\n        preserve_topology: Whether to preserve topology during simplification\n\n    Returns:\n        GeoDataFrame or list of shapely Polygons with regularized building footprints\n    \"\"\"\n    from shapely import wkt\n    from shapely.affinity import rotate, translate\n    from shapely.geometry import Polygon, shape\n\n    regularized_buildings = []\n\n    # Check if we're dealing with a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        geom_objects = building_polygons.geometry\n    else:\n        geom_objects = building_polygons\n\n    for building in geom_objects:\n        # Handle potential string representations of geometries\n        if isinstance(building, str):\n            try:\n                # Try to parse as WKT\n                building = wkt.loads(building)\n            except Exception:\n                print(f\"Failed to parse geometry string: {building[:30]}...\")\n                continue\n\n        # Ensure we have a valid geometry\n        if not hasattr(building, \"simplify\"):\n            print(f\"Invalid geometry type: {type(building)}\")\n            continue\n\n        # Step 1: Simplify to remove noise and small vertices\n        simplified = building.simplify(\n            simplify_tolerance, preserve_topology=preserve_topology\n        )\n\n        if orthogonalize:\n            # Make sure we have a valid polygon with an exterior\n            if not hasattr(simplified, \"exterior\") or simplified.exterior is None:\n                print(f\"Simplified geometry has no exterior: {simplified}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            # Step 2: Get the dominant angle to rotate building\n            coords = np.array(simplified.exterior.coords)\n\n            # Make sure we have enough coordinates for angle calculation\n            if len(coords) &lt; 3:\n                print(f\"Not enough coordinates for angle calculation: {len(coords)}\")\n                regularized_buildings.append(building)  # Use original instead\n                continue\n\n            segments = np.diff(coords, axis=0)\n            angles = np.arctan2(segments[:, 1], segments[:, 0]) * 180 / np.pi\n\n            # Find most common angle classes (0, 90, 180, 270 degrees)\n            binned_angles = np.round(angles / 90) * 90\n            dominant_angle = np.bincount(binned_angles.astype(int) % 180).argmax()\n\n            # Step 3: Rotate to align with axes, regularize, then rotate back\n            rotated = rotate(simplified, -dominant_angle, origin=\"centroid\")\n\n            # Step 4: Rectify coordinates to enforce right angles\n            ext_coords = np.array(rotated.exterior.coords)\n            rect_coords = []\n\n            # Regularize each vertex to create orthogonal corners\n            for i in range(len(ext_coords) - 1):\n                rect_coords.append(ext_coords[i])\n\n                # Check if we need to add a right-angle vertex\n                angle = (\n                    np.arctan2(\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 1]\n                        - ext_coords[i, 1],\n                        ext_coords[(i + 1) % (len(ext_coords) - 1), 0]\n                        - ext_coords[i, 0],\n                    )\n                    * 180\n                    / np.pi\n                )\n\n                if abs(angle % 90) &gt; angle_tolerance and abs(angle % 90) &lt; (\n                    90 - angle_tolerance\n                ):\n                    # Add intermediate point to create right angle\n                    rect_coords.append(\n                        [\n                            ext_coords[(i + 1) % (len(ext_coords) - 1), 0],\n                            ext_coords[i, 1],\n                        ]\n                    )\n\n            # Close the polygon by adding the first point again\n            rect_coords.append(rect_coords[0])\n\n            # Create regularized polygon and rotate back\n            regularized = Polygon(rect_coords)\n            final_building = rotate(regularized, dominant_angle, origin=\"centroid\")\n        else:\n            final_building = simplified\n\n        regularized_buildings.append(final_building)\n\n    # If input was a GeoDataFrame, return a GeoDataFrame\n    if isinstance(building_polygons, gpd.GeoDataFrame):\n        return gpd.GeoDataFrame(\n            geometry=regularized_buildings, crs=building_polygons.crs\n        )\n    else:\n        return regularized_buildings\n</code></pre>"},{"location":"utils/#geoai.utils.regularize","title":"<code>regularize(data, parallel_threshold=1.0, target_crs=None, simplify=True, simplify_tolerance=0.5, allow_45_degree=True, diagonal_threshold_reduction=15, allow_circles=True, circle_threshold=0.9, num_cores=1, include_metadata=False, output_path=None, **kwargs)</code>","text":"<p>Regularizes polygon geometries in a GeoDataFrame by aligning edges.</p> <p>Aligns edges to be parallel or perpendicular (optionally also 45 degrees) to their main direction. Handles reprojection, initial simplification, regularization, geometry cleanup, and parallel processing.</p> <p>This function is a wrapper around the <code>regularize_geodataframe</code> function from the <code>buildingregulariser</code> package. Credits to the original author Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[GeoDataFrame, str]</code> <p>Input GeoDataFrame with polygon or multipolygon geometries, or a file path to the GeoDataFrame.</p> required <code>parallel_threshold</code> <code>float</code> <p>Distance threshold for merging nearly parallel adjacent edges during regularization. Defaults to 1.0.</p> <code>1.0</code> <code>target_crs</code> <code>Optional[Union[str, CRS]]</code> <p>Target Coordinate Reference System for processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a projected CRS. Defaults to None.</p> <code>None</code> <code>simplify</code> <code>bool</code> <p>If True, applies initial simplification to the geometry before regularization. Defaults to True.</p> <code>True</code> <code>simplify_tolerance</code> <code>float</code> <p>Tolerance for the initial simplification step (if <code>simplify</code> is True). Also used for geometry cleanup steps. Defaults to 0.5.</p> <code>0.5</code> <code>allow_45_degree</code> <code>bool</code> <p>If True, allows edges to be oriented at 45-degree angles relative to the main direction during regularization. Defaults to True.</p> <code>True</code> <code>diagonal_threshold_reduction</code> <code>float</code> <p>Reduction factor in degrees to reduce the likelihood of diagonal edges being created. Larger values reduce the likelihood of diagonal edges. Defaults to 15.</p> <code>15</code> <code>allow_circles</code> <code>bool</code> <p>If True, attempts to detect polygons that are nearly circular and replaces them with perfect circles. Defaults to True.</p> <code>True</code> <code>circle_threshold</code> <code>float</code> <p>Intersection over Union (IoU) threshold used for circle detection (if <code>allow_circles</code> is True). Value between 0 and 1. Defaults to 0.9.</p> <code>0.9</code> <code>num_cores</code> <code>int</code> <p>Number of CPU cores to use for parallel processing. If 1, processing is done sequentially. Defaults to 1.</p> <code>1</code> <code>include_metadata</code> <code>bool</code> <p>If True, includes metadata about the regularization process in the output GeoDataFrame. Defaults to False.</p> <code>False</code> <code>output_path</code> <code>Optional[str]</code> <p>Path to save the output GeoDataFrame. If None, the output is not saved. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to the <code>to_file</code> method when saving the output.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are</p> <code>Any</code> <p>preserved. Geometries that failed processing might be dropped.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.</p> Source code in <code>geoai/utils.py</code> <pre><code>def regularize(\n    data: Union[gpd.GeoDataFrame, str],\n    parallel_threshold: float = 1.0,\n    target_crs: Optional[Union[str, \"pyproj.CRS\"]] = None,\n    simplify: bool = True,\n    simplify_tolerance: float = 0.5,\n    allow_45_degree: bool = True,\n    diagonal_threshold_reduction: float = 15,\n    allow_circles: bool = True,\n    circle_threshold: float = 0.9,\n    num_cores: int = 1,\n    include_metadata: bool = False,\n    output_path: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Regularizes polygon geometries in a GeoDataFrame by aligning edges.\n\n    Aligns edges to be parallel or perpendicular (optionally also 45 degrees)\n    to their main direction. Handles reprojection, initial simplification,\n    regularization, geometry cleanup, and parallel processing.\n\n    This function is a wrapper around the `regularize_geodataframe` function\n    from the `buildingregulariser` package. Credits to the original author\n    Nick Wright. Check out the repo at https://github.com/DPIRD-DMA/Building-Regulariser.\n\n    Args:\n        data (Union[gpd.GeoDataFrame, str]): Input GeoDataFrame with polygon or multipolygon geometries,\n            or a file path to the GeoDataFrame.\n        parallel_threshold (float, optional): Distance threshold for merging nearly parallel adjacent edges\n            during regularization. Defaults to 1.0.\n        target_crs (Optional[Union[str, \"pyproj.CRS\"]], optional): Target Coordinate Reference System for\n            processing. If None, uses the input GeoDataFrame's CRS. Processing is more reliable in a\n            projected CRS. Defaults to None.\n        simplify (bool, optional): If True, applies initial simplification to the geometry before\n            regularization. Defaults to True.\n        simplify_tolerance (float, optional): Tolerance for the initial simplification step (if `simplify`\n            is True). Also used for geometry cleanup steps. Defaults to 0.5.\n        allow_45_degree (bool, optional): If True, allows edges to be oriented at 45-degree angles relative\n            to the main direction during regularization. Defaults to True.\n        diagonal_threshold_reduction (float, optional): Reduction factor in degrees to reduce the likelihood\n            of diagonal edges being created. Larger values reduce the likelihood of diagonal edges.\n            Defaults to 15.\n        allow_circles (bool, optional): If True, attempts to detect polygons that are nearly circular and\n            replaces them with perfect circles. Defaults to True.\n        circle_threshold (float, optional): Intersection over Union (IoU) threshold used for circle detection\n            (if `allow_circles` is True). Value between 0 and 1. Defaults to 0.9.\n        num_cores (int, optional): Number of CPU cores to use for parallel processing. If 1, processing is\n            done sequentially. Defaults to 1.\n        include_metadata (bool, optional): If True, includes metadata about the regularization process in the\n            output GeoDataFrame. Defaults to False.\n        output_path (Optional[str], optional): Path to save the output GeoDataFrame. If None, the output is\n            not saved. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to the `to_file` method when saving the output.\n\n    Returns:\n        gpd.GeoDataFrame: A new GeoDataFrame with regularized polygon geometries. Original attributes are\n        preserved. Geometries that failed processing might be dropped.\n\n    Raises:\n        ValueError: If the input data is not a GeoDataFrame or a file path, or if the input GeoDataFrame is empty.\n    \"\"\"\n    try:\n        from buildingregulariser import regularize_geodataframe\n    except ImportError:\n        install_package(\"buildingregulariser\")\n        from buildingregulariser import regularize_geodataframe\n\n    if isinstance(data, str):\n        data = gpd.read_file(data)\n    elif not isinstance(data, gpd.GeoDataFrame):\n        raise ValueError(\"Input data must be a GeoDataFrame or a file path.\")\n\n    # Check if the input data is empty\n    if data.empty:\n        raise ValueError(\"Input GeoDataFrame is empty.\")\n\n    gdf = regularize_geodataframe(\n        data,\n        parallel_threshold=parallel_threshold,\n        target_crs=target_crs,\n        simplify=simplify,\n        simplify_tolerance=simplify_tolerance,\n        allow_45_degree=allow_45_degree,\n        diagonal_threshold_reduction=diagonal_threshold_reduction,\n        allow_circles=allow_circles,\n        circle_threshold=circle_threshold,\n        num_cores=num_cores,\n        include_metadata=include_metadata,\n    )\n\n    if output_path:\n        gdf.to_file(output_path, **kwargs)\n\n    return gdf\n</code></pre>"},{"location":"utils/#geoai.utils.rowcol_to_xy","title":"<code>rowcol_to_xy(src_fp, rows=None, cols=None, boxes=None, zs=None, offset='center', output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Converts a list of (row, col) coordinates to (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>rows</code> <code>list</code> <p>A list of row coordinates. Defaults to None.</p> <code>None</code> <code>cols</code> <code>list</code> <p>A list of col coordinates. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list</code> <p>A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]</p> <code>None</code> <code>zs</code> <code>Optional[List[float]]</code> <p>zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.</p> <code>None</code> <code>offset</code> <code>str</code> <p>Determines if the returned coordinates are for the center of the pixel or for a corner.</p> <code>'center'</code> <code>output</code> <code>str</code> <p>The output vector file path. Defaults to None.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to rasterio.transform.xy.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>A list of (x, y) coordinates.</p> Source code in <code>geoai/utils.py</code> <pre><code>def rowcol_to_xy(\n    src_fp: str,\n    rows: Optional[List[int]] = None,\n    cols: Optional[List[int]] = None,\n    boxes: Optional[List[List[int]]] = None,\n    zs: Optional[List[float]] = None,\n    offset: str = \"center\",\n    output: Optional[str] = None,\n    dst_crs: str = \"EPSG:4326\",\n    **kwargs: Any,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Converts a list of (row, col) coordinates to (x, y) coordinates.\n\n    Args:\n        src_fp (str): The source raster file path.\n        rows (list, optional): A list of row coordinates. Defaults to None.\n        cols (list, optional): A list of col coordinates. Defaults to None.\n        boxes (list, optional): A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]\n        zs: zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.\n        offset (str, optional): Determines if the returned coordinates are for the center of the pixel or for a corner.\n        output (str, optional): The output vector file path. Defaults to None.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.xy.\n\n    Returns:\n        A list of (x, y) coordinates.\n    \"\"\"\n\n    if boxes is not None:\n        rows = []\n        cols = []\n\n        for box in boxes:\n            rows.append(box[1])\n            rows.append(box[3])\n            cols.append(box[0])\n            cols.append(box[2])\n\n    if rows is None or cols is None:\n        raise ValueError(\"rows and cols must be provided.\")\n\n    with rasterio.open(src_fp) as src:\n        xs, ys = rasterio.transform.xy(src.transform, rows, cols, zs, offset, **kwargs)\n        src_crs = src.crs\n\n    if boxes is None:\n        return [[x, y] for x, y in zip(xs, ys)]\n    else:\n        result = [[xs[i], ys[i + 1], xs[i + 1], ys[i]] for i in range(0, len(xs), 2)]\n\n        if output is not None:\n            boxes_to_vector(result, src_crs, dst_crs, output)\n        else:\n            return result\n</code></pre>"},{"location":"utils/#geoai.utils.stack_bands","title":"<code>stack_bands(input_files, output_file, resolution=None, dtype=None, temp_vrt='stack.vrt', overwrite=False, compress='DEFLATE', output_format='COG', extra_gdal_translate_args=None)</code>","text":"<p>Stack bands from multiple images into a single multi-band GeoTIFF.</p> <p>Parameters:</p> Name Type Description Default <code>input_files</code> <code>List[str]</code> <p>List of input image paths.</p> required <code>output_file</code> <code>str</code> <p>Path to the output stacked image.</p> required <code>resolution</code> <code>float</code> <p>Output resolution. If None, inferred from first image.</p> <code>None</code> <code>dtype</code> <code>str</code> <p>Output data type (e.g., \"UInt16\", \"Float32\").</p> <code>None</code> <code>temp_vrt</code> <code>str</code> <p>Temporary VRT filename.</p> <code>'stack.vrt'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the output file.</p> <code>False</code> <code>compress</code> <code>str</code> <p>Compression method.</p> <code>'DEFLATE'</code> <code>output_format</code> <code>str</code> <p>GDAL output format (default is \"COG\").</p> <code>'COG'</code> <code>extra_gdal_translate_args</code> <code>List[str]</code> <p>Extra arguments for gdal_translate.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the output file.</p> Source code in <code>geoai/utils.py</code> <pre><code>def stack_bands(\n    input_files: List[str],\n    output_file: str,\n    resolution: Optional[float] = None,\n    dtype: Optional[str] = None,  # e.g., \"UInt16\", \"Float32\"\n    temp_vrt: str = \"stack.vrt\",\n    overwrite: bool = False,\n    compress: str = \"DEFLATE\",\n    output_format: str = \"COG\",\n    extra_gdal_translate_args: Optional[List[str]] = None,\n) -&gt; str:\n    \"\"\"\n    Stack bands from multiple images into a single multi-band GeoTIFF.\n\n    Args:\n        input_files (List[str]): List of input image paths.\n        output_file (str): Path to the output stacked image.\n        resolution (float, optional): Output resolution. If None, inferred from first image.\n        dtype (str, optional): Output data type (e.g., \"UInt16\", \"Float32\").\n        temp_vrt (str): Temporary VRT filename.\n        overwrite (bool): Whether to overwrite the output file.\n        compress (str): Compression method.\n        output_format (str): GDAL output format (default is \"COG\").\n        extra_gdal_translate_args (List[str], optional): Extra arguments for gdal_translate.\n\n    Returns:\n        str: Path to the output file.\n    \"\"\"\n    import leafmap\n\n    if not input_files:\n        raise ValueError(\"No input files provided.\")\n    elif isinstance(input_files, str):\n        input_files = leafmap.find_files(input_files, \".tif\")\n\n    if os.path.exists(output_file) and not overwrite:\n        print(f\"Output file already exists: {output_file}\")\n        return output_file\n\n    # Infer resolution if not provided\n    if resolution is None:\n        resolution_x, resolution_y = get_raster_resolution(input_files[0])\n    else:\n        resolution_x = resolution_y = resolution\n\n    # Step 1: Build VRT\n    vrt_cmd = [\"gdalbuildvrt\", \"-separate\", temp_vrt] + input_files\n    subprocess.run(vrt_cmd, check=True)\n\n    # Step 2: Translate VRT to output GeoTIFF\n    translate_cmd = [\n        \"gdal_translate\",\n        \"-tr\",\n        str(resolution_x),\n        str(resolution_y),\n        temp_vrt,\n        output_file,\n        \"-of\",\n        output_format,\n        \"-co\",\n        f\"COMPRESS={compress}\",\n    ]\n\n    if dtype:\n        translate_cmd.insert(1, \"-ot\")\n        translate_cmd.insert(2, dtype)\n\n    if extra_gdal_translate_args:\n        translate_cmd += extra_gdal_translate_args\n\n    subprocess.run(translate_cmd, check=True)\n\n    # Step 3: Clean up VRT\n    if os.path.exists(temp_vrt):\n        os.remove(temp_vrt)\n\n    return output_file\n</code></pre>"},{"location":"utils/#geoai.utils.temp_file_path","title":"<code>temp_file_path(ext)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>ext</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The temporary file path.</p> Source code in <code>geoai/utils.py</code> <pre><code>def temp_file_path(ext: str) -&gt; str:\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        ext (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not ext.startswith(\".\"):\n        ext = \".\" + ext\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{ext}\")\n\n    return file_path\n</code></pre>"},{"location":"utils/#geoai.utils.try_common_architectures","title":"<code>try_common_architectures(state_dict)</code>","text":"<p>Try to load the state_dict into common architectures to see which one fits.</p> <p>Parameters:</p> Name Type Description Default <code>state_dict</code> <code>Dict[str, Any]</code> <p>The model's state dictionary</p> required Source code in <code>geoai/utils.py</code> <pre><code>def try_common_architectures(state_dict: Dict[str, Any]) -&gt; Optional[str]:\n    \"\"\"\n    Try to load the state_dict into common architectures to see which one fits.\n\n    Args:\n        state_dict: The model's state dictionary\n    \"\"\"\n    import torchinfo\n\n    # Test models and their initializations\n    models_to_try = {\n        \"FCN-ResNet50\": lambda: fcn_resnet50(num_classes=9),\n        \"DeepLabV3-ResNet50\": lambda: deeplabv3_resnet50(num_classes=9),\n    }\n\n    print(\"\\nTrying to load state_dict into common architectures:\")\n\n    for name, model_fn in models_to_try.items():\n        try:\n            model = model_fn()\n            # Sometimes state_dict keys have 'model.' prefix\n            if all(k.startswith(\"model.\") for k in state_dict.keys()):\n                cleaned_state_dict = {k[6:]: v for k, v in state_dict.items()}\n                model.load_state_dict(cleaned_state_dict, strict=False)\n            else:\n                model.load_state_dict(state_dict, strict=False)\n\n            print(\n                f\"- {name}: Successfully loaded (may have missing or unexpected keys)\"\n            )\n\n            # Generate model summary\n            print(f\"\\nSummary of {name} architecture:\")\n            summary = torchinfo.summary(model, input_size=(1, 3, 224, 224), verbose=0)\n            print(summary)\n\n        except Exception as e:\n            print(f\"- {name}: Failed to load - {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.vector_to_geojson","title":"<code>vector_to_geojson(filename, output=None, **kwargs)</code>","text":"<p>Converts a vector file to a geojson file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <code>output</code> <code>str</code> <p>The output geojson file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>str</code> <p>The geojson dictionary.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_geojson(\n    filename: str, output: Optional[str] = None, **kwargs: Any\n) -&gt; str:\n    \"\"\"Converts a vector file to a geojson file.\n\n    Args:\n        filename (str): The vector file path.\n        output (str, optional): The output geojson file path. Defaults to None.\n\n    Returns:\n        dict: The geojson dictionary.\n    \"\"\"\n\n    if filename.startswith(\"http\"):\n        filename = download_file(filename)\n\n    gdf = gpd.read_file(filename, **kwargs)\n    if output is None:\n        return gdf.__geo_interface__\n    else:\n        gdf.to_file(output, driver=\"GeoJSON\")\n</code></pre>"},{"location":"utils/#geoai.utils.vector_to_raster","title":"<code>vector_to_raster(vector_path, output_path=None, reference_raster=None, attribute_field=None, output_shape=None, transform=None, pixel_size=None, bounds=None, crs=None, all_touched=False, fill_value=0, dtype=np.uint8, nodata=None, plot_result=False)</code>","text":"<p>Convert vector data to a raster.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str or GeoDataFrame</code> <p>Path to the input vector file or a GeoDataFrame.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output raster file. If None, returns the array without saving.</p> <code>None</code> <code>reference_raster</code> <code>str</code> <p>Path to a reference raster for dimensions, transform and CRS.</p> <code>None</code> <code>attribute_field</code> <code>str</code> <p>Field name in the vector data to use for pixel values. If None, all vector features will be burned with value 1.</p> <code>None</code> <code>output_shape</code> <code>tuple</code> <p>Shape of the output raster as (height, width). Required if reference_raster is not provided.</p> <code>None</code> <code>transform</code> <code>Affine</code> <p>Affine transformation matrix. Required if reference_raster is not provided.</p> <code>None</code> <code>pixel_size</code> <code>float or tuple</code> <p>Pixel size (resolution) as single value or (x_res, y_res). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>bounds</code> <code>tuple</code> <p>Bounds of the output raster as (left, bottom, right, top). Used to calculate transform if transform is not provided.</p> <code>None</code> <code>crs</code> <code>str or CRS</code> <p>Coordinate reference system of the output raster. Required if reference_raster is not provided.</p> <code>None</code> <code>all_touched</code> <code>bool</code> <p>If True, all pixels touched by geometries will be burned in. If False, only pixels whose center is within the geometry will be burned in.</p> <code>False</code> <code>fill_value</code> <code>int</code> <p>Value to fill the raster with before burning in features.</p> <code>0</code> <code>dtype</code> <code>dtype</code> <p>Data type of the output raster.</p> <code>uint8</code> <code>nodata</code> <code>int</code> <p>No data value for the output raster.</p> <code>None</code> <code>plot_result</code> <code>bool</code> <p>Whether to plot the resulting raster.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy.ndarray: The rasterized data array if output_path is None, else None.</p> Source code in <code>geoai/utils.py</code> <pre><code>def vector_to_raster(\n    vector_path: Union[str, gpd.GeoDataFrame],\n    output_path: Optional[str] = None,\n    reference_raster: Optional[str] = None,\n    attribute_field: Optional[str] = None,\n    output_shape: Optional[Tuple[int, int]] = None,\n    transform: Optional[Any] = None,\n    pixel_size: Optional[float] = None,\n    bounds: Optional[List[float]] = None,\n    crs: Optional[str] = None,\n    all_touched: bool = False,\n    fill_value: Union[int, float] = 0,\n    dtype: Any = np.uint8,\n    nodata: Optional[Union[int, float]] = None,\n    plot_result: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convert vector data to a raster.\n\n    Args:\n        vector_path (str or GeoDataFrame): Path to the input vector file or a GeoDataFrame.\n        output_path (str): Path to save the output raster file. If None, returns the array without saving.\n        reference_raster (str): Path to a reference raster for dimensions, transform and CRS.\n        attribute_field (str): Field name in the vector data to use for pixel values.\n            If None, all vector features will be burned with value 1.\n        output_shape (tuple): Shape of the output raster as (height, width).\n            Required if reference_raster is not provided.\n        transform (affine.Affine): Affine transformation matrix.\n            Required if reference_raster is not provided.\n        pixel_size (float or tuple): Pixel size (resolution) as single value or (x_res, y_res).\n            Used to calculate transform if transform is not provided.\n        bounds (tuple): Bounds of the output raster as (left, bottom, right, top).\n            Used to calculate transform if transform is not provided.\n        crs (str or CRS): Coordinate reference system of the output raster.\n            Required if reference_raster is not provided.\n        all_touched (bool): If True, all pixels touched by geometries will be burned in.\n            If False, only pixels whose center is within the geometry will be burned in.\n        fill_value (int): Value to fill the raster with before burning in features.\n        dtype (numpy.dtype): Data type of the output raster.\n        nodata (int): No data value for the output raster.\n        plot_result (bool): Whether to plot the resulting raster.\n\n    Returns:\n        numpy.ndarray: The rasterized data array if output_path is None, else None.\n    \"\"\"\n    # Load vector data\n    if isinstance(vector_path, gpd.GeoDataFrame):\n        gdf = vector_path\n    else:\n        gdf = gpd.read_file(vector_path)\n\n    # Check if vector data is empty\n    if gdf.empty:\n        warnings.warn(\"The input vector data is empty. Creating an empty raster.\")\n\n    # Get CRS from vector data if not provided\n    if crs is None and reference_raster is None:\n        crs = gdf.crs\n\n    # Get transform and output shape from reference raster if provided\n    if reference_raster is not None:\n        with rasterio.open(reference_raster) as src:\n            transform = src.transform\n            output_shape = src.shape\n            crs = src.crs\n            if nodata is None:\n                nodata = src.nodata\n    else:\n        # Check if we have all required parameters\n        if transform is None:\n            if pixel_size is None or bounds is None:\n                raise ValueError(\n                    \"Either reference_raster, transform, or both pixel_size and bounds must be provided.\"\n                )\n\n            # Calculate transform from pixel size and bounds\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n                y_res = abs(y_res) * -1  # Convert to negative for north-up raster\n\n            left, bottom, right, top = bounds\n            transform = rasterio.transform.from_bounds(\n                left,\n                bottom,\n                right,\n                top,\n                int((right - left) / x_res),\n                int((top - bottom) / abs(y_res)),\n            )\n\n        if output_shape is None:\n            # Calculate output shape from bounds and pixel size\n            if bounds is None or pixel_size is None:\n                raise ValueError(\n                    \"output_shape must be provided if reference_raster is not provided and \"\n                    \"cannot be calculated from bounds and pixel_size.\"\n                )\n\n            if isinstance(pixel_size, (int, float)):\n                x_res = y_res = float(pixel_size)\n            else:\n                x_res, y_res = pixel_size\n\n            left, bottom, right, top = bounds\n            width = int((right - left) / x_res)\n            height = int((top - bottom) / abs(y_res))\n            output_shape = (height, width)\n\n    # Ensure CRS is set\n    if crs is None:\n        raise ValueError(\n            \"CRS must be provided either directly, from reference_raster, or from input vector data.\"\n        )\n\n    # Reproject vector data if its CRS doesn't match the output CRS\n    if gdf.crs != crs:\n        print(f\"Reprojecting vector data from {gdf.crs} to {crs}\")\n        gdf = gdf.to_crs(crs)\n\n    # Create empty raster filled with fill_value\n    raster_data = np.full(output_shape, fill_value, dtype=dtype)\n\n    # Burn vector features into raster\n    if not gdf.empty:\n        # Prepare shapes for burning\n        if attribute_field is not None and attribute_field in gdf.columns:\n            # Use attribute field for values\n            shapes = [\n                (geom, value) for geom, value in zip(gdf.geometry, gdf[attribute_field])\n            ]\n        else:\n            # Burn with value 1\n            shapes = [(geom, 1) for geom in gdf.geometry]\n\n        # Burn shapes into raster\n        burned = features.rasterize(\n            shapes=shapes,\n            out_shape=output_shape,\n            transform=transform,\n            fill=fill_value,\n            all_touched=all_touched,\n            dtype=dtype,\n        )\n\n        # Update raster data\n        raster_data = burned\n\n    # Save raster if output path is provided\n    if output_path is not None:\n        # Create directory if it doesn't exist\n        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)\n\n        # Define metadata\n        metadata = {\n            \"driver\": \"GTiff\",\n            \"height\": output_shape[0],\n            \"width\": output_shape[1],\n            \"count\": 1,\n            \"dtype\": raster_data.dtype,\n            \"crs\": crs,\n            \"transform\": transform,\n        }\n\n        # Add nodata value if provided\n        if nodata is not None:\n            metadata[\"nodata\"] = nodata\n\n        # Write raster\n        with rasterio.open(output_path, \"w\", **metadata) as dst:\n            dst.write(raster_data, 1)\n\n        print(f\"Rasterized data saved to {output_path}\")\n\n    # Plot result if requested\n    if plot_result:\n        fig, ax = plt.subplots(figsize=(10, 10))\n\n        # Plot raster\n        im = ax.imshow(raster_data, cmap=\"viridis\")\n        plt.colorbar(im, ax=ax, label=attribute_field if attribute_field else \"Value\")\n\n        # Plot vector boundaries for reference\n        if output_path is not None:\n            # Get the extent of the raster\n            with rasterio.open(output_path) as src:\n                bounds = src.bounds\n                raster_bbox = box(*bounds)\n        else:\n            # Calculate extent from transform and shape\n            height, width = output_shape\n            left, top = transform * (0, 0)\n            right, bottom = transform * (width, height)\n            raster_bbox = box(left, bottom, right, top)\n\n        # Clip vector to raster extent for clarity in plot\n        if not gdf.empty:\n            gdf_clipped = gpd.clip(gdf, raster_bbox)\n            if not gdf_clipped.empty:\n                gdf_clipped.boundary.plot(ax=ax, color=\"red\", linewidth=1)\n\n        plt.title(\"Rasterized Vector Data\")\n        plt.tight_layout()\n        plt.show()\n\n    return raster_data\n</code></pre>"},{"location":"utils/#geoai.utils.view_image","title":"<code>view_image(image, transpose=False, bdx=None, clip_percentiles=(2, 98), gamma=None, figsize=(10, 5), axis_off=True, title=None, **kwargs)</code>","text":"<p>Visualize an image using matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[ndarray, Tensor]</code> <p>The image to visualize.</p> required <code>transpose</code> <code>bool</code> <p>Whether to transpose the image. Defaults to False.</p> <code>False</code> <code>bdx</code> <code>Optional[int]</code> <p>The band index to visualize. Defaults to None.</p> <code>None</code> <code>figsize</code> <code>Tuple[int, int]</code> <p>The size of the figure. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>axis_off</code> <code>bool</code> <p>Whether to turn off the axis. Defaults to True.</p> <code>True</code> <code>title</code> <code>Optional[str]</code> <p>The title of the plot. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plt.imshow().</p> <code>{}</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_image(\n    image: Union[np.ndarray, torch.Tensor],\n    transpose: bool = False,\n    bdx: Optional[int] = None,\n    clip_percentiles: Optional[Tuple[float, float]] = (2, 98),\n    gamma: Optional[float] = None,\n    figsize: Tuple[int, int] = (10, 5),\n    axis_off: bool = True,\n    title: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Visualize an image using matplotlib.\n\n    Args:\n        image (Union[np.ndarray, torch.Tensor]): The image to visualize.\n        transpose (bool, optional): Whether to transpose the image. Defaults to False.\n        bdx (Optional[int], optional): The band index to visualize. Defaults to None.\n        figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n        axis_off (bool, optional): Whether to turn off the axis. Defaults to True.\n        title (Optional[str], optional): The title of the plot. Defaults to None.\n        **kwargs (Any): Additional keyword arguments for plt.imshow().\n\n    Returns:\n        None\n    \"\"\"\n\n    if isinstance(image, torch.Tensor):\n        image = image.cpu().numpy()\n    elif isinstance(image, str):\n        image = rasterio.open(image).read().transpose(1, 2, 0)\n\n    ax = plt.figure(figsize=figsize)\n\n    if transpose:\n        image = image.transpose(1, 2, 0)\n\n    if bdx is not None:\n        image = image[:, :, bdx]\n\n    if len(image.shape) &gt; 2 and image.shape[2] &gt; 3:\n        image = image[:, :, 0:3]\n\n    if clip_percentiles is not None:\n        p_low, p_high = clip_percentiles\n        lower = np.percentile(image, p_low)\n        upper = np.percentile(image, p_high)\n        image = np.clip((image - lower) / (upper - lower), 0, 1)\n\n    if gamma is not None:\n        image = np.power(image, gamma)\n\n    plt.imshow(image, **kwargs)\n    if axis_off:\n        plt.axis(\"off\")\n    if title is not None:\n        plt.title(title)\n    plt.show()\n    plt.close()\n\n    return ax\n</code></pre>"},{"location":"utils/#geoai.utils.view_raster","title":"<code>view_raster(source, indexes=None, colormap=None, vmin=None, vmax=None, nodata=None, attribution=None, layer_name='Raster', layer_index=None, zoom_to_layer=True, visible=True, opacity=1.0, array_args=None, client_args={'cors_all': False}, basemap='OpenStreetMap', basemap_args=None, backend='ipyleaflet', **kwargs)</code>","text":"<p>Visualize a raster using leafmap.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The source of the raster.</p> required <code>indexes</code> <code>Optional[int]</code> <p>The band indexes to visualize. Defaults to None.</p> <code>None</code> <code>colormap</code> <code>Optional[str]</code> <p>The colormap to apply. Defaults to None.</p> <code>None</code> <code>vmin</code> <code>Optional[float]</code> <p>The minimum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>vmax</code> <code>Optional[float]</code> <p>The maximum value for colormap scaling. Defaults to None.</p> <code>None</code> <code>nodata</code> <code>Optional[float]</code> <p>The nodata value. Defaults to None.</p> <code>None</code> <code>attribution</code> <code>Optional[str]</code> <p>The attribution for the raster. Defaults to None.</p> <code>None</code> <code>layer_name</code> <code>Optional[str]</code> <p>The name of the layer. Defaults to \"Raster\".</p> <code>'Raster'</code> <code>layer_index</code> <code>Optional[int]</code> <p>The index of the layer. Defaults to None.</p> <code>None</code> <code>zoom_to_layer</code> <code>Optional[bool]</code> <p>Whether to zoom to the layer. Defaults to True.</p> <code>True</code> <code>visible</code> <code>Optional[bool]</code> <p>Whether the layer is visible. Defaults to True.</p> <code>True</code> <code>opacity</code> <code>Optional[float]</code> <p>The opacity of the layer. Defaults to 1.0.</p> <code>1.0</code> <code>array_args</code> <code>Optional[Dict]</code> <p>Additional arguments for array processing. Defaults to {}.</p> <code>None</code> <code>client_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the client. Defaults to {\"cors_all\": False}.</p> <code>{'cors_all': False}</code> <code>basemap</code> <code>Optional[str]</code> <p>The basemap to use. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>basemap_args</code> <code>Optional[Dict]</code> <p>Additional arguments for the basemap. Defaults to None.</p> <code>None</code> <code>backend</code> <code>Optional[str]</code> <p>The backend to use. Defaults to \"ipyleaflet\".</p> <code>'ipyleaflet'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>leafmap.Map: The map object with the raster layer added.</p> Source code in <code>geoai/utils.py</code> <pre><code>def view_raster(\n    source: str,\n    indexes: Optional[int] = None,\n    colormap: Optional[str] = None,\n    vmin: Optional[float] = None,\n    vmax: Optional[float] = None,\n    nodata: Optional[float] = None,\n    attribution: Optional[str] = None,\n    layer_name: Optional[str] = \"Raster\",\n    layer_index: Optional[int] = None,\n    zoom_to_layer: Optional[bool] = True,\n    visible: Optional[bool] = True,\n    opacity: Optional[float] = 1.0,\n    array_args: Optional[Dict] = None,\n    client_args: Optional[Dict] = {\"cors_all\": False},\n    basemap: Optional[str] = \"OpenStreetMap\",\n    basemap_args: Optional[Dict] = None,\n    backend: Optional[str] = \"ipyleaflet\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize a raster using leafmap.\n\n    Args:\n        source (str): The source of the raster.\n        indexes (Optional[int], optional): The band indexes to visualize. Defaults to None.\n        colormap (Optional[str], optional): The colormap to apply. Defaults to None.\n        vmin (Optional[float], optional): The minimum value for colormap scaling. Defaults to None.\n        vmax (Optional[float], optional): The maximum value for colormap scaling. Defaults to None.\n        nodata (Optional[float], optional): The nodata value. Defaults to None.\n        attribution (Optional[str], optional): The attribution for the raster. Defaults to None.\n        layer_name (Optional[str], optional): The name of the layer. Defaults to \"Raster\".\n        layer_index (Optional[int], optional): The index of the layer. Defaults to None.\n        zoom_to_layer (Optional[bool], optional): Whether to zoom to the layer. Defaults to True.\n        visible (Optional[bool], optional): Whether the layer is visible. Defaults to True.\n        opacity (Optional[float], optional): The opacity of the layer. Defaults to 1.0.\n        array_args (Optional[Dict], optional): Additional arguments for array processing. Defaults to {}.\n        client_args (Optional[Dict], optional): Additional arguments for the client. Defaults to {\"cors_all\": False}.\n        basemap (Optional[str], optional): The basemap to use. Defaults to \"OpenStreetMap\".\n        basemap_args (Optional[Dict], optional): Additional arguments for the basemap. Defaults to None.\n        backend (Optional[str], optional): The backend to use. Defaults to \"ipyleaflet\".\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        leafmap.Map: The map object with the raster layer added.\n    \"\"\"\n\n    if backend == \"folium\":\n        import leafmap.foliumap as leafmap\n    else:\n        import leafmap.leafmap as leafmap\n\n    if basemap_args is None:\n        basemap_args = {}\n\n    if array_args is None:\n        array_args = {}\n\n    m = leafmap.Map()\n\n    if isinstance(basemap, str):\n        if basemap.lower().endswith(\".tif\"):\n            if basemap.lower().startswith(\"http\"):\n                if \"name\" not in basemap_args:\n                    basemap_args[\"name\"] = \"Basemap\"\n                m.add_cog_layer(basemap, **basemap_args)\n            else:\n                if \"layer_name\" not in basemap_args:\n                    basemap_args[\"layer_name\"] = \"Basemap\"\n                m.add_raster(basemap, **basemap_args)\n    else:\n        m.add_basemap(basemap, **basemap_args)\n\n    if isinstance(source, dict):\n        source = dict_to_image(source)\n\n    if (\n        isinstance(source, str)\n        and source.lower().endswith(\".tif\")\n        and source.startswith(\"http\")\n    ):\n        if indexes is not None:\n            kwargs[\"bidx\"] = indexes\n        if colormap is not None:\n            kwargs[\"colormap_name\"] = colormap\n        if attribution is None:\n            attribution = \"TiTiler\"\n\n        m.add_cog_layer(\n            source,\n            name=layer_name,\n            opacity=opacity,\n            attribution=attribution,\n            zoom_to_layer=zoom_to_layer,\n            **kwargs,\n        )\n    else:\n        m.add_raster(\n            source=source,\n            indexes=indexes,\n            colormap=colormap,\n            vmin=vmin,\n            vmax=vmax,\n            nodata=nodata,\n            attribution=attribution,\n            layer_name=layer_name,\n            layer_index=layer_index,\n            zoom_to_layer=zoom_to_layer,\n            visible=visible,\n            opacity=opacity,\n            array_args=array_args,\n            client_args=client_args,\n            **kwargs,\n        )\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.view_vector","title":"<code>view_vector(vector_data, column=None, cmap='viridis', figsize=(10, 10), title=None, legend=True, basemap=False, basemap_type='streets', alpha=0.7, edge_color='black', classification='quantiles', n_classes=5, highlight_index=None, highlight_color='red', scheme=None, save_path=None, dpi=300)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>column</code> <code>str</code> <p>Column to use for choropleth mapping. If None, a single color will be used. Defaults to None.</p> <code>None</code> <code>cmap</code> <code>str or Colormap</code> <p>Colormap to use for choropleth mapping. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height) in inches. Defaults to (10, 10).</p> <code>(10, 10)</code> <code>title</code> <code>str</code> <p>Title for the plot. Defaults to None.</p> <code>None</code> <code>legend</code> <code>bool</code> <p>Whether to display a legend. Defaults to True.</p> <code>True</code> <code>basemap</code> <code>bool</code> <p>Whether to add a web basemap. Requires contextily. Defaults to False.</p> <code>False</code> <code>basemap_type</code> <code>str</code> <p>Type of basemap to use. Options: 'streets', 'satellite'. Defaults to 'streets'.</p> <code>'streets'</code> <code>alpha</code> <code>float</code> <p>Transparency of the vector features, between 0-1. Defaults to 0.7.</p> <code>0.7</code> <code>edge_color</code> <code>str</code> <p>Color for feature edges. Defaults to \"black\".</p> <code>'black'</code> <code>classification</code> <code>str</code> <p>Classification method for choropleth maps. Options: \"quantiles\", \"equal_interval\", \"natural_breaks\". Defaults to \"quantiles\".</p> <code>'quantiles'</code> <code>n_classes</code> <code>int</code> <p>Number of classes for choropleth maps. Defaults to 5.</p> <code>5</code> <code>highlight_index</code> <code>list</code> <p>List of indices to highlight. Defaults to None.</p> <code>None</code> <code>highlight_color</code> <code>str</code> <p>Color to use for highlighted features. Defaults to \"red\".</p> <code>'red'</code> <code>scheme</code> <code>str</code> <p>MapClassify classification scheme. Overrides classification parameter if provided. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, the figure is not saved. Defaults to None.</p> <code>None</code> <code>dpi</code> <code>int</code> <p>DPI for saved figure. Defaults to 300.</p> <code>300</code> <p>Returns:</p> Type Description <code>Any</code> <p>matplotlib.axes.Axes: The Axes object containing the plot.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    column: Optional[str] = None,\n    cmap: str = \"viridis\",\n    figsize: Tuple[int, int] = (10, 10),\n    title: Optional[str] = None,\n    legend: bool = True,\n    basemap: bool = False,\n    basemap_type: str = \"streets\",\n    alpha: float = 0.7,\n    edge_color: str = \"black\",\n    classification: str = \"quantiles\",\n    n_classes: int = 5,\n    highlight_index: Optional[int] = None,\n    highlight_color: str = \"red\",\n    scheme: Optional[str] = None,\n    save_path: Optional[str] = None,\n    dpi: int = 300,\n) -&gt; Any:\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        column (str, optional): Column to use for choropleth mapping. If None,\n            a single color will be used. Defaults to None.\n        cmap (str or matplotlib.colors.Colormap, optional): Colormap to use for\n            choropleth mapping. Defaults to \"viridis\".\n        figsize (tuple, optional): Figure size as (width, height) in inches.\n            Defaults to (10, 10).\n        title (str, optional): Title for the plot. Defaults to None.\n        legend (bool, optional): Whether to display a legend. Defaults to True.\n        basemap (bool, optional): Whether to add a web basemap. Requires contextily.\n            Defaults to False.\n        basemap_type (str, optional): Type of basemap to use. Options: 'streets', 'satellite'.\n            Defaults to 'streets'.\n        alpha (float, optional): Transparency of the vector features, between 0-1.\n            Defaults to 0.7.\n        edge_color (str, optional): Color for feature edges. Defaults to \"black\".\n        classification (str, optional): Classification method for choropleth maps.\n            Options: \"quantiles\", \"equal_interval\", \"natural_breaks\".\n            Defaults to \"quantiles\".\n        n_classes (int, optional): Number of classes for choropleth maps.\n            Defaults to 5.\n        highlight_index (list, optional): List of indices to highlight.\n            Defaults to None.\n        highlight_color (str, optional): Color to use for highlighted features.\n            Defaults to \"red\".\n        scheme (str, optional): MapClassify classification scheme. Overrides\n            classification parameter if provided. Defaults to None.\n        save_path (str, optional): Path to save the figure. If None, the figure\n            is not saved. Defaults to None.\n        dpi (int, optional): DPI for saved figure. Defaults to 300.\n\n    Returns:\n        matplotlib.axes.Axes: The Axes object containing the plot.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector(cities, \"population\", cmap=\"Reds\", basemap=True)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector(roads, \"type\", basemap=True, figsize=(12, 8))\n    \"\"\"\n    import contextily as ctx\n\n    if isinstance(vector_data, str):\n        vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    # Make a copy to avoid changing the original data\n    gdf = vector_data.copy()\n\n    # Set up figure and axis\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Determine geometry type\n    geom_type = gdf.geometry.iloc[0].geom_type\n\n    # Plotting parameters\n    plot_kwargs = {\"alpha\": alpha, \"ax\": ax}\n\n    # Set up keyword arguments based on geometry type\n    if \"Point\" in geom_type:\n        plot_kwargs[\"markersize\"] = 50\n        plot_kwargs[\"edgecolor\"] = edge_color\n    elif \"Line\" in geom_type:\n        plot_kwargs[\"linewidth\"] = 1\n    elif \"Polygon\" in geom_type:\n        plot_kwargs[\"edgecolor\"] = edge_color\n\n    # Classification options\n    if column is not None:\n        if scheme is not None:\n            # Use mapclassify scheme if provided\n            plot_kwargs[\"scheme\"] = scheme\n        else:\n            # Use classification parameter\n            if classification == \"quantiles\":\n                plot_kwargs[\"scheme\"] = \"quantiles\"\n            elif classification == \"equal_interval\":\n                plot_kwargs[\"scheme\"] = \"equal_interval\"\n            elif classification == \"natural_breaks\":\n                plot_kwargs[\"scheme\"] = \"fisher_jenks\"\n\n        plot_kwargs[\"k\"] = n_classes\n        plot_kwargs[\"cmap\"] = cmap\n        plot_kwargs[\"column\"] = column\n        plot_kwargs[\"legend\"] = legend\n\n    # Plot the main data\n    gdf.plot(**plot_kwargs)\n\n    # Highlight specific features if requested\n    if highlight_index is not None:\n        gdf.iloc[highlight_index].plot(\n            ax=ax, color=highlight_color, edgecolor=\"black\", linewidth=2, zorder=5\n        )\n\n    if basemap:\n        try:\n            basemap_options = {\n                \"streets\": ctx.providers.OpenStreetMap.Mapnik,\n                \"satellite\": ctx.providers.Esri.WorldImagery,\n            }\n            ctx.add_basemap(ax, crs=gdf.crs, source=basemap_options[basemap_type])\n        except Exception as e:\n            print(f\"Could not add basemap: {e}\")\n\n    # Set title if provided\n    if title:\n        ax.set_title(title, fontsize=14)\n\n    # Remove axes if not needed\n    ax.set_axis_off()\n\n    # Adjust layout\n    plt.tight_layout()\n\n    # Save figure if a path is provided\n    if save_path:\n        plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n\n    return ax\n</code></pre>"},{"location":"utils/#geoai.utils.view_vector_interactive","title":"<code>view_vector_interactive(vector_data, layer_name='Vector Layer', tiles_args=None, **kwargs)</code>","text":"<p>Visualize vector datasets with options for styling, classification, basemaps and more.</p> <p>This function visualizes GeoDataFrame objects with customizable symbology. It supports different vector types (points, lines, polygons), attribute-based classification, and background basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>vector_data</code> <code>GeoDataFrame</code> <p>The vector dataset to visualize.</p> required <code>layer_name</code> <code>str</code> <p>The name of the layer. Defaults to \"Vector Layer\".</p> <code>'Vector Layer'</code> <code>tiles_args</code> <code>dict</code> <p>Additional arguments for the localtileserver client. get_folium_tile_layer function. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments to pass to GeoDataFrame.explore() function. See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>folium.Map: The map object with the vector data added.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import geopandas as gpd\n&gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n&gt;&gt;&gt; view_vector_interactive(cities)\n</code></pre> <pre><code>&gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n&gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n</code></pre> Source code in <code>geoai/utils.py</code> <pre><code>def view_vector_interactive(\n    vector_data: Union[str, gpd.GeoDataFrame],\n    layer_name: str = \"Vector Layer\",\n    tiles_args: Optional[Dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    Visualize vector datasets with options for styling, classification, basemaps and more.\n\n    This function visualizes GeoDataFrame objects with customizable symbology.\n    It supports different vector types (points, lines, polygons), attribute-based\n    classification, and background basemaps.\n\n    Args:\n        vector_data (geopandas.GeoDataFrame): The vector dataset to visualize.\n        layer_name (str, optional): The name of the layer. Defaults to \"Vector Layer\".\n        tiles_args (dict, optional): Additional arguments for the localtileserver client.\n            get_folium_tile_layer function. Defaults to None.\n        **kwargs: Additional keyword arguments to pass to GeoDataFrame.explore() function.\n            See https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html\n\n    Returns:\n        folium.Map: The map object with the vector data added.\n\n    Examples:\n        &gt;&gt;&gt; import geopandas as gpd\n        &gt;&gt;&gt; cities = gpd.read_file(\"cities.shp\")\n        &gt;&gt;&gt; view_vector_interactive(cities)\n\n        &gt;&gt;&gt; roads = gpd.read_file(\"roads.shp\")\n        &gt;&gt;&gt; view_vector_interactive(roads, figsize=(12, 8))\n    \"\"\"\n    import folium\n    import folium.plugins as plugins\n    from leafmap import cog_tile\n    from localtileserver import TileClient, get_folium_tile_layer\n\n    google_tiles = {\n        \"Roadmap\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Maps\",\n        },\n        \"Satellite\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Satellite\",\n        },\n        \"Terrain\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Terrain\",\n        },\n        \"Hybrid\": {\n            \"url\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n            \"attribution\": \"Google\",\n            \"name\": \"Google Hybrid\",\n        },\n    }\n\n    basemap_layer_name = None\n    raster_layer = None\n\n    if \"tiles\" in kwargs and isinstance(kwargs[\"tiles\"], str):\n        if kwargs[\"tiles\"].title() in google_tiles:\n            basemap_layer_name = google_tiles[kwargs[\"tiles\"].title()][\"name\"]\n            kwargs[\"tiles\"] = google_tiles[kwargs[\"tiles\"].title()][\"url\"]\n            kwargs[\"attr\"] = \"Google\"\n        elif kwargs[\"tiles\"].lower().endswith(\".tif\"):\n            if tiles_args is None:\n                tiles_args = {}\n            if kwargs[\"tiles\"].lower().startswith(\"http\"):\n                basemap_layer_name = \"Remote Raster\"\n                kwargs[\"tiles\"] = cog_tile(kwargs[\"tiles\"], **tiles_args)\n                kwargs[\"attr\"] = \"TiTiler\"\n            else:\n                basemap_layer_name = \"Local Raster\"\n                client = TileClient(kwargs[\"tiles\"])\n                raster_layer = get_folium_tile_layer(client, **tiles_args)\n                kwargs[\"tiles\"] = raster_layer.tiles\n                kwargs[\"attr\"] = \"localtileserver\"\n\n    if \"max_zoom\" not in kwargs:\n        kwargs[\"max_zoom\"] = 30\n\n    if isinstance(vector_data, str):\n        if vector_data.endswith(\".parquet\"):\n            vector_data = gpd.read_parquet(vector_data)\n        else:\n            vector_data = gpd.read_file(vector_data)\n\n    # Check if input is a GeoDataFrame\n    if not isinstance(vector_data, gpd.GeoDataFrame):\n        raise TypeError(\"Input data must be a GeoDataFrame\")\n\n    layer_control = kwargs.pop(\"layer_control\", True)\n    fullscreen_control = kwargs.pop(\"fullscreen_control\", True)\n\n    m = vector_data.explore(**kwargs)\n\n    # Change the layer name\n    for layer in m._children.values():\n        if isinstance(layer, folium.GeoJson):\n            layer.layer_name = layer_name\n        if isinstance(layer, folium.TileLayer) and basemap_layer_name:\n            layer.layer_name = basemap_layer_name\n\n    if layer_control:\n        m.add_child(folium.LayerControl())\n\n    if fullscreen_control:\n        plugins.Fullscreen().add_to(m)\n\n    return m\n</code></pre>"},{"location":"utils/#geoai.utils.visualize_vector_by_attribute","title":"<code>visualize_vector_by_attribute(vector_path, attribute_name, cmap='viridis', figsize=(10, 8))</code>","text":"<p>Create a thematic map visualization of vector data based on an attribute.</p> <p>Parameters:</p> Name Type Description Default <code>vector_path</code> <code>str</code> <p>Path to the vector file</p> required <code>attribute_name</code> <code>str</code> <p>Name of the attribute to visualize</p> required <code>cmap</code> <code>str</code> <p>Matplotlib colormap name. Defaults to 'viridis'.</p> <code>'viridis'</code> <code>figsize</code> <code>tuple</code> <p>Figure size as (width, height). Defaults to (10, 8).</p> <code>(10, 8)</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if visualization was successful, False otherwise</p> Source code in <code>geoai/utils.py</code> <pre><code>def visualize_vector_by_attribute(\n    vector_path: str,\n    attribute_name: str,\n    cmap: str = \"viridis\",\n    figsize: Tuple[int, int] = (10, 8),\n) -&gt; bool:\n    \"\"\"Create a thematic map visualization of vector data based on an attribute.\n\n    Args:\n        vector_path (str): Path to the vector file\n        attribute_name (str): Name of the attribute to visualize\n        cmap (str, optional): Matplotlib colormap name. Defaults to 'viridis'.\n        figsize (tuple, optional): Figure size as (width, height). Defaults to (10, 8).\n\n    Returns:\n        bool: True if visualization was successful, False otherwise\n    \"\"\"\n    try:\n        # Read the vector data\n        gdf = gpd.read_file(vector_path)\n\n        # Check if attribute exists\n        if attribute_name not in gdf.columns:\n            print(f\"Attribute '{attribute_name}' not found in the dataset\")\n            return False\n\n        # Create the plot\n        fig, ax = plt.subplots(figsize=figsize)\n\n        # Determine plot type based on data type\n        if pd.api.types.is_numeric_dtype(gdf[attribute_name]):\n            # Continuous data\n            gdf.plot(column=attribute_name, cmap=cmap, legend=True, ax=ax)\n        else:\n            # Categorical data\n            gdf.plot(column=attribute_name, categorical=True, legend=True, ax=ax)\n\n        # Add title and labels\n        ax.set_title(f\"{os.path.basename(vector_path)} - {attribute_name}\")\n        ax.set_xlabel(\"Longitude\")\n        ax.set_ylabel(\"Latitude\")\n\n        # Add basemap or additional elements if available\n        # Note: Additional options could be added here for more complex maps\n\n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"Error visualizing data: {str(e)}\")\n</code></pre>"},{"location":"utils/#geoai.utils.write_colormap","title":"<code>write_colormap(image, colormap, output=None)</code>","text":"<p>Write a colormap to an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, ndarray]</code> <p>The image to write the colormap to.</p> required <code>colormap</code> <code>Union[str, Dict]</code> <p>The colormap to write to the image.</p> required <code>output</code> <code>Optional[str]</code> <p>The output file path.</p> <code>None</code> Source code in <code>geoai/utils.py</code> <pre><code>def write_colormap(\n    image: Union[str, np.ndarray],\n    colormap: Union[str, Dict],\n    output: Optional[str] = None,\n) -&gt; Optional[str]:\n    \"\"\"Write a colormap to an image.\n\n    Args:\n        image: The image to write the colormap to.\n        colormap: The colormap to write to the image.\n        output: The output file path.\n    \"\"\"\n    if isinstance(colormap, str):\n        colormap = leafmap.get_image_colormap(colormap)\n    leafmap.write_image_colormap(image, colormap, output)\n</code></pre>"},{"location":"examples/AI_agents/","title":"AI agents","text":"<p>AI Agents for Geospatial Analysis and Visualization</p> <p></p> <p>Uncomment the following line to install leafmap if needed.</p> In\u00a0[\u00a0]: Copied! <pre># %pip install \"geoai-py[agents]\"\n</pre> # %pip install \"geoai-py[agents]\" In\u00a0[\u00a0]: Copied! <pre>from geoai import Map\nfrom geoai.agents import (\n    GeoAgent,\n    create_ollama_model,\n    create_openai_model,\n    create_anthropic_model,\n    create_bedrock_model,\n)\n</pre> from geoai import Map from geoai.agents import (     GeoAgent,     create_ollama_model,     create_openai_model,     create_anthropic_model,     create_bedrock_model, ) In\u00a0[\u00a0]: Copied! <pre>model = create_ollama_model(model=\"llama3.1\")\n</pre> model = create_ollama_model(model=\"llama3.1\") In\u00a0[\u00a0]: Copied! <pre>agent = GeoAgent(model=model)\n</pre> agent = GeoAgent(model=model) In\u00a0[\u00a0]: Copied! <pre>agent.model\n</pre> agent.model In\u00a0[\u00a0]: Copied! <pre>agent.session.m\n</pre> agent.session.m In\u00a0[\u00a0]: Copied! <pre>agent.ask(\"Add OpenTopoMap basemap\")\n</pre> agent.ask(\"Add OpenTopoMap basemap\") In\u00a0[\u00a0]: Copied! <pre>agent.ask(\"Fly to Chicago\")\n</pre> agent.ask(\"Fly to Chicago\") In\u00a0[\u00a0]: Copied! <pre>agent.model\n</pre> agent.model In\u00a0[\u00a0]: Copied! <pre>custom_map = Map(center=[-83.92, 35.96], zoom=11, projection=\"globe\")\ncustom_map\n</pre> custom_map = Map(center=[-83.92, 35.96], zoom=11, projection=\"globe\") custom_map In\u00a0[\u00a0]: Copied! <pre>agent = GeoAgent(model=model, map_instance=custom_map)\nagent.ask(\"Add basemap Esri.WorldImagery\")\n</pre> agent = GeoAgent(model=model, map_instance=custom_map) agent.ask(\"Add basemap Esri.WorldImagery\") In\u00a0[\u00a0]: Copied! <pre>agent.ask(\"Remove Esri.WorldImagery\")\n</pre> agent.ask(\"Remove Esri.WorldImagery\") In\u00a0[\u00a0]: Copied! <pre>m = Map(center=[-100, 40], zoom=3, projection=\"globe\")\nagent = GeoAgent(model=model, map_instance=m)\nagent.show_ui()\n</pre> m = Map(center=[-100, 40], zoom=3, projection=\"globe\") agent = GeoAgent(model=model, map_instance=m) agent.show_ui()"},{"location":"examples/AI_agents/#installation","title":"Installation\u00b6","text":""},{"location":"examples/AI_agents/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/AI_agents/#create-a-model","title":"Create a model\u00b6","text":"<p>You can create a model with the following functions:</p> <ul> <li><code>create_ollama_model</code>: Create a model using Ollama. You will need to install Ollama) separately and pull the model you want to use, such as <code>llama3.1</code>.</li> <li><code>create_openai_model</code>: Create a model using OpenAI. You will need an OpenAI API key. Set it in the <code>OPENAI_API_KEY</code> environment variable.</li> <li><code>create_anthropic_model</code>: Create a model using Anthropic. You will need an Anthropic API key. Set it in the <code>ANTHROPIC_API_KEY</code> environment variable.</li> <li><code>create_bedrock_model</code>: Create a model using Bedrock. You will need an AWS account and the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables set.</li> </ul>"},{"location":"examples/AI_agents/#create-an-agent-with-default-map","title":"Create an agent with default map\u00b6","text":""},{"location":"examples/AI_agents/#create-an-agent-with-a-custom-map","title":"Create an agent with a custom map\u00b6","text":""},{"location":"examples/AI_agents/#show-the-agent-ui","title":"Show the agent UI\u00b6","text":""},{"location":"examples/AlphaEarth/","title":"AlphaEarth","text":"<p>Visualize AlphaEarth satellite embeddings in 3D</p> <p></p> <p>Google DeepMind has released a new satellite embedding dataset called AlphaEarth. This dataset contains annual satellite embeddings from 2017 to 2024, with each pixel representing a 10x10 meter area. The dataset is available on Google Earth Engine, and can be used to train machine learning models to classify satellite imagery.</p> <ul> <li>News release: https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/</li> <li>Dataset: https://developers.google.com/earth-engine/datasets/catalog/GOOGLE_SATELLITE_EMBEDDING_V1_ANNUAL#description</li> <li>Paper: https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/alphaearth-foundations.pdf</li> <li>Blog post: https://medium.com/google-earth/ai-powered-pixels-introducing-googles-satellite-embedding-dataset-31744c1f4650</li> <li>Tutorials: https://developers.google.com/earth-engine/tutorials/community/satellite-embedding-01-introduction</li> <li>Similarity search: https://earthengine-ai.projects.earthengine.app/view/embedding-similarity-search</li> <li>Clustering: https://code.earthengine.google.com/b0871454add885294f633f731b90f946</li> </ul> <p>Uncomment the following line to install leafmap if needed.</p> In\u00a0[\u00a0]: Copied! <pre># %pip install -U leafmap geemap\n</pre> # %pip install -U leafmap geemap In\u00a0[\u00a0]: Copied! <pre>import ee\nimport geoai\n</pre> import ee import geoai <p>To use the AlphaEarth satellite embeddings, you will need to authenticate with Earth Engine.</p> <p>If you don't have an Earth Engine account, you can create one at https://earthengine.google.com.</p> <p>Once you have an Earth Engine account, you can authenticate with Earth Engine by running the following code:</p> In\u00a0[\u00a0]: Copied! <pre>ee.Authenticate()\nee.Initialize(project=\"your-ee-project\")\n</pre> ee.Authenticate() ee.Initialize(project=\"your-ee-project\") In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(projection=\"globe\", sidebar_visible=True)\nm.add_basemap(\"USGS.Imagery\")\nm.add_alphaearth_gui()\nm\n</pre> m = geoai.Map(projection=\"globe\", sidebar_visible=True) m.add_basemap(\"USGS.Imagery\") m.add_alphaearth_gui() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(projection=\"globe\", sidebar_visible=True)\nm.add_basemap(\"USGS.Imagery\")\nm\n</pre> m = geoai.Map(projection=\"globe\", sidebar_visible=True) m.add_basemap(\"USGS.Imagery\") m In\u00a0[\u00a0]: Copied! <pre>lon = -121.8036\nlat = 39.0372\nm.set_center(lon, lat, zoom=12)\n</pre> lon = -121.8036 lat = 39.0372 m.set_center(lon, lat, zoom=12) In\u00a0[\u00a0]: Copied! <pre>point = ee.Geometry.Point(lon, lat)\ndataset = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\")\n</pre> point = ee.Geometry.Point(lon, lat) dataset = ee.ImageCollection(\"GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL\") In\u00a0[\u00a0]: Copied! <pre>image1 = dataset.filterDate(\"2017-01-01\", \"2018-01-01\").filterBounds(point).first()\nimage2 = dataset.filterDate(\"2024-01-01\", \"2025-01-01\").filterBounds(point).first()\n</pre> image1 = dataset.filterDate(\"2017-01-01\", \"2018-01-01\").filterBounds(point).first() image2 = dataset.filterDate(\"2024-01-01\", \"2025-01-01\").filterBounds(point).first() In\u00a0[\u00a0]: Copied! <pre>vis_params = {\"min\": -0.3, \"max\": 0.3, \"bands\": [\"A01\", \"A16\", \"A09\"]}\nm.add_ee_layer(image1, vis_params, name=\"Year 1 embeddings\")\nm.add_ee_layer(image2, vis_params, name=\"Year 2 embeddings\")\n</pre> vis_params = {\"min\": -0.3, \"max\": 0.3, \"bands\": [\"A01\", \"A16\", \"A09\"]} m.add_ee_layer(image1, vis_params, name=\"Year 1 embeddings\") m.add_ee_layer(image2, vis_params, name=\"Year 2 embeddings\") In\u00a0[\u00a0]: Copied! <pre>dot_prod = image1.multiply(image2).reduce(ee.Reducer.sum())\n</pre> dot_prod = image1.multiply(image2).reduce(ee.Reducer.sum()) In\u00a0[\u00a0]: Copied! <pre>vis_params = {\"min\": 0, \"max\": 1, \"palette\": [\"white\", \"black\"]}\nm.add_ee_layer(dot_prod, vis_params, name=\"Similarity\")\nm\n</pre> vis_params = {\"min\": 0, \"max\": 1, \"palette\": [\"white\", \"black\"]} m.add_ee_layer(dot_prod, vis_params, name=\"Similarity\") m <p></p>"},{"location":"examples/DINOv3/","title":"DINOv3","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\nraster_path = geoai.download_file(url)\n</pre> url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) raster_path = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre>processor = geoai.DINOv3GeoProcessor(\n    model_name=\"dinov3_vitl16\",\n)\n</pre> processor = geoai.DINOv3GeoProcessor(     model_name=\"dinov3_vitl16\", ) In\u00a0[\u00a0]: Copied! <pre>image = \"naip_rgb_train.tif\"\nfeatures, h_patches, w_patches = processor.extract_features(image)\n</pre> image = \"naip_rgb_train.tif\" features, h_patches, w_patches = processor.extract_features(image) In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap()\nm.add_dinov3_gui(image, processor, features)\nm\n</pre> m = geoai.LeafMap() m.add_dinov3_gui(image, processor, features) m"},{"location":"examples/DINOv3/#find-similar-patches-with-dinov3","title":"Find similar patches with DINOv3\u00b6","text":"<p>This notebook demonstrates how to find similar patches with DINOv3.</p>"},{"location":"examples/DINOv3/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/DINOv3/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/DINOv3/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/DINOv3/#initialize-the-dinov3-processor","title":"Initialize the DINOv3 processor\u00b6","text":""},{"location":"examples/DINOv3/#extract-features","title":"Extract features\u00b6","text":""},{"location":"examples/DINOv3/#find-similar-patches","title":"Find similar patches\u00b6","text":""},{"location":"examples/DINOv3_visualization/","title":"DINOv3 visualization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport geoai\nfrom geoai.dinov3 import DINOv3GeoProcessor, visualize_similarity_results\n\n%matplotlib inline\n</pre> import numpy as np import matplotlib.pyplot as plt from PIL import Image import geoai from geoai.dinov3 import DINOv3GeoProcessor, visualize_similarity_results  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/spaces/pszemraj/dinov3-viz-sat493m/resolve/main/aralsea_tmo_2000238_lrg.jpg\"\ntest_image_path = geoai.download_file(url)\n</pre> url = \"https://huggingface.co/spaces/pszemraj/dinov3-viz-sat493m/resolve/main/aralsea_tmo_2000238_lrg.jpg\" test_image_path = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre># Create and display test image\ntest_image = Image.open(test_image_path)\n\nplt.figure(figsize=(8, 8))\nplt.imshow(test_image)\nplt.axis(\"off\")\nplt.show()\n</pre> # Create and display test image test_image = Image.open(test_image_path)  plt.figure(figsize=(8, 8)) plt.imshow(test_image) plt.axis(\"off\") plt.show() In\u00a0[\u00a0]: Copied! <pre># Query coordinates\nquery_coords = (1250, 1250)\n\n# Side-by-side visualization\nresults = visualize_similarity_results(\n    input_image=test_image_path,\n    query_coords=query_coords,\n    overlay=False,  # Side-by-side view\n    figsize=(15, 6),\n    colormap=\"turbo\",\n    show_query_point=True,\n)\n\nplt.show()\nprint(f\"Query patch coordinates: {results['patch_coords']}\")\nprint(f\"Patch grid size: {results['patch_grid_size']}\")\n</pre> # Query coordinates query_coords = (1250, 1250)  # Side-by-side visualization results = visualize_similarity_results(     input_image=test_image_path,     query_coords=query_coords,     overlay=False,  # Side-by-side view     figsize=(15, 6),     colormap=\"turbo\",     show_query_point=True, )  plt.show() print(f\"Query patch coordinates: {results['patch_coords']}\") print(f\"Patch grid size: {results['patch_grid_size']}\") In\u00a0[\u00a0]: Copied! <pre># Overlay visualization\nresults_overlay = visualize_similarity_results(\n    input_image=test_image_path,\n    query_coords=query_coords,\n    overlay=True,  # Overlay view\n    alpha=0.6,  # Transparency\n    colormap=\"jet\",\n    show_query_point=True,\n)\n\nplt.show()\n</pre> # Overlay visualization results_overlay = visualize_similarity_results(     input_image=test_image_path,     query_coords=query_coords,     overlay=True,  # Overlay view     alpha=0.6,  # Transparency     colormap=\"jet\",     show_query_point=True, )  plt.show() In\u00a0[\u00a0]: Copied! <pre># Initialize processor\nprocessor = DINOv3GeoProcessor()\n\n# Load and preprocess image\ndata, metadata = processor.load_image(test_image_path)\nimage = processor.preprocess_image_for_dinov3(data)\nfeatures, h_patches, w_patches = processor.extract_features(image)\n\nprint(f\"Image size: {image.size}\")\nprint(f\"Features shape: {features.shape}\")\nprint(f\"Patch grid: {h_patches} x {w_patches}\")\n</pre> # Initialize processor processor = DINOv3GeoProcessor()  # Load and preprocess image data, metadata = processor.load_image(test_image_path) image = processor.preprocess_image_for_dinov3(data) features, h_patches, w_patches = processor.extract_features(image)  print(f\"Image size: {image.size}\") print(f\"Features shape: {features.shape}\") print(f\"Patch grid: {h_patches} x {w_patches}\") In\u00a0[\u00a0]: Copied! <pre># Convert pixel coordinates to patch coordinates\nimg_w, img_h = data.shape[2], data.shape[1]\npatch_x = int((query_coords[0] / img_w) * w_patches)\npatch_y = int((query_coords[1] / img_h) * h_patches)\n\nprint(f\"Query pixel coordinates: {query_coords}\")\nprint(f\"Query patch coordinates: ({patch_x}, {patch_y})\")\n\n# Compute similarity\nsimilarities = processor.compute_patch_similarity(features, patch_x, patch_y)\nsimilarity_array = similarities.cpu().numpy()\n\nprint(f\"Similarity range: {similarity_array.min():.3f} - {similarity_array.max():.3f}\")\n</pre> # Convert pixel coordinates to patch coordinates img_w, img_h = data.shape[2], data.shape[1] patch_x = int((query_coords[0] / img_w) * w_patches) patch_y = int((query_coords[1] / img_h) * h_patches)  print(f\"Query pixel coordinates: {query_coords}\") print(f\"Query patch coordinates: ({patch_x}, {patch_y})\")  # Compute similarity similarities = processor.compute_patch_similarity(features, patch_x, patch_y) similarity_array = similarities.cpu().numpy()  print(f\"Similarity range: {similarity_array.min():.3f} - {similarity_array.max():.3f}\") In\u00a0[\u00a0]: Copied! <pre># Visualize patch grid\nfig = processor.visualize_patches(\n    image=image, features=features, patch_coords=(patch_x, patch_y), figsize=(12, 8)\n)\n\nplt.show()\n</pre> # Visualize patch grid fig = processor.visualize_patches(     image=image, features=features, patch_coords=(patch_x, patch_y), figsize=(12, 8) )  plt.show() In\u00a0[\u00a0]: Copied! <pre># Create side-by-side visualization\nfig = processor.visualize_similarity(\n    source=test_image_path,\n    similarity_data=similarity_array,\n    query_coords=query_coords,\n    patch_coords=(patch_x, patch_y),\n    overlay=False,\n    colormap=\"jet\",\n    figsize=(15, 6),\n)\n\nplt.show()\n</pre> # Create side-by-side visualization fig = processor.visualize_similarity(     source=test_image_path,     similarity_data=similarity_array,     query_coords=query_coords,     patch_coords=(patch_x, patch_y),     overlay=False,     colormap=\"jet\",     figsize=(15, 6), )  plt.show() In\u00a0[\u00a0]: Copied! <pre># Create overlay visualization\nfig = processor.visualize_similarity(\n    source=test_image_path,\n    similarity_data=similarity_array,\n    query_coords=query_coords,\n    overlay=True,\n    alpha=0.5,\n    colormap=\"jet\",\n)\n\nplt.show()\n</pre> # Create overlay visualization fig = processor.visualize_similarity(     source=test_image_path,     similarity_data=similarity_array,     query_coords=query_coords,     overlay=True,     alpha=0.5,     colormap=\"jet\", )  plt.show() In\u00a0[\u00a0]: Copied! <pre># Create overlay as numpy array\noverlay_img = processor.create_similarity_overlay(\n    source=test_image_path, similarity_data=similarity_array, colormap=\"jet\", alpha=0.6\n)\n\n# Display the overlay\nplt.figure(figsize=(10, 10))\nplt.imshow(overlay_img)\nplt.title(\"Programmatically Created Overlay\")\nplt.axis(\"off\")\nplt.show()\n\n# Save overlay as image file\noverlay_pil = Image.fromarray((overlay_img * 255).astype(np.uint8))\noverlay_pil.save(\"demo_overlay.png\")\nprint(\"Overlay saved to demo_overlay.png\")\n</pre> # Create overlay as numpy array overlay_img = processor.create_similarity_overlay(     source=test_image_path, similarity_data=similarity_array, colormap=\"jet\", alpha=0.6 )  # Display the overlay plt.figure(figsize=(10, 10)) plt.imshow(overlay_img) plt.title(\"Programmatically Created Overlay\") plt.axis(\"off\") plt.show()  # Save overlay as image file overlay_pil = Image.fromarray((overlay_img * 255).astype(np.uint8)) overlay_pil.save(\"demo_overlay.png\") print(\"Overlay saved to demo_overlay.png\") In\u00a0[\u00a0]: Copied! <pre># Test multiple query points\nquery_points = [\n    (128, 128),  # Red circle\n    (384, 128),  # Green circle\n    (128, 384),  # Blue circle\n    (384, 384),  # Yellow circle\n]\n\ncolors = [\"Red\", \"Green\", \"Blue\", \"Yellow\"]\n\nfig, axes = plt.subplots(2, 2, figsize=(20, 16))\naxes = axes.flatten()\n\nfor i, (coords, color) in enumerate(zip(query_points, colors)):\n    # Quick visualization for each point\n    results = visualize_similarity_results(\n        input_image=test_image_path,\n        query_coords=coords,\n        overlay=True,\n        alpha=0.6,\n        show_query_point=True,\n    )\n\n    # Close the generated figure and create our own subplot\n    plt.close(results[\"visualization\"])\n\n    # Get the similarity data and create our own visualization\n    similarity_data = results[\"image_dict\"][\"image\"][0]\n    data, _ = processor.load_image(test_image_path)\n    display_img = np.transpose(data, (1, 2, 0))\n\n    # Show on subplot\n    axes[i].imshow(display_img / 255.0)\n    im = axes[i].imshow(similarity_data, cmap=\"turbo\", alpha=0.6, vmin=0, vmax=1)\n    axes[i].plot(\n        coords[0],\n        coords[1],\n        \"r*\",\n        markersize=15,\n        markeredgecolor=\"white\",\n        markeredgewidth=2,\n    )\n    axes[i].set_title(f\"Query: {color} Circle at {coords}\")\n    axes[i].axis(\"off\")\n\n    # Add colorbar\n    plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)\n\nplt.tight_layout()\nplt.show()\n</pre> # Test multiple query points query_points = [     (128, 128),  # Red circle     (384, 128),  # Green circle     (128, 384),  # Blue circle     (384, 384),  # Yellow circle ]  colors = [\"Red\", \"Green\", \"Blue\", \"Yellow\"]  fig, axes = plt.subplots(2, 2, figsize=(20, 16)) axes = axes.flatten()  for i, (coords, color) in enumerate(zip(query_points, colors)):     # Quick visualization for each point     results = visualize_similarity_results(         input_image=test_image_path,         query_coords=coords,         overlay=True,         alpha=0.6,         show_query_point=True,     )      # Close the generated figure and create our own subplot     plt.close(results[\"visualization\"])      # Get the similarity data and create our own visualization     similarity_data = results[\"image_dict\"][\"image\"][0]     data, _ = processor.load_image(test_image_path)     display_img = np.transpose(data, (1, 2, 0))      # Show on subplot     axes[i].imshow(display_img / 255.0)     im = axes[i].imshow(similarity_data, cmap=\"turbo\", alpha=0.6, vmin=0, vmax=1)     axes[i].plot(         coords[0],         coords[1],         \"r*\",         markersize=15,         markeredgecolor=\"white\",         markeredgewidth=2,     )     axes[i].set_title(f\"Query: {color} Circle at {coords}\")     axes[i].axis(\"off\")      # Add colorbar     plt.colorbar(im, ax=axes[i], fraction=0.046, pad=0.04)  plt.tight_layout() plt.show()"},{"location":"examples/DINOv3_visualization/#dinov3-similarity-visualization","title":"DINOv3 Similarity Visualization\u00b6","text":"<p>This notebook demonstrates the new visualization functionality for DINOv3 similarity analysis in the geoai package.</p>"},{"location":"examples/DINOv3_visualization/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/DINOv3_visualization/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/DINOv3_visualization/#method-1-quick-visualization-with-visualize_similarity_results","title":"Method 1: Quick Visualization with visualize_similarity_results()\u00b6","text":"<p>The easiest way to create and visualize similarity results in one step.</p>"},{"location":"examples/DINOv3_visualization/#method-2-step-by-step-with-dinov3geoprocessor","title":"Method 2: Step-by-step with DINOv3GeoProcessor\u00b6","text":"<p>For more control over the process, you can use the processor methods directly.</p>"},{"location":"examples/DINOv3_visualization/#method-3-create-overlay-image-array","title":"Method 3: Create Overlay Image Array\u00b6","text":"<p>You can also create overlay images programmatically for further processing.</p>"},{"location":"examples/DINOv3_visualization/#test-different-query-points","title":"Test Different Query Points\u00b6","text":"<p>Let's test similarity analysis with different query points.</p>"},{"location":"examples/DINOv3_wetlands/","title":"DINOv3 wetlands","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609931_ne_14_1_20100629_subset.tif\"\nimage = geoai.download_file(url)\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609931_ne_14_1_20100629_subset.tif\" image = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre>processor = geoai.DINOv3GeoProcessor(\n    model_name=\"dinov3_vitl16\",\n)\n</pre> processor = geoai.DINOv3GeoProcessor(     model_name=\"dinov3_vitl16\", ) In\u00a0[\u00a0]: Copied! <pre>features, h_patches, w_patches = processor.extract_features(image)\n</pre> features, h_patches, w_patches = processor.extract_features(image) In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap()\nm.add_dinov3_gui(image, processor, features)\nm\n</pre> m = geoai.LeafMap() m.add_dinov3_gui(image, processor, features) m"},{"location":"examples/DINOv3_wetlands/#find-similar-patches-with-dinov3","title":"Find similar patches with DINOv3\u00b6","text":"<p>This notebook demonstrates how to find similar patches with DINOv3.</p>"},{"location":"examples/DINOv3_wetlands/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/DINOv3_wetlands/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/DINOv3_wetlands/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/DINOv3_wetlands/#initialize-the-dinov3-processor","title":"Initialize the DINOv3 processor\u00b6","text":""},{"location":"examples/DINOv3_wetlands/#extract-features","title":"Extract features\u00b6","text":""},{"location":"examples/DINOv3_wetlands/#find-similar-patches","title":"Find similar patches\u00b6","text":""},{"location":"examples/JPEG2000/","title":"JPEG2000","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.jp2\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.jp2\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.jp2\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.jp2\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"buildings\"\n</pre> out_folder = \"buildings\" In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=5,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and building     batch_size=8,     num_epochs=5,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmasks_path = \"naip_test_semantic_prediction.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> # Define paths masks_path = \"naip_test_semantic_prediction.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run semantic segmentation inference\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> # Run semantic segmentation inference geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"naip_test_semantic_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"naip_test_semantic_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=\"Esri.WorldImagery\")\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=\"Esri.WorldImagery\") In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=\"Esri.WorldImagery\")\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=\"Esri.WorldImagery\") In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, )"},{"location":"examples/JPEG2000/#train-a-semantic-segmentation-model-with-jpeg-2000-images","title":"Train a Semantic Segmentation Model with JPEG-2000 Images\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models with JPEG-2000 images. JPEG2000 is a lossless image compression standard that is widely used in remote sensing and GIS applications. It is a more efficient and flexible format than JPEG, which is a lossy image compression standard.</p> <p>To support JPEG2000 images, we need to install the <code>libgdal-jp2openjpeg</code> package with the following command:</p> <pre>conda install conda-forge::libgdal-jp2openjpeg\n</pre> <p>To generate JPEG2000 images, you can use the following command:</p> <pre>gdal_translate -of JP2OpenJPEG input.tif output.jp2\n</pre>"},{"location":"examples/JPEG2000/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/JPEG2000/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/JPEG2000/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same dataset as the Mask R-CNN example for consistency.</p>"},{"location":"examples/JPEG2000/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"examples/JPEG2000/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p>"},{"location":"examples/JPEG2000/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"examples/JPEG2000/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"examples/JPEG2000/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/JPEG2000/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/JPEG2000/#model-performance-analysis","title":"Model Performance Analysis\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/STAC_agents/","title":"STAC agents","text":"<p>STAC Agent - Natural Language Satellite Imagery Search</p> <p></p> <p>This notebook demonstrates how to use the STAC (SpatioTemporal Asset Catalog) Agent to search for satellite and aerial imagery using natural language queries.</p> <p>The STAC Agent can:</p> <ul> <li>Understand natural language queries about geospatial data</li> <li>Search the Microsoft Planetary Computer STAC catalog</li> <li>Convert location names to coordinates</li> <li>Find appropriate collections and items</li> <li>Return structured results with item IDs, URLs, and metadata</li> </ul> <p>Uncomment the following line to install geoai if needed.</p> In\u00a0[\u00a0]: Copied! <pre># %pip install \"geoai-py[agents]\"\n</pre> # %pip install \"geoai-py[agents]\" In\u00a0[\u00a0]: Copied! <pre>import json\nfrom geoai import Map\nfrom geoai.agents import (\n    STACAgent,\n    STACTools,\n    create_ollama_model,\n    create_openai_model,\n    create_anthropic_model,\n    create_bedrock_model,\n)\n</pre> import json from geoai import Map from geoai.agents import (     STACAgent,     STACTools,     create_ollama_model,     create_openai_model,     create_anthropic_model,     create_bedrock_model, ) In\u00a0[\u00a0]: Copied! <pre>model = create_ollama_model(model=\"llama3.1\")\n</pre> model = create_ollama_model(model=\"llama3.1\") In\u00a0[\u00a0]: Copied! <pre>tools = STACTools()\n</pre> tools = STACTools() In\u00a0[\u00a0]: Copied! <pre>tools.list_collections()\n</pre> tools.list_collections() In\u00a0[\u00a0]: Copied! <pre>tools.list_collections(filter_keyword=\"landsat\")\n</pre> tools.list_collections(filter_keyword=\"landsat\") In\u00a0[\u00a0]: Copied! <pre>agent = STACAgent(model=model)\n</pre> agent = STACAgent(model=model) In\u00a0[\u00a0]: Copied! <pre>result = agent.ask(\"Find Sentinel-2 imagery over San Francisco in August 2025\")\njson.loads(result)\n</pre> result = agent.ask(\"Find Sentinel-2 imagery over San Francisco in August 2025\") json.loads(result) In\u00a0[\u00a0]: Copied! <pre>m = Map(center=[-100, 40], zoom=3, projection=\"globe\")\nagent = STACAgent(model=model, map_instance=m)\nagent.show_ui()\n</pre> m = Map(center=[-100, 40], zoom=3, projection=\"globe\") agent = STACAgent(model=model, map_instance=m) agent.show_ui()"},{"location":"examples/STAC_agents/#installation","title":"Installation\u00b6","text":""},{"location":"examples/STAC_agents/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/STAC_agents/#create-a-model","title":"Create a model\u00b6","text":"<p>You can create a model with the following functions:</p> <ul> <li><code>create_ollama_model</code>: Create a model using Ollama. You will need to install Ollama) separately and pull the model you want to use, such as <code>llama3.1</code>.</li> <li><code>create_openai_model</code>: Create a model using OpenAI. You will need an OpenAI API key. Set it in the <code>OPENAI_API_KEY</code> environment variable.</li> <li><code>create_anthropic_model</code>: Create a model using Anthropic. You will need an Anthropic API key. Set it in the <code>ANTHROPIC_API_KEY</code> environment variable.</li> <li><code>create_bedrock_model</code>: Create a model using Bedrock. You will need an AWS account and the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables set.</li> </ul>"},{"location":"examples/STAC_agents/#call-stac-tools-directly","title":"Call STAC tools directly\u00b6","text":""},{"location":"examples/STAC_agents/#show-the-agent-ui","title":"Show the agent UI\u00b6","text":""},{"location":"examples/_template/","title":"template","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/_template/#template","title":"Template\u00b6","text":""},{"location":"examples/_template/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/_template/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/_template/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/_template/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/_template/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/_template/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/_template/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/batch_segmentation/","title":"Batch segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\" In\u00a0[\u00a0]: Copied! <pre>out_folder = geoai.download_file(url)\n</pre> out_folder = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre># Test train_segmentation_model with automatic size detection\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/masks\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and water\n    batch_size=8,\n    num_epochs=30,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Test train_segmentation_model with automatic size detection geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/masks\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and water     batch_size=8,     num_epochs=30,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>index = 3\ntest_image_path = f\"{out_folder}/images/water_body_{index}.jpg\"\nground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\"\nprediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> index = 3 test_image_path = f\"{out_folder}/images/water_body_{index}.jpg\" ground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\" prediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run semantic segmentation inference\ngeoai.semantic_segmentation(\n    input_path=test_image_path,\n    output_path=prediction_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> # Run semantic segmentation inference geoai.semantic_segmentation(     input_path=test_image_path,     output_path=prediction_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>fig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",\n    show_plot=True,\n)\n</pre> fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",     show_plot=True, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\" In\u00a0[\u00a0]: Copied! <pre>data_dir = geoai.download_file(url)\nimages_dir = f\"{data_dir}/images\"\nmasks_dir = f\"{data_dir}/masks\"\npredictions_dir = f\"{data_dir}/predictions\"\n</pre> data_dir = geoai.download_file(url) images_dir = f\"{data_dir}/images\" masks_dir = f\"{data_dir}/masks\" predictions_dir = f\"{data_dir}/predictions\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4,     quiet=True, )"},{"location":"examples/batch_segmentation/#batch-segmentation","title":"Batch Segmentation\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models and run batch segmentation.</p>"},{"location":"examples/batch_segmentation/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/batch_segmentation/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/batch_segmentation/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the waterbody dataset from Kaggle. You will need to create an account and download the dataset. I have already downloaded the dataset and saved a copy on Hugging Face. Let's download the dataset:</p>"},{"location":"examples/batch_segmentation/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p> <p>Let's train the module using U-Net with ResNet34 encoder:</p>"},{"location":"examples/batch_segmentation/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/batch_segmentation/#run-inference-on-a-single-image","title":"Run inference on a single image\u00b6","text":"<p>You can run inference on a new image using the <code>semantic_segmentation</code> function. I don't have a new image to test on, so I'll use one of the training images. In reality, you would use your own images not used in training.</p>"},{"location":"examples/batch_segmentation/#run-inference-on-multiple-images","title":"Run inference on multiple images\u00b6","text":"<p>First, let's download the test images and masks.</p>"},{"location":"examples/building_detection_lidar/","title":"Building detection lidar","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import os\nimport geoai\n</pre> import os import geoai In\u00a0[\u00a0]: Copied! <pre>train_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_naip.tif\"\ntrain_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_hag.tif\"\ntrain_building_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_buildings_train.geojson\"\ntest_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_naip.tif\"\ntest_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_hag.tif\"\n</pre> train_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_naip.tif\" train_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_hag.tif\" train_building_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_buildings_train.geojson\" test_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_naip.tif\" test_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_hag.tif\" In\u00a0[\u00a0]: Copied! <pre>train_aerial_path = geoai.download_file(train_aerial_url)\ntrain_LiDAR_path = geoai.download_file(train_LiDAR_url)\ntrain_building_path = geoai.download_file(train_building_url)\ntest_aerial_path = geoai.download_file(test_aerial_url)\ntest_LiDAR_path = geoai.download_file(test_LiDAR_url)\n</pre> train_aerial_path = geoai.download_file(train_aerial_url) train_LiDAR_path = geoai.download_file(train_LiDAR_url) train_building_path = geoai.download_file(train_building_url) test_aerial_path = geoai.download_file(test_aerial_url) test_LiDAR_path = geoai.download_file(test_LiDAR_url) In\u00a0[\u00a0]: Copied! <pre>os.environ[\"TITILER_ENDPOINT\"] = \"https://giswqs-titiler-endpoint.hf.space\"\n</pre> os.environ[\"TITILER_ENDPOINT\"] = \"https://giswqs-titiler-endpoint.hf.space\" In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_building_path, tiles=train_aerial_url)\n</pre> geoai.view_vector_interactive(train_building_path, tiles=train_aerial_url) <p>Visualize the building footprints with the height above ground (HAG) data derived from LiDAR data.</p> In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_building_path, tiles=train_LiDAR_url)\n</pre> geoai.view_vector_interactive(train_building_path, tiles=train_LiDAR_url) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = \"las_vegas_train_naip_hag.tif\"\ngeoai.stack_bands(\n    input_files=[train_aerial_path, train_LiDAR_path],\n    output_file=train_raster_path,\n    resolution=None,  # Automatically inferred from first image\n    overwrite=True,\n    dtype=\"Byte\",  # or \"UInt16\", \"Float32\"\n)\n</pre> train_raster_path = \"las_vegas_train_naip_hag.tif\" geoai.stack_bands(     input_files=[train_aerial_path, train_LiDAR_path],     output_file=train_raster_path,     resolution=None,  # Automatically inferred from first image     overwrite=True,     dtype=\"Byte\",  # or \"UInt16\", \"Float32\" ) In\u00a0[\u00a0]: Copied! <pre>test_raster_path = \"las_vegas_test_naip_hag.tif\"\ngeoai.stack_bands(\n    input_files=[test_aerial_path, test_LiDAR_path],\n    output_file=test_raster_path,\n    resolution=None,  # Automatically inferred from first image\n    overwrite=True,\n    dtype=\"Byte\",  # or \"UInt16\", \"Float32\"\n)\n</pre> test_raster_path = \"las_vegas_test_naip_hag.tif\" geoai.stack_bands(     input_files=[test_aerial_path, test_LiDAR_path],     output_file=test_raster_path,     resolution=None,  # Automatically inferred from first image     overwrite=True,     dtype=\"Byte\",  # or \"UInt16\", \"Float32\" ) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"buildings\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_building_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"buildings\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_building_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=5,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=50,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=5,     num_classes=2,  # background and building     batch_size=8,     num_epochs=50,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmasks_path = \"building_masks.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> # Define paths masks_path = \"building_masks.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run semantic segmentation inference\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=5,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=8,\n)\n</pre> # Run semantic segmentation inference geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=5,     num_classes=2,     window_size=512,     overlap=256,     batch_size=8, ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"building_masks.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"building_masks.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\nprint(f\"Number of buildings: {len(gdf_props)}\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") print(f\"Number of buildings: {len(gdf_props)}\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, nodata=0, basemap=test_aerial_url, backend=\"ipyleaflet\")\n</pre> geoai.view_raster(masks_path, nodata=0, basemap=test_aerial_url, backend=\"ipyleaflet\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_aerial_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_aerial_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\nprint(f\"Number of buildings: {len(gdf_filtered)}\")\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] print(f\"Number of buildings: {len(gdf_filtered)}\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_aerial_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_aerial_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_aerial_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_aerial_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_aerial_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_aerial_url, )"},{"location":"examples/building_detection_lidar/#building-detection-from-aerial-imagery-and-lidar-data","title":"Building Detection from Aerial Imagery and LiDAR Data\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models for building detection from NAIP aerial imagery and height above ground (HAG) data derived from LiDAR data with just a few lines of code. You can adapt this notebook to segment other objects of interest (such as trees, cars, etc.) from aerial imagery and LiDAR data.</p>"},{"location":"examples/building_detection_lidar/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/building_detection_lidar/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_detection_lidar/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same dataset as the Mask R-CNN example for consistency.</p>"},{"location":"examples/building_detection_lidar/#visualize-sample-data","title":"Visualize sample data\u00b6","text":"<p>Visualize the building footprints with the aerial imagery.</p>"},{"location":"examples/building_detection_lidar/#stack-bands","title":"Stack bands\u00b6","text":"<p>Stack the NAIP and HAG bands into a single image.</p>"},{"location":"examples/building_detection_lidar/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"examples/building_detection_lidar/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p> <p>Let's train a U-Net with ResNet34 encoder</p>"},{"location":"examples/building_detection_lidar/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"examples/building_detection_lidar/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"examples/building_detection_lidar/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"examples/building_detection_lidar/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/building_detection_lidar/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/building_detection_lidar/#performance-metrics","title":"Performance Metrics\u00b6","text":"<p>IoU (Intersection over Union) and Dice score are both popular metrics used to evaluate the similarity between two binary masks\u2014often in image segmentation tasks. While they are related, they are not the same.</p>"},{"location":"examples/building_detection_lidar/#definitions","title":"\ud83d\udd38 Definitions\u00b6","text":""},{"location":"examples/building_detection_lidar/#iou-jaccard-index","title":"IoU (Jaccard Index)\u00b6","text":"<p>$$ \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} $$</p> <ul> <li>Measures the overlap between predicted region $A$ and ground truth region $B$ relative to their union.</li> <li>Ranges from 0 (no overlap) to 1 (perfect overlap).</li> </ul>"},{"location":"examples/building_detection_lidar/#dice-score-f1-score-for-sets","title":"Dice Score (F1 Score for Sets)\u00b6","text":"<p>$$ \\text{Dice} = \\frac{2|A \\cap B|}{|A| + |B|} $$</p> <ul> <li>Measures the overlap between $A$ and $B$, but gives more weight to the intersection.</li> <li>Also ranges from 0 to 1.</li> </ul>"},{"location":"examples/building_detection_lidar/#key-differences","title":"\ud83d\udd38 Key Differences\u00b6","text":"Metric Formula Penalizes Sensitivity IoU $\\frac{TP}{TP + FP + FN}$ FP and FN equally Less sensitive to small objects Dice $\\frac{2TP}{2TP + FP + FN}$ Less harsh on small mismatches More sensitive to small overlaps <p>TP: True Positive, FP: False Positive, FN: False Negative</p>"},{"location":"examples/building_detection_lidar/#relationship","title":"\ud83d\udd38 Relationship\u00b6","text":"<p>Dice and IoU are mathematically related:</p> <p>$$ \\text{Dice} = \\frac{2 \\cdot \\text{IoU}}{1 + \\text{IoU}} \\quad \\text{or} \\quad \\text{IoU} = \\frac{\\text{Dice}}{2 - \\text{Dice}} $$</p>"},{"location":"examples/building_detection_lidar/#when-to-use-what","title":"\ud83d\udd38 When to Use What\u00b6","text":"<ul> <li>IoU: Common in object detection and semantic segmentation benchmarks (e.g., COCO, Pascal VOC).</li> <li>Dice: Preferred in medical imaging and when class imbalance is an issue, due to its sensitivity to small regions.</li> </ul>"},{"location":"examples/building_footprints_africa/","title":"Building footprints africa","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_africa.tif\"\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_africa.tif\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\")\n</pre> extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\") In\u00a0[\u00a0]: Copied! <pre>masks_path = extractor.generate_masks(\n    raster_path,\n    output_dir=\"building_masks.tif\",\n    min_object_area=1000,\n    confidence_threshold=0.5,\n    threshold=0.5,\n)\n</pre> masks_path = extractor.generate_masks(     raster_path,     output_dir=\"building_masks.tif\",     min_object_area=1000,     confidence_threshold=0.5,     threshold=0.5, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_url)\n</pre> geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.orthogonalize(\n    input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0\n)\n</pre> gdf = geoai.orthogonalize(     input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.2}, tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.2}, tiles=raster_url )"},{"location":"examples/building_footprints_africa/#building-footprint-extraction-for-africa","title":"Building Footprint Extraction for Africa\u00b6","text":""},{"location":"examples/building_footprints_africa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_africa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_africa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_africa/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/building_footprints_africa/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_africa/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/building_footprints_africa/#add-geometric-attributes","title":"Add geometric attributes\u00b6","text":""},{"location":"examples/building_footprints_africa/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/building_footprints_china/","title":"Building footprints china","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_china.tif\"\n)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_china.tif\" ) In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\")\n</pre> extractor = geoai.BuildingFootprintExtractor(model_path=\"building_footprints_usa.pth\") In\u00a0[\u00a0]: Copied! <pre>masks_path = extractor.generate_masks(\n    raster_path,\n    output_dir=\"building_masks.tif\",\n    min_object_area=1000,\n    confidence_threshold=0.5,\n    mask_threshold=0.6,\n    threshold=0.5,\n)\n</pre> masks_path = extractor.generate_masks(     raster_path,     output_dir=\"building_masks.tif\",     min_object_area=1000,     confidence_threshold=0.5,     mask_threshold=0.6,     threshold=0.5, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_path)\n</pre> geoai.view_raster(masks_path, opacity=0.7, colormap=\"tab20\", basemap=raster_path) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.orthogonalize(\n    input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0\n)\n</pre> gdf = geoai.orthogonalize(     input_path=masks_path, output_path=\"building_footprints.geojson\", epsilon=1.0 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=raster_url )"},{"location":"examples/building_footprints_china/#building-footprint-extraction-for-china","title":"Building Footprint Extraction for China\u00b6","text":""},{"location":"examples/building_footprints_china/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_china/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_china/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_china/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/building_footprints_china/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_china/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/building_footprints_china/#add-geometric-attributes","title":"Add geometric attributes\u00b6","text":""},{"location":"examples/building_footprints_china/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/building_footprints_usa/","title":"Building footprints usa","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) In\u00a0[\u00a0]: Copied! <pre>extractor = geoai.BuildingFootprintExtractor()\n</pre> extractor = geoai.BuildingFootprintExtractor() In\u00a0[\u00a0]: Copied! <pre>mask_path = extractor.save_masks_as_geotiff(\n    raster_path=raster_path,\n    output_path=\"building_masks.tif\",\n    confidence_threshold=0.5,\n    mask_threshold=0.5,\n)\n</pre> mask_path = extractor.save_masks_as_geotiff(     raster_path=raster_path,     output_path=\"building_masks.tif\",     confidence_threshold=0.5,     mask_threshold=0.5, ) <p>Convert raster to vector</p> In\u00a0[\u00a0]: Copied! <pre>gdf = extractor.masks_to_vector(\n    mask_path=mask_path,\n    output_path=\"building_masks.geojson\",\n    simplify_tolerance=1.0,\n)\n</pre> gdf = extractor.masks_to_vector(     mask_path=mask_path,     output_path=\"building_masks.geojson\",     simplify_tolerance=1.0, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_buildings.geojson\"\ngdf = extractor.process_raster(\n    raster_path,\n    output_path=\"buildings.geojson\",\n    batch_size=4,\n    confidence_threshold=0.5,\n    overlap=0.25,\n    nms_iou_threshold=0.5,\n    min_object_area=100,\n    max_object_area=None,\n    mask_threshold=0.5,\n    simplify_tolerance=1.0,\n)\n</pre> output_path = \"naip_buildings.geojson\" gdf = extractor.process_raster(     raster_path,     output_path=\"buildings.geojson\",     batch_size=4,     confidence_threshold=0.5,     overlap=0.25,     nms_iou_threshold=0.5,     min_object_area=100,     max_object_area=None,     mask_threshold=0.5,     simplify_tolerance=1.0, ) In\u00a0[\u00a0]: Copied! <pre>gdf_regularized = extractor.regularize_buildings(\n    gdf=gdf,\n    min_area=100,\n    angle_threshold=15,\n    orthogonality_threshold=0.3,\n    rectangularity_threshold=0.7,\n)\n</pre> gdf_regularized = extractor.regularize_buildings(     gdf=gdf,     min_area=100,     angle_threshold=15,     orthogonality_threshold=0.3,     rectangularity_threshold=0.7, ) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, column=\"confidence\", layer_name=\"Building\", tiles=\"Satellite\"\n)\n</pre> geoai.view_vector_interactive(     gdf, column=\"confidence\", layer_name=\"Building\", tiles=\"Satellite\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, column=\"confidence\", layer_name=\"Building\", tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf, column=\"confidence\", layer_name=\"Building\", tiles=raster_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_regularized, column=\"confidence\", layer_name=\"Building\", tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf_regularized, column=\"confidence\", layer_name=\"Building\", tiles=raster_url ) In\u00a0[\u00a0]: Copied! <pre>extractor.visualize_results(raster_path, gdf, output_path=\"naip_buildings.png\")\n</pre> extractor.visualize_results(raster_path, gdf, output_path=\"naip_buildings.png\") In\u00a0[\u00a0]: Copied! <pre>extractor.visualize_results(\n    raster_path, gdf_regularized, output_path=\"naip_buildings_regularized.png\"\n)\n</pre> extractor.visualize_results(     raster_path, gdf_regularized, output_path=\"naip_buildings_regularized.png\" )"},{"location":"examples/building_footprints_usa/#building-footprint-extraction-for-the-usa","title":"Building Footprint Extraction for the USA\u00b6","text":""},{"location":"examples/building_footprints_usa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_footprints_usa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/building_footprints_usa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_footprints_usa/#initialize-building-footprint-extraction-pretrained-model","title":"Initialize building footprint extraction pretrained model\u00b6","text":""},{"location":"examples/building_footprints_usa/#extract-building-footprints","title":"Extract building footprints\u00b6","text":""},{"location":"examples/building_footprints_usa/#option-1-extract-building-footprints-as-raster","title":"Option 1: Extract building footprints as raster\u00b6","text":""},{"location":"examples/building_footprints_usa/#option-2-extract-building-footprints-as-vector","title":"Option 2: Extract building footprints as vector\u00b6","text":""},{"location":"examples/building_footprints_usa/#regularize-building-footprints","title":"Regularize building footprints\u00b6","text":""},{"location":"examples/building_footprints_usa/#visualize-building-footprints","title":"Visualize building footprints\u00b6","text":""},{"location":"examples/building_regularization/","title":"Building regularization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>naip_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nmasks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_building_masks.tif\"\n</pre> naip_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_building_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>masks_path = geoai.download_file(masks_url)\nnaip_path = geoai.download_file(naip_url)\n</pre> masks_path = geoai.download_file(masks_url) naip_path = geoai.download_file(naip_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(masks_path, figsize=(18, 10))\n</pre> geoai.view_image(masks_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=masks_url,\n    right_layer=naip_url,\n    left_label=\"Building Masks\",\n    right_label=\"NAIP Imagery\",\n    left_args={\n        \"colormap\": {\"255\": \"#0000ff\"},\n        \"opacity\": 0.5,\n    },\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=masks_url,     right_layer=naip_url,     left_label=\"Building Masks\",     right_label=\"NAIP Imagery\",     left_args={         \"colormap\": {\"255\": \"#0000ff\"},         \"opacity\": 0.5,     },     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.raster_to_vector(masks_path, output_path=\"naip_building_masks.geojson\")\n</pre> gdf = geoai.raster_to_vector(masks_path, output_path=\"naip_building_masks.geojson\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf, style_kwds={\"color\": \"blue\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf,\n    right_layer=naip_url,\n    left_label=\"Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"blue\"}, \"fillOpacity\": 0.4},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf,     right_layer=naip_url,     left_label=\"Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"blue\"}, \"fillOpacity\": 0.4},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf_regularized = geoai.regularization(\n    building_polygons=gdf,\n    angle_tolerance=10,\n    simplify_tolerance=0.5,\n    orthogonalize=True,\n    preserve_topology=True,\n)\n</pre> gdf_regularized = geoai.regularization(     building_polygons=gdf,     angle_tolerance=10,     simplify_tolerance=0.5,     orthogonalize=True,     preserve_topology=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_regularized, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_regularized, style_kwds={\"color\": \"red\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_regularized,\n    right_layer=naip_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"red\"}, \"fillOpacity\": 0.4},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_regularized,     right_layer=naip_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"red\"}, \"fillOpacity\": 0.4},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf_hybrid = geoai.hybrid_regularization(gdf)\n</pre> gdf_hybrid = geoai.hybrid_regularization(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_hybrid, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_hybrid, style_kwds={\"color\": \"green\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_regularized,\n    right_layer=naip_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"green\", \"fillOpacity\": 0.4}},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_regularized,     right_layer=naip_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"green\", \"fillOpacity\": 0.4}},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf_adaptive = geoai.adaptive_regularization(\n    building_polygons=gdf,\n    simplify_tolerance=0.5,\n    area_threshold=0.9,\n    preserve_shape=True,\n)\n</pre> gdf_adaptive = geoai.adaptive_regularization(     building_polygons=gdf,     simplify_tolerance=0.5,     area_threshold=0.9,     preserve_shape=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_adaptive, style_kwds={\"color\": \"yellow\", \"fillOpacity\": 0.4}, tiles=naip_url\n)\n</pre> geoai.view_vector_interactive(     gdf_adaptive, style_kwds={\"color\": \"yellow\", \"fillOpacity\": 0.4}, tiles=naip_url ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_adaptive,\n    right_layer=naip_url,\n    left_label=\"Adaptive Regularization Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"yellow\", \"fillOpacity\": 0.4}},\n    basemap=naip_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_adaptive,     right_layer=naip_url,     left_label=\"Adaptive Regularization Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"yellow\", \"fillOpacity\": 0.4}},     basemap=naip_url, ) In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"SATELLITE\")\nm.add_gdf(gdf, layer_name=\"Original\")\nm.add_gdf(\n    gdf_regularized, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\"\n)\nm.add_gdf(gdf_hybrid, style={\"color\": \"green\", \"fillOpacity\": 0}, layer_name=\"Hybrid\")\nm.add_gdf(\n    gdf_adaptive, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Adaptive\"\n)\nlegend = {\n    \"Original\": \"blue\",\n    \"Regularized\": \"red\",\n    \"Hybrid\": \"green\",\n    \"Adaptive\": \"yellow\",\n}\nm.add_legend(title=\"Building Footprints\", legend_dict=legend)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"SATELLITE\") m.add_gdf(gdf, layer_name=\"Original\") m.add_gdf(     gdf_regularized, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\" ) m.add_gdf(gdf_hybrid, style={\"color\": \"green\", \"fillOpacity\": 0}, layer_name=\"Hybrid\") m.add_gdf(     gdf_adaptive, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Adaptive\" ) legend = {     \"Original\": \"blue\",     \"Regularized\": \"red\",     \"Hybrid\": \"green\",     \"Adaptive\": \"yellow\", } m.add_legend(title=\"Building Footprints\", legend_dict=legend) m"},{"location":"examples/building_regularization/#building-regularization","title":"Building Regularization\u00b6","text":""},{"location":"examples/building_regularization/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/building_regularization/#import-package","title":"Import package\u00b6","text":""},{"location":"examples/building_regularization/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/building_regularization/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/building_regularization/#building-regularization","title":"Building regularization\u00b6","text":""},{"location":"examples/building_regularization/#hybrid-regularization","title":"Hybrid regularization\u00b6","text":""},{"location":"examples/building_regularization/#adaptive-regularization","title":"Adaptive regularization\u00b6","text":""},{"location":"examples/building_regularization/#compare-regularization-methods","title":"Compare regularization methods\u00b6","text":""},{"location":"examples/car_detection/","title":"Car detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\"\n)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\" ) In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.CarDetector()\n</pre> detector = geoai.CarDetector() In\u00a0[\u00a0]: Copied! <pre>mask_path = detector.generate_masks(\n    raster_path=raster_path,\n    output_path=\"cars_masks.tif\",\n    confidence_threshold=0.3,\n    mask_threshold=0.5,\n    overlap=0.25,\n    chip_size=(400, 400),\n)\n</pre> mask_path = detector.generate_masks(     raster_path=raster_path,     output_path=\"cars_masks.tif\",     confidence_threshold=0.3,     mask_threshold=0.5,     overlap=0.25,     chip_size=(400, 400), ) <p>Convert the image masks to polygons and save the output GeoJSON file.</p> In\u00a0[\u00a0]: Copied! <pre>gdf = detector.vectorize_masks(\n    masks_path=\"cars_masks.tif\",\n    output_path=\"cars.geojson\",\n    min_object_area=100,\n    max_object_area=2000,\n)\n</pre> gdf = detector.vectorize_masks(     masks_path=\"cars_masks.tif\",     output_path=\"cars.geojson\",     min_object_area=100,     max_object_area=2000, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filter = gdf[\n    (gdf[\"area_m2\"] &gt; 8) &amp; (gdf[\"area_m2\"] &lt; 60) &amp; (gdf[\"minor_length_m\"] &gt; 1)\n]\n</pre> gdf_filter = gdf[     (gdf[\"area_m2\"] &gt; 8) &amp; (gdf[\"area_m2\"] &lt; 60) &amp; (gdf[\"minor_length_m\"] &gt; 1) ] In\u00a0[\u00a0]: Copied! <pre>len(gdf_filter)\n</pre> len(gdf_filter) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, tiles=raster_url) <p></p>"},{"location":"examples/car_detection/#car-detection","title":"Car Detection\u00b6","text":""},{"location":"examples/car_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/car_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/car_detection/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We will download a sample image from Hugging Face Hub to use for car detection. You can find more high-resolution images from OpenAerialMap.</p>"},{"location":"examples/car_detection/#visualize-the-image","title":"Visualize the image\u00b6","text":""},{"location":"examples/car_detection/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/car_detection/#extract-cars","title":"Extract cars\u00b6","text":"<p>Extract cars from the image using the model and save the output image.</p>"},{"location":"examples/car_detection/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/car_detection/#visualize-initial-results","title":"Visualize initial results\u00b6","text":""},{"location":"examples/car_detection/#filter-cars-by-area","title":"Filter cars by area\u00b6","text":""},{"location":"examples/car_detection/#visualiza-final-results","title":"Visualiza final results\u00b6","text":""},{"location":"examples/catalog_search_agent/","title":"Catalog search agent","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"geoai-py[agents]\"\n</pre> # %pip install \"geoai-py[agents]\" In\u00a0[\u00a0]: Copied! <pre>import json\nfrom geoai.agents import (\n    CatalogAgent,\n    CatalogTools,\n    create_ollama_model,\n    create_openai_model,\n    create_anthropic_model,\n)\n</pre> import json from geoai.agents import (     CatalogAgent,     CatalogTools,     create_ollama_model,     create_openai_model,     create_anthropic_model, ) In\u00a0[\u00a0]: Copied! <pre>catalog_url = \"https://raw.githubusercontent.com/opengeos/Earth-Engine-Catalog/refs/heads/master/gee_catalog.json\"\n</pre> catalog_url = \"https://raw.githubusercontent.com/opengeos/Earth-Engine-Catalog/refs/heads/master/gee_catalog.json\" In\u00a0[\u00a0]: Copied! <pre>model = create_ollama_model(model=\"llama3.1\")\n</pre> model = create_ollama_model(model=\"llama3.1\") In\u00a0[\u00a0]: Copied! <pre>tools = CatalogTools(catalog_url=catalog_url)\n</pre> tools = CatalogTools(catalog_url=catalog_url) In\u00a0[\u00a0]: Copied! <pre>stats = json.loads(tools.get_catalog_stats())\nprint(f\"Total datasets: {stats['total_datasets']}\")\nprint(f\"\\nDataset types:\")\nfor dtype, count in stats.get(\"dataset_types\", {}).items():\n    print(f\"  {dtype}: {count}\")\n</pre> stats = json.loads(tools.get_catalog_stats()) print(f\"Total datasets: {stats['total_datasets']}\") print(f\"\\nDataset types:\") for dtype, count in stats.get(\"dataset_types\", {}).items():     print(f\"  {dtype}: {count}\") In\u00a0[\u00a0]: Copied! <pre>result = json.loads(tools.search_datasets(keywords=\"landcover\", max_results=5))\nprint(f\"Found {result['dataset_count']} datasets\\n\")\n\nfor ds in result[\"datasets\"]:\n    print(f\"ID: {ds['id']}\")\n    print(f\"Title: {ds['title']}\")\n    print(f\"Provider: {ds.get('provider', 'N/A')}\")\n    print(\"-\" * 80)\n</pre> result = json.loads(tools.search_datasets(keywords=\"landcover\", max_results=5)) print(f\"Found {result['dataset_count']} datasets\\n\")  for ds in result[\"datasets\"]:     print(f\"ID: {ds['id']}\")     print(f\"Title: {ds['title']}\")     print(f\"Provider: {ds.get('provider', 'N/A')}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre># Search by location name\nresult = json.loads(\n    tools.search_by_region(location=\"California\", keywords=\"elevation\", max_results=5)\n)\n\nprint(f\"Found {result['dataset_count']} datasets covering California\\n\")\n\nfor ds in result[\"datasets\"]:\n    print(f\"ID: {ds['id']}\")\n    print(f\"Title: {ds['title']}\")\n    print(f\"Bbox: {ds.get('bbox', 'N/A')}\")\n    print(\"-\" * 80)\n</pre> # Search by location name result = json.loads(     tools.search_by_region(location=\"California\", keywords=\"elevation\", max_results=5) )  print(f\"Found {result['dataset_count']} datasets covering California\\n\")  for ds in result[\"datasets\"]:     print(f\"ID: {ds['id']}\")     print(f\"Title: {ds['title']}\")     print(f\"Bbox: {ds.get('bbox', 'N/A')}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre># Search by bounding box coordinates\n# San Francisco Bay Area: [west, south, east, north]\nresult = json.loads(\n    tools.search_by_region(\n        bbox=[-122.5, 37.5, -122.0, 38.0], keywords=\"landcover\", max_results=3\n    )\n)\n\nprint(f\"Found {result['dataset_count']} datasets\\n\")\n\nfor ds in result[\"datasets\"]:\n    print(f\"ID: {ds['id']}\")\n    print(f\"Title: {ds['title']}\")\n    print(\"-\" * 80)\n</pre> # Search by bounding box coordinates # San Francisco Bay Area: [west, south, east, north] result = json.loads(     tools.search_by_region(         bbox=[-122.5, 37.5, -122.0, 38.0], keywords=\"landcover\", max_results=3     ) )  print(f\"Found {result['dataset_count']} datasets\\n\")  for ds in result[\"datasets\"]:     print(f\"ID: {ds['id']}\")     print(f\"Title: {ds['title']}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre># Convert location name to bounding box\nlocation_info = json.loads(tools.geocode_location(\"New York City\"))\nprint(f\"Name: {location_info['name']}\")\nprint(f\"Bbox: {location_info['bbox']}\")\nprint(f\"Center: {location_info['center']}\")\n</pre> # Convert location name to bounding box location_info = json.loads(tools.geocode_location(\"New York City\")) print(f\"Name: {location_info['name']}\") print(f\"Bbox: {location_info['bbox']}\") print(f\"Center: {location_info['center']}\") In\u00a0[\u00a0]: Copied! <pre>result = json.loads(\n    tools.search_datasets(\n        keywords=\"elevation\", dataset_type=\"image\", provider=\"NASA\", max_results=5\n    )\n)\n\nprint(f\"Found {result['dataset_count']} datasets\\n\")\n\nfor ds in result[\"datasets\"]:\n    print(f\"ID: {ds['id']}\")\n    print(f\"Title: {ds['title']}\")\n    print(f\"Type: {ds.get('type', 'N/A')}\")\n    print(\"-\" * 80)\n</pre> result = json.loads(     tools.search_datasets(         keywords=\"elevation\", dataset_type=\"image\", provider=\"NASA\", max_results=5     ) )  print(f\"Found {result['dataset_count']} datasets\\n\")  for ds in result[\"datasets\"]:     print(f\"ID: {ds['id']}\")     print(f\"Title: {ds['title']}\")     print(f\"Type: {ds.get('type', 'N/A')}\")     print(\"-\" * 80) In\u00a0[\u00a0]: Copied! <pre>providers = json.loads(tools.list_providers())\nprint(f\"Total providers: {providers['count']}\\n\")\nprint(\"Sample providers:\")\nfor p in providers[\"providers\"][:10]:\n    print(f\"  - {p}\")\n</pre> providers = json.loads(tools.list_providers()) print(f\"Total providers: {providers['count']}\\n\") print(\"Sample providers:\") for p in providers[\"providers\"][:10]:     print(f\"  - {p}\") In\u00a0[\u00a0]: Copied! <pre>types = json.loads(tools.list_dataset_types())\nprint(f\"Available dataset types: {types['types']}\")\n</pre> types = json.loads(tools.list_dataset_types()) print(f\"Available dataset types: {types['types']}\") In\u00a0[\u00a0]: Copied! <pre>agent = CatalogAgent(model=model, catalog_url=catalog_url)\n</pre> agent = CatalogAgent(model=model, catalog_url=catalog_url) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Find datasets about landcover from NASA\")\nprint(response)\n</pre> response = agent.ask(\"Find datasets about landcover from NASA\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Show me elevation data\")\nprint(response)\n</pre> response = agent.ask(\"Show me elevation data\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Find landcover datasets covering California\")\nprint(response)\n</pre> response = agent.ask(\"Find landcover datasets covering California\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Show me elevation data for San Francisco\")\nprint(response)\n</pre> response = agent.ask(\"Show me elevation data for San Francisco\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Find land cover datasets from NASA that cover New York City\")\nprint(response)\n</pre> response = agent.ask(\"Find land cover datasets from NASA that cover New York City\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"What types of datasets are available?\")\nprint(response)\n</pre> response = agent.ask(\"What types of datasets are available?\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Find image collections about forests\")\nprint(response)\n</pre> response = agent.ask(\"Find image collections about forests\") print(response) In\u00a0[\u00a0]: Copied! <pre>response = agent.ask(\"Find landcover datasets from 2022 onwards\")\nprint(response)\n</pre> response = agent.ask(\"Find landcover datasets from 2022 onwards\") print(response) In\u00a0[\u00a0]: Copied! <pre>datasets = agent.search_datasets(keywords=\"sentinel\", max_results=5)\n\nfor ds in datasets:\n    print(f\"ID: {ds['id']}\")\n    print(f\"Title: {ds['title']}\")\n    print(f\"Provider: {ds.get('provider', 'N/A')}\")\n    if ds.get(\"snippet\"):\n        print(f\"Code: {ds['snippet']}\")\n    print(\"-\" * 80)\n</pre> datasets = agent.search_datasets(keywords=\"sentinel\", max_results=5)  for ds in datasets:     print(f\"ID: {ds['id']}\")     print(f\"Title: {ds['title']}\")     print(f\"Provider: {ds.get('provider', 'N/A')}\")     if ds.get(\"snippet\"):         print(f\"Code: {ds['snippet']}\")     print(\"-\" * 80)"},{"location":"examples/catalog_search_agent/#catalog-search-agent-find-datasets-with-natural-language","title":"Catalog Search Agent - Find Datasets with Natural Language\u00b6","text":"<p>This notebook demonstrates how to use the Catalog Search Agent to find datasets in data catalogs using natural language queries.</p> <p>The Catalog Agent can:</p> <ul> <li>Search through data catalogs (TSV, CSV, JSON formats)</li> <li>Search by geographic region (location names or bounding boxes)</li> <li>Understand natural language queries</li> <li>Filter by keywords, dataset type, and provider</li> <li>Return structured results with metadata</li> <li>Work with Earth Engine Data Catalog and custom catalogs</li> </ul> <p>Uncomment the following line to install geoai if needed.</p>"},{"location":"examples/catalog_search_agent/#installation","title":"Installation\u00b6","text":""},{"location":"examples/catalog_search_agent/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/catalog_search_agent/#load-earth-engine-data-catalog","title":"Load Earth Engine Data Catalog\u00b6","text":"<p>The Earth Engine Data Catalog is available in JSON format from the opengeos/Earth-Engine-Catalog repository.</p> <p>Important: Use the JSON format (not TSV) to enable spatial search capabilities with bounding box information.</p>"},{"location":"examples/catalog_search_agent/#create-a-model","title":"Create a model\u00b6","text":"<p>You can create a model with the following functions:</p> <ul> <li><code>create_ollama_model</code>: Create a model using Ollama. You will need to install Ollama separately and pull the model you want to use, such as <code>llama3.1</code>.</li> <li><code>create_openai_model</code>: Create a model using OpenAI. You will need an OpenAI API key. Set it in the <code>OPENAI_API_KEY</code> environment variable.</li> <li><code>create_anthropic_model</code>: Create a model using Anthropic. You will need an Anthropic API key. Set it in the <code>ANTHROPIC_API_KEY</code> environment variable.</li> </ul>"},{"location":"examples/catalog_search_agent/#use-catalogtools-directly-fast","title":"Use CatalogTools Directly (Fast)\u00b6","text":"<p>For faster searches without LLM overhead, you can use <code>CatalogTools</code> directly:</p>"},{"location":"examples/catalog_search_agent/#get-catalog-statistics","title":"Get catalog statistics\u00b6","text":""},{"location":"examples/catalog_search_agent/#search-for-datasets-by-keyword","title":"Search for datasets by keyword\u00b6","text":""},{"location":"examples/catalog_search_agent/#search-by-geographic-region-new","title":"Search by geographic region (NEW!)\u00b6","text":"<p>Find datasets covering a specific location or bounding box:</p>"},{"location":"examples/catalog_search_agent/#geocode-location-names","title":"Geocode location names\u00b6","text":""},{"location":"examples/catalog_search_agent/#search-with-filters","title":"Search with filters\u00b6","text":""},{"location":"examples/catalog_search_agent/#list-available-providers","title":"List available providers\u00b6","text":""},{"location":"examples/catalog_search_agent/#list-dataset-types","title":"List dataset types\u00b6","text":""},{"location":"examples/catalog_search_agent/#use-catalogagent-with-natural-language-llm","title":"Use CatalogAgent with Natural Language (LLM)\u00b6","text":"<p>For natural language queries, create a <code>CatalogAgent</code> that uses an LLM to understand and execute searches:</p>"},{"location":"examples/catalog_search_agent/#ask-natural-language-questions","title":"Ask natural language questions\u00b6","text":""},{"location":"examples/catalog_search_agent/#spatial-search-with-natural-language-new","title":"Spatial search with natural language (NEW!)\u00b6","text":""},{"location":"examples/catalog_search_agent/#get-structured-results-programmatically","title":"Get structured results programmatically\u00b6","text":""},{"location":"examples/change_detection/","title":"Change detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom geoai.change_detection import ChangeDetection\n</pre> import geoai import os import numpy as np import matplotlib.pyplot as plt from pathlib import Path from geoai.change_detection import ChangeDetection In\u00a0[\u00a0]: Copied! <pre># Check if CUDA is available\ndevice = geoai.get_device()\nprint(f\"Using device: {device}\")\n\n# Set up paths\nout_folder = \"change_detection_results\"\nPath(out_folder).mkdir(exist_ok=True)\n\nprint(f\"Working directory: {out_folder}\")\n</pre> # Check if CUDA is available device = geoai.get_device() print(f\"Using device: {device}\")  # Set up paths out_folder = \"change_detection_results\" Path(out_folder).mkdir(exist_ok=True)  print(f\"Working directory: {out_folder}\") In\u00a0[\u00a0]: Copied! <pre># Download NAIP imagery\nnaip_2019_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_naip_2019_a.tif\"\nnaip_2022_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_naip_2022_a.tif\"\n\nnaip_2019_path = geoai.download_file(naip_2019_url)\nnaip_2022_path = geoai.download_file(naip_2022_url)\n\nprint(f\"Downloaded 2019 NAIP: {naip_2019_path}\")\nprint(f\"Downloaded 2022 NAIP: {naip_2022_path}\")\n</pre> # Download NAIP imagery naip_2019_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_naip_2019_a.tif\" naip_2022_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_naip_2022_a.tif\"  naip_2019_path = geoai.download_file(naip_2019_url) naip_2022_path = geoai.download_file(naip_2022_url)  print(f\"Downloaded 2019 NAIP: {naip_2019_path}\") print(f\"Downloaded 2022 NAIP: {naip_2022_path}\") In\u00a0[\u00a0]: Copied! <pre># Check raster information\ngeoai.get_raster_info(naip_2019_path)\n</pre> # Check raster information geoai.get_raster_info(naip_2019_path) In\u00a0[\u00a0]: Copied! <pre># View the images\ngeoai.view_raster(naip_2019_path)\n</pre> # View the images geoai.view_raster(naip_2019_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(naip_2022_path)\n</pre> geoai.view_raster(naip_2022_path) In\u00a0[\u00a0]: Copied! <pre># Make sure model directory exists\nPath(\"~/.cache/torch/hub/checkpoints/\").expanduser().mkdir(parents=True, exist_ok=True)\n\n# Initialize change detection\ndetector = ChangeDetection(sam_model_type=\"vit_h\")\n\n# Configure parameters (following the torchange example)\ndetector.set_hyperparameters(\n    change_confidence_threshold=145,\n    use_normalized_feature=True,\n    bitemporal_match=True,\n)\n\ndetector.set_mask_generator_params(\n    points_per_side=32,\n    stability_score_thresh=0.95,\n)\n\nprint(\"Change detection system initialized!\")\n</pre> # Make sure model directory exists Path(\"~/.cache/torch/hub/checkpoints/\").expanduser().mkdir(parents=True, exist_ok=True)  # Initialize change detection detector = ChangeDetection(sam_model_type=\"vit_h\")  # Configure parameters (following the torchange example) detector.set_hyperparameters(     change_confidence_threshold=145,     use_normalized_feature=True,     bitemporal_match=True, )  detector.set_mask_generator_params(     points_per_side=32,     stability_score_thresh=0.95, )  print(\"Change detection system initialized!\") In\u00a0[\u00a0]: Copied! <pre># Run change detection\nresults = detector.detect_changes(\n    naip_2019_path,\n    naip_2022_path,\n    output_path=f\"{out_folder}/binary_mask.tif\",\n    export_probability=True,\n    probability_output_path=f\"{out_folder}/probability_mask.tif\",\n    export_instance_masks=True,\n    instance_masks_output_path=f\"{out_folder}/instance_masks.tif\",\n    return_detailed_results=True,\n    return_results=False,\n)\n\nprint(f\"Change detection completed!\")\nprint(f\"Total instances detected: {results['summary']['total_masks']}\")\nprint(f\"Image size: {results['summary']['original_shape']}\")\n</pre> # Run change detection results = detector.detect_changes(     naip_2019_path,     naip_2022_path,     output_path=f\"{out_folder}/binary_mask.tif\",     export_probability=True,     probability_output_path=f\"{out_folder}/probability_mask.tif\",     export_instance_masks=True,     instance_masks_output_path=f\"{out_folder}/instance_masks.tif\",     return_detailed_results=True,     return_results=False, )  print(f\"Change detection completed!\") print(f\"Total instances detected: {results['summary']['total_masks']}\") print(f\"Image size: {results['summary']['original_shape']}\") In\u00a0[\u00a0]: Copied! <pre># Display statistics\nif \"statistics\" in results and results[\"statistics\"]:\n    print(\"Quality Statistics:\")\n    for metric, stats in results[\"statistics\"].items():\n        print(f\"  {metric}: mean={stats['mean']:.3f}, std={stats['std']:.3f}\")\n\n# Show top instances\nif \"masks\" in results and len(results[\"masks\"]) &gt; 0:\n    print(\"\\nTop 5 detected instances:\")\n    for i, mask in enumerate(results[\"masks\"][:5]):\n        print(\n            f\"  {i+1}. Instance {mask['mask_id']}: \"\n            f\"IoU={mask['iou_pred']:.3f}, \"\n            f\"Stability={mask['stability_score']:.3f}, \"\n            f\"Area={mask['area']} pixels\"\n        )\n</pre> # Display statistics if \"statistics\" in results and results[\"statistics\"]:     print(\"Quality Statistics:\")     for metric, stats in results[\"statistics\"].items():         print(f\"  {metric}: mean={stats['mean']:.3f}, std={stats['std']:.3f}\")  # Show top instances if \"masks\" in results and len(results[\"masks\"]) &gt; 0:     print(\"\\nTop 5 detected instances:\")     for i, mask in enumerate(results[\"masks\"][:5]):         print(             f\"  {i+1}. Instance {mask['mask_id']}: \"             f\"IoU={mask['iou_pred']:.3f}, \"             f\"Stability={mask['stability_score']:.3f}, \"             f\"Area={mask['area']} pixels\"         ) In\u00a0[\u00a0]: Copied! <pre># probability visualization\ndetector.visualize_results(\n    naip_2019_path,\n    naip_2022_path,\n    f\"{out_folder}/binary_mask.tif\",\n    f\"{out_folder}/probability_mask.tif\",\n)\n</pre> # probability visualization detector.visualize_results(     naip_2019_path,     naip_2022_path,     f\"{out_folder}/binary_mask.tif\",     f\"{out_folder}/probability_mask.tif\", ) In\u00a0[\u00a0]: Copied! <pre># Create split comparison visualization\ndetector.create_split_comparison(\n    naip_2019_path,\n    naip_2022_path,\n    f\"{out_folder}/binary_mask.tif\",\n    f\"{out_folder}/probability_mask.tif\",\n    f\"{out_folder}/split_comparison.png\",\n)\n</pre> # Create split comparison visualization detector.create_split_comparison(     naip_2019_path,     naip_2022_path,     f\"{out_folder}/binary_mask.tif\",     f\"{out_folder}/probability_mask.tif\",     f\"{out_folder}/split_comparison.png\", ) In\u00a0[\u00a0]: Copied! <pre># Analyze individual instances\ninstance_stats = detector.analyze_instances(\n    f\"{out_folder}/instance_masks.tif\",\n    f\"{out_folder}/instance_masks_scores.tif\",\n    f\"{out_folder}/instance_analysis.png\",\n)\n</pre> # Analyze individual instances instance_stats = detector.analyze_instances(     f\"{out_folder}/instance_masks.tif\",     f\"{out_folder}/instance_masks_scores.tif\",     f\"{out_folder}/instance_analysis.png\", ) In\u00a0[\u00a0]: Copied! <pre># Create comprehensive analysis report\ndetector.create_comprehensive_report(results, f\"{out_folder}/comprehensive_report.png\")\n</pre> # Create comprehensive analysis report detector.create_comprehensive_report(results, f\"{out_folder}/comprehensive_report.png\") In\u00a0[\u00a0]: Copied! <pre># Alternative: Run complete analysis in one step\n# This method does everything - detection, file outputs, and all visualizations\ncomplete_results = detector.run_complete_analysis(\n    naip_2019_path, naip_2022_path, \"complete_analysis_results\"\n)\n</pre> # Alternative: Run complete analysis in one step # This method does everything - detection, file outputs, and all visualizations complete_results = detector.run_complete_analysis(     naip_2019_path, naip_2022_path, \"complete_analysis_results\" )"},{"location":"examples/change_detection/#change-detection-with-instance-segmentation","title":"Change Detection with Instance Segmentation\u00b6","text":"<p>This notebook demonstrates the change detection functionality in GeoAI, which provides instance segmentation and confidence scoring for individual change objects.</p> <p>The change detection functionality builds upon the torchange package developed by Dr. Zhuo Zheng. We have made it much easier to analyze remote sensing imagery and visualize the results.</p>"},{"location":"examples/change_detection/#overview","title":"Overview\u00b6","text":"<p>The change detection system provides:</p> <ul> <li>Instance Segmentation: Each change object gets a unique ID</li> <li>Confidence Scores: Individual confidence values for each detected instance</li> <li>Proper GeoTIFF Output: Maintains spatial reference information</li> </ul>"},{"location":"examples/change_detection/#key-features","title":"Key Features\u00b6","text":"<ul> <li>Instance-level change detection with unique IDs</li> <li>Confidence scoring for quality assessment</li> <li>Support for large GeoTIFF files</li> <li>Comprehensive analysis capabilities</li> </ul>"},{"location":"examples/change_detection/#install-packages","title":"Install packages\u00b6","text":""},{"location":"examples/change_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/change_detection/#setup","title":"Setup\u00b6","text":""},{"location":"examples/change_detection/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use NAIP imagery for Las Vegas to demonstrate change detection.</p>"},{"location":"examples/change_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/change_detection/#initialize-change-detection","title":"Initialize Change Detection\u00b6","text":"<p>Create the change detection system with optimal parameters.</p>"},{"location":"examples/change_detection/#run-change-detection","title":"Run Change Detection\u00b6","text":"<p>Execute change detection with instance segmentation and confidence scoring.</p>"},{"location":"examples/change_detection/#analyze-results","title":"Analyze Results\u00b6","text":"<p>Display key statistics and quality metrics.</p>"},{"location":"examples/change_detection/#visualizations","title":"Visualizations\u00b6","text":"<p>Use the integrated visualization methods for comprehensive analysis.</p>"},{"location":"examples/change_detection/#comprehensive-analysis-report","title":"Comprehensive Analysis Report\u00b6","text":"<p>Generate a detailed analysis report combining all metrics.</p>"},{"location":"examples/change_detection/#one-click-complete-analysis","title":"One-Click Complete Analysis\u00b6","text":"<p>For ultimate simplicity, use the complete analysis method.</p>"},{"location":"examples/change_detection/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated the change detection functionality in GeoAI with integrated visualization methods:</p>"},{"location":"examples/change_detection/#key-features-used","title":"Key Features Used:\u00b6","text":"<ol> <li>Change Detection: Instance segmentation with confidence scoring</li> <li>Integrated Visualizations: Built-in methods for comprehensive analysis</li> <li>Simplified API: Clean, streamlined interface following geoai patterns</li> <li>Complete Analysis: One-click method for full analysis workflow</li> </ol>"},{"location":"examples/change_detection/#output-files-generated","title":"Output Files Generated:\u00b6","text":"<ul> <li><code>binary_mask.tif</code>: Traditional binary change detection</li> <li><code>probability_mask.tif</code>: Probability-weighted change detection</li> <li><code>instance_masks.tif</code>: Instance segmentation with unique IDs</li> <li><code>instance_masks_scores.tif</code>: Confidence scores for each instance</li> <li><code>enhanced_probability_results.png</code>: Comprehensive visualization</li> <li><code>split_comparison.png</code>: Before/after split comparison</li> <li><code>instance_analysis.png</code>: Individual instance analysis</li> <li><code>comprehensive_report.png</code>: Complete analysis report</li> </ul>"},{"location":"examples/change_detection/#advantages-over-traditional-methods","title":"Advantages Over Traditional Methods:\u00b6","text":"<ol> <li>Instance-Level Analysis: Each change object has unique ID and metrics</li> <li>Quality Assessment: Confidence scores for filtering and ranking</li> <li>Rich Visualizations: Multiple analysis perspectives in one package</li> <li>Simplified Workflow: Integrated methods reduce code complexity</li> <li>Flexible Usage: From simple detection to comprehensive analysis</li> </ol>"},{"location":"examples/clean_segmentation_results/","title":"Clean segmentation results","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U \"geoai-py[extra]\"\n</pre> # %pip install -U \"geoai-py[extra]\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport rasterio\nfrom rasterio.transform import from_bounds\nimport tempfile\nimport os\n\n# Import GeoAI multiclean utilities\n# You can import from geoai directly (convenience imports)\nfrom geoai import (\n    clean_segmentation_mask,\n    clean_raster,\n    clean_raster_batch,\n    compare_masks,\n)\n\n# Or import from the tools subpackage directly\n# from geoai.tools.multiclean import (\n#     clean_segmentation_mask,\n#     clean_raster,\n#     clean_raster_batch,\n#     compare_masks,\n# )\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import rasterio from rasterio.transform import from_bounds import tempfile import os  # Import GeoAI multiclean utilities # You can import from geoai directly (convenience imports) from geoai import (     clean_segmentation_mask,     clean_raster,     clean_raster_batch,     compare_masks, )  # Or import from the tools subpackage directly # from geoai.tools.multiclean import ( #     clean_segmentation_mask, #     clean_raster, #     clean_raster_batch, #     compare_masks, # ) In\u00a0[\u00a0]: Copied! <pre>def create_noisy_segmentation(size=(512, 512), num_classes=3, noise_level=0.1):\n    \"\"\"\n    Create a synthetic segmentation mask with noise.\n\n    Args:\n        size: Tuple of (height, width)\n        num_classes: Number of segmentation classes\n        noise_level: Fraction of pixels to add noise (0-1)\n\n    Returns:\n        Noisy segmentation mask\n    \"\"\"\n    np.random.seed(42)\n\n    # Create base segmentation with smooth regions\n    mask = np.zeros(size, dtype=np.int32)\n\n    # Create class regions\n    mask[: size[0] // 2, :] = 0  # Background\n    mask[size[0] // 2 :, : size[1] // 2] = 1  # Class 1\n    mask[size[0] // 2 :, size[1] // 2 :] = 2  # Class 2\n\n    # Add noise - small random islands\n    num_noise_pixels = int(size[0] * size[1] * noise_level)\n    noise_y = np.random.randint(0, size[0], num_noise_pixels)\n    noise_x = np.random.randint(0, size[1], num_noise_pixels)\n    noise_classes = np.random.randint(0, num_classes, num_noise_pixels)\n    mask[noise_y, noise_x] = noise_classes\n\n    # Add some edge roughness by randomly changing boundary pixels\n    from scipy.ndimage import binary_erosion, binary_dilation\n\n    for class_id in range(num_classes):\n        class_mask = mask == class_id\n        # Find edges\n        eroded = binary_erosion(class_mask)\n        edges = class_mask &amp; ~eroded\n        # Randomly toggle some edge pixels\n        edge_coords = np.where(edges)\n        if len(edge_coords[0]) &gt; 0:\n            num_toggle = int(len(edge_coords[0]) * 0.3)\n            toggle_idx = np.random.choice(\n                len(edge_coords[0]), num_toggle, replace=False\n            )\n            toggle_y = edge_coords[0][toggle_idx]\n            toggle_x = edge_coords[1][toggle_idx]\n            mask[toggle_y, toggle_x] = (mask[toggle_y, toggle_x] + 1) % num_classes\n\n    return mask\n\n\n# Create noisy mask\nnoisy_mask = create_noisy_segmentation(size=(512, 512), num_classes=3, noise_level=0.05)\nprint(f\"Created noisy mask with shape: {noisy_mask.shape}\")\nprint(f\"Classes: {np.unique(noisy_mask)}\")\n</pre> def create_noisy_segmentation(size=(512, 512), num_classes=3, noise_level=0.1):     \"\"\"     Create a synthetic segmentation mask with noise.      Args:         size: Tuple of (height, width)         num_classes: Number of segmentation classes         noise_level: Fraction of pixels to add noise (0-1)      Returns:         Noisy segmentation mask     \"\"\"     np.random.seed(42)      # Create base segmentation with smooth regions     mask = np.zeros(size, dtype=np.int32)      # Create class regions     mask[: size[0] // 2, :] = 0  # Background     mask[size[0] // 2 :, : size[1] // 2] = 1  # Class 1     mask[size[0] // 2 :, size[1] // 2 :] = 2  # Class 2      # Add noise - small random islands     num_noise_pixels = int(size[0] * size[1] * noise_level)     noise_y = np.random.randint(0, size[0], num_noise_pixels)     noise_x = np.random.randint(0, size[1], num_noise_pixels)     noise_classes = np.random.randint(0, num_classes, num_noise_pixels)     mask[noise_y, noise_x] = noise_classes      # Add some edge roughness by randomly changing boundary pixels     from scipy.ndimage import binary_erosion, binary_dilation      for class_id in range(num_classes):         class_mask = mask == class_id         # Find edges         eroded = binary_erosion(class_mask)         edges = class_mask &amp; ~eroded         # Randomly toggle some edge pixels         edge_coords = np.where(edges)         if len(edge_coords[0]) &gt; 0:             num_toggle = int(len(edge_coords[0]) * 0.3)             toggle_idx = np.random.choice(                 len(edge_coords[0]), num_toggle, replace=False             )             toggle_y = edge_coords[0][toggle_idx]             toggle_x = edge_coords[1][toggle_idx]             mask[toggle_y, toggle_x] = (mask[toggle_y, toggle_x] + 1) % num_classes      return mask   # Create noisy mask noisy_mask = create_noisy_segmentation(size=(512, 512), num_classes=3, noise_level=0.05) print(f\"Created noisy mask with shape: {noisy_mask.shape}\") print(f\"Classes: {np.unique(noisy_mask)}\") In\u00a0[\u00a0]: Copied! <pre># Create color map for visualization\ncolors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green\ncmap = ListedColormap(colors)\n\nplt.figure(figsize=(10, 10))\nplt.imshow(noisy_mask, cmap=cmap, interpolation=\"nearest\")\nplt.title(\"Noisy Segmentation Mask\", fontsize=16)\nplt.colorbar(label=\"Class\", ticks=[0, 1, 2])\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.tight_layout()\nplt.show()\n</pre> # Create color map for visualization colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"]  # Blue, Orange, Green cmap = ListedColormap(colors)  plt.figure(figsize=(10, 10)) plt.imshow(noisy_mask, cmap=cmap, interpolation=\"nearest\") plt.title(\"Noisy Segmentation Mask\", fontsize=16) plt.colorbar(label=\"Class\", ticks=[0, 1, 2]) plt.xlabel(\"X\") plt.ylabel(\"Y\") plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Apply MultiClean\ncleaned_mask = clean_segmentation_mask(\n    noisy_mask,\n    class_values=[0, 1, 2],  # Classes to process\n    smooth_edge_size=3,  # Kernel size for edge smoothing\n    min_island_size=100,  # Remove islands smaller than 100 pixels\n    connectivity=8,  # Use 8-connectivity (includes diagonals)\n    fill_nan=False,  # Don't fill NaN values (we don't have any)\n)\n\nprint(f\"Cleaned mask shape: {cleaned_mask.shape}\")\nprint(f\"Classes: {np.unique(cleaned_mask)}\")\n</pre> # Apply MultiClean cleaned_mask = clean_segmentation_mask(     noisy_mask,     class_values=[0, 1, 2],  # Classes to process     smooth_edge_size=3,  # Kernel size for edge smoothing     min_island_size=100,  # Remove islands smaller than 100 pixels     connectivity=8,  # Use 8-connectivity (includes diagonals)     fill_nan=False,  # Don't fill NaN values (we don't have any) )  print(f\"Cleaned mask shape: {cleaned_mask.shape}\") print(f\"Classes: {np.unique(cleaned_mask)}\") In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n\n# Noisy mask\nim1 = axes[0].imshow(noisy_mask, cmap=cmap, interpolation=\"nearest\")\naxes[0].set_title(\"Before Cleaning (Noisy)\", fontsize=16)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"Y\")\nplt.colorbar(im1, ax=axes[0], label=\"Class\", ticks=[0, 1, 2])\n\n# Cleaned mask\nim2 = axes[1].imshow(cleaned_mask, cmap=cmap, interpolation=\"nearest\")\naxes[1].set_title(\"After Cleaning (Smooth)\", fontsize=16)\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y\")\nplt.colorbar(im2, ax=axes[1], label=\"Class\", ticks=[0, 1, 2])\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(1, 2, figsize=(20, 10))  # Noisy mask im1 = axes[0].imshow(noisy_mask, cmap=cmap, interpolation=\"nearest\") axes[0].set_title(\"Before Cleaning (Noisy)\", fontsize=16) axes[0].set_xlabel(\"X\") axes[0].set_ylabel(\"Y\") plt.colorbar(im1, ax=axes[0], label=\"Class\", ticks=[0, 1, 2])  # Cleaned mask im2 = axes[1].imshow(cleaned_mask, cmap=cmap, interpolation=\"nearest\") axes[1].set_title(\"After Cleaning (Smooth)\", fontsize=16) axes[1].set_xlabel(\"X\") axes[1].set_ylabel(\"Y\") plt.colorbar(im2, ax=axes[1], label=\"Class\", ticks=[0, 1, 2])  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>pixels_changed, total_pixels, change_percentage = compare_masks(\n    noisy_mask, cleaned_mask\n)\n\nprint(\"Cleaning Statistics:\")\nprint(f\"  Total pixels: {total_pixels:,}\")\nprint(f\"  Pixels changed: {pixels_changed:,}\")\nprint(f\"  Change percentage: {change_percentage:.2f}%\")\n</pre> pixels_changed, total_pixels, change_percentage = compare_masks(     noisy_mask, cleaned_mask )  print(\"Cleaning Statistics:\") print(f\"  Total pixels: {total_pixels:,}\") print(f\"  Pixels changed: {pixels_changed:,}\") print(f\"  Change percentage: {change_percentage:.2f}%\") In\u00a0[\u00a0]: Copied! <pre># Select a region to zoom in\ny_start, y_end = 200, 350\nx_start, x_end = 200, 350\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\n\n# Zoomed noisy region\nim1 = axes[0].imshow(\n    noisy_mask[y_start:y_end, x_start:x_end], cmap=cmap, interpolation=\"nearest\"\n)\naxes[0].set_title(\"Before Cleaning (Zoomed)\", fontsize=14)\naxes[0].set_xlabel(\"X\")\naxes[0].set_ylabel(\"Y\")\nplt.colorbar(im1, ax=axes[0], label=\"Class\", ticks=[0, 1, 2])\n\n# Zoomed cleaned region\nim2 = axes[1].imshow(\n    cleaned_mask[y_start:y_end, x_start:x_end], cmap=cmap, interpolation=\"nearest\"\n)\naxes[1].set_title(\"After Cleaning (Zoomed)\", fontsize=14)\naxes[1].set_xlabel(\"X\")\naxes[1].set_ylabel(\"Y\")\nplt.colorbar(im2, ax=axes[1], label=\"Class\", ticks=[0, 1, 2])\n\nplt.tight_layout()\nplt.show()\n</pre> # Select a region to zoom in y_start, y_end = 200, 350 x_start, x_end = 200, 350  fig, axes = plt.subplots(1, 2, figsize=(16, 8))  # Zoomed noisy region im1 = axes[0].imshow(     noisy_mask[y_start:y_end, x_start:x_end], cmap=cmap, interpolation=\"nearest\" ) axes[0].set_title(\"Before Cleaning (Zoomed)\", fontsize=14) axes[0].set_xlabel(\"X\") axes[0].set_ylabel(\"Y\") plt.colorbar(im1, ax=axes[0], label=\"Class\", ticks=[0, 1, 2])  # Zoomed cleaned region im2 = axes[1].imshow(     cleaned_mask[y_start:y_end, x_start:x_end], cmap=cmap, interpolation=\"nearest\" ) axes[1].set_title(\"After Cleaning (Zoomed)\", fontsize=14) axes[1].set_xlabel(\"X\") axes[1].set_ylabel(\"Y\") plt.colorbar(im2, ax=axes[1], label=\"Class\", ticks=[0, 1, 2])  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Create masks with different parameters\nparams = [\n    {\"smooth_edge_size\": 0, \"min_island_size\": 0, \"title\": \"No Cleaning\"},\n    {\"smooth_edge_size\": 0, \"min_island_size\": 100, \"title\": \"Island Removal Only\"},\n    {\"smooth_edge_size\": 3, \"min_island_size\": 0, \"title\": \"Edge Smoothing Only\"},\n    {\"smooth_edge_size\": 3, \"min_island_size\": 100, \"title\": \"Full Cleaning\"},\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 16))\naxes = axes.flatten()\n\nfor i, param in enumerate(params):\n    if i == 0:\n        # No cleaning - just show original\n        mask = noisy_mask\n    else:\n        # Apply cleaning with specified parameters\n        mask = clean_segmentation_mask(\n            noisy_mask,\n            class_values=[0, 1, 2],\n            smooth_edge_size=param[\"smooth_edge_size\"],\n            min_island_size=param[\"min_island_size\"],\n            connectivity=8,\n        )\n\n    im = axes[i].imshow(mask, cmap=cmap, interpolation=\"nearest\")\n    axes[i].set_title(param[\"title\"], fontsize=14)\n    axes[i].set_xlabel(\"X\")\n    axes[i].set_ylabel(\"Y\")\n    plt.colorbar(im, ax=axes[i], label=\"Class\", ticks=[0, 1, 2])\n\nplt.tight_layout()\nplt.show()\n</pre> # Create masks with different parameters params = [     {\"smooth_edge_size\": 0, \"min_island_size\": 0, \"title\": \"No Cleaning\"},     {\"smooth_edge_size\": 0, \"min_island_size\": 100, \"title\": \"Island Removal Only\"},     {\"smooth_edge_size\": 3, \"min_island_size\": 0, \"title\": \"Edge Smoothing Only\"},     {\"smooth_edge_size\": 3, \"min_island_size\": 100, \"title\": \"Full Cleaning\"}, ]  fig, axes = plt.subplots(2, 2, figsize=(16, 16)) axes = axes.flatten()  for i, param in enumerate(params):     if i == 0:         # No cleaning - just show original         mask = noisy_mask     else:         # Apply cleaning with specified parameters         mask = clean_segmentation_mask(             noisy_mask,             class_values=[0, 1, 2],             smooth_edge_size=param[\"smooth_edge_size\"],             min_island_size=param[\"min_island_size\"],             connectivity=8,         )      im = axes[i].imshow(mask, cmap=cmap, interpolation=\"nearest\")     axes[i].set_title(param[\"title\"], fontsize=14)     axes[i].set_xlabel(\"X\")     axes[i].set_ylabel(\"Y\")     plt.colorbar(im, ax=axes[i], label=\"Class\", ticks=[0, 1, 2])  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Create a temporary directory for our test files\ntmpdir = tempfile.mkdtemp()\nprint(f\"Working directory: {tmpdir}\")\n\n# Save noisy mask as GeoTIFF\ninput_tif = os.path.join(tmpdir, \"noisy_segmentation.tif\")\noutput_tif = os.path.join(tmpdir, \"cleaned_segmentation.tif\")\n\n# Create a simple transform (geographic coordinates)\ntransform = from_bounds(\n    west=-120.0,\n    south=35.0,\n    east=-119.0,\n    north=36.0,\n    width=noisy_mask.shape[1],\n    height=noisy_mask.shape[0],\n)\n\n# Write noisy mask to GeoTIFF\nwith rasterio.open(\n    input_tif,\n    \"w\",\n    driver=\"GTiff\",\n    height=noisy_mask.shape[0],\n    width=noisy_mask.shape[1],\n    count=1,\n    dtype=noisy_mask.dtype,\n    crs=\"EPSG:4326\",\n    transform=transform,\n    compress=\"lzw\",\n) as dst:\n    dst.write(noisy_mask, 1)\n\nprint(f\"Saved noisy mask to: {input_tif}\")\n</pre> # Create a temporary directory for our test files tmpdir = tempfile.mkdtemp() print(f\"Working directory: {tmpdir}\")  # Save noisy mask as GeoTIFF input_tif = os.path.join(tmpdir, \"noisy_segmentation.tif\") output_tif = os.path.join(tmpdir, \"cleaned_segmentation.tif\")  # Create a simple transform (geographic coordinates) transform = from_bounds(     west=-120.0,     south=35.0,     east=-119.0,     north=36.0,     width=noisy_mask.shape[1],     height=noisy_mask.shape[0], )  # Write noisy mask to GeoTIFF with rasterio.open(     input_tif,     \"w\",     driver=\"GTiff\",     height=noisy_mask.shape[0],     width=noisy_mask.shape[1],     count=1,     dtype=noisy_mask.dtype,     crs=\"EPSG:4326\",     transform=transform,     compress=\"lzw\", ) as dst:     dst.write(noisy_mask, 1)  print(f\"Saved noisy mask to: {input_tif}\") In\u00a0[\u00a0]: Copied! <pre># Clean the GeoTIFF\nclean_raster(\n    input_path=input_tif,\n    output_path=output_tif,\n    class_values=[0, 1, 2],\n    smooth_edge_size=3,\n    min_island_size=100,\n    connectivity=8,\n)\n\nprint(f\"Cleaned raster saved to: {output_tif}\")\n</pre> # Clean the GeoTIFF clean_raster(     input_path=input_tif,     output_path=output_tif,     class_values=[0, 1, 2],     smooth_edge_size=3,     min_island_size=100,     connectivity=8, )  print(f\"Cleaned raster saved to: {output_tif}\") In\u00a0[\u00a0]: Copied! <pre># Verify the output preserves geospatial metadata\nwith rasterio.open(input_tif) as src_in:\n    print(\"Input metadata:\")\n    print(f\"  CRS: {src_in.crs}\")\n    print(f\"  Transform: {src_in.transform}\")\n    print(f\"  Bounds: {src_in.bounds}\")\n\nprint()\n\nwith rasterio.open(output_tif) as src_out:\n    print(\"Output metadata:\")\n    print(f\"  CRS: {src_out.crs}\")\n    print(f\"  Transform: {src_out.transform}\")\n    print(f\"  Bounds: {src_out.bounds}\")\n\n    # Read cleaned data\n    cleaned_from_file = src_out.read(1)\n\nprint(\"\\n\u2713 Geospatial metadata preserved!\")\n</pre> # Verify the output preserves geospatial metadata with rasterio.open(input_tif) as src_in:     print(\"Input metadata:\")     print(f\"  CRS: {src_in.crs}\")     print(f\"  Transform: {src_in.transform}\")     print(f\"  Bounds: {src_in.bounds}\")  print()  with rasterio.open(output_tif) as src_out:     print(\"Output metadata:\")     print(f\"  CRS: {src_out.crs}\")     print(f\"  Transform: {src_out.transform}\")     print(f\"  Bounds: {src_out.bounds}\")      # Read cleaned data     cleaned_from_file = src_out.read(1)  print(\"\\n\u2713 Geospatial metadata preserved!\") In\u00a0[\u00a0]: Copied! <pre># Create multiple test files\ninput_files = []\nfor i in range(3):\n    # Create different noisy masks\n    test_mask = create_noisy_segmentation(\n        size=(256, 256), num_classes=3, noise_level=0.05 + i * 0.02\n    )\n\n    # Save to file\n    filepath = os.path.join(tmpdir, f\"test_mask_{i}.tif\")\n\n    with rasterio.open(\n        filepath,\n        \"w\",\n        driver=\"GTiff\",\n        height=test_mask.shape[0],\n        width=test_mask.shape[1],\n        count=1,\n        dtype=test_mask.dtype,\n        crs=\"EPSG:4326\",\n        transform=from_bounds(-120, 35, -119, 36, 256, 256),\n    ) as dst:\n        dst.write(test_mask, 1)\n\n    input_files.append(filepath)\n\nprint(f\"Created {len(input_files)} test files\")\n</pre> # Create multiple test files input_files = [] for i in range(3):     # Create different noisy masks     test_mask = create_noisy_segmentation(         size=(256, 256), num_classes=3, noise_level=0.05 + i * 0.02     )      # Save to file     filepath = os.path.join(tmpdir, f\"test_mask_{i}.tif\")      with rasterio.open(         filepath,         \"w\",         driver=\"GTiff\",         height=test_mask.shape[0],         width=test_mask.shape[1],         count=1,         dtype=test_mask.dtype,         crs=\"EPSG:4326\",         transform=from_bounds(-120, 35, -119, 36, 256, 256),     ) as dst:         dst.write(test_mask, 1)      input_files.append(filepath)  print(f\"Created {len(input_files)} test files\") In\u00a0[\u00a0]: Copied! <pre># Batch clean all files\noutput_dir = os.path.join(tmpdir, \"batch_cleaned\")\n\noutput_files = clean_raster_batch(\n    input_paths=input_files,\n    output_dir=output_dir,\n    class_values=[0, 1, 2],\n    smooth_edge_size=2,\n    min_island_size=50,\n    connectivity=8,\n    suffix=\"_cleaned\",\n    verbose=True,\n)\n\nprint(f\"\\nProcessed {len(output_files)} files\")\nprint(\"Output files:\")\nfor f in output_files:\n    print(f\"  - {os.path.basename(f)}\")\n</pre> # Batch clean all files output_dir = os.path.join(tmpdir, \"batch_cleaned\")  output_files = clean_raster_batch(     input_paths=input_files,     output_dir=output_dir,     class_values=[0, 1, 2],     smooth_edge_size=2,     min_island_size=50,     connectivity=8,     suffix=\"_cleaned\",     verbose=True, )  print(f\"\\nProcessed {len(output_files)} files\") print(\"Output files:\") for f in output_files:     print(f\"  - {os.path.basename(f)}\") In\u00a0[\u00a0]: Copied! <pre>def segment_and_clean_workflow(image_path, output_path):\n    \"\"\"\n    Example workflow: Segmentation + Cleaning\n\n    In a real application, this would:\n    1. Load an image\n    2. Run semantic segmentation model (e.g., UNet, DeepLab)\n    3. Get raw predictions (often noisy)\n    4. Apply MultiClean to smooth and denoise\n    5. Save final result\n    \"\"\"\n    # For this example, we'll use our synthetic data\n    # In practice, you would:\n    # - Load the image with rasterio or PIL\n    # - Run your trained segmentation model\n    # - Get the prediction mask\n\n    # Simulate noisy model predictions\n    raw_predictions = create_noisy_segmentation(\n        size=(512, 512), num_classes=3, noise_level=0.08\n    )\n\n    # Apply MultiClean post-processing\n    cleaned_predictions = clean_segmentation_mask(\n        raw_predictions,\n        class_values=[0, 1, 2],\n        smooth_edge_size=3,\n        min_island_size=100,\n        connectivity=8,\n    )\n\n    return raw_predictions, cleaned_predictions\n\n\n# Run the workflow\nraw, cleaned = segment_and_clean_workflow(None, None)\n\n# Compare\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\n\naxes[0].imshow(raw, cmap=cmap, interpolation=\"nearest\")\naxes[0].set_title(\"Raw Model Predictions\", fontsize=14)\naxes[0].axis(\"off\")\n\naxes[1].imshow(cleaned, cmap=cmap, interpolation=\"nearest\")\naxes[1].set_title(\"After MultiClean Post-Processing\", fontsize=14)\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n# Quantify improvement\nchanged, total, pct = compare_masks(raw, cleaned)\nprint(f\"\\nPost-processing changed {pct:.2f}% of pixels\")\n</pre> def segment_and_clean_workflow(image_path, output_path):     \"\"\"     Example workflow: Segmentation + Cleaning      In a real application, this would:     1. Load an image     2. Run semantic segmentation model (e.g., UNet, DeepLab)     3. Get raw predictions (often noisy)     4. Apply MultiClean to smooth and denoise     5. Save final result     \"\"\"     # For this example, we'll use our synthetic data     # In practice, you would:     # - Load the image with rasterio or PIL     # - Run your trained segmentation model     # - Get the prediction mask      # Simulate noisy model predictions     raw_predictions = create_noisy_segmentation(         size=(512, 512), num_classes=3, noise_level=0.08     )      # Apply MultiClean post-processing     cleaned_predictions = clean_segmentation_mask(         raw_predictions,         class_values=[0, 1, 2],         smooth_edge_size=3,         min_island_size=100,         connectivity=8,     )      return raw_predictions, cleaned_predictions   # Run the workflow raw, cleaned = segment_and_clean_workflow(None, None)  # Compare fig, axes = plt.subplots(1, 2, figsize=(16, 8))  axes[0].imshow(raw, cmap=cmap, interpolation=\"nearest\") axes[0].set_title(\"Raw Model Predictions\", fontsize=14) axes[0].axis(\"off\")  axes[1].imshow(cleaned, cmap=cmap, interpolation=\"nearest\") axes[1].set_title(\"After MultiClean Post-Processing\", fontsize=14) axes[1].axis(\"off\")  plt.tight_layout() plt.show()  # Quantify improvement changed, total, pct = compare_masks(raw, cleaned) print(f\"\\nPost-processing changed {pct:.2f}% of pixels\") In\u00a0[\u00a0]: Copied! <pre># Cleanup temporary files\nimport shutil\n\nshutil.rmtree(tmpdir)\nprint(\"Cleaned up temporary files\")\n</pre> # Cleanup temporary files import shutil  shutil.rmtree(tmpdir) print(\"Cleaned up temporary files\")"},{"location":"examples/clean_segmentation_results/#cleaning-segmentation-results-with-multiclean","title":"Cleaning Segmentation Results with MultiClean\u00b6","text":"<p>This notebook demonstrates how to use MultiClean integration in GeoAI to post-process and clean segmentation results. MultiClean performs morphological operations to:</p> <ul> <li>Smooth edges - Reduce jagged boundaries using morphological opening</li> <li>Remove noise - Eliminate small isolated components (islands)</li> <li>Fill gaps - Replace invalid pixels with nearest valid class</li> </ul> <p>MultiClean is particularly useful for cleaning up noisy predictions from deep learning segmentation models.</p>"},{"location":"examples/clean_segmentation_results/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/clean_segmentation_results/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/clean_segmentation_results/#1-create-a-synthetic-noisy-segmentation-mask","title":"1. Create a Synthetic Noisy Segmentation Mask\u00b6","text":"<p>First, let's create a synthetic segmentation mask with realistic noise patterns that might occur in deep learning predictions.</p>"},{"location":"examples/clean_segmentation_results/#2-visualize-the-noisy-mask","title":"2. Visualize the Noisy Mask\u00b6","text":"<p>Let's visualize the noisy segmentation mask.</p>"},{"location":"examples/clean_segmentation_results/#3-clean-the-segmentation-mask","title":"3. Clean the Segmentation Mask\u00b6","text":"<p>Now let's apply MultiClean to remove noise and smooth edges.</p>"},{"location":"examples/clean_segmentation_results/#4-compare-before-and-after","title":"4. Compare Before and After\u00b6","text":"<p>Let's visualize the noisy and cleaned masks side by side.</p>"},{"location":"examples/clean_segmentation_results/#5-quantify-the-changes","title":"5. Quantify the Changes\u00b6","text":"<p>Use the <code>compare_masks</code> function to quantify how much the mask changed.</p>"},{"location":"examples/clean_segmentation_results/#6-zoom-in-on-a-region","title":"6. Zoom In on a Region\u00b6","text":"<p>Let's zoom in to see the edge smoothing and noise removal in detail.</p>"},{"location":"examples/clean_segmentation_results/#7-experiment-with-different-parameters","title":"7. Experiment with Different Parameters\u00b6","text":"<p>Let's see how different cleaning parameters affect the results.</p>"},{"location":"examples/clean_segmentation_results/#8-working-with-geotiff-files","title":"8. Working with GeoTIFF Files\u00b6","text":"<p>MultiClean can also process GeoTIFF files directly while preserving geospatial metadata.</p>"},{"location":"examples/clean_segmentation_results/#9-batch-processing-multiple-files","title":"9. Batch Processing Multiple Files\u00b6","text":"<p>You can process multiple segmentation files at once using <code>clean_raster_batch</code>.</p>"},{"location":"examples/clean_segmentation_results/#10-integration-with-segmentation-workflows","title":"10. Integration with Segmentation Workflows\u00b6","text":"<p>MultiClean is designed to be used as a post-processing step after semantic segmentation. Here's an example workflow:</p>"},{"location":"examples/clean_segmentation_results/#11-best-practices-and-tips","title":"11. Best Practices and Tips\u00b6","text":""},{"location":"examples/clean_segmentation_results/#choosing-parameters","title":"Choosing Parameters\u00b6","text":"<ul> <li>smooth_edge_size: Start with 2-3 pixels. Larger values create smoother boundaries but may over-smooth fine details.</li> <li>min_island_size: Depends on your minimum object size. Set to the smallest valid object area in pixels.</li> <li>connectivity: Use 8 for natural objects (smoother results), 4 for grid-aligned objects.</li> <li>fill_nan: Set to True if your predictions have nodata/NaN values that should be filled.</li> </ul>"},{"location":"examples/clean_segmentation_results/#performance-tips","title":"Performance Tips\u00b6","text":"<ul> <li>Use max_workers parameter for parallel processing on multi-core systems</li> <li>Process large rasters in tiles if memory is limited</li> <li>For batch processing, use <code>clean_raster_batch</code> instead of loops</li> </ul>"},{"location":"examples/clean_segmentation_results/#when-to-use-multiclean","title":"When to Use MultiClean\u00b6","text":"<p>\u2713 After semantic segmentation to remove noise \u2713 When edge boundaries are jagged or noisy \u2713 To remove small false positive detections \u2713 For cleaning up classification rasters</p> <p>\u2717 Don't use if you need to preserve exact boundaries \u2717 Not suitable for instance segmentation (use on semantic masks only)</p>"},{"location":"examples/clean_segmentation_results/#summary","title":"Summary\u00b6","text":"<p>In this notebook, we demonstrated:</p> <ol> <li>\u2705 Basic usage of <code>clean_segmentation_mask()</code> for numpy arrays</li> <li>\u2705 Visualizing before/after comparisons</li> <li>\u2705 Quantifying changes with <code>compare_masks()</code></li> <li>\u2705 Experimenting with different cleaning parameters</li> <li>\u2705 Processing GeoTIFF files with <code>clean_raster()</code></li> <li>\u2705 Batch processing with <code>clean_raster_batch()</code></li> <li>\u2705 Integration with segmentation workflows</li> </ol> <p>MultiClean is a powerful tool for post-processing segmentation results, helping you achieve cleaner, more professional outputs from your deep learning models.</p>"},{"location":"examples/clean_segmentation_results/#references","title":"References\u00b6","text":"<ul> <li>MultiClean GitHub Repository</li> <li>GeoAI Documentation</li> </ul>"},{"location":"examples/cloud_detection/","title":"Cloud detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U \"geoai-py[extra]\"\n</pre> # %pip install -U \"geoai-py[extra]\" In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport rasterio\nfrom rasterio.transform import from_bounds\nimport tempfile\nimport os\n\n# Import GeoAI cloud mask utilities\n# Import from the tools subpackage\nfrom geoai.tools.cloudmask import (\n    predict_cloud_mask,\n    predict_cloud_mask_from_raster,\n    predict_cloud_mask_batch,\n    calculate_cloud_statistics,\n    create_cloud_free_mask,\n    CLEAR,\n    THICK_CLOUD,\n    THIN_CLOUD,\n    CLOUD_SHADOW,\n)\n</pre> import numpy as np import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import rasterio from rasterio.transform import from_bounds import tempfile import os  # Import GeoAI cloud mask utilities # Import from the tools subpackage from geoai.tools.cloudmask import (     predict_cloud_mask,     predict_cloud_mask_from_raster,     predict_cloud_mask_batch,     calculate_cloud_statistics,     create_cloud_free_mask,     CLEAR,     THICK_CLOUD,     THIN_CLOUD,     CLOUD_SHADOW, ) In\u00a0[\u00a0]: Copied! <pre>def create_synthetic_satellite_image(size=(512, 512), cloud_coverage=0.3):\n    \"\"\"\n    Create synthetic satellite imagery (R, G, NIR bands).\n\n    Args:\n        size: Image dimensions (height, width)\n        cloud_coverage: Fraction of image covered by clouds (0-1)\n\n    Returns:\n        3D array with shape (3, height, width) containing R, G, NIR bands\n    \"\"\"\n    np.random.seed(42)\n\n    # Create base reflectance values typical of vegetation\n    # Red: low (absorbed by chlorophyll)\n    # Green: medium\n    # NIR: high (reflected by vegetation)\n    red = np.random.rand(*size) * 2000 + 500  # 500-2500\n    green = np.random.rand(*size) * 3000 + 1000  # 1000-4000\n    nir = np.random.rand(*size) * 5000 + 3000  # 3000-8000\n\n    # Add some spatial structure (vegetation patches)\n    from scipy.ndimage import gaussian_filter\n\n    red = gaussian_filter(red, sigma=20)\n    green = gaussian_filter(green, sigma=20)\n    nir = gaussian_filter(nir, sigma=20)\n\n    # Add cloud patterns\n    if cloud_coverage &gt; 0:\n        # Create cloud mask\n        cloud_base = np.random.rand(*size)\n        cloud_base = gaussian_filter(cloud_base, sigma=30)\n        cloud_mask = cloud_base &gt; (1 - cloud_coverage)\n\n        # Clouds have high reflectance in all bands\n        cloud_value = 8000\n        red[cloud_mask] = cloud_value + np.random.rand(cloud_mask.sum()) * 1000\n        green[cloud_mask] = cloud_value + np.random.rand(cloud_mask.sum()) * 1000\n        nir[cloud_mask] = cloud_value + np.random.rand(cloud_mask.sum()) * 1000\n\n    # Stack into (3, H, W) format\n    image = np.stack([red, green, nir], axis=0).astype(np.float32)\n\n    return image\n\n\n# Create synthetic image\nimage = create_synthetic_satellite_image(size=(512, 512), cloud_coverage=0.2)\nprint(f\"Created synthetic image with shape: {image.shape}\")\nprint(f\"Value range: {image.min():.0f} - {image.max():.0f}\")\n</pre> def create_synthetic_satellite_image(size=(512, 512), cloud_coverage=0.3):     \"\"\"     Create synthetic satellite imagery (R, G, NIR bands).      Args:         size: Image dimensions (height, width)         cloud_coverage: Fraction of image covered by clouds (0-1)      Returns:         3D array with shape (3, height, width) containing R, G, NIR bands     \"\"\"     np.random.seed(42)      # Create base reflectance values typical of vegetation     # Red: low (absorbed by chlorophyll)     # Green: medium     # NIR: high (reflected by vegetation)     red = np.random.rand(*size) * 2000 + 500  # 500-2500     green = np.random.rand(*size) * 3000 + 1000  # 1000-4000     nir = np.random.rand(*size) * 5000 + 3000  # 3000-8000      # Add some spatial structure (vegetation patches)     from scipy.ndimage import gaussian_filter      red = gaussian_filter(red, sigma=20)     green = gaussian_filter(green, sigma=20)     nir = gaussian_filter(nir, sigma=20)      # Add cloud patterns     if cloud_coverage &gt; 0:         # Create cloud mask         cloud_base = np.random.rand(*size)         cloud_base = gaussian_filter(cloud_base, sigma=30)         cloud_mask = cloud_base &gt; (1 - cloud_coverage)          # Clouds have high reflectance in all bands         cloud_value = 8000         red[cloud_mask] = cloud_value + np.random.rand(cloud_mask.sum()) * 1000         green[cloud_mask] = cloud_value + np.random.rand(cloud_mask.sum()) * 1000         nir[cloud_mask] = cloud_value + np.random.rand(cloud_mask.sum()) * 1000      # Stack into (3, H, W) format     image = np.stack([red, green, nir], axis=0).astype(np.float32)      return image   # Create synthetic image image = create_synthetic_satellite_image(size=(512, 512), cloud_coverage=0.2) print(f\"Created synthetic image with shape: {image.shape}\") print(f\"Value range: {image.min():.0f} - {image.max():.0f}\") In\u00a0[\u00a0]: Copied! <pre>def visualize_rgb(image, title=\"RGB Composite\"):\n    \"\"\"\n    Visualize RGB composite from satellite image.\n\n    Args:\n        image: Array with shape (3, H, W) or (H, W, 3)\n        title: Plot title\n    \"\"\"\n    # Convert to (H, W, 3) if needed\n    if image.shape[0] == 3:\n        rgb = image[:3].transpose(1, 2, 0)  # Take R, G, (NIR-&gt;B for vis)\n    else:\n        rgb = image[:, :, :3]\n\n    # Normalize to 0-1 for display\n    rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())\n\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_norm)\n    plt.title(title, fontsize=16)\n    plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\nvisualize_rgb(image, \"Synthetic Satellite Image (RGB)\")\n</pre> def visualize_rgb(image, title=\"RGB Composite\"):     \"\"\"     Visualize RGB composite from satellite image.      Args:         image: Array with shape (3, H, W) or (H, W, 3)         title: Plot title     \"\"\"     # Convert to (H, W, 3) if needed     if image.shape[0] == 3:         rgb = image[:3].transpose(1, 2, 0)  # Take R, G, (NIR-&gt;B for vis)     else:         rgb = image[:, :, :3]      # Normalize to 0-1 for display     rgb_norm = (rgb - rgb.min()) / (rgb.max() - rgb.min())      plt.figure(figsize=(10, 10))     plt.imshow(rgb_norm)     plt.title(title, fontsize=16)     plt.axis(\"off\")     plt.tight_layout()     plt.show()   visualize_rgb(image, \"Synthetic Satellite Image (RGB)\") In\u00a0[\u00a0]: Copied! <pre># Predict cloud mask\n# Note: First run will download the model (may take a moment)\ncloud_mask = predict_cloud_mask(\n    image,\n    batch_size=1,\n    inference_device=\"cpu\",  # Use \"cuda\" if GPU available\n    inference_dtype=\"fp32\",  # Use \"bf16\" for faster inference on supported hardware\n    patch_size=1000,\n    model_version=3,  # Model versions: 1, 2, or 3\n)\n\nprint(f\"Cloud mask shape: {cloud_mask.shape}\")\nprint(f\"Classes found: {np.unique(cloud_mask)}\")\n</pre> # Predict cloud mask # Note: First run will download the model (may take a moment) cloud_mask = predict_cloud_mask(     image,     batch_size=1,     inference_device=\"cpu\",  # Use \"cuda\" if GPU available     inference_dtype=\"fp32\",  # Use \"bf16\" for faster inference on supported hardware     patch_size=1000,     model_version=3,  # Model versions: 1, 2, or 3 )  print(f\"Cloud mask shape: {cloud_mask.shape}\") print(f\"Classes found: {np.unique(cloud_mask)}\") In\u00a0[\u00a0]: Copied! <pre># Create colormap for cloud classes\n# Clear (blue), Thick Cloud (white), Thin Cloud (light gray), Shadow (dark gray)\ncolors = [\"#4575b4\", \"#ffffff\", \"#d3d3d3\", \"#404040\"]\ncmap = ListedColormap(colors)\n\nplt.figure(figsize=(12, 10))\nim = plt.imshow(cloud_mask, cmap=cmap, interpolation=\"nearest\", vmin=0, vmax=3)\nplt.title(\"Cloud Mask Classification\", fontsize=16)\ncbar = plt.colorbar(im, ticks=[0, 1, 2, 3])\ncbar.ax.set_yticklabels([\"Clear\", \"Thick Cloud\", \"Thin Cloud\", \"Shadow\"])\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.tight_layout()\nplt.show()\n</pre> # Create colormap for cloud classes # Clear (blue), Thick Cloud (white), Thin Cloud (light gray), Shadow (dark gray) colors = [\"#4575b4\", \"#ffffff\", \"#d3d3d3\", \"#404040\"] cmap = ListedColormap(colors)  plt.figure(figsize=(12, 10)) im = plt.imshow(cloud_mask, cmap=cmap, interpolation=\"nearest\", vmin=0, vmax=3) plt.title(\"Cloud Mask Classification\", fontsize=16) cbar = plt.colorbar(im, ticks=[0, 1, 2, 3]) cbar.ax.set_yticklabels([\"Clear\", \"Thick Cloud\", \"Thin Cloud\", \"Shadow\"]) plt.xlabel(\"X\") plt.ylabel(\"Y\") plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>stats = calculate_cloud_statistics(cloud_mask)\n\nprint(\"Cloud Coverage Statistics:\")\nprint(f\"  Total pixels: {stats['total_pixels']:,}\")\nprint(f\"  Clear pixels: {stats['clear_pixels']:,} ({stats['clear_percent']:.1f}%)\")\nprint(f\"  Thick cloud: {stats['thick_cloud_pixels']:,}\")\nprint(f\"  Thin cloud: {stats['thin_cloud_pixels']:,}\")\nprint(f\"  Cloud shadow: {stats['shadow_pixels']:,}\")\nprint(f\"  \\nTotal cloud coverage: {stats['cloud_percent']:.1f}%\")\nprint(f\"  Shadow coverage: {stats['shadow_percent']:.1f}%\")\n</pre> stats = calculate_cloud_statistics(cloud_mask)  print(\"Cloud Coverage Statistics:\") print(f\"  Total pixels: {stats['total_pixels']:,}\") print(f\"  Clear pixels: {stats['clear_pixels']:,} ({stats['clear_percent']:.1f}%)\") print(f\"  Thick cloud: {stats['thick_cloud_pixels']:,}\") print(f\"  Thin cloud: {stats['thin_cloud_pixels']:,}\") print(f\"  Cloud shadow: {stats['shadow_pixels']:,}\") print(f\"  \\nTotal cloud coverage: {stats['cloud_percent']:.1f}%\") print(f\"  Shadow coverage: {stats['shadow_percent']:.1f}%\") In\u00a0[\u00a0]: Copied! <pre># Strict cloud-free mask (only clear pixels)\ncloud_free_strict = create_cloud_free_mask(\n    cloud_mask, include_thin_clouds=False, include_shadows=False\n)\n\n# Relaxed cloud-free mask (accept thin clouds and shadows)\ncloud_free_relaxed = create_cloud_free_mask(\n    cloud_mask, include_thin_clouds=True, include_shadows=True\n)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\n\naxes[0].imshow(cloud_free_strict, cmap=\"RdYlGn\", interpolation=\"nearest\")\naxes[0].set_title(\n    f\"Strict Cloud-Free Mask\\n({cloud_free_strict.sum() / cloud_free_strict.size * 100:.1f}% usable)\",\n    fontsize=14,\n)\naxes[0].axis(\"off\")\n\naxes[1].imshow(cloud_free_relaxed, cmap=\"RdYlGn\", interpolation=\"nearest\")\naxes[1].set_title(\n    f\"Relaxed Cloud-Free Mask\\n({cloud_free_relaxed.sum() / cloud_free_relaxed.size * 100:.1f}% usable)\",\n    fontsize=14,\n)\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> # Strict cloud-free mask (only clear pixels) cloud_free_strict = create_cloud_free_mask(     cloud_mask, include_thin_clouds=False, include_shadows=False )  # Relaxed cloud-free mask (accept thin clouds and shadows) cloud_free_relaxed = create_cloud_free_mask(     cloud_mask, include_thin_clouds=True, include_shadows=True )  fig, axes = plt.subplots(1, 2, figsize=(16, 7))  axes[0].imshow(cloud_free_strict, cmap=\"RdYlGn\", interpolation=\"nearest\") axes[0].set_title(     f\"Strict Cloud-Free Mask\\n({cloud_free_strict.sum() / cloud_free_strict.size * 100:.1f}% usable)\",     fontsize=14, ) axes[0].axis(\"off\")  axes[1].imshow(cloud_free_relaxed, cmap=\"RdYlGn\", interpolation=\"nearest\") axes[1].set_title(     f\"Relaxed Cloud-Free Mask\\n({cloud_free_relaxed.sum() / cloud_free_relaxed.size * 100:.1f}% usable)\",     fontsize=14, ) axes[1].axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(2, 2, figsize=(14, 14))\naxes = axes.flatten()\n\nclass_names = [\"Clear\", \"Thick Cloud\", \"Thin Cloud\", \"Cloud Shadow\"]\nclass_values = [CLEAR, THICK_CLOUD, THIN_CLOUD, CLOUD_SHADOW]\n\nfor i, (name, value) in enumerate(zip(class_names, class_values)):\n    # Create binary mask for this class\n    class_mask = (cloud_mask == value).astype(np.uint8)\n    count = class_mask.sum()\n    percent = count / class_mask.size * 100\n\n    axes[i].imshow(class_mask, cmap=\"gray\", interpolation=\"nearest\")\n    axes[i].set_title(f\"{name}\\n{count:,} pixels ({percent:.1f}%)\", fontsize=12)\n    axes[i].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(2, 2, figsize=(14, 14)) axes = axes.flatten()  class_names = [\"Clear\", \"Thick Cloud\", \"Thin Cloud\", \"Cloud Shadow\"] class_values = [CLEAR, THICK_CLOUD, THIN_CLOUD, CLOUD_SHADOW]  for i, (name, value) in enumerate(zip(class_names, class_values)):     # Create binary mask for this class     class_mask = (cloud_mask == value).astype(np.uint8)     count = class_mask.sum()     percent = count / class_mask.size * 100      axes[i].imshow(class_mask, cmap=\"gray\", interpolation=\"nearest\")     axes[i].set_title(f\"{name}\\n{count:,} pixels ({percent:.1f}%)\", fontsize=12)     axes[i].axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Create temporary directory\ntmpdir = tempfile.mkdtemp()\nprint(f\"Working directory: {tmpdir}\")\n\n# Save synthetic image as GeoTIFF\ninput_tif = os.path.join(tmpdir, \"satellite_image.tif\")\noutput_tif = os.path.join(tmpdir, \"cloud_mask.tif\")\n\n# Create geographic transform\ntransform = from_bounds(\n    west=-120.0,\n    south=35.0,\n    east=-119.0,\n    north=36.0,\n    width=image.shape[2],\n    height=image.shape[1],\n)\n\n# Write image to GeoTIFF (3 bands: R, G, NIR)\nwith rasterio.open(\n    input_tif,\n    \"w\",\n    driver=\"GTiff\",\n    height=image.shape[1],\n    width=image.shape[2],\n    count=3,\n    dtype=image.dtype,\n    crs=\"EPSG:4326\",\n    transform=transform,\n    compress=\"lzw\",\n) as dst:\n    for i in range(3):\n        dst.write(image[i], i + 1)\n\nprint(f\"Saved satellite image to: {input_tif}\")\n</pre> # Create temporary directory tmpdir = tempfile.mkdtemp() print(f\"Working directory: {tmpdir}\")  # Save synthetic image as GeoTIFF input_tif = os.path.join(tmpdir, \"satellite_image.tif\") output_tif = os.path.join(tmpdir, \"cloud_mask.tif\")  # Create geographic transform transform = from_bounds(     west=-120.0,     south=35.0,     east=-119.0,     north=36.0,     width=image.shape[2],     height=image.shape[1], )  # Write image to GeoTIFF (3 bands: R, G, NIR) with rasterio.open(     input_tif,     \"w\",     driver=\"GTiff\",     height=image.shape[1],     width=image.shape[2],     count=3,     dtype=image.dtype,     crs=\"EPSG:4326\",     transform=transform,     compress=\"lzw\", ) as dst:     for i in range(3):         dst.write(image[i], i + 1)  print(f\"Saved satellite image to: {input_tif}\") In\u00a0[\u00a0]: Copied! <pre># Predict cloud mask from GeoTIFF\npredict_cloud_mask_from_raster(\n    input_path=input_tif,\n    output_path=output_tif,\n    red_band=1,  # Band indices for R, G, NIR\n    green_band=2,\n    nir_band=3,\n    inference_device=\"cpu\",\n)\n\nprint(f\"Cloud mask saved to: {output_tif}\")\n</pre> # Predict cloud mask from GeoTIFF predict_cloud_mask_from_raster(     input_path=input_tif,     output_path=output_tif,     red_band=1,  # Band indices for R, G, NIR     green_band=2,     nir_band=3,     inference_device=\"cpu\", )  print(f\"Cloud mask saved to: {output_tif}\") In\u00a0[\u00a0]: Copied! <pre># Verify output and metadata preservation\nwith rasterio.open(input_tif) as src_in:\n    print(\"Input metadata:\")\n    print(f\"  CRS: {src_in.crs}\")\n    print(f\"  Transform: {src_in.transform}\")\n    print(f\"  Bounds: {src_in.bounds}\")\n    print(f\"  Bands: {src_in.count}\")\n\nprint()\n\nwith rasterio.open(output_tif) as src_out:\n    print(\"Output metadata:\")\n    print(f\"  CRS: {src_out.crs}\")\n    print(f\"  Transform: {src_out.transform}\")\n    print(f\"  Bounds: {src_out.bounds}\")\n    print(f\"  Bands: {src_out.count}\")\n\n    # Read and verify cloud mask\n    mask_from_file = src_out.read(1)\n    print(f\"  Classes: {np.unique(mask_from_file)}\")\n\nprint(\"\\n\u2713 Geospatial metadata preserved!\")\n</pre> # Verify output and metadata preservation with rasterio.open(input_tif) as src_in:     print(\"Input metadata:\")     print(f\"  CRS: {src_in.crs}\")     print(f\"  Transform: {src_in.transform}\")     print(f\"  Bounds: {src_in.bounds}\")     print(f\"  Bands: {src_in.count}\")  print()  with rasterio.open(output_tif) as src_out:     print(\"Output metadata:\")     print(f\"  CRS: {src_out.crs}\")     print(f\"  Transform: {src_out.transform}\")     print(f\"  Bounds: {src_out.bounds}\")     print(f\"  Bands: {src_out.count}\")      # Read and verify cloud mask     mask_from_file = src_out.read(1)     print(f\"  Classes: {np.unique(mask_from_file)}\")  print(\"\\n\u2713 Geospatial metadata preserved!\") In\u00a0[\u00a0]: Copied! <pre># Create multiple test images with different cloud coverage\ninput_files = []\ncloud_coverages = [0.1, 0.3, 0.5]\n\nfor i, coverage in enumerate(cloud_coverages):\n    # Create image with specific cloud coverage\n    test_image = create_synthetic_satellite_image(\n        size=(256, 256), cloud_coverage=coverage\n    )\n\n    # Save to file\n    filepath = os.path.join(tmpdir, f\"scene_{i}_clouds{int(coverage*100)}.tif\")\n\n    transform = from_bounds(-120, 35, -119, 36, 256, 256)\n    with rasterio.open(\n        filepath,\n        \"w\",\n        driver=\"GTiff\",\n        height=256,\n        width=256,\n        count=3,\n        dtype=test_image.dtype,\n        crs=\"EPSG:4326\",\n        transform=transform,\n    ) as dst:\n        for j in range(3):\n            dst.write(test_image[j], j + 1)\n\n    input_files.append(filepath)\n\nprint(f\"Created {len(input_files)} test images\")\nfor f in input_files:\n    print(f\"  - {os.path.basename(f)}\")\n</pre> # Create multiple test images with different cloud coverage input_files = [] cloud_coverages = [0.1, 0.3, 0.5]  for i, coverage in enumerate(cloud_coverages):     # Create image with specific cloud coverage     test_image = create_synthetic_satellite_image(         size=(256, 256), cloud_coverage=coverage     )      # Save to file     filepath = os.path.join(tmpdir, f\"scene_{i}_clouds{int(coverage*100)}.tif\")      transform = from_bounds(-120, 35, -119, 36, 256, 256)     with rasterio.open(         filepath,         \"w\",         driver=\"GTiff\",         height=256,         width=256,         count=3,         dtype=test_image.dtype,         crs=\"EPSG:4326\",         transform=transform,     ) as dst:         for j in range(3):             dst.write(test_image[j], j + 1)      input_files.append(filepath)  print(f\"Created {len(input_files)} test images\") for f in input_files:     print(f\"  - {os.path.basename(f)}\") In\u00a0[\u00a0]: Copied! <pre># Batch process all images\noutput_dir = os.path.join(tmpdir, \"cloud_masks\")\n\noutput_files = predict_cloud_mask_batch(\n    input_paths=input_files,\n    output_dir=output_dir,\n    red_band=1,\n    green_band=2,\n    nir_band=3,\n    inference_device=\"cpu\",\n    suffix=\"_cloudmask\",\n    verbose=True,\n)\n\nprint(f\"\\nProcessed {len(output_files)} images\")\n</pre> # Batch process all images output_dir = os.path.join(tmpdir, \"cloud_masks\")  output_files = predict_cloud_mask_batch(     input_paths=input_files,     output_dir=output_dir,     red_band=1,     green_band=2,     nir_band=3,     inference_device=\"cpu\",     suffix=\"_cloudmask\",     verbose=True, )  print(f\"\\nProcessed {len(output_files)} images\") In\u00a0[\u00a0]: Copied! <pre># Analyze results from batch processing\nprint(\"\\nCloud Coverage Analysis:\")\nprint(\"-\" * 60)\n\nfor output_file in output_files:\n    with rasterio.open(output_file) as src:\n        mask = src.read(1)\n\n    stats = calculate_cloud_statistics(mask)\n\n    filename = os.path.basename(output_file)\n    print(f\"\\n{filename}:\")\n    print(f\"  Clear: {stats['clear_percent']:.1f}%\")\n    print(f\"  Cloud: {stats['cloud_percent']:.1f}%\")\n    print(f\"  Shadow: {stats['shadow_percent']:.1f}%\")\n</pre> # Analyze results from batch processing print(\"\\nCloud Coverage Analysis:\") print(\"-\" * 60)  for output_file in output_files:     with rasterio.open(output_file) as src:         mask = src.read(1)      stats = calculate_cloud_statistics(mask)      filename = os.path.basename(output_file)     print(f\"\\n{filename}:\")     print(f\"  Clear: {stats['clear_percent']:.1f}%\")     print(f\"  Cloud: {stats['cloud_percent']:.1f}%\")     print(f\"  Shadow: {stats['shadow_percent']:.1f}%\") In\u00a0[\u00a0]: Copied! <pre># Cleanup temporary files\nimport shutil\n\nshutil.rmtree(tmpdir)\nprint(\"Cleaned up temporary files\")\n</pre> # Cleanup temporary files import shutil  shutil.rmtree(tmpdir) print(\"Cleaned up temporary files\")"},{"location":"examples/cloud_detection/#cloud-detection-with-omnicloudmask","title":"Cloud Detection with OmniCloudMask\u00b6","text":"<p>This notebook demonstrates how to use OmniCloudMask integration in GeoAI for detecting clouds and cloud shadows in satellite imagery. OmniCloudMask performs semantic segmentation to classify pixels into four categories:</p> <ul> <li>0: Clear - Cloud-free pixels</li> <li>1: Thick Cloud - Opaque cloud cover</li> <li>2: Thin Cloud - Semi-transparent cloud cover</li> <li>3: Cloud Shadow - Shadows cast by clouds</li> </ul> <p>OmniCloudMask supports Sentinel-2, Landsat 8, PlanetScope, and Maxar imagery at 10-50m spatial resolution.</p>"},{"location":"examples/cloud_detection/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/cloud_detection/#import-libraries","title":"Import Libraries\u00b6","text":""},{"location":"examples/cloud_detection/#1-create-synthetic-satellite-imagery","title":"1. Create Synthetic Satellite Imagery\u00b6","text":"<p>For demonstration purposes, let's create synthetic satellite imagery with RGB and NIR bands.</p>"},{"location":"examples/cloud_detection/#2-visualize-the-input-image","title":"2. Visualize the Input Image\u00b6","text":"<p>Let's visualize the RGB composite of our synthetic satellite image.</p>"},{"location":"examples/cloud_detection/#3-predict-cloud-mask","title":"3. Predict Cloud Mask\u00b6","text":"<p>Now let's use OmniCloudMask to detect clouds and cloud shadows.</p>"},{"location":"examples/cloud_detection/#4-visualize-cloud-mask","title":"4. Visualize Cloud Mask\u00b6","text":"<p>Let's visualize the cloud detection results with a color-coded map.</p>"},{"location":"examples/cloud_detection/#5-calculate-cloud-statistics","title":"5. Calculate Cloud Statistics\u00b6","text":"<p>Let's quantify the cloud coverage and other statistics.</p>"},{"location":"examples/cloud_detection/#6-create-cloud-free-mask","title":"6. Create Cloud-Free Mask\u00b6","text":"<p>Create a binary mask showing which pixels are usable (cloud-free).</p>"},{"location":"examples/cloud_detection/#7-compare-different-classes","title":"7. Compare Different Classes\u00b6","text":"<p>Let's visualize each cloud class separately.</p>"},{"location":"examples/cloud_detection/#8-working-with-geotiff-files","title":"8. Working with GeoTIFF Files\u00b6","text":"<p>OmniCloudMask can process GeoTIFF files directly while preserving geospatial metadata.</p>"},{"location":"examples/cloud_detection/#9-batch-processing-multiple-images","title":"9. Batch Processing Multiple Images\u00b6","text":"<p>Process multiple satellite images at once using batch processing.</p>"},{"location":"examples/cloud_detection/#10-use-case-filter-usable-scenes","title":"10. Use Case: Filter Usable Scenes\u00b6","text":"<p>A common workflow is to filter satellite scenes based on cloud coverage threshold.</p>"},{"location":"examples/cloud_detection/#11-best-practices-and-tips","title":"11. Best Practices and Tips\u00b6","text":""},{"location":"examples/cloud_detection/#input-requirements","title":"Input Requirements\u00b6","text":"<ul> <li>Bands: Requires Red, Green, and NIR bands</li> <li>Resolution: Optimized for 10-50m spatial resolution</li> <li>Sensors: Validated on Sentinel-2, Landsat 8, PlanetScope, Maxar</li> <li>Values: Works with reflectance (0-1) or digital numbers</li> </ul>"},{"location":"examples/cloud_detection/#performance-tips","title":"Performance Tips\u00b6","text":"<ul> <li>Use inference_dtype='bf16' for 2-3x speedup on supported hardware</li> <li>Use inference_device='cuda' if GPU available</li> <li>Adjust patch_size based on available memory</li> <li>Use batch_size &gt; 1 for faster batch processing</li> </ul>"},{"location":"examples/cloud_detection/#model-versions","title":"Model Versions\u00b6","text":"<ul> <li>v3.0 (default): Expanded training dataset for higher accuracy.</li> <li>v2.0: A smaller faster model ensemble with improved robustness.</li> <li>v1.0: Baseline model release supporting the OmniCloudMask paper.</li> </ul>"},{"location":"examples/cloud_detection/#when-to-use-omnicloudmask","title":"When to Use OmniCloudMask\u00b6","text":"<p>\u2713 Pre-processing satellite imagery for analysis \u2713 Filtering scenes based on cloud coverage \u2713 Creating cloud-free composites \u2713 Quality assessment of satellite data \u2713 Time series analysis (exclude cloudy observations)</p>"},{"location":"examples/cloud_detection/#sensor-specific-band-indices","title":"Sensor-Specific Band Indices\u00b6","text":"<p>Sentinel-2:</p> <pre>red_band=4, green_band=3, nir_band=8\n</pre> <p>Landsat 8/9:</p> <pre>red_band=4, green_band=3, nir_band=5\n</pre> <p>PlanetScope:</p> <pre>red_band=3, green_band=2, nir_band=4\n</pre>"},{"location":"examples/cloud_detection/#summary","title":"Summary\u00b6","text":"<p>In this notebook, we demonstrated:</p> <ol> <li>\u2705 Predicting cloud masks from numpy arrays</li> <li>\u2705 Visualizing cloud detection results</li> <li>\u2705 Calculating cloud coverage statistics</li> <li>\u2705 Creating cloud-free masks</li> <li>\u2705 Processing GeoTIFF files while preserving metadata</li> <li>\u2705 Batch processing multiple scenes</li> <li>\u2705 Filtering scenes based on cloud coverage</li> </ol> <p>OmniCloudMask is a powerful tool for cloud detection in satellite imagery, essential for many remote sensing workflows.</p>"},{"location":"examples/cloud_detection/#references","title":"References\u00b6","text":"<ul> <li>OmniCloudMask GitHub Repository</li> <li>GeoAI Documentation</li> </ul>"},{"location":"examples/create_training_data/","title":"Create training data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import os\nimport geoai\n</pre> import os import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train_tiles.zip\"\ndownload_dir = geoai.download_file(url)\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train_tiles.zip\" download_dir = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre># List available data\ndata_dir = os.path.join(download_dir, \"data\")\n\nprint(\"Images:\")\nfor f in sorted(os.listdir(f\"{data_dir}/images\")):\n    print(f\"  - {f}\")\n\nprint(\"\\nMasks (single file):\")\nfor f in sorted(os.listdir(f\"{data_dir}/masks1\")):\n    print(f\"  - {f}\")\n\nprint(\"\\nMasks (multiple files):\")\nfor f in sorted(os.listdir(f\"{data_dir}/masks2\")):\n    print(f\"  - {f}\")\n</pre> # List available data data_dir = os.path.join(download_dir, \"data\")  print(\"Images:\") for f in sorted(os.listdir(f\"{data_dir}/images\")):     print(f\"  - {f}\")  print(\"\\nMasks (single file):\") for f in sorted(os.listdir(f\"{data_dir}/masks1\")):     print(f\"  - {f}\")  print(\"\\nMasks (multiple files):\") for f in sorted(os.listdir(f\"{data_dir}/masks2\")):     print(f\"  - {f}\") In\u00a0[\u00a0]: Copied! <pre># Load and display first image\nimage_path = f\"{data_dir}/images/naip_rgb_train_tile1.tif\"\nmask_path = f\"{data_dir}/masks2/naip_rgb_train_tile1.geojson\"\n\nfig, axes, info = geoai.display_image_with_vector(image_path, mask_path)\nprint(f\"Number of buildings: {info['num_features']}\")\n</pre> # Load and display first image image_path = f\"{data_dir}/images/naip_rgb_train_tile1.tif\" mask_path = f\"{data_dir}/masks2/naip_rgb_train_tile1.geojson\"  fig, axes, info = geoai.display_image_with_vector(image_path, mask_path) print(f\"Number of buildings: {info['num_features']}\") In\u00a0[\u00a0]: Copied! <pre># Use single mask file for all images\nstats = geoai.export_geotiff_tiles_batch(\n    images_folder=f\"{data_dir}/images\",\n    masks_file=f\"{data_dir}/masks1/naip_train_buildings.geojson\",\n    output_folder=\"output/method1_single_mask\",\n    tile_size=256,\n    stride=256,  # No overlap\n    class_value_field=\"class\",\n    skip_empty_tiles=True,  # Skip tiles with no buildings\n    max_tiles=20,  # Limit for demo purposes\n    quiet=False,\n)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Results:\")\nprint(f\"  Images processed: {stats['processed_pairs']}\")\nprint(f\"  Total tiles generated: {stats['total_tiles']}\")\nprint(f\"  Tiles with features: {stats['tiles_with_features']}\")\nprint(\n    f\"  Feature percentage: {stats['tiles_with_features']/stats['total_tiles']*100:.1f}%\"\n)\n</pre> # Use single mask file for all images stats = geoai.export_geotiff_tiles_batch(     images_folder=f\"{data_dir}/images\",     masks_file=f\"{data_dir}/masks1/naip_train_buildings.geojson\",     output_folder=\"output/method1_single_mask\",     tile_size=256,     stride=256,  # No overlap     class_value_field=\"class\",     skip_empty_tiles=True,  # Skip tiles with no buildings     max_tiles=20,  # Limit for demo purposes     quiet=False, )  print(f\"\\n{'='*60}\") print(\"Results:\") print(f\"  Images processed: {stats['processed_pairs']}\") print(f\"  Total tiles generated: {stats['total_tiles']}\") print(f\"  Tiles with features: {stats['tiles_with_features']}\") print(     f\"  Feature percentage: {stats['tiles_with_features']/stats['total_tiles']*100:.1f}%\" ) In\u00a0[\u00a0]: Copied! <pre># Use multiple mask files matched by sorted order\nstats = geoai.export_geotiff_tiles_batch(\n    images_folder=f\"{data_dir}/images\",\n    masks_folder=f\"{data_dir}/masks2\",\n    output_folder=\"output/method2_sorted_order\",\n    tile_size=256,\n    stride=256,\n    class_value_field=\"class\",\n    skip_empty_tiles=True,\n    match_by_name=False,  # Match by sorted order\n    max_tiles=20,\n)\n\nprint(f\"\\n{'='*60}\")\nprint(\"Results:\")\nprint(f\"  Images processed: {stats['processed_pairs']}\")\nprint(f\"  Total tiles generated: {stats['total_tiles']}\")\nprint(f\"  Tiles with features: {stats['tiles_with_features']}\")\n</pre> # Use multiple mask files matched by sorted order stats = geoai.export_geotiff_tiles_batch(     images_folder=f\"{data_dir}/images\",     masks_folder=f\"{data_dir}/masks2\",     output_folder=\"output/method2_sorted_order\",     tile_size=256,     stride=256,     class_value_field=\"class\",     skip_empty_tiles=True,     match_by_name=False,  # Match by sorted order     max_tiles=20, )  print(f\"\\n{'='*60}\") print(\"Results:\") print(f\"  Images processed: {stats['processed_pairs']}\") print(f\"  Total tiles generated: {stats['total_tiles']}\") print(f\"  Tiles with features: {stats['tiles_with_features']}\") In\u00a0[\u00a0]: Copied! <pre>stats = geoai.export_geotiff_tiles_batch(\n    images_folder=\"data/images\",\n    masks_folder=\"data/masks2\",\n    output_folder=\"output/method3_filename_match\",\n    tile_size=256,\n    stride=256,\n    class_value_field=\"class\",\n    skip_empty_tiles=True,\n    match_by_name=True,  # Match by filename\n)\n\nprint(\"Method 3 requires matching base filenames between images and masks.\")\nprint(\"Example: 'image001.tif' pairs with 'image001.geojson'\")\n</pre> stats = geoai.export_geotiff_tiles_batch(     images_folder=\"data/images\",     masks_folder=\"data/masks2\",     output_folder=\"output/method3_filename_match\",     tile_size=256,     stride=256,     class_value_field=\"class\",     skip_empty_tiles=True,     match_by_name=True,  # Match by filename )  print(\"Method 3 requires matching base filenames between images and masks.\") print(\"Example: 'image001.tif' pairs with 'image001.geojson'\") In\u00a0[\u00a0]: Copied! <pre>output_dir = \"output/method1_single_mask\"\nfig = geoai.display_training_tiles(output_dir, num_tiles=6)\n</pre> output_dir = \"output/method1_single_mask\" fig = geoai.display_training_tiles(output_dir, num_tiles=6) In\u00a0[\u00a0]: Copied! <pre># Advanced example with custom parameters\nstats = geoai.export_geotiff_tiles_batch(\n    images_folder=f\"{data_dir}/images\",\n    masks_file=f\"{data_dir}/masks1/naip_train_buildings.geojson\",\n    output_folder=\"output/advanced_example\",\n    tile_size=512,  # Larger tiles\n    stride=256,  # 50% overlap for better coverage\n    class_value_field=\"class\",  # Field containing class labels\n    buffer_radius=0.5,  # Add 0.5m buffer around buildings\n    skip_empty_tiles=True,  # Skip tiles with no features\n    all_touched=True,  # Include pixels touching features\n    max_tiles=10,  # Limit number of tiles per image\n    quiet=False,  # Show progress\n)\n\nprint(f\"\\nGenerated {stats['total_tiles']} tiles with 50% overlap\")\nprint(f\"Output structure:\")\nprint(f\"  - output/advanced_example/images/  (image tiles)\")\nprint(f\"  - output/advanced_example/masks/   (mask tiles)\")\n</pre> # Advanced example with custom parameters stats = geoai.export_geotiff_tiles_batch(     images_folder=f\"{data_dir}/images\",     masks_file=f\"{data_dir}/masks1/naip_train_buildings.geojson\",     output_folder=\"output/advanced_example\",     tile_size=512,  # Larger tiles     stride=256,  # 50% overlap for better coverage     class_value_field=\"class\",  # Field containing class labels     buffer_radius=0.5,  # Add 0.5m buffer around buildings     skip_empty_tiles=True,  # Skip tiles with no features     all_touched=True,  # Include pixels touching features     max_tiles=10,  # Limit number of tiles per image     quiet=False,  # Show progress )  print(f\"\\nGenerated {stats['total_tiles']} tiles with 50% overlap\") print(f\"Output structure:\") print(f\"  - output/advanced_example/images/  (image tiles)\") print(f\"  - output/advanced_example/masks/   (mask tiles)\") In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"output/method3_filename_match/images\",\n    labels_dir=f\"output/method3_filename_match/masks\",\n    output_dir=f\"output/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=20,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"output/method3_filename_match/images\",     labels_dir=f\"output/method3_filename_match/masks\",     output_dir=f\"output/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and building     batch_size=8,     num_epochs=20,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"output/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"output/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, )"},{"location":"examples/create_training_data/#creating-training-data-for-deep-learning","title":"Creating Training Data for Deep Learning\u00b6","text":"<p>This notebook demonstrates how to create training data (image and mask tiles) from georeferenced imagery and vector annotations using the improved <code>export_geotiff_tiles_batch</code> function.</p> <p>The function now supports three different input modes:</p> <ol> <li>Single vector file covering all images - Most efficient for large annotation files</li> <li>Multiple vector files matched by filename - Good for paired datasets</li> <li>Multiple vector files matched by sorted order - Good for sequential datasets</li> </ol>"},{"location":"examples/create_training_data/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/create_training_data/#setup","title":"Setup\u00b6","text":"<p>Import the required functions and check the sample data structure.</p>"},{"location":"examples/create_training_data/#download-sample-data","title":"Download Sample Data\u00b6","text":""},{"location":"examples/create_training_data/#explore-sample-data","title":"Explore Sample Data\u00b6","text":"<p>The sample data contains:</p> <ul> <li>images/: Two NAIP RGB image tiles</li> <li>masks1/: Single GeoJSON file with all building annotations</li> <li>masks2/: Separate GeoJSON files for each image tile</li> </ul>"},{"location":"examples/create_training_data/#visualize-sample-image-and-annotations","title":"Visualize Sample Image and Annotations\u00b6","text":"<p>Let's look at one of the images and its corresponding building annotations.</p>"},{"location":"examples/create_training_data/#method-1-single-vector-file-covering-all-images","title":"Method 1: Single Vector File Covering All Images\u00b6","text":"<p>This is the most efficient method when you have one large annotation file covering multiple image tiles. The function automatically:</p> <ul> <li>Loads the vector file once</li> <li>Spatially filters features for each image based on bounds</li> <li>Generates tiles only where features exist</li> </ul>"},{"location":"examples/create_training_data/#method-2-multiple-vector-files-matched-by-sorted-order","title":"Method 2: Multiple Vector Files Matched by Sorted Order\u00b6","text":"<p>This method pairs images and masks alphabetically by sorted order. The 1st image pairs with the 1st mask, 2nd with 2nd, etc.</p>"},{"location":"examples/create_training_data/#method-3-multiple-vector-files-matched-by-filename","title":"Method 3: Multiple Vector Files Matched by Filename\u00b6","text":"<p>This method pairs images and masks by matching their base filenames (e.g., <code>image1.tif</code> \u2192 <code>image1.geojson</code>).</p> <p>Note: This requires images and masks to have matching base names. The sample dataset doesn't have matching names, so this example creates a compatible structure first.</p>"},{"location":"examples/create_training_data/#visualize-generated-tiles","title":"Visualize Generated Tiles\u00b6","text":"<p>Let's look at some of the generated training tiles.</p>"},{"location":"examples/create_training_data/#advanced-usage-custom-parameters","title":"Advanced Usage: Custom Parameters\u00b6","text":"<p>The function supports many parameters for customization:</p>"},{"location":"examples/create_training_data/#train-a-segmentation-model","title":"Train a Segmentation Model\u00b6","text":""},{"location":"examples/create_training_data/#summary","title":"Summary\u00b6","text":"<p>The improved <code>export_geotiff_tiles_batch</code> function provides flexible options for creating training data:</p> Method Use Case Parameter Single vector file One annotation file covering all images <code>masks_file=\"path/to/file.geojson\"</code> Multiple files (by name) Paired files with matching names <code>masks_folder=\"path/to/masks\", match_by_name=True</code> Multiple files (by order) Paired files in sorted order <code>masks_folder=\"path/to/masks\", match_by_name=False</code> <p>Key Features:</p> <ul> <li>Supports both raster and vector masks</li> <li>Automatic CRS reprojection</li> <li>Spatial filtering for single mask files</li> <li>Configurable tile size, stride, and overlap</li> <li>Optional empty tile filtering</li> <li>Buffer support for vector annotations</li> <li>Detailed statistics reporting</li> </ul>"},{"location":"examples/create_vector/","title":"Create vector","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(style=\"liberty\")\nraster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nm.add_cog_layer(raster_url, name=\"NAIP\")\nm.add_layer_control()\nm.add_draw_control(\n    controls=[\"point\", \"polygon\", \"line_string\", \"trash\"], position=\"top-right\"\n)\n</pre> m = geoai.Map(style=\"liberty\") raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) m.add_cog_layer(raster_url, name=\"NAIP\") m.add_layer_control() m.add_draw_control(     controls=[\"point\", \"polygon\", \"line_string\", \"trash\"], position=\"top-right\" ) In\u00a0[\u00a0]: Copied! <pre>properties = {\n    \"Type\": [\"Residential\", \"Commercial\", \"Industrial\"],\n    \"Area\": 3000,\n    \"Name\": \"Building\",\n    \"City\": \"Seattle\",\n}\n</pre> properties = {     \"Type\": [\"Residential\", \"Commercial\", \"Industrial\"],     \"Area\": 3000,     \"Name\": \"Building\",     \"City\": \"Seattle\", } In\u00a0[\u00a0]: Copied! <pre>widget = geoai.create_vector_data(m, properties, file_ext=\"gpkg\")\nwidget\n</pre> widget = geoai.create_vector_data(m, properties, file_ext=\"gpkg\") widget"},{"location":"examples/create_vector/#create-vector-labels-for-training-deep-learning-models","title":"Create vector labels for training deep learning models\u00b6","text":""},{"location":"examples/create_vector/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/create_vector/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/create_vector/#add-sample-datasets","title":"Add sample datasets\u00b6","text":""},{"location":"examples/create_vector/#set-default-properties","title":"Set default properties\u00b6","text":""},{"location":"examples/create_vector/#display-the-interactive-widget","title":"Display the interactive widget\u00b6","text":""},{"location":"examples/data_augmentation/","title":"Data augmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install -U geoai\n</pre> # %pip install -U geoai In\u00a0[\u00a0]: Copied! <pre>import os\nimport geoai\nimport numpy as np\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n</pre> import os import geoai import numpy as np import rasterio import matplotlib.pyplot as plt from pathlib import Path In\u00a0[\u00a0]: Copied! <pre># Download NAIP imagery and building footprints\ntrain_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n\nsample_image = geoai.download_file(train_raster_url)\nsample_vector = geoai.download_file(train_vector_url)\n\nprint(f\"Downloaded sample image: {sample_image}\")\nprint(f\"Downloaded sample labels: {sample_vector}\")\n</pre> # Download NAIP imagery and building footprints train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"  sample_image = geoai.download_file(train_raster_url) sample_vector = geoai.download_file(train_vector_url)  print(f\"Downloaded sample image: {sample_image}\") print(f\"Downloaded sample labels: {sample_vector}\") In\u00a0[\u00a0]: Copied! <pre>print(\"Image information:\")\ngeoai.get_raster_info(sample_image)\n</pre> print(\"Image information:\") geoai.get_raster_info(sample_image) In\u00a0[\u00a0]: Copied! <pre># Visualize on interactive map\ngeoai.view_vector_interactive(sample_vector, tiles=sample_image)\n</pre> # Visualize on interactive map geoai.view_vector_interactive(sample_vector, tiles=sample_image) In\u00a0[\u00a0]: Copied! <pre># Create output directory\noutput_dir = \"buildings_augmentation_demo\"\n\n# Export without augmentation\noutput_no_aug = f\"{output_dir}/tiles_no_augmentation\"\n\ngeoai.export_geotiff_tiles(\n    sample_image,\n    output_no_aug,\n    in_class_data=sample_vector,\n    tile_size=256,\n    stride=128,\n    apply_augmentation=False,  # No augmentation\n)\n\n# Count tiles\nimage_tiles = list(Path(output_no_aug, \"images\").glob(\"*.tif\"))\nlabel_tiles = list(Path(output_no_aug, \"labels\").glob(\"*.tif\"))\n\nprint(f\"\\nWithout augmentation:\")\nprint(f\"  Image tiles: {len(image_tiles)}\")\nprint(f\"  Label tiles: {len(label_tiles)}\")\n</pre> # Create output directory output_dir = \"buildings_augmentation_demo\"  # Export without augmentation output_no_aug = f\"{output_dir}/tiles_no_augmentation\"  geoai.export_geotiff_tiles(     sample_image,     output_no_aug,     in_class_data=sample_vector,     tile_size=256,     stride=128,     apply_augmentation=False,  # No augmentation )  # Count tiles image_tiles = list(Path(output_no_aug, \"images\").glob(\"*.tif\")) label_tiles = list(Path(output_no_aug, \"labels\").glob(\"*.tif\"))  print(f\"\\nWithout augmentation:\") print(f\"  Image tiles: {len(image_tiles)}\") print(f\"  Label tiles: {len(label_tiles)}\") In\u00a0[\u00a0]: Copied! <pre># Export with default augmentation - generate 3 augmented versions per tile\noutput_with_aug = f\"{output_dir}/tiles_with_augmentation\"\n\ngeoai.export_geotiff_tiles(\n    sample_image,\n    output_with_aug,\n    in_class_data=sample_vector,\n    tile_size=256,\n    stride=128,\n    apply_augmentation=True,  # Enable augmentation\n    augmentation_count=3,  # Generate 3 augmented versions per tile\n)\n\n# Count tiles\naug_image_tiles = list(Path(output_with_aug, \"images\").glob(\"*.tif\"))\naug_label_tiles = list(Path(output_with_aug, \"labels\").glob(\"*.tif\"))\n\nprint(f\"\\nWith augmentation (3 per tile):\")\nprint(f\"  Image tiles: {len(aug_image_tiles)} (original + augmented)\")\nprint(f\"  Label tiles: {len(aug_label_tiles)} (original + augmented)\")\nprint(f\"  \\nThis is {len(aug_image_tiles) / len(image_tiles):.1f}x more training data!\")\n</pre> # Export with default augmentation - generate 3 augmented versions per tile output_with_aug = f\"{output_dir}/tiles_with_augmentation\"  geoai.export_geotiff_tiles(     sample_image,     output_with_aug,     in_class_data=sample_vector,     tile_size=256,     stride=128,     apply_augmentation=True,  # Enable augmentation     augmentation_count=3,  # Generate 3 augmented versions per tile )  # Count tiles aug_image_tiles = list(Path(output_with_aug, \"images\").glob(\"*.tif\")) aug_label_tiles = list(Path(output_with_aug, \"labels\").glob(\"*.tif\"))  print(f\"\\nWith augmentation (3 per tile):\") print(f\"  Image tiles: {len(aug_image_tiles)} (original + augmented)\") print(f\"  Label tiles: {len(aug_label_tiles)} (original + augmented)\") print(f\"  \\nThis is {len(aug_image_tiles) / len(image_tiles):.1f}x more training data!\") In\u00a0[\u00a0]: Copied! <pre># Load original tile and some augmented versions\ntile_files = sorted([f for f in Path(output_with_aug, \"images\").glob(\"*.tif\")])\nlabel_files = sorted([f for f in Path(output_with_aug, \"labels\").glob(\"*.tif\")])\n\n# Get first 4 tiles (1 original + 3 augmented)\nn_display = min(4, len(tile_files))\n\nfig, axes = plt.subplots(2, n_display, figsize=(15, 8))\nif n_display == 1:\n    axes = axes.reshape(2, 1)\n\nfor i in range(n_display):\n    # Load and display image tile\n    with rasterio.open(tile_files[i]) as src:\n        img = src.read()  # Read all bands\n        # If RGB, display as color\n        if img.shape[0] &gt;= 3:\n            img_display = np.transpose(img[:3], (1, 2, 0))\n            axes[0, i].imshow(img_display)\n        else:\n            axes[0, i].imshow(img[0], cmap=\"gray\")\n\n        title = \"Original\" if i == 0 else f\"Augmented {i}\"\n        axes[0, i].set_title(f\"{title}\\n{tile_files[i].name}\")\n        axes[0, i].axis(\"off\")\n\n    # Load and display label tile\n    with rasterio.open(label_files[i]) as src:\n        label = src.read(1)\n        axes[1, i].imshow(label, cmap=\"tab10\", vmin=0, vmax=10)\n        axes[1, i].set_title(f\"Label\\n{label_files[i].name}\")\n        axes[1, i].axis(\"off\")\n\nfig.suptitle(\"Original Tile vs Augmented Versions\", fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\n    \"Notice how the augmented tiles have different orientations, colors, and brightness\"\n)\nprint(\"while the labels are transformed consistently with the images.\")\n</pre> # Load original tile and some augmented versions tile_files = sorted([f for f in Path(output_with_aug, \"images\").glob(\"*.tif\")]) label_files = sorted([f for f in Path(output_with_aug, \"labels\").glob(\"*.tif\")])  # Get first 4 tiles (1 original + 3 augmented) n_display = min(4, len(tile_files))  fig, axes = plt.subplots(2, n_display, figsize=(15, 8)) if n_display == 1:     axes = axes.reshape(2, 1)  for i in range(n_display):     # Load and display image tile     with rasterio.open(tile_files[i]) as src:         img = src.read()  # Read all bands         # If RGB, display as color         if img.shape[0] &gt;= 3:             img_display = np.transpose(img[:3], (1, 2, 0))             axes[0, i].imshow(img_display)         else:             axes[0, i].imshow(img[0], cmap=\"gray\")          title = \"Original\" if i == 0 else f\"Augmented {i}\"         axes[0, i].set_title(f\"{title}\\n{tile_files[i].name}\")         axes[0, i].axis(\"off\")      # Load and display label tile     with rasterio.open(label_files[i]) as src:         label = src.read(1)         axes[1, i].imshow(label, cmap=\"tab10\", vmin=0, vmax=10)         axes[1, i].set_title(f\"Label\\n{label_files[i].name}\")         axes[1, i].axis(\"off\")  fig.suptitle(\"Original Tile vs Augmented Versions\", fontsize=14, y=1.02) plt.tight_layout() plt.show()  print(     \"Notice how the augmented tiles have different orientations, colors, and brightness\" ) print(\"while the labels are transformed consistently with the images.\") In\u00a0[\u00a0]: Copied! <pre>import albumentations as A\n\n# Define custom augmentation pipeline\ncustom_transforms = A.Compose(\n    [\n        A.HorizontalFlip(p=1.0),  # Always flip horizontally\n        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n        A.HueSaturationValue(\n            hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5\n        ),\n    ]\n)\n\n# Export with custom augmentation\noutput_custom_aug = f\"{output_dir}/tiles_custom_augmentation\"\n\ngeoai.export_geotiff_tiles(\n    sample_image,\n    output_custom_aug,\n    in_class_data=sample_vector,\n    tile_size=256,\n    stride=128,\n    apply_augmentation=True,\n    augmentation_count=2,\n    augmentation_transforms=custom_transforms,  # Use custom transforms\n)\n\ncustom_image_tiles = list(Path(output_custom_aug, \"images\").glob(\"*.tif\"))\nprint(f\"\\nWith custom augmentation (2 per tile):\")\nprint(f\"  Image tiles: {len(custom_image_tiles)}\")\n</pre> import albumentations as A  # Define custom augmentation pipeline custom_transforms = A.Compose(     [         A.HorizontalFlip(p=1.0),  # Always flip horizontally         A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),         A.HueSaturationValue(             hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5         ),     ] )  # Export with custom augmentation output_custom_aug = f\"{output_dir}/tiles_custom_augmentation\"  geoai.export_geotiff_tiles(     sample_image,     output_custom_aug,     in_class_data=sample_vector,     tile_size=256,     stride=128,     apply_augmentation=True,     augmentation_count=2,     augmentation_transforms=custom_transforms,  # Use custom transforms )  custom_image_tiles = list(Path(output_custom_aug, \"images\").glob(\"*.tif\")) print(f\"\\nWith custom augmentation (2 per tile):\") print(f\"  Image tiles: {len(custom_image_tiles)}\") In\u00a0[\u00a0]: Copied! <pre>from geoai.utils import get_default_augmentation_transforms\n\n# Get default transforms\ndefault_transforms = get_default_augmentation_transforms(\n    tile_size=256, include_normalize=False  # Don't normalize for visualization\n)\n\nprint(\"Default augmentation pipeline:\")\nprint(default_transforms)\n\n# Apply to a sample image\nwith rasterio.open(tile_files[0]) as src:\n    original_img = src.read()\n    # Convert to HWC format for albumentations\n    img_hwc = np.transpose(original_img, (1, 2, 0))\n\n# Apply augmentation\naugmented = default_transforms(image=img_hwc)\naug_img = augmented[\"image\"]\n\n# Visualize\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].imshow(img_hwc)\naxes[0].set_title(\"Original\")\naxes[0].axis(\"off\")\n\naxes[1].imshow(aug_img)\naxes[1].set_title(\"Augmented (random transform)\")\naxes[1].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> from geoai.utils import get_default_augmentation_transforms  # Get default transforms default_transforms = get_default_augmentation_transforms(     tile_size=256, include_normalize=False  # Don't normalize for visualization )  print(\"Default augmentation pipeline:\") print(default_transforms)  # Apply to a sample image with rasterio.open(tile_files[0]) as src:     original_img = src.read()     # Convert to HWC format for albumentations     img_hwc = np.transpose(original_img, (1, 2, 0))  # Apply augmentation augmented = default_transforms(image=img_hwc) aug_img = augmented[\"image\"]  # Visualize fig, axes = plt.subplots(1, 2, figsize=(10, 5)) axes[0].imshow(img_hwc) axes[0].set_title(\"Original\") axes[0].axis(\"off\")  axes[1].imshow(aug_img) axes[1].set_title(\"Augmented (random transform)\") axes[1].axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>from geoai.train import train_segmentation_model\n\n# Train with enhanced default augmentation\nmodel = train_segmentation_model(\n    images_dir=f\"{output_dir}/tiles_with_augmentation/images\",\n    labels_dir=f\"{output_dir}/tiles_with_augmentation/labels\",\n    output_dir=f\"{output_dir}/tiles_with_augmentation/training_output\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_classes=4,\n    batch_size=8,\n    num_epochs=20,\n    # The following default augmentations are applied automatically:\n    # - Horizontal flips (50%)\n    # - Vertical flips (50%)\n    # - Random 90\u00b0 rotations (50%)\n    # - Brightness adjustment (50%)\n    # - Contrast adjustment (50%)\n)\n</pre> from geoai.train import train_segmentation_model  # Train with enhanced default augmentation model = train_segmentation_model(     images_dir=f\"{output_dir}/tiles_with_augmentation/images\",     labels_dir=f\"{output_dir}/tiles_with_augmentation/labels\",     output_dir=f\"{output_dir}/tiles_with_augmentation/training_output\",     architecture=\"unet\",     encoder_name=\"resnet34\",     num_classes=4,     batch_size=8,     num_epochs=20,     # The following default augmentations are applied automatically:     # - Horizontal flips (50%)     # - Vertical flips (50%)     # - Random 90\u00b0 rotations (50%)     # - Brightness adjustment (50%)     # - Contrast adjustment (50%) ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{output_dir}/tiles_with_augmentation/training_output/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{output_dir}/tiles_with_augmentation/training_output/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>from geoai.train import (\n    train_segmentation_model,\n    SemanticTransforms,\n    SemanticRandomHorizontalFlip,\n    SemanticToTensor,\n)\n\n# Define custom training transforms\ncustom_train_transforms = SemanticTransforms(\n    [\n        SemanticToTensor(),\n        SemanticRandomHorizontalFlip(0.5),\n        # Add more custom transforms here...\n    ]\n)\n\n# Train with custom augmentation\nmodel = train_segmentation_model(\n    images_dir=f\"{output_dir}/tiles_no_augmentation/images\",\n    labels_dir=f\"{output_dir}/tiles_no_augmentation/labels\",\n    output_dir=f\"{output_dir}/tiles_no_augmentation/training_output\",\n    architecture=\"unet\",\n    num_classes=4,\n    train_transforms=custom_train_transforms,  # Use custom transforms\n    num_epochs=20,\n)\n</pre> from geoai.train import (     train_segmentation_model,     SemanticTransforms,     SemanticRandomHorizontalFlip,     SemanticToTensor, )  # Define custom training transforms custom_train_transforms = SemanticTransforms(     [         SemanticToTensor(),         SemanticRandomHorizontalFlip(0.5),         # Add more custom transforms here...     ] )  # Train with custom augmentation model = train_segmentation_model(     images_dir=f\"{output_dir}/tiles_no_augmentation/images\",     labels_dir=f\"{output_dir}/tiles_no_augmentation/labels\",     output_dir=f\"{output_dir}/tiles_no_augmentation/training_output\",     architecture=\"unet\",     num_classes=4,     train_transforms=custom_train_transforms,  # Use custom transforms     num_epochs=20, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{output_dir}/tiles_no_augmentation/training_output/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{output_dir}/tiles_no_augmentation/training_output/training_history.pth\",     figsize=(15, 5),     verbose=True, )"},{"location":"examples/data_augmentation/#data-augmentation-for-geospatial-training","title":"Data Augmentation for Geospatial Training\u00b6","text":"<p>This notebook demonstrates how to use data augmentation when preparing training tiles and training segmentation models. Data augmentation helps improve model generalization by creating variations of training data through transformations like flips, rotations, and photometric adjustments.</p>"},{"location":"examples/data_augmentation/#key-features","title":"Key Features\u00b6","text":"<ul> <li>Tile Export with Augmentation: Generate augmented versions of tiles during export</li> <li>Default Augmentation Transforms: Use pre-configured transforms optimized for remote sensing</li> <li>Custom Augmentation: Define your own augmentation pipeline</li> <li>Enhanced Training Defaults: Improved default augmentations for segmentation model training</li> </ul>"},{"location":"examples/data_augmentation/#install-package","title":"Install package\u00b6","text":"<p>Uncomment the following line to install the geoai package if needed:</p>"},{"location":"examples/data_augmentation/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/data_augmentation/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same NAIP imagery and building footprint dataset as used in the train_segmentation_model.ipynb example. This is real aerial imagery with building annotations.</p>"},{"location":"examples/data_augmentation/#visualize-sample-data","title":"Visualize sample data\u00b6","text":"<p>Let's visualize the NAIP imagery and building footprints.</p>"},{"location":"examples/data_augmentation/#part-1-export-tiles-without-augmentation-baseline","title":"Part 1: Export Tiles WITHOUT Augmentation (Baseline)\u00b6","text":"<p>First, let's export tiles without augmentation to establish a baseline.</p>"},{"location":"examples/data_augmentation/#part-2-export-tiles-with-default-augmentation","title":"Part 2: Export Tiles WITH Default Augmentation\u00b6","text":"<p>Now let's export tiles with default augmentation. The <code>get_default_augmentation_transforms()</code> function provides sensible defaults for remote sensing data:</p> <p>Geometric Transforms:</p> <ul> <li>Horizontal/Vertical Flips (50% probability each)</li> <li>Random 90\u00b0 Rotations (50% probability)</li> <li>Shift-Scale-Rotate (50% probability)</li> </ul> <p>Photometric Transforms:</p> <ul> <li>Random Brightness/Contrast (50% probability)</li> <li>HSV Color Adjustments (30% probability)</li> <li>Gaussian Noise (20% probability)</li> <li>Gaussian Blur (20% probability)</li> </ul>"},{"location":"examples/data_augmentation/#visualize-original-vs-augmented-tiles","title":"Visualize Original vs Augmented Tiles\u00b6","text":"<p>Let's compare an original tile with its augmented versions to see the transformations.</p>"},{"location":"examples/data_augmentation/#part-3-custom-augmentation-pipeline","title":"Part 3: Custom Augmentation Pipeline\u00b6","text":"<p>You can also define your own custom augmentation transforms using albumentations. This is useful when you want specific augmentations for your use case.</p>"},{"location":"examples/data_augmentation/#part-4-using-get_default_augmentation_transforms","title":"Part 4: Using get_default_augmentation_transforms()\u00b6","text":"<p>You can also access the default augmentation transforms directly to use in your own workflows.</p>"},{"location":"examples/data_augmentation/#part-5-training-with-enhanced-default-augmentation","title":"Part 5: Training with Enhanced Default Augmentation\u00b6","text":"<p>The <code>train_segmentation_model()</code> function now uses improved default augmentations that include:</p> <ul> <li>Horizontal and vertical flips</li> <li>Random 90\u00b0 rotations</li> <li>Brightness and contrast adjustments</li> </ul> <p>Here's an example of how you would train with these defaults (we won't actually run training in this demo):</p>"},{"location":"examples/data_augmentation/#part-6-custom-training-transforms","title":"Part 6: Custom Training Transforms\u00b6","text":"<p>If you want to use custom augmentations during training, you can define your own transform functions and pass them to <code>train_segmentation_model()</code>.</p>"},{"location":"examples/data_augmentation/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated:</p> <ol> <li>Tile Export with Augmentation: Using <code>export_geotiff_tiles()</code> with <code>apply_augmentation=True</code> to generate augmented training data</li> <li>Default Augmentation: Using <code>get_default_augmentation_transforms()</code> for sensible defaults optimized for remote sensing</li> <li>Custom Augmentation: Defining custom augmentation pipelines with albumentations</li> <li>Training with Augmentation: How the enhanced defaults work in <code>train_segmentation_model()</code></li> </ol>"},{"location":"examples/data_augmentation/#key-benefits-of-data-augmentation","title":"Key Benefits of Data Augmentation:\u00b6","text":"<ul> <li>More Training Data: Generate 2-5x more training samples from existing data</li> <li>Better Generalization: Models learn to handle variations in orientation, lighting, and appearance</li> <li>Reduced Overfitting: More diverse training data helps prevent memorization</li> <li>Improved Accuracy: Typically results in 2-5% better validation accuracy</li> </ul>"},{"location":"examples/data_augmentation/#best-practices","title":"Best Practices:\u00b6","text":"<ol> <li>Start with default augmentation - it works well for most remote sensing tasks</li> <li>Use 2-5 augmented versions per tile (more isn't always better)</li> <li>Ensure augmentations match your domain (e.g., avoid vertical flips if imagery has a consistent \"up\" direction)</li> <li>Monitor validation performance to ensure augmentations help rather than hurt</li> </ol> <p>For more information, see the geoai documentation.</p>"},{"location":"examples/data_visualization/","title":"Data visualization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>from torchgeo.datasets import NAIP\nfrom torchgeo.samplers import RandomGeoSampler, GridGeoSampler\nfrom geoai.utils import view_image, view_raster, dict_to_image\nfrom geoai.download import download_naip\n</pre> from torchgeo.datasets import NAIP from torchgeo.samplers import RandomGeoSampler, GridGeoSampler from geoai.utils import view_image, view_raster, dict_to_image from geoai.download import download_naip In\u00a0[\u00a0]: Copied! <pre>root = \"naip_data\"\n</pre> root = \"naip_data\" In\u00a0[\u00a0]: Copied! <pre>bbox = (-117.6029, 47.65, -117.5936, 47.6563)\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=root,\n    max_items=1,\n)\n</pre> bbox = (-117.6029, 47.65, -117.5936, 47.6563) downloaded_files = download_naip(     bbox=bbox,     output_dir=root,     max_items=1, ) <ul> <li>torchgeo.datasets.NAIP: Provides access to the National Agriculture Imagery Program (NAIP) dataset, which offers high-resolution aerial imagery across the United States.</li> <li>torchgeo.samplers: Contains sampling strategies for geospatial data:<ul> <li>RandomGeoSampler: Samples random patches from the dataset</li> <li>GridGeoSampler: Samples patches in a grid pattern with specified stride</li> </ul> </li> <li>geoai.utils: Custom utility functions for visualization:<ul> <li>view_image: Visualizes tensor images</li> <li>view_raster: Displays georeferenced data on an interactive map</li> <li>dict_to_image: Converts dictionary representation to image format</li> </ul> </li> </ul> <p>Load the NAIP dataset from the specified root directory:</p> In\u00a0[\u00a0]: Copied! <pre>dataset = NAIP(root)\n</pre> dataset = NAIP(root) <p>Examine the dataset object to understand its properties:</p> In\u00a0[\u00a0]: Copied! <pre>dataset\n</pre> dataset <p>This will display information about the NAIP dataset including available imagery dates, coverage area, and other metadata.</p> <p>Check the Coordinate Reference System (CRS) used by the dataset:</p> In\u00a0[\u00a0]: Copied! <pre>dataset.crs\n</pre> dataset.crs <p>The CRS defines how the geospatial data is projected onto a coordinate system, which is essential for accurate visualization and analysis.</p> In\u00a0[\u00a0]: Copied! <pre>train_sampler = RandomGeoSampler(dataset, size=256, length=1000)\n</pre> train_sampler = RandomGeoSampler(dataset, size=256, length=1000) <p>This creates a sampler that will randomly select 1000 patches, each 256x256 pixels in size. This sampling strategy is commonly used for training machine learning models where you need a diverse set of examples.</p> <p>Extract a bounding box from the random sampler:</p> In\u00a0[\u00a0]: Copied! <pre>train_bbox = next(iter(train_sampler))\ntrain_bbox\n</pre> train_bbox = next(iter(train_sampler)) train_bbox <p>The bounding box contains the coordinates defining the spatial extent of our randomly sampled patch.</p> <p>Load the actual image data corresponding to the randomly selected bounding box:</p> In\u00a0[\u00a0]: Copied! <pre>train_image = dataset[next(iter(train_sampler))][\"image\"]\n</pre> train_image = dataset[next(iter(train_sampler))][\"image\"] <p>Examine the complete data dictionary returned for a sample, which includes both the image and metadata:</p> In\u00a0[\u00a0]: Copied! <pre>dataset[next(iter(train_sampler))]\n</pre> dataset[next(iter(train_sampler))] <p>This returns a dictionary containing the image tensor and associated metadata such as the bounding box, CRS, and other properties.</p> In\u00a0[\u00a0]: Copied! <pre>view_image(\n    train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\"\n)\n</pre> view_image(     train_image, transpose=True, scale_factor=(1 / 250), title=\"Random GeoSampler\" ) <ul> <li>transpose=True: Rearranges the dimensions for proper display (from [C,H,W] to [H,W,C])</li> <li>scale_factor=(1/250): Scales the pixel values for better visualization</li> <li>title=\"Random GeoSampler\": Adds a descriptive title to the plot</li> </ul> In\u00a0[\u00a0]: Copied! <pre>test_sampler = GridGeoSampler(dataset, size=256, stride=128)\n</pre> test_sampler = GridGeoSampler(dataset, size=256, stride=128) <p>This sampler extracts 256x256 pixel patches in a grid pattern with a stride of 128 pixels, meaning patches will overlap by 128 pixels. Grid sampling is typically used for inference or testing, where systematic coverage of the area is important.</p> <p>Extract a bounding box from the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_bbox = next(iter(test_sampler))\ntest_bbox\n</pre> test_bbox = next(iter(test_sampler)) test_bbox <p>Load the image data for a patch selected by the grid sampler:</p> In\u00a0[\u00a0]: Copied! <pre>test_image = dataset[next(iter(test_sampler))][\"image\"]\n</pre> test_image = dataset[next(iter(test_sampler))][\"image\"] In\u00a0[\u00a0]: Copied! <pre>view_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\")\n</pre> view_image(test_image, transpose=True, scale_factor=(1 / 250), title=\"Grid GeoSampler\") <p>The visualization shows a systematically sampled patch from the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>data = dataset[next(iter(test_sampler))]\n</pre> data = dataset[next(iter(test_sampler))] <p>Visualize the raster data on an interactive map with Esri.WorldImagery imagery as the background:</p> In\u00a0[\u00a0]: Copied! <pre>view_raster(data, basemap=\"Esri.WorldImagery\")\n</pre> view_raster(data, basemap=\"Esri.WorldImagery\") <p>This interactive visualization places the sampled data in its real-world geographic context, allowing you to see how it aligns with the Esri.WorldImagery imagery.</p>"},{"location":"examples/data_visualization/#data-visualization","title":"Data visualization\u00b6","text":"<p>This notebook demonstrates how to work with geospatial imagery data using TorchGeo and GeoAI. We'll explore how to load data, sample it using different strategies, and visualize the results.</p>"},{"location":"examples/data_visualization/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/data_visualization/#importing-required-libraries","title":"Importing Required Libraries\u00b6","text":"<p>First, we import the necessary libraries for our geospatial data visualization workflow:</p>"},{"location":"examples/data_visualization/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/data_visualization/#setting-up-the-dataset","title":"Setting Up the Dataset\u00b6","text":""},{"location":"examples/data_visualization/#random-sampling-of-geospatial-data","title":"Random Sampling of Geospatial Data\u00b6","text":"<p>Create a random sampler to extract patches from the dataset:</p>"},{"location":"examples/data_visualization/#visualizing-randomly-sampled-data","title":"Visualizing Randomly Sampled Data\u00b6","text":"<p>Display the randomly sampled image:</p>"},{"location":"examples/data_visualization/#grid-sampling-of-geospatial-data","title":"Grid Sampling of Geospatial Data\u00b6","text":"<p>Create a grid sampler to extract patches in a systematic pattern:</p>"},{"location":"examples/data_visualization/#visualizing-grid-sampled-data","title":"Visualizing Grid Sampled Data\u00b6","text":"<p>Display the image from the grid sampler:</p>"},{"location":"examples/data_visualization/#advanced-visualization-with-geospatial-context","title":"Advanced Visualization with Geospatial Context\u00b6","text":"<p>Load a complete data sample including all metadata:</p>"},{"location":"examples/data_visualization/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ol> <li>TorchGeo provides a flexible framework for working with geospatial datasets like NAIP.</li> <li>Different sampling strategies (random vs. grid) serve different purposes in geospatial machine learning workflows.</li> <li>Visualization tools help understand the data in both pixel space (view_image) and geographic space (view_raster).</li> <li>Working with geospatial data requires attention to coordinate reference systems (CRS) and proper handling of georeferenced data.</li> </ol>"},{"location":"examples/download_data/","title":"Download data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom geoai.download import (\n    download_naip,\n    download_overture_buildings,\n    extract_building_stats,\n)\n</pre> import leafmap from geoai.download import (     download_naip,     download_overture_buildings,     extract_building_stats, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[47.6526, -117.5923], zoom=16)\nm.add_basemap(\"Esri.WorldImagery\")\nm\n</pre> m = leafmap.Map(center=[47.6526, -117.5923], zoom=16) m.add_basemap(\"Esri.WorldImagery\") m <p>Use the drawing tools to draw a rectangle on the map. If no rectangle is drawn, the default ROI will be used.</p> In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = (-117.6029, 47.65, -117.5936, 47.6563)\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = (-117.6029, 47.65, -117.5936, 47.6563) In\u00a0[\u00a0]: Copied! <pre># Download NAIP imagery for the specified region\ndownloaded_files = download_naip(\n    bbox=bbox,\n    output_dir=\"naip_data\",\n    max_items=1,\n    # year=2020,\n)\n\nprint(f\"Downloaded {len(downloaded_files)} files.\")\n</pre> # Download NAIP imagery for the specified region downloaded_files = download_naip(     bbox=bbox,     output_dir=\"naip_data\",     max_items=1,     # year=2020, )  print(f\"Downloaded {len(downloaded_files)} files.\") In\u00a0[\u00a0]: Copied! <pre># Download buildings\ndata_file = download_overture_buildings(\n    bbox=bbox,\n    output=\"buildings.geojson\",\n)\n</pre> # Download buildings data_file = download_overture_buildings(     bbox=bbox,     output=\"buildings.geojson\", ) In\u00a0[\u00a0]: Copied! <pre>stats = extract_building_stats(data_file)\nprint(stats)\n</pre> stats = extract_building_stats(data_file) print(stats) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"naip_data/m_4711720_sw_11_060_20230701_20230911.tif\", layer_name=\"NAIP\")\nm.add_geojson(\"buildings.geojson\", layer_name=\"Buildings\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"naip_data/m_4711720_sw_11_060_20230701_20230911.tif\", layer_name=\"NAIP\") m.add_geojson(\"buildings.geojson\", layer_name=\"Buildings\") m <p></p>"},{"location":"examples/download_data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/download_data/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/download_data/#import-libraries","title":"Import Libraries\u00b6","text":"<p>These modules allow downloading NAIP imagery and extracting building data statistics.</p>"},{"location":"examples/download_data/#define-bounding-box","title":"Define Bounding Box\u00b6","text":"<p>Define the geographic extent (longitude and latitude) for data downloads.</p>"},{"location":"examples/download_data/#download-naip-imagery","title":"Download NAIP Imagery\u00b6","text":"<p>Fetch NAIP aerial imagery for the specified bounding box. The <code>max_items</code> parameter limits the number of downloaded files.</p>"},{"location":"examples/download_data/#download-building-data","title":"Download Building Data\u00b6","text":"<p>Retrieve building footprint data in GeoJSON format within the bounding box. The <code>verbose</code> flag provides detailed output.</p>"},{"location":"examples/download_data/#extract-building-statistics","title":"Extract Building Statistics\u00b6","text":"<p>If the building data file is successfully downloaded, extract and display relevant statistics such as area, count, and footprint details.</p>"},{"location":"examples/download_data/#visualize-datasets","title":"Visualize Datasets\u00b6","text":""},{"location":"examples/download_naip/","title":"Download naip","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap(center=[47.031260, -99.156360], zoom=14)\nm.add_basemap(\"Esri.WorldImagery\")\nm\n</pre> m = geoai.LeafMap(center=[47.031260, -99.156360], zoom=14) m.add_basemap(\"Esri.WorldImagery\") m <p>Use the drawing tool to select an area of interest (AOI) on the map. The selected area will be used to search for NAIP imagery.</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-99.1705, 47.0149, -99.1296, 47.0365]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-99.1705, 47.0149, -99.1296, 47.0365] In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=bbox,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=bbox, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_items(items=items)\n</pre> geoai.view_pc_items(items=items) In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"])\n</pre> geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"])"},{"location":"examples/download_naip/#download-naip-imagery","title":"Download NAIP Imagery\u00b6","text":""},{"location":"examples/download_naip/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/download_naip/#import-library","title":"Import library\u00b6","text":""},{"location":"examples/download_naip/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/download_naip/#search-for-naip-imagery","title":"Search for NAIP imagery\u00b6","text":""},{"location":"examples/download_naip/#visualize-the-search-results","title":"Visualize the search results\u00b6","text":""},{"location":"examples/download_naip/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/download_sentinel2/","title":"Download sentinel2","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre># Example for Sentinel-2 item\nitem_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2B_MSIL2A_20250228T173149_R055_T14SLH_20250228T212633\"\n</pre> # Example for Sentinel-2 item item_url = \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-2-l2a/items/S2B_MSIL2A_20250228T173149_R055_T14SLH_20250228T212633\" In\u00a0[\u00a0]: Copied! <pre># Specify which bands to download (Sentinel-2 bands)\nbands_to_download = [\n    \"B01\",\n    \"B02\",\n    \"B03\",\n    \"B04\",\n    \"B05\",\n    \"B06\",\n    \"B07\",\n    \"B08\",\n    \"B8A\",\n    \"B09\",\n    \"B11\",\n    \"B12\",\n    \"AOT\",\n    \"WVP\",\n    \"SCL\",\n]\n</pre> # Specify which bands to download (Sentinel-2 bands) bands_to_download = [     \"B01\",     \"B02\",     \"B03\",     \"B04\",     \"B05\",     \"B06\",     \"B07\",     \"B08\",     \"B8A\",     \"B09\",     \"B11\",     \"B12\",     \"AOT\",     \"WVP\",     \"SCL\", ] In\u00a0[\u00a0]: Copied! <pre># Create a directory for the downloaded bands\noutput_directory = \"sentinel2_bands\"\n</pre> # Create a directory for the downloaded bands output_directory = \"sentinel2_bands\" In\u00a0[\u00a0]: Copied! <pre># Download the bands, save them to the output directory, and create a merged GeoTIFF.\n# The download process may take a while. Please be patient.\ndownloaded_bands = geoai.download_pc_stac_item(\n    item_url=item_url,\n    bands=bands_to_download,\n    output_dir=output_directory,\n    show_progress=True,\n    merge_bands=True,\n    merged_filename=\"sentinel2_all_bands.tif\",\n    overwrite=False,  # Skip files that already exist\n    cell_size=10,  # Resample all bands to 10m resolution\n)\n</pre> # Download the bands, save them to the output directory, and create a merged GeoTIFF. # The download process may take a while. Please be patient. downloaded_bands = geoai.download_pc_stac_item(     item_url=item_url,     bands=bands_to_download,     output_dir=output_directory,     show_progress=True,     merge_bands=True,     merged_filename=\"sentinel2_all_bands.tif\",     overwrite=False,  # Skip files that already exist     cell_size=10,  # Resample all bands to 10m resolution ) In\u00a0[\u00a0]: Copied! <pre># Print the paths to the downloaded files\nfor band, path in downloaded_bands.items():\n    print(f\"Downloaded {band}: {path}\")\n</pre> # Print the paths to the downloaded files for band, path in downloaded_bands.items():     print(f\"Downloaded {band}: {path}\") In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/sentinel2a_kansas.tif\"\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/sentinel2a_kansas.tif\" In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url, bidx=[8, 4, 3], rescale=\"1000,4000\")\n</pre> geoai.view_raster(raster_url, bidx=[8, 4, 3], rescale=\"1000,4000\")"},{"location":"examples/download_sentinel2/#download-sentinel-2-data-from-planetary-computer","title":"Download Sentinel-2 Data from Planetary Computer\u00b6","text":"<p>The notebook demonstrates how to download Sentinel-2 data from the Planetary Computer using the <code>geoai</code> package.</p>"},{"location":"examples/download_sentinel2/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/download_sentinel2/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/download_sentinel2/#search-data","title":"Search data\u00b6","text":"<p>Go to the Planetary Computer and search for the data you are interested in. Copy the STAC item link and paste it to the code below.</p>"},{"location":"examples/download_sentinel2/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/download_sentinel2/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/edit_vector/","title":"Edit vector","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map()\nm.add_basemap(\"Esri.WorldImagery\")\nurl = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> m = geoai.Map() m.add_basemap(\"Esri.WorldImagery\") url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>properties = {\n    \"class\": [\"apartments\", \"terrace\", \"detached\", \"house\", \"shed\", None],\n    \"height\": 0.0,\n}\n</pre> properties = {     \"class\": [\"apartments\", \"terrace\", \"detached\", \"house\", \"shed\", None],     \"height\": 0.0, } In\u00a0[\u00a0]: Copied! <pre>widget = geoai.edit_vector_data(m, url, properties=properties)\nm.add_layer_control()\nwidget\n</pre> widget = geoai.edit_vector_data(m, url, properties=properties) m.add_layer_control() widget"},{"location":"examples/edit_vector/#edit-vector-labels-for-training-deep-learning-models","title":"Edit vector labels for training deep learning models\u00b6","text":""},{"location":"examples/edit_vector/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/edit_vector/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/edit_vector/#add-sample-datasets","title":"Add sample datasets\u00b6","text":""},{"location":"examples/edit_vector/#set-default-properties","title":"Set default properties\u00b6","text":""},{"location":"examples/edit_vector/#display-the-interactive-widget","title":"Display the interactive widget\u00b6","text":""},{"location":"examples/export_training_data_formats/","title":"Export training data formats","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport json\nfrom pathlib import Path\n</pre> import geoai import json from pathlib import Path In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(train_raster_path)\n</pre> geoai.get_raster_info(train_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path) In\u00a0[\u00a0]: Copied! <pre>pascal_output = \"buildings_pascal_voc\"\n\nstats = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=pascal_output,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n    metadata_format=\"PASCAL_VOC\",\n    # max_tiles=10,  # Limit for demo purposes\n)\n</pre> pascal_output = \"buildings_pascal_voc\"  stats = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=pascal_output,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0,     metadata_format=\"PASCAL_VOC\",     # max_tiles=10,  # Limit for demo purposes ) In\u00a0[\u00a0]: Copied! <pre># List annotation files\nxml_files = list(Path(f\"{pascal_output}/annotations\").glob(\"*.xml\"))\nprint(f\"Found {len(xml_files)} XML annotation files\")\n\n# Display first annotation file\nif xml_files:\n    with open(xml_files[0], \"r\") as f:\n        print(f\"\\nSample annotation ({xml_files[0].name}):\\n\")\n        print(f.read())\n</pre> # List annotation files xml_files = list(Path(f\"{pascal_output}/annotations\").glob(\"*.xml\")) print(f\"Found {len(xml_files)} XML annotation files\")  # Display first annotation file if xml_files:     with open(xml_files[0], \"r\") as f:         print(f\"\\nSample annotation ({xml_files[0].name}):\\n\")         print(f.read()) In\u00a0[\u00a0]: Copied! <pre>coco_output = \"buildings_coco\"\n\nstats = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=coco_output,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n    metadata_format=\"COCO\",\n    # max_tiles=10,\n)\n</pre> coco_output = \"buildings_coco\"  stats = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=coco_output,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0,     metadata_format=\"COCO\",     # max_tiles=10, ) In\u00a0[\u00a0]: Copied! <pre># Load COCO annotations\ncoco_file = f\"{coco_output}/annotations/instances.json\"\nwith open(coco_file, \"r\") as f:\n    coco_data = json.load(f)\n\nprint(f\"COCO Dataset Summary:\")\nprint(f\"  Images: {len(coco_data['images'])}\")\nprint(f\"  Annotations: {len(coco_data['annotations'])}\")\nprint(f\"  Categories: {len(coco_data['categories'])}\")\n\n# Display categories\nprint(f\"\\nCategories:\")\nfor cat in coco_data[\"categories\"]:\n    print(f\"  {cat}\")\n\n# Display first image\nif coco_data[\"images\"]:\n    print(f\"\\nFirst image:\")\n    print(f\"  {coco_data['images'][0]}\")\n\n# Display first annotation\nif coco_data[\"annotations\"]:\n    print(f\"\\nFirst annotation:\")\n    print(f\"  {coco_data['annotations'][0]}\")\n</pre> # Load COCO annotations coco_file = f\"{coco_output}/annotations/instances.json\" with open(coco_file, \"r\") as f:     coco_data = json.load(f)  print(f\"COCO Dataset Summary:\") print(f\"  Images: {len(coco_data['images'])}\") print(f\"  Annotations: {len(coco_data['annotations'])}\") print(f\"  Categories: {len(coco_data['categories'])}\")  # Display categories print(f\"\\nCategories:\") for cat in coco_data[\"categories\"]:     print(f\"  {cat}\")  # Display first image if coco_data[\"images\"]:     print(f\"\\nFirst image:\")     print(f\"  {coco_data['images'][0]}\")  # Display first annotation if coco_data[\"annotations\"]:     print(f\"\\nFirst annotation:\")     print(f\"  {coco_data['annotations'][0]}\") In\u00a0[\u00a0]: Copied! <pre>yolo_output = \"buildings_yolo\"\n\nstats = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=yolo_output,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n    metadata_format=\"YOLO\",\n    # max_tiles=10,\n)\n</pre> yolo_output = \"buildings_yolo\"  stats = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=yolo_output,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0,     metadata_format=\"YOLO\",     # max_tiles=10, ) In\u00a0[\u00a0]: Copied! <pre># Load classes\nclasses_file = f\"{yolo_output}/classes.txt\"\nwith open(classes_file, \"r\") as f:\n    classes = f.read().strip().split(\"\\n\")\n\nprint(f\"Classes ({len(classes)}):\")\nfor i, cls in enumerate(classes):\n    print(f\"  {i}: {cls}\")\n\n# List annotation files\ntxt_files = list(Path(f\"{yolo_output}/labels\").glob(\"*.txt\"))\nprint(f\"\\nFound {len(txt_files)} YOLO annotation files\")\n\n# Display first annotation file\nif txt_files:\n    with open(txt_files[0], \"r\") as f:\n        lines = f.readlines()\n    print(f\"\\nSample annotation ({txt_files[0].name}):\")\n    print(f\"  Format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\")\n    for line in lines[:5]:  # Show first 5 objects\n        print(f\"  {line.strip()}\")\n    if len(lines) &gt; 5:\n        print(f\"  ... and {len(lines) - 5} more objects\")\n</pre> # Load classes classes_file = f\"{yolo_output}/classes.txt\" with open(classes_file, \"r\") as f:     classes = f.read().strip().split(\"\\n\")  print(f\"Classes ({len(classes)}):\") for i, cls in enumerate(classes):     print(f\"  {i}: {cls}\")  # List annotation files txt_files = list(Path(f\"{yolo_output}/labels\").glob(\"*.txt\")) print(f\"\\nFound {len(txt_files)} YOLO annotation files\")  # Display first annotation file if txt_files:     with open(txt_files[0], \"r\") as f:         lines = f.readlines()     print(f\"\\nSample annotation ({txt_files[0].name}):\")     print(f\"  Format:  \")     for line in lines[:5]:  # Show first 5 objects         print(f\"  {line.strip()}\")     if len(lines) &gt; 5:         print(f\"  ... and {len(lines) - 5} more objects\") In\u00a0[\u00a0]: Copied! <pre># Train semantic segmentation model with COCO format\ngeoai.train_segmentation_model(\n    images_dir=f\"{coco_output}/images\",\n    labels_dir=f\"{coco_output}/annotations/instances.json\",  # Path to COCO JSON\n    output_dir=\"models_coco\",\n    input_format=\"coco\",  # Specify COCO format\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_epochs=20,  # Reduced for demo\n    batch_size=8,\n    verbose=True,\n)\n</pre> # Train semantic segmentation model with COCO format geoai.train_segmentation_model(     images_dir=f\"{coco_output}/images\",     labels_dir=f\"{coco_output}/annotations/instances.json\",  # Path to COCO JSON     output_dir=\"models_coco\",     input_format=\"coco\",  # Specify COCO format     architecture=\"unet\",     encoder_name=\"resnet34\",     num_epochs=20,  # Reduced for demo     batch_size=8,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"models_coco/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"models_coco/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Train instance segmentation model with COCO format\ngeoai.train_instance_segmentation_model(\n    images_dir=f\"{coco_output}/images\",\n    labels_dir=f\"{coco_output}/annotations/instances.json\",\n    output_dir=\"models_maskrcnn_coco\",\n    input_format=\"coco\",\n    num_epochs=20,\n    batch_size=8,\n)\n</pre> # Train instance segmentation model with COCO format geoai.train_instance_segmentation_model(     images_dir=f\"{coco_output}/images\",     labels_dir=f\"{coco_output}/annotations/instances.json\",     output_dir=\"models_maskrcnn_coco\",     input_format=\"coco\",     num_epochs=20,     batch_size=8, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"models_maskrcnn_coco/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"models_maskrcnn_coco/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Train semantic segmentation model with YOLO format\ngeoai.train_segmentation_model(\n    images_dir=yolo_output,  # Root directory containing images/ and labels/\n    labels_dir=\"\",  # Not used for YOLO format\n    output_dir=\"models_yolo\",\n    input_format=\"yolo\",  # Specify YOLO format\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_epochs=20,\n    batch_size=8,\n    verbose=True,\n)\n</pre> # Train semantic segmentation model with YOLO format geoai.train_segmentation_model(     images_dir=yolo_output,  # Root directory containing images/ and labels/     labels_dir=\"\",  # Not used for YOLO format     output_dir=\"models_yolo\",     input_format=\"yolo\",  # Specify YOLO format     architecture=\"unet\",     encoder_name=\"resnet34\",     num_epochs=20,     batch_size=8,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"models_yolo/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"models_yolo/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Train instance segmentation model with YOLO format\ngeoai.train_instance_segmentation_model(\n    images_dir=yolo_output,\n    labels_dir=\"\",\n    output_dir=\"models_maskrcnn_yolo\",\n    input_format=\"yolo\",\n    num_epochs=20,\n    batch_size=8,\n)\n</pre> # Train instance segmentation model with YOLO format geoai.train_instance_segmentation_model(     images_dir=yolo_output,     labels_dir=\"\",     output_dir=\"models_maskrcnn_yolo\",     input_format=\"yolo\",     num_epochs=20,     batch_size=8, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"models_maskrcnn_yolo/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"models_maskrcnn_yolo/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Standard directory format (default behavior)\ngeoai.train_segmentation_model(\n    images_dir=f\"{pascal_output}/images\",\n    labels_dir=f\"{pascal_output}/labels\",\n    output_dir=\"models_directory\",\n    # input_format=\"directory\" is the default, can be omitted\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_epochs=20,\n    batch_size=8,\n    verbose=True,\n)\n</pre> # Standard directory format (default behavior) geoai.train_segmentation_model(     images_dir=f\"{pascal_output}/images\",     labels_dir=f\"{pascal_output}/labels\",     output_dir=\"models_directory\",     # input_format=\"directory\" is the default, can be omitted     architecture=\"unet\",     encoder_name=\"resnet34\",     num_epochs=20,     batch_size=8,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"models_directory/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"models_directory/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Train with TIMM encoder using COCO format\ngeoai.train_timm_segmentation_model(\n    images_dir=f\"{coco_output}/images\",\n    labels_dir=f\"{coco_output}/annotations/instances.json\",\n    output_dir=\"models_timm_coco\",\n    input_format=\"coco\",  # Specify COCO format\n    encoder_name=\"efficientnet-b3\",  # TIMM encoder\n    architecture=\"unet\",\n    encoder_weights=\"imagenet\",\n    num_epochs=20,\n    batch_size=8,\n    verbose=True,\n)\n</pre> # Train with TIMM encoder using COCO format geoai.train_timm_segmentation_model(     images_dir=f\"{coco_output}/images\",     labels_dir=f\"{coco_output}/annotations/instances.json\",     output_dir=\"models_timm_coco\",     input_format=\"coco\",  # Specify COCO format     encoder_name=\"efficientnet-b3\",  # TIMM encoder     architecture=\"unet\",     encoder_weights=\"imagenet\",     num_epochs=20,     batch_size=8,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Or with YOLO format\ngeoai.train_timm_segmentation_model(\n    images_dir=yolo_output,\n    labels_dir=\"\",\n    output_dir=\"models_timm_yolo\",\n    input_format=\"yolo\",\n    encoder_name=\"efficientnet-b3\",\n    num_epochs=20,\n)\n</pre> # Or with YOLO format geoai.train_timm_segmentation_model(     images_dir=yolo_output,     labels_dir=\"\",     output_dir=\"models_timm_yolo\",     input_format=\"yolo\",     encoder_name=\"efficientnet-b3\",     num_epochs=20, )"},{"location":"examples/export_training_data_formats/#export-training-data-in-multiple-formats-pascal-voc-coco-yolo","title":"Export Training Data in Multiple Formats (PASCAL VOC, COCO, YOLO)\u00b6","text":"<p>This notebook demonstrates how to export geospatial training data in three popular object detection formats:</p> <ul> <li>PASCAL VOC: XML-based format, widely used in computer vision</li> <li>COCO: JSON-based format, standard for object detection benchmarks</li> <li>YOLO: Text-based format with normalized coordinates, optimized for YOLO models</li> </ul>"},{"location":"examples/export_training_data_formats/#install-packages","title":"Install packages\u00b6","text":"<p>Ensure the required packages are installed.</p>"},{"location":"examples/export_training_data_formats/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/export_training_data_formats/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same building detection dataset from the segmentation example.</p>"},{"location":"examples/export_training_data_formats/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/export_training_data_formats/#format-1-pascal-voc-xml","title":"Format 1: PASCAL VOC (XML)\u00b6","text":"<p>PASCAL VOC format stores annotations in XML files with bounding boxes and class labels. This is the default format and is widely used in traditional object detection frameworks.</p> <p>Output structure:</p> <pre><code>pascal_voc_output/\n\u251c\u2500\u2500 images/          # GeoTIFF tiles\n\u251c\u2500\u2500 labels/          # Label masks (GeoTIFF)\n\u2514\u2500\u2500 annotations/     # XML annotation files\n</code></pre>"},{"location":"examples/export_training_data_formats/#examine-pascal-voc-output","title":"Examine PASCAL VOC output\u00b6","text":""},{"location":"examples/export_training_data_formats/#format-2-coco-json","title":"Format 2: COCO (JSON)\u00b6","text":"<p>COCO format uses a single JSON file containing all annotations, images, and categories. This is the standard format for modern object detection benchmarks.</p> <p>Output structure:</p> <pre><code>coco_output/\n\u251c\u2500\u2500 images/              # GeoTIFF tiles\n\u251c\u2500\u2500 labels/              # Label masks (GeoTIFF)\n\u2514\u2500\u2500 annotations/\n    \u2514\u2500\u2500 instances.json   # COCO annotations\n</code></pre> <p>COCO JSON structure:</p> <pre>{\n  \"images\": [{\"id\": 0, \"file_name\": \"tile_000000.tif\", \"width\": 512, \"height\": 512}],\n  \"annotations\": [{\"id\": 1, \"image_id\": 0, \"category_id\": 1, \"bbox\": [x, y, w, h]}],\n  \"categories\": [{\"id\": 1, \"name\": \"building\", \"supercategory\": \"object\"}]\n}\n</pre>"},{"location":"examples/export_training_data_formats/#examine-coco-output","title":"Examine COCO output\u00b6","text":""},{"location":"examples/export_training_data_formats/#format-3-yolo-text","title":"Format 3: YOLO (Text)\u00b6","text":"<p>YOLO format uses text files with normalized bounding box coordinates. Each image has a corresponding <code>.txt</code> file with one line per object.</p> <p>Output structure:</p> <pre><code>yolo_output/\n\u251c\u2500\u2500 images/           # GeoTIFF tiles\n\u251c\u2500\u2500 labels/           # Label masks (GeoTIFF) + YOLO .txt files\n\u2514\u2500\u2500 classes.txt       # Class names (one per line)\n</code></pre> <p>YOLO annotation format (normalized coordinates 0-1):</p> <pre><code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\n0 0.5 0.5 0.3 0.2\n</code></pre>"},{"location":"examples/export_training_data_formats/#examine-yolo-output","title":"Examine YOLO output\u00b6","text":""},{"location":"examples/export_training_data_formats/#format-comparison","title":"Format Comparison\u00b6","text":""},{"location":"examples/export_training_data_formats/#when-to-use-each-format","title":"When to Use Each Format\u00b6","text":"Format Best For Pros Cons PASCAL VOC Traditional CV frameworks, quick inspection Human-readable XML, one file per image Verbose, not ideal for large datasets COCO Modern object detection, benchmarking, complex datasets Efficient JSON, supports multiple annotations types Single file can be large, requires parsing YOLO YOLO models (v3-v8), real-time detection Compact, fast to parse, normalized coordinates Less human-readable, limited metadata"},{"location":"examples/export_training_data_formats/#coordinate-systems","title":"Coordinate Systems\u00b6","text":"<ul> <li>PASCAL VOC: Absolute pixel coordinates <code>[xmin, ymin, xmax, ymax]</code></li> <li>COCO: Absolute pixel coordinates <code>[x, y, width, height]</code> (top-left corner)</li> <li>YOLO: Normalized coordinates <code>[x_center, y_center, width, height]</code> (0-1 range)</li> </ul>"},{"location":"examples/export_training_data_formats/#geoai-extensions","title":"GeoAI Extensions\u00b6","text":"<p>All formats preserve geospatial information:</p> <ul> <li>PASCAL VOC: CRS, transform, and bounds in <code>&lt;georeference&gt;</code> element</li> <li>COCO: CRS and transform as custom fields in image metadata</li> <li>YOLO: Georeferenced GeoTIFF tiles maintain spatial context</li> </ul>"},{"location":"examples/export_training_data_formats/#multi-class-example","title":"Multi-Class Example\u00b6","text":"<p>The formats also support multi-class datasets. Here's how class information is stored:</p> <p>PASCAL VOC:</p> <pre>&lt;object&gt;\n  &lt;name&gt;building&lt;/name&gt;\n  &lt;bndbox&gt;...&lt;/bndbox&gt;\n&lt;/object&gt;\n</pre> <p>COCO:</p> <pre>{\n  \"categories\": [\n    {\"id\": 1, \"name\": \"building\", \"supercategory\": \"object\"},\n    {\"id\": 2, \"name\": \"road\", \"supercategory\": \"object\"}\n  ]\n}\n</pre> <p>YOLO:</p> <pre><code>classes.txt:\nbuilding\nroad\n\nannotations:\n0 0.5 0.5 0.3 0.2  # class_id 0 = building\n1 0.7 0.3 0.2 0.1  # class_id 1 = road\n</code></pre>"},{"location":"examples/export_training_data_formats/#summary","title":"Summary\u00b6","text":"<p>The <code>export_geotiff_tiles</code> function now supports three popular annotation formats:</p> <ul> <li>\u2705 PASCAL VOC (XML) - Traditional, human-readable</li> <li>\u2705 COCO (JSON) - Modern benchmark standard</li> <li>\u2705 YOLO (TXT) - Lightweight, optimized for YOLO</li> </ul> <p>All formats maintain geospatial context through georeferenced GeoTIFF tiles, making them ideal for training object detection models on remote sensing imagery.</p> <p>Choose the format that best fits your model training framework:</p> <ul> <li>Use COCO for detectron2, MMDetection, or benchmark comparisons</li> <li>Use YOLO for YOLOv5, YOLOv8, or ultralytics</li> <li>Use PASCAL VOC for TensorFlow Object Detection API or legacy frameworks</li> </ul>"},{"location":"examples/export_training_data_formats/#using-exported-data-for-training","title":"Using Exported Data for Training\u00b6","text":"<p>The training functions in GeoAI now support all three annotation formats directly! Here's how to use them for training models.</p>"},{"location":"examples/export_training_data_formats/#training-with-coco-format","title":"Training with COCO Format\u00b6","text":"<p>Use <code>input_format=\"coco\"</code> and point <code>labels_dir</code> to the <code>instances.json</code> file:</p>"},{"location":"examples/export_training_data_formats/#training-with-yolo-format","title":"Training with YOLO Format\u00b6","text":"<p>Use <code>input_format=\"yolo\"</code> and point <code>images_dir</code> to the root directory containing <code>images/</code> and <code>labels/</code> subdirectories:</p>"},{"location":"examples/export_training_data_formats/#training-with-directory-format-default","title":"Training with Directory Format (Default)\u00b6","text":"<p>The default behavior uses separate <code>images_dir</code> and <code>labels_dir</code> directories:</p>"},{"location":"examples/export_training_data_formats/#training-summary","title":"Training Summary\u00b6","text":"<p>Both <code>train_segmentation_model()</code> and <code>train_instance_segmentation_model()</code> functions now accept the <code>input_format</code> parameter to load data in any of these formats:</p> Input Format <code>input_format</code> Value <code>images_dir</code> <code>labels_dir</code> COCO <code>\"coco\"</code> Path to images directory Path to <code>instances.json</code> YOLO <code>\"yolo\"</code> Root directory with <code>images/</code> and <code>labels/</code> Empty string <code>\"\"</code> or not used Directory <code>\"directory\"</code> (default) Path to images directory Path to labels directory"},{"location":"examples/export_training_data_formats/#benefits","title":"Benefits\u00b6","text":"<ul> <li>Maximum Flexibility: Use any annotation format without conversion</li> <li>Geospatial Preservation: All formats maintain georeferencing through GeoTIFF tiles</li> <li>Framework Compatibility: Export in one format, train in another</li> <li>Consistent API: Same training functions work with all formats</li> </ul>"},{"location":"examples/export_training_data_formats/#example-workflow","title":"Example Workflow\u00b6","text":"<ol> <li>Export training data in COCO format for sharing with collaborators</li> <li>Export same data in YOLO format for YOLOv8 experiments</li> <li>Train both semantic and instance segmentation models using the same data</li> <li>All while maintaining full geospatial context for deployment on satellite imagery</li> </ol> <p>This provides a complete end-to-end workflow for geospatial deep learning!</p>"},{"location":"examples/export_training_data_formats/#using-timm-models-with-multiple-formats","title":"Using TIMM Models with Multiple Formats\u00b6","text":"<p>The <code>train_timm_segmentation_model()</code> function also supports all three annotation formats, providing access to a wider range of encoder backbones from the TIMM library (e.g., EfficientNet, ConvNeXt, Swin Transformer):</p>"},{"location":"examples/geometric_properties/","title":"Geometric properties","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_buildings_masks.geojson\"\nraster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\n</pre> vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_buildings_masks.geojson\" raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.read_vector(vector_url)\n</pre> gdf = geoai.read_vector(vector_url) In\u00a0[\u00a0]: Copied! <pre>gdf.head()\n</pre> gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>gdf_props.head()\n</pre> gdf_props.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"elongation\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &lt; 2000) &amp; (gdf_props[\"elongation\"] &lt; 5)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &lt; 2000) &amp; (gdf_props[\"elongation\"] &lt; 5)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"elongation\", tiles=raster_url)"},{"location":"examples/geometric_properties/#geometric-properties","title":"Geometric Properties\u00b6","text":"<p>This notebook demonstrates how to calculate geometric properties of objects in a vector dataset and filter out unwanted objects based on these properties.</p>"},{"location":"examples/geometric_properties/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/geometric_properties/#import-package","title":"Import package\u00b6","text":""},{"location":"examples/geometric_properties/#load-data","title":"Load data\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/geometric_properties/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-geometric-properties","title":"Visualize geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#filter-objects-based-on-geometric-properties","title":"Filter objects based on geometric properties\u00b6","text":""},{"location":"examples/geometric_properties/#visualize-filtered-objects","title":"Visualize filtered objects\u00b6","text":""},{"location":"examples/globe_projection/","title":"Globe projection","text":"<p>Visualize geospatial data on a 3D globe</p> <p>Uncomment the following line to install geoai if needed.</p> In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(center=[-100, 40], zoom=3, style=\"liberty\")\nm.add_globe_control()\nm\n</pre> m = geoai.Map(center=[-100, 40], zoom=3, style=\"liberty\") m.add_globe_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(center=[-100, 40], zoom=3, style=\"positron\", projection=\"globe\")\nm.add_basemap(\"Esri.WorldImagery\")\nm.add_overture_3d_buildings()\nm\n</pre> m = geoai.Map(center=[-100, 40], zoom=3, style=\"positron\", projection=\"globe\") m.add_basemap(\"Esri.WorldImagery\") m.add_overture_3d_buildings() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(\n    center=[19.43, 49.49], zoom=3, pitch=60, style=\"positron\", projection=\"globe\"\n)\nsource = {\n    \"type\": \"geojson\",\n    \"data\": \"https://docs.maptiler.com/sdk-js/assets/Mean_age_of_women_at_first_marriage_in_2019.geojson\",\n}\nm.add_source(\"countries\", source)\nlayer = {\n    \"id\": \"eu-countries\",\n    \"source\": \"countries\",\n    \"type\": \"fill-extrusion\",\n    \"paint\": {\n        \"fill-extrusion-color\": [\n            \"interpolate\",\n            [\"linear\"],\n            [\"get\", \"age\"],\n            23.0,\n            \"#fff5eb\",\n            24.0,\n            \"#fee6ce\",\n            25.0,\n            \"#fdd0a2\",\n            26.0,\n            \"#fdae6b\",\n            27.0,\n            \"#fd8d3c\",\n            28.0,\n            \"#f16913\",\n            29.0,\n            \"#d94801\",\n            30.0,\n            \"#8c2d04\",\n        ],\n        \"fill-extrusion-opacity\": 1,\n        \"fill-extrusion-height\": [\"*\", [\"get\", \"age\"], 5000],\n    },\n}\nfirst_symbol_layer_id = m.find_first_symbol_layer()[\"id\"]\nm.add_layer(layer, first_symbol_layer_id)\nm.add_layer_control()\nm\n</pre> m = geoai.Map(     center=[19.43, 49.49], zoom=3, pitch=60, style=\"positron\", projection=\"globe\" ) source = {     \"type\": \"geojson\",     \"data\": \"https://docs.maptiler.com/sdk-js/assets/Mean_age_of_women_at_first_marriage_in_2019.geojson\", } m.add_source(\"countries\", source) layer = {     \"id\": \"eu-countries\",     \"source\": \"countries\",     \"type\": \"fill-extrusion\",     \"paint\": {         \"fill-extrusion-color\": [             \"interpolate\",             [\"linear\"],             [\"get\", \"age\"],             23.0,             \"#fff5eb\",             24.0,             \"#fee6ce\",             25.0,             \"#fdd0a2\",             26.0,             \"#fdae6b\",             27.0,             \"#fd8d3c\",             28.0,             \"#f16913\",             29.0,             \"#d94801\",             30.0,             \"#8c2d04\",         ],         \"fill-extrusion-opacity\": 1,         \"fill-extrusion-height\": [\"*\", [\"get\", \"age\"], 5000],     }, } first_symbol_layer_id = m.find_first_symbol_layer()[\"id\"] m.add_layer(layer, first_symbol_layer_id) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>data = \"https://github.com/opengeos/datasets/releases/download/vector/countries.geojson\"\n</pre> data = \"https://github.com/opengeos/datasets/releases/download/vector/countries.geojson\" In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(style=\"liberty\", projection=\"globe\")\nfirst_symbol_id = m.find_first_symbol_layer()[\"id\"]\nm.add_data(\n    data,\n    column=\"POP_EST\",\n    scheme=\"Quantiles\",\n    cmap=\"Blues\",\n    legend_title=\"Population\",\n    name=\"Population\",\n    before_id=first_symbol_id,\n    extrude=True,\n    scale_factor=1000,\n)\nm.add_layer_control()\nm\n</pre> m = geoai.Map(style=\"liberty\", projection=\"globe\") first_symbol_id = m.find_first_symbol_layer()[\"id\"] m.add_data(     data,     column=\"POP_EST\",     scheme=\"Quantiles\",     cmap=\"Blues\",     legend_title=\"Population\",     name=\"Population\",     before_id=first_symbol_id,     extrude=True,     scale_factor=1000, ) m.add_layer_control() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(style=\"liberty\", projection=\"globe\")\ntrain_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\nm.add_cog_layer(train_raster_url, name=\"NAIP\")\npaint = {\"fill-color\": \"#ff0000\", \"fill-opacity\": 0.4, \"fill-outline-color\": \"#ffff00\"}\nm.add_geojson(train_vector_url, name=\"Buildings\", layer_type=\"fill\", paint=paint)\nm.add_layer_control()\nm\n</pre> m = geoai.Map(style=\"liberty\", projection=\"globe\") train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" m.add_cog_layer(train_raster_url, name=\"NAIP\") paint = {\"fill-color\": \"#ff0000\", \"fill-opacity\": 0.4, \"fill-outline-color\": \"#ffff00\"} m.add_geojson(train_vector_url, name=\"Buildings\", layer_type=\"fill\", paint=paint) m.add_layer_control() m In\u00a0[\u00a0]: Copied! <pre>collection = \"landsat-8-c2-l2\"\nitem = \"LC08_L2SP_047027_20201204_02_T1\"\n</pre> collection = \"landsat-8-c2-l2\" item = \"LC08_L2SP_047027_20201204_02_T1\" In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(projection=\"globe\")\nm.add_stac_layer(\n    collection=collection, item=item, assets=\"SR_B7,SR_B5,SR_B4\", name=\"False color\"\n)\nm\n</pre> m = geoai.Map(projection=\"globe\") m.add_stac_layer(     collection=collection, item=item, assets=\"SR_B7,SR_B5,SR_B4\", name=\"False color\" ) m In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(projection=\"globe\")\nm.add_stac_layer(\n    collection=collection,\n    item=item,\n    assets=[\"SR_B5\", \"SR_B4\", \"SR_B3\"],\n    name=\"Color infrared\",\n)\nm\n</pre> m = geoai.Map(projection=\"globe\") m.add_stac_layer(     collection=collection,     item=item,     assets=[\"SR_B5\", \"SR_B4\", \"SR_B3\"],     name=\"Color infrared\", ) m In\u00a0[\u00a0]: Copied! <pre>m = geoai.Map(projection=\"globe\")\nm.add_stac_layer(\n    collection=collection, item=item, assets=\"SR_B5,SR_B4,SR_B3\", name=\"Color infrared\"\n)\nm.add_stac_layer(\n    collection=collection,\n    item=item,\n    expression=\"(SR_B5-SR_B4)/(SR_B5+SR_B4)\",\n    rescale=\"-1,1\",\n    colormap_name=\"greens\",\n    name=\"NDVI Green\",\n)\nm\n</pre> m = geoai.Map(projection=\"globe\") m.add_stac_layer(     collection=collection, item=item, assets=\"SR_B5,SR_B4,SR_B3\", name=\"Color infrared\" ) m.add_stac_layer(     collection=collection,     item=item,     expression=\"(SR_B5-SR_B4)/(SR_B5+SR_B4)\",     rescale=\"-1,1\",     colormap_name=\"greens\",     name=\"NDVI Green\", ) m"},{"location":"examples/globe_projection/#import-library","title":"Import library\u00b6","text":""},{"location":"examples/globe_projection/#add-globe-control","title":"Add globe control\u00b6","text":""},{"location":"examples/globe_projection/#use-globe-projection","title":"Use globe projection\u00b6","text":""},{"location":"examples/globe_projection/#create-3d-choropleth-maps","title":"Create 3D choropleth maps\u00b6","text":""},{"location":"examples/globe_projection/#vector-data","title":"Vector data\u00b6","text":""},{"location":"examples/globe_projection/#planetary-computer","title":"Planetary Computer\u00b6","text":""},{"location":"examples/grounded_sam/","title":"Grounded sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nfrom geoai.segment import GroundedSAM\n</pre> import geoai from geoai.segment import GroundedSAM In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/aerial.tif\"\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/aerial.tif\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>grounded_sam = GroundedSAM(\n    detector_id=\"IDEA-Research/grounding-dino-tiny\",\n    segmenter_id=\"facebook/sam-vit-base\",\n    device=None,  # Will auto-detect CUDA if available\n    tile_size=1024,\n    overlap=128,\n    threshold=0.3,\n)\n</pre> grounded_sam = GroundedSAM(     detector_id=\"IDEA-Research/grounding-dino-tiny\",     segmenter_id=\"facebook/sam-vit-base\",     device=None,  # Will auto-detect CUDA if available     tile_size=1024,     overlap=128,     threshold=0.3, ) In\u00a0[\u00a0]: Copied! <pre>text_prompts = [\"building\", \"car\", \"tree\"]\noutput_file = \"segmented_objects.tif\"\n</pre> text_prompts = [\"building\", \"car\", \"tree\"] output_file = \"segmented_objects.tif\" In\u00a0[\u00a0]: Copied! <pre>result_files = grounded_sam.segment_image(\n    input_path=raster_path,\n    output_path=output_file,\n    text_prompts=text_prompts,\n    polygon_refinement=True,\n    export_boxes=True,\n    export_polygons=True,\n    smoothing_sigma=1.0,\n    nms_threshold=0.5,\n    min_polygon_area=50,\n    simplify_tolerance=1.0,\n)\n</pre> result_files = grounded_sam.segment_image(     input_path=raster_path,     output_path=output_file,     text_prompts=text_prompts,     polygon_refinement=True,     export_boxes=True,     export_polygons=True,     smoothing_sigma=1.0,     nms_threshold=0.5,     min_polygon_area=50,     simplify_tolerance=1.0, ) In\u00a0[\u00a0]: Copied! <pre>result_files\n</pre> result_files In\u00a0[\u00a0]: Copied! <pre>polygons = result_files[\"polygons\"]\nboxes = result_files[\"boxes\"]\n</pre> polygons = result_files[\"polygons\"] boxes = result_files[\"boxes\"] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    polygons, basemap=raster_url, column=\"label\", cmap=\"tab20\"\n)\n</pre> geoai.view_vector_interactive(     polygons, basemap=raster_url, column=\"label\", cmap=\"tab20\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(output_file, indexes=[2], basemap=raster_url, layer_name=\"Building\")\n</pre> geoai.view_raster(output_file, indexes=[2], basemap=raster_url, layer_name=\"Building\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(output_file, indexes=[3], basemap=raster_url, layer_name=\"Car\")\n</pre> geoai.view_raster(output_file, indexes=[3], basemap=raster_url, layer_name=\"Car\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(output_file, indexes=[4], basemap=raster_url, layer_name=\"Tree\")\n</pre> geoai.view_raster(output_file, indexes=[4], basemap=raster_url, layer_name=\"Tree\")"},{"location":"examples/grounded_sam/#image-segmentation-with-grounded-sam","title":"Image Segmentation with Grounded SAM\u00b6","text":"<p>This notebook demonstrates how to use the <code>GroundedSAM</code> class to segment images using Grounded SAM. To learn more about Grounded SAM, please refer to the Grounded SAM documentation.</p>"},{"location":"examples/grounded_sam/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/grounded_sam/#import-library","title":"Import library\u00b6","text":""},{"location":"examples/grounded_sam/#download-sample-data","title":"Download sample data\u00b6","text":"<p>The sample data is from OpenAerialMap. Credits to the provider Jason McMinn.</p>"},{"location":"examples/grounded_sam/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/grounded_sam/#image-segmentation","title":"Image Segmentation\u00b6","text":""},{"location":"examples/grounded_sam/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/image_chips/","title":"Image chips","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(raster_path, figsize=(18, 10))\n</pre> geoai.view_image(raster_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector(vector_path, basemap=True, alpha=0.5, figsize=(18, 10))\n</pre> geoai.view_vector(vector_path, basemap=True, alpha=0.5, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(vector_path)\n</pre> geoai.view_vector_interactive(vector_path) In\u00a0[\u00a0]: Copied! <pre>output_path = vector_path.replace(\".geojson\", \".tif\")\ngeoai.vector_to_raster(vector_path, output_path, reference_raster=raster_path)\n</pre> output_path = vector_path.replace(\".geojson\", \".tif\") geoai.vector_to_raster(vector_path, output_path, reference_raster=raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(output_path, figsize=(18, 10))\n</pre> geoai.view_image(output_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=raster_path,\n    out_folder=\"output\",\n    in_class_data=vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n    create_overview=True,\n    quiet=True,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=raster_path,     out_folder=\"output\",     in_class_data=vector_path,     tile_size=512,     stride=256,     buffer_radius=0,     create_overview=True,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/overview.png\", figsize=(18, 10))\n</pre> geoai.view_image(\"output/overview.png\", figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/images/tile_000000.tif\")\n</pre> geoai.view_image(\"output/images/tile_000000.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/labels/tile_000000.tif\")\n</pre> geoai.view_image(\"output/labels/tile_000000.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/images/tile_000001.tif\")\n</pre> geoai.view_image(\"output/images/tile_000001.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(\"output/labels/tile_000001.tif\")\n</pre> geoai.view_image(\"output/labels/tile_000001.tif\")"},{"location":"examples/image_chips/#generate-image-chips","title":"Generate Image Chips\u00b6","text":""},{"location":"examples/image_chips/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/image_chips/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"examples/image_chips/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/image_chips/#preview-data","title":"Preview data\u00b6","text":""},{"location":"examples/image_chips/#convert-vector-to-raster","title":"Convert vector to raster\u00b6","text":""},{"location":"examples/image_chips/#generate-image-chips","title":"Generate image chips\u00b6","text":""},{"location":"examples/image_chips/#preview-image-chips","title":"Preview image chips\u00b6","text":""},{"location":"examples/image_tiling/","title":"Image tiling","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/landcover-sample-data.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/landcover-sample-data.zip\" In\u00a0[\u00a0]: Copied! <pre>data_dir = geoai.download_file(url)\n</pre> data_dir = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/images\"\nmasks_dir = f\"{data_dir}/masks\"\ntiles_dir = f\"{data_dir}/tiles\"\n</pre> images_dir = f\"{data_dir}/images\" masks_dir = f\"{data_dir}/masks\" tiles_dir = f\"{data_dir}/tiles\" In\u00a0[\u00a0]: Copied! <pre>result = geoai.export_geotiff_tiles_batch(\n    images_folder=images_dir,\n    masks_folder=masks_dir,\n    output_folder=tiles_dir,\n    tile_size=512,\n    stride=128,\n    quiet=True,\n)\n# print(result)\n</pre> result = geoai.export_geotiff_tiles_batch(     images_folder=images_dir,     masks_folder=masks_dir,     output_folder=tiles_dir,     tile_size=512,     stride=128,     quiet=True, ) # print(result) <p>You will find the generated tiles in the <code>tiles</code> folder, which includes two subfolders: <code>images</code> and <code>masks</code>. You can use these tiles for training a segmentation model.</p>"},{"location":"examples/image_tiling/#generate-image-chips-from-folders-of-images-and-masks","title":"Generate Image Chips From Folders of Images and Masks\u00b6","text":""},{"location":"examples/image_tiling/#install-package","title":"Install Package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/image_tiling/#import-packages","title":"Import Packages\u00b6","text":""},{"location":"examples/image_tiling/#download-sample-data","title":"Download Sample Data\u00b6","text":""},{"location":"examples/image_tiling/#generate-image-chips","title":"Generate Image Chips\u00b6","text":""},{"location":"examples/instance_segmentation/","title":"Instance segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport os\nfrom pathlib import Path\n</pre> import geoai import os from pathlib import Path In\u00a0[\u00a0]: Copied! <pre># Check if CUDA is available\ndevice = geoai.get_device()\nprint(f\"Using device: {device}\")\n\n# Set up paths\nout_folder = \"instance_segmentation_buildings\"\nmodels_dir = Path(out_folder) / \"models\"\noutput_dir = Path(out_folder) / \"output\"\n\n# Create directories if they don't exist\nmodels_dir.mkdir(parents=True, exist_ok=True)\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Working directory: {out_folder}\")\nprint(f\"Models will be saved to: {models_dir}\")\nprint(f\"Output will be saved to: {output_dir}\")\n</pre> # Check if CUDA is available device = geoai.get_device() print(f\"Using device: {device}\")  # Set up paths out_folder = \"instance_segmentation_buildings\" models_dir = Path(out_folder) / \"models\" output_dir = Path(out_folder) / \"output\"  # Create directories if they don't exist models_dir.mkdir(parents=True, exist_ok=True) output_dir.mkdir(parents=True, exist_ok=True)  print(f\"Working directory: {out_folder}\") print(f\"Models will be saved to: {models_dir}\") print(f\"Output will be saved to: {output_dir}\") In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n\nprint(f\"Downloaded training raster: {train_raster_path}\")\nprint(f\"Downloaded training vector: {train_vector_path}\")\nprint(f\"Downloaded test raster: {test_raster_path}\")\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url)  print(f\"Downloaded training raster: {train_raster_path}\") print(f\"Downloaded training vector: {train_vector_path}\") print(f\"Downloaded test raster: {test_raster_path}\") In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(train_raster_path)\n</pre> geoai.get_raster_info(train_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre># Create training tiles\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n\nprint(f\"Created {len(tiles)} training tiles\")\nprint(f\"Images saved to: {out_folder}/images\")\nprint(f\"Labels saved to: {out_folder}/labels\")\n</pre> # Create training tiles tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, )  print(f\"Created {len(tiles)} training tiles\") print(f\"Images saved to: {out_folder}/images\") print(f\"Labels saved to: {out_folder}/labels\") In\u00a0[\u00a0]: Copied! <pre># Create a model for binary segmentation (background + buildings)\nmodel = geoai.get_instance_segmentation_model(\n    num_classes=2,  # background + buildings\n    num_channels=3,  # RGB channels\n    pretrained=True,\n)\n\nprint(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\nprint(f\"Model device: {next(model.parameters()).device}\")\nprint(f\"Model type: {type(model)}\")\n</pre> # Create a model for binary segmentation (background + buildings) model = geoai.get_instance_segmentation_model(     num_classes=2,  # background + buildings     num_channels=3,  # RGB channels     pretrained=True, )  print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\") print(f\"Model device: {next(model.parameters()).device}\") print(f\"Model type: {type(model)}\") In\u00a0[\u00a0]: Copied! <pre># Training configuration\ntraining_config = {\n    \"images_dir\": f\"{out_folder}/images\",\n    \"labels_dir\": f\"{out_folder}/labels\",\n    \"output_dir\": str(models_dir),\n    \"num_classes\": 2,  # background + buildings\n    \"num_channels\": 3,  # RGB\n    \"batch_size\": 2,  # Small batch size for demo\n    \"num_epochs\": 20,  # Few epochs for demo\n    \"learning_rate\": 0.005,\n    \"val_split\": 0.2,\n    \"visualize\": True,\n    \"device\": device,\n    \"verbose\": True,\n}\n\nprint(\"Training configuration:\")\nfor key, value in training_config.items():\n    print(f\"  {key}: {value}\")\n</pre> # Training configuration training_config = {     \"images_dir\": f\"{out_folder}/images\",     \"labels_dir\": f\"{out_folder}/labels\",     \"output_dir\": str(models_dir),     \"num_classes\": 2,  # background + buildings     \"num_channels\": 3,  # RGB     \"batch_size\": 2,  # Small batch size for demo     \"num_epochs\": 20,  # Few epochs for demo     \"learning_rate\": 0.005,     \"val_split\": 0.2,     \"visualize\": True,     \"device\": device,     \"verbose\": True, }  print(\"Training configuration:\") for key, value in training_config.items():     print(f\"  {key}: {value}\") In\u00a0[\u00a0]: Copied! <pre># Train the model\nprint(\"Starting training...\")\ngeoai.train_instance_segmentation_model(**training_config)\nprint(\"Training completed!\")\n</pre> # Train the model print(\"Starting training...\") geoai.train_instance_segmentation_model(**training_config) print(\"Training completed!\") In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=str(models_dir / \"training_history.pth\"),\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=str(models_dir / \"training_history.pth\"),     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmodel_path = str(models_dir / \"best_model.pth\")\noutput_path = str(output_dir / \"instance_segmentation_result.tif\")\n\n# Check if model exists\nif os.path.exists(model_path):\n    print(f\"Model found at: {model_path}\")\nelse:\n    print(f\"Model not found at: {model_path}\")\n    print(\"Please ensure training completed successfully\")\n</pre> # Define paths model_path = str(models_dir / \"best_model.pth\") output_path = str(output_dir / \"instance_segmentation_result.tif\")  # Check if model exists if os.path.exists(model_path):     print(f\"Model found at: {model_path}\") else:     print(f\"Model not found at: {model_path}\")     print(\"Please ensure training completed successfully\") In\u00a0[\u00a0]: Copied! <pre># Single image inference with improved parameters\ninference_config = {\n    \"input_path\": test_raster_path,\n    \"output_path\": output_path,\n    \"model_path\": model_path,\n    \"window_size\": 512,\n    \"overlap\": 128,  # Reduced overlap to minimize artifacts\n    \"confidence_threshold\": 0.5,\n    \"batch_size\": 2,\n    \"num_channels\": 3,\n    \"num_classes\": 2,\n    \"device\": device,\n}\n\nprint(\"Running inference with sliding window processing...\")\nresult_path, inference_time = geoai.instance_segmentation(**inference_config)\nprint(f\"Inference completed in {inference_time:.2f} seconds!\")\n</pre> # Single image inference with improved parameters inference_config = {     \"input_path\": test_raster_path,     \"output_path\": output_path,     \"model_path\": model_path,     \"window_size\": 512,     \"overlap\": 128,  # Reduced overlap to minimize artifacts     \"confidence_threshold\": 0.5,     \"batch_size\": 2,     \"num_channels\": 3,     \"num_classes\": 2,     \"device\": device, }  print(\"Running inference with sliding window processing...\") result_path, inference_time = geoai.instance_segmentation(**inference_config) print(f\"Inference completed in {inference_time:.2f} seconds!\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    output_path, nodata=0, colormap=\"tab20\", opacity=0.7, basemap=test_raster_url\n)\n</pre> geoai.view_raster(     output_path, nodata=0, colormap=\"tab20\", opacity=0.7, basemap=test_raster_url ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"building_predictions.geojson\"\ngdf = geoai.orthogonalize(output_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"building_predictions.geojson\" gdf = geoai.orthogonalize(output_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre># Add geometric properties\ngdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> # Add geometric properties gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre># Interactive visualization with area information\ngeoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_url)\n</pre> # Interactive visualization with area information geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre># Filter out small buildings and visualize\ngdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\nprint(f\"Buildings after filtering (area &gt; 50 m\u00b2): {len(gdf_filtered)}\")\n\ngeoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_url)\n</pre> # Filter out small buildings and visualize gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] print(f\"Buildings after filtering (area &gt; 50 m\u00b2): {len(gdf_filtered)}\")  geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre># Create a split map comparison\ngeoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.3}},\n    basemap=test_raster_url,\n)\n</pre> # Create a split map comparison geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.3}},     basemap=test_raster_url, )"},{"location":"examples/instance_segmentation/#instance-segmentation-with-geoai","title":"Instance Segmentation with GeoAI\u00b6","text":"<p>This notebook demonstrates how to use the new instance segmentation functionality in GeoAI for training models and running inference on geospatial data.</p>"},{"location":"examples/instance_segmentation/#overview","title":"Overview\u00b6","text":"<p>Instance segmentation combines object detection and semantic segmentation to identify and segment individual objects in images. This is particularly useful for:</p> <ul> <li>Building detection and segmentation</li> <li>Vehicle counting and tracking</li> <li>Infrastructure mapping</li> <li>Object delineation in satellite imagery</li> </ul>"},{"location":"examples/instance_segmentation/#new-functions","title":"New Functions\u00b6","text":"<p>GeoAI now provides clear wrapper functions for instance segmentation:</p>"},{"location":"examples/instance_segmentation/#training","title":"Training\u00b6","text":"<ul> <li><code>train_instance_segmentation_model()</code> - Train a Mask R-CNN model</li> </ul>"},{"location":"examples/instance_segmentation/#inference","title":"Inference\u00b6","text":"<ul> <li><code>instance_segmentation()</code> - Run inference on a single GeoTIFF</li> <li><code>instance_segmentation_batch()</code> - Run inference on multiple GeoTIFFs</li> </ul>"},{"location":"examples/instance_segmentation/#model-creation","title":"Model Creation\u00b6","text":"<ul> <li><code>get_instance_segmentation_model()</code> - Create a Mask R-CNN model with custom parameters</li> </ul>"},{"location":"examples/instance_segmentation/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/instance_segmentation/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/instance_segmentation/#setup","title":"Setup\u00b6","text":"<p>First, let's check our environment and set up paths.</p>"},{"location":"examples/instance_segmentation/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same dataset as the semantic segmentation example for consistency.</p>"},{"location":"examples/instance_segmentation/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/instance_segmentation/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create training tiles for instance segmentation. Note that for instance segmentation, we need to ensure each building instance has a unique pixel value in the label.</p>"},{"location":"examples/instance_segmentation/#1-model-creation","title":"1. Model Creation\u00b6","text":"<p>Let's create an instance segmentation model with custom parameters.</p>"},{"location":"examples/instance_segmentation/#2-training-instance-segmentation-model","title":"2. Training Instance Segmentation Model\u00b6","text":"<p>Now let's train the instance segmentation model using our prepared data.</p>"},{"location":"examples/instance_segmentation/#3-running-inference","title":"3. Running Inference\u00b6","text":"<p>Once we have a trained model, we can run inference on new images.</p>"},{"location":"examples/instance_segmentation/#4-vectorize-and-visualize-results","title":"4. Vectorize and Visualize Results\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"examples/instance_segmentation/#8-key-parameters-guide","title":"8. Key Parameters Guide\u00b6","text":"<p>Here are the key parameters you can adjust:</p>"},{"location":"examples/instance_segmentation/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated the new instance segmentation functionality in GeoAI:</p> <ol> <li>Model Creation: Created Mask R-CNN models with custom parameters</li> <li>Training: Trained an instance segmentation model on building data</li> <li>Inference: Ran inference on test images</li> <li>Visualization: Converted results to vectors and visualized them</li> <li>Analysis: Compared with semantic segmentation approaches</li> </ol> <p>The new functions provide a cleaner API while maintaining backward compatibility with existing code. They're built on top of the robust MaskRCNN implementation already present in GeoAI.</p>"},{"location":"examples/instance_segmentation/#available-functions","title":"Available Functions:\u00b6","text":"<ul> <li><code>geoai.train_instance_segmentation_model()</code> - Train Mask R-CNN models</li> <li><code>geoai.instance_segmentation()</code> - Single image inference</li> <li><code>geoai.instance_segmentation_batch()</code> - Batch processing</li> <li><code>geoai.get_instance_segmentation_model()</code> - Create custom models</li> </ul>"},{"location":"examples/lightly_train_self_supervised/","title":"Lightly train self supervised","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py lightly-train\n</pre> # %pip install geoai-py lightly-train In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport os\nimport numpy as np\nimport torch\nfrom pathlib import Path\n</pre> import geoai import os import numpy as np import torch from pathlib import Path In\u00a0[\u00a0]: Copied! <pre># Download sample geospatial imagery for training\nsample_urls = [\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\",\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\",\n]\n\n# Create directories for our workflow\ndata_dir = \"lightly_train_data\"\noutput_dir = \"lightly_train_output\"\nembeddings_dir = \"lightly_embeddings\"\n\nos.makedirs(data_dir, exist_ok=True)\nos.makedirs(output_dir, exist_ok=True)\nos.makedirs(embeddings_dir, exist_ok=True)\n\n# Download and prepare training images\nimage_paths = []\nfor url in sample_urls:\n    image_path = geoai.download_file(url)\n    image_paths.append(image_path)\n    print(f\"Downloaded: {image_path}\")\n</pre> # Download sample geospatial imagery for training sample_urls = [     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\",     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\", ]  # Create directories for our workflow data_dir = \"lightly_train_data\" output_dir = \"lightly_train_output\" embeddings_dir = \"lightly_embeddings\"  os.makedirs(data_dir, exist_ok=True) os.makedirs(output_dir, exist_ok=True) os.makedirs(embeddings_dir, exist_ok=True)  # Download and prepare training images image_paths = [] for url in sample_urls:     image_path = geoai.download_file(url)     image_paths.append(image_path)     print(f\"Downloaded: {image_path}\") In\u00a0[\u00a0]: Copied! <pre># # Extract image patches for training\n# for i, image_path in enumerate(image_paths):\n#     patch_output_dir = f\"{data_dir}/patches_{i}\"\n\n#     # Extract patches from the raster\n#     geoai.export_geotiff_tiles(\n#         in_raster=image_path,\n#         out_folder=patch_output_dir,\n#         tile_size=224,  # Common size for vision models\n#         stride=112,  # 50% overlap\n#     )\n\n#     print(f\"Extracted patches from {image_path} to {patch_output_dir}\")\n</pre> # # Extract image patches for training # for i, image_path in enumerate(image_paths): #     patch_output_dir = f\"{data_dir}/patches_{i}\"  #     # Extract patches from the raster #     geoai.export_geotiff_tiles( #         in_raster=image_path, #         out_folder=patch_output_dir, #         tile_size=224,  # Common size for vision models #         stride=112,  # 50% overlap #     )  #     print(f\"Extracted patches from {image_path} to {patch_output_dir}\") In\u00a0[\u00a0]: Copied! <pre># Count total training images\nimport glob\n\nall_patches = glob.glob(f\"{data_dir}/**/images/*.tif\", recursive=True)\nprint(f\"\\nTotal training patches: {len(all_patches)}\")\n</pre> # Count total training images import glob  all_patches = glob.glob(f\"{data_dir}/**/images/*.tif\", recursive=True) print(f\"\\nTotal training patches: {len(all_patches)}\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Display a few sample patches\nsample_patches = all_patches[:6]  # Take first 6 patches\n\nfig, axes = plt.subplots(2, 3, figsize=(12, 8))\nfig.suptitle(\"Sample Training Patches for Self-Supervised Learning\", fontsize=16)\n\nfor i, patch_path in enumerate(sample_patches):\n    row, col = i // 3, i % 3\n    img = Image.open(patch_path)\n    axes[row, col].imshow(img)\n    axes[row, col].set_title(f\"Patch {i+1}\")\n    axes[row, col].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Each patch is {img.size} pixels\")\n</pre> import matplotlib.pyplot as plt from PIL import Image  # Display a few sample patches sample_patches = all_patches[:6]  # Take first 6 patches  fig, axes = plt.subplots(2, 3, figsize=(12, 8)) fig.suptitle(\"Sample Training Patches for Self-Supervised Learning\", fontsize=16)  for i, patch_path in enumerate(sample_patches):     row, col = i // 3, i % 3     img = Image.open(patch_path)     axes[row, col].imshow(img)     axes[row, col].set_title(f\"Patch {i+1}\")     axes[row, col].axis(\"off\")  plt.tight_layout() plt.show()  print(f\"Each patch is {img.size} pixels\") In\u00a0[\u00a0]: Copied! <pre># Train a self-supervised model using Lightly Train\nmodel_path = geoai.lightly_train_model(\n    data_dir=f\"{data_dir}/patches_0/images\",  # Use patches from first image\n    output_dir=output_dir,\n    model=\"torchvision/resnet50\",  # Base architecture\n    method=\"simclr\",  # Use SimCLR for CNN models like ResNet\n    epochs=10,  # Small number for demo\n    batch_size=32,  # Adjust based on your GPU\n    optim_args={\"lr\": 1e-4},  # Pass learning rate through optim_args\n    overwrite=True,\n)\n\nprint(f\"\\nModel training completed! Pretrained model saved to: {model_path}\")\n</pre> # Train a self-supervised model using Lightly Train model_path = geoai.lightly_train_model(     data_dir=f\"{data_dir}/patches_0/images\",  # Use patches from first image     output_dir=output_dir,     model=\"torchvision/resnet50\",  # Base architecture     method=\"simclr\",  # Use SimCLR for CNN models like ResNet     epochs=10,  # Small number for demo     batch_size=32,  # Adjust based on your GPU     optim_args={\"lr\": 1e-4},  # Pass learning rate through optim_args     overwrite=True, )  print(f\"\\nModel training completed! Pretrained model saved to: {model_path}\") In\u00a0[\u00a0]: Copied! <pre># Load the pretrained model\npretrained_model = geoai.load_lightly_pretrained_model(\n    model_path=model_path, model_architecture=\"torchvision/resnet50\"\n)\n\nprint(f\"Loaded pretrained model: {type(pretrained_model)}\")\nprint(f\"Model parameters: {sum(p.numel() for p in pretrained_model.parameters()):,}\")\n\n# The model is now ready for fine-tuning on your specific task\nprint(\"\\nModel is ready for fine-tuning on downstream tasks!\")\n</pre> # Load the pretrained model pretrained_model = geoai.load_lightly_pretrained_model(     model_path=model_path, model_architecture=\"torchvision/resnet50\" )  print(f\"Loaded pretrained model: {type(pretrained_model)}\") print(f\"Model parameters: {sum(p.numel() for p in pretrained_model.parameters()):,}\")  # The model is now ready for fine-tuning on your specific task print(\"\\nModel is ready for fine-tuning on downstream tasks!\") In\u00a0[\u00a0]: Copied! <pre># Generate embeddings for our images\n# Note: We use the checkpoint file (.ckpt) for embedding generation\ncheckpoint_path = os.path.join(output_dir, \"checkpoints\", \"last.ckpt\")\n\nembeddings_path = geoai.lightly_embed_images(\n    data_dir=f\"{data_dir}/patches_0/images\",\n    model_path=checkpoint_path,\n    output_path=f\"{embeddings_dir}/image_embeddings.pt\",\n    batch_size=32,\n    overwrite=True,\n)\n\nprint(f\"Embeddings saved to: {embeddings_path}\")\n</pre> # Generate embeddings for our images # Note: We use the checkpoint file (.ckpt) for embedding generation checkpoint_path = os.path.join(output_dir, \"checkpoints\", \"last.ckpt\")  embeddings_path = geoai.lightly_embed_images(     data_dir=f\"{data_dir}/patches_0/images\",     model_path=checkpoint_path,     output_path=f\"{embeddings_dir}/image_embeddings.pt\",     batch_size=32,     overwrite=True, )  print(f\"Embeddings saved to: {embeddings_path}\") In\u00a0[\u00a0]: Copied! <pre># Load and analyze the embeddings\nif os.path.exists(embeddings_path):\n    # Load PyTorch tensor embeddings\n    embeddings = torch.load(embeddings_path)\n\n    # Convert to numpy for analysis if it's a tensor\n    if isinstance(embeddings, torch.Tensor):\n        embeddings_np = embeddings.cpu().numpy()[\"embeddings\"]\n    else:\n        embeddings_np = embeddings[\"embeddings\"]\n\n    print(f\"Embeddings shape: {embeddings_np.shape}\")\n    print(f\"Embedding dimension: {embeddings_np.shape[1]}\")\n    print(f\"Number of images embedded: {embeddings_np.shape[0]}\")\n\n    # Basic statistics\n    print(f\"\\nEmbedding statistics:\")\n    print(f\"Mean: {embeddings_np.mean():.4f}\")\n    print(f\"Std: {embeddings_np.std():.4f}\")\n    print(f\"Min: {embeddings_np.min():.4f}\")\n    print(f\"Max: {embeddings_np.max():.4f}\")\nelse:\n    print(\n        \"Embeddings file not found. This might be due to the embedding generation process.\"\n    )\n</pre> # Load and analyze the embeddings if os.path.exists(embeddings_path):     # Load PyTorch tensor embeddings     embeddings = torch.load(embeddings_path)      # Convert to numpy for analysis if it's a tensor     if isinstance(embeddings, torch.Tensor):         embeddings_np = embeddings.cpu().numpy()[\"embeddings\"]     else:         embeddings_np = embeddings[\"embeddings\"]      print(f\"Embeddings shape: {embeddings_np.shape}\")     print(f\"Embedding dimension: {embeddings_np.shape[1]}\")     print(f\"Number of images embedded: {embeddings_np.shape[0]}\")      # Basic statistics     print(f\"\\nEmbedding statistics:\")     print(f\"Mean: {embeddings_np.mean():.4f}\")     print(f\"Std: {embeddings_np.std():.4f}\")     print(f\"Min: {embeddings_np.min():.4f}\")     print(f\"Max: {embeddings_np.max():.4f}\") else:     print(         \"Embeddings file not found. This might be due to the embedding generation process.\"     ) In\u00a0[\u00a0]: Copied! <pre># Visualize embeddings using t-SNE\nif os.path.exists(embeddings_path):\n    try:\n        from sklearn.manifold import TSNE\n        import matplotlib.pyplot as plt\n\n        # Apply t-SNE for dimensionality reduction\n        tsne = TSNE(\n            n_components=2, random_state=42, perplexity=min(30, len(embeddings_np) - 1)\n        )\n        embeddings_2d = tsne.fit_transform(embeddings_np)\n\n        # Create visualization\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(\n            embeddings_2d[:, 0],\n            embeddings_2d[:, 1],\n            c=range(len(embeddings_2d)),\n            cmap=\"viridis\",\n            alpha=0.7,\n        )\n        plt.colorbar(scatter)\n        plt.title(\"t-SNE Visualization of Self-Supervised Embeddings\")\n        plt.xlabel(\"t-SNE Component 1\")\n        plt.ylabel(\"t-SNE Component 2\")\n        plt.grid(True, alpha=0.3)\n        plt.show()\n\n        print(\"t-SNE visualization shows how the model groups similar image patches.\")\n        print(\n            \"Clusters indicate that the model has learned meaningful representations!\"\n        )\n\n    except ImportError:\n        print(\"scikit-learn not available for t-SNE visualization.\")\n        print(\"Install with: pip install scikit-learn\")\nelse:\n    print(\"Embeddings not available for visualization.\")\n</pre> # Visualize embeddings using t-SNE if os.path.exists(embeddings_path):     try:         from sklearn.manifold import TSNE         import matplotlib.pyplot as plt          # Apply t-SNE for dimensionality reduction         tsne = TSNE(             n_components=2, random_state=42, perplexity=min(30, len(embeddings_np) - 1)         )         embeddings_2d = tsne.fit_transform(embeddings_np)          # Create visualization         plt.figure(figsize=(10, 8))         scatter = plt.scatter(             embeddings_2d[:, 0],             embeddings_2d[:, 1],             c=range(len(embeddings_2d)),             cmap=\"viridis\",             alpha=0.7,         )         plt.colorbar(scatter)         plt.title(\"t-SNE Visualization of Self-Supervised Embeddings\")         plt.xlabel(\"t-SNE Component 1\")         plt.ylabel(\"t-SNE Component 2\")         plt.grid(True, alpha=0.3)         plt.show()          print(\"t-SNE visualization shows how the model groups similar image patches.\")         print(             \"Clusters indicate that the model has learned meaningful representations!\"         )      except ImportError:         print(\"scikit-learn not available for t-SNE visualization.\")         print(\"Install with: pip install scikit-learn\") else:     print(\"Embeddings not available for visualization.\") In\u00a0[\u00a0]: Copied! <pre># Uncomment to clean up temporary files\n# import shutil\n# shutil.rmtree(data_dir, ignore_errors=True)\n# shutil.rmtree(output_dir, ignore_errors=True)\n# shutil.rmtree(embeddings_dir, ignore_errors=True)\n# print(\"Temporary files cleaned up.\")\n</pre> # Uncomment to clean up temporary files # import shutil # shutil.rmtree(data_dir, ignore_errors=True) # shutil.rmtree(output_dir, ignore_errors=True) # shutil.rmtree(embeddings_dir, ignore_errors=True) # print(\"Temporary files cleaned up.\")"},{"location":"examples/lightly_train_self_supervised/#self-supervised-learning-with-lightly-train","title":"Self-Supervised Learning with Lightly Train\u00b6","text":"<p>This notebook demonstrates how to use Lightly Train for self-supervised pretraining on unlabeled geospatial imagery. You'll learn how to:</p> <ol> <li>Train a self-supervised model using various methods (DINOv2, DINO, SimCLR)</li> <li>Load and use the pretrained model</li> <li>Generate embeddings for downstream tasks</li> </ol> <p>Self-supervised learning is particularly useful for geospatial applications where labeled data is scarce or expensive to obtain.</p>"},{"location":"examples/lightly_train_self_supervised/#install-packages","title":"Install packages\u00b6","text":"<p>To use the <code>geoai-py</code> package with Lightly Train support, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/lightly_train_self_supervised/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/lightly_train_self_supervised/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use unlabeled satellite imagery for self-supervised pretraining. For this example, we'll download some sample NAIP imagery.</p>"},{"location":"examples/lightly_train_self_supervised/#prepare-training-data","title":"Prepare training data\u00b6","text":"<p>For self-supervised learning, we need to extract image patches from our geospatial imagery. These patches will be used as unlabeled training data.</p>"},{"location":"examples/lightly_train_self_supervised/#visualize-sample-training-data","title":"Visualize sample training data\u00b6","text":"<p>Let's take a look at some of the extracted patches that will be used for training.</p>"},{"location":"examples/lightly_train_self_supervised/#train-self-supervised-model","title":"Train self-supervised model\u00b6","text":"<p>Now we'll train a self-supervised model using Lightly Train. We'll use the SimCLR method, which works well with CNN models like ResNet.</p> <p>Method compatibility:</p> <ul> <li>SimCLR or DINO: Works with CNN models (ResNet, EfficientNet, etc.)</li> <li>DINOv2: Requires Vision Transformer (ViT) models only</li> </ul> <p>Note: Training will take some time depending on your hardware. For demonstration purposes, we're using a small number of epochs. In practice, you might want to train for 100+ epochs.</p>"},{"location":"examples/lightly_train_self_supervised/#load-pretrained-model","title":"Load pretrained model\u00b6","text":"<p>Once training is complete, we can load the pretrained model for use in downstream tasks.</p>"},{"location":"examples/lightly_train_self_supervised/#generate-embeddings","title":"Generate embeddings\u00b6","text":"<p>We can also use the pretrained model checkpoint to generate embeddings for our images. These embeddings capture rich representations learned through self-supervised training.</p> <p>Note: We need to use the checkpoint file (.ckpt) for embedding generation, not the exported model (.pt).</p>"},{"location":"examples/lightly_train_self_supervised/#analyze-embeddings","title":"Analyze embeddings\u00b6","text":"<p>Let's load and analyze the generated embeddings to understand what our model has learned.</p>"},{"location":"examples/lightly_train_self_supervised/#visualize-embeddings-with-t-sne","title":"Visualize embeddings with t-SNE\u00b6","text":"<p>Let's use t-SNE to visualize the high-dimensional embeddings in 2D space. This helps us understand how well the model groups similar images.</p>"},{"location":"examples/lightly_train_self_supervised/#next-steps","title":"Next steps\u00b6","text":"<p>Now that you have a pretrained self-supervised model, here are some ways you can use it:</p>"},{"location":"examples/lightly_train_self_supervised/#1-fine-tuning-for-specific-tasks","title":"1. Fine-tuning for specific tasks\u00b6","text":"<pre># Load your pretrained model\nmodel = geoai.load_lightly_pretrained_model(\n    model_path=\"path/to/your/model.pt\",\n    model_architecture=\"torchvision/resnet50\"\n)\n\n# Replace the final layer for your specific task\n# For example, for binary classification:\nimport torch.nn as nn\nmodel.fc = nn.Linear(model.fc.in_features, 2)  # 2 classes\n\n# Fine-tune with your labeled data using standard PyTorch training\n</pre>"},{"location":"examples/lightly_train_self_supervised/#2-feature-extraction","title":"2. Feature extraction\u00b6","text":"<pre># Use the model as a feature extractor\nmodel.eval()\n# Remove the final classification layer\nfeature_extractor = nn.Sequential(*list(model.children())[:-1])\n\n# Extract features for any new images\nwith torch.no_grad():\n    features = feature_extractor(your_image_tensor)\n</pre>"},{"location":"examples/lightly_train_self_supervised/#3-similarity-search","title":"3. Similarity search\u00b6","text":"<pre># Use embeddings for finding similar images\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Find most similar images to a query image\nsimilarities = cosine_similarity([query_embedding], embeddings)\nmost_similar_indices = similarities.argsort()[0][-5:]  # Top 5 similar\n</pre>"},{"location":"examples/lightly_train_self_supervised/#4-different-architectures-and-methods","title":"4. Different architectures and methods\u00b6","text":"<p>Try different combinations for your specific use case:</p> <p>For CNN models (ResNet, EfficientNet):</p> <pre># SimCLR method (recommended for CNNs)\nmodel_path = geoai.lightly_train_model(\n    data_dir=\"path/to/images\",\n    output_dir=\"output\",\n    model=\"torchvision/resnet50\",\n    method=\"simclr\",\n    epochs=100\n)\n\n# Or DINO method (also works with CNNs)\nmodel_path = geoai.lightly_train_model(\n    data_dir=\"path/to/images\",\n    output_dir=\"output\",\n    model=\"timm/efficientnet_b0\",\n    method=\"dino\",\n    epochs=100\n)\n</pre> <p>For Vision Transformer (ViT) models:</p> <pre># DINOv2 (recommended for ViTs, excellent for geospatial imagery)\nmodel_path = geoai.lightly_train_model(\n    data_dir=\"path/to/images\",\n    output_dir=\"output\",\n    model=\"timm/vit_base_patch16_224\",\n    method=\"dinov2\",\n    epochs=100\n)\n</pre> <p>Model architectures:</p> <ul> <li><code>\"torchvision/resnet50\"</code> - Good general purpose CNN</li> <li><code>\"torchvision/resnet101\"</code> - Larger CNN for more complex features</li> <li><code>\"timm/efficientnet_b0\"</code> - Efficient CNN architecture</li> <li><code>\"timm/vit_base_patch16_224\"</code> - Vision Transformer (use with dinov2)</li> </ul> <p>Self-supervised methods:</p> <ul> <li><code>\"simclr\"</code> - Contrastive learning, works with CNNs</li> <li><code>\"dino\"</code> - Works with both CNNs and ViTs</li> <li><code>\"dinov2\"</code> - Advanced method, requires ViT models</li> <li><code>\"dinov2_distillation\"</code> - Enhanced DINOv2, requires ViT models</li> </ul>"},{"location":"examples/lightly_train_self_supervised/#cleanup-optional","title":"Cleanup (optional)\u00b6","text":"<p>Remove temporary files if needed:</p>"},{"location":"examples/load_model_checkpoint/","title":"Load model checkpoint","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport os\n</pre> import geoai import os In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"checkpoint_test\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"checkpoint_test\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Initial training - just 10 epochs\nprint(\"Starting initial training for 10 epochs...\")\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/initial_training\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    batch_size=4,  # Smaller batch size for faster testing\n    num_epochs=10,  # Just 10 epochs for testing\n    learning_rate=0.001,\n    val_split=0.2,\n    save_best_only=False,  # Save checkpoints every 10 epochs\n    verbose=True,\n    plot_curves=True,\n)\nprint(\"Initial training completed!\")\n</pre> # Initial training - just 10 epochs print(\"Starting initial training for 10 epochs...\") geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/initial_training\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     batch_size=4,  # Smaller batch size for faster testing     num_epochs=10,  # Just 10 epochs for testing     learning_rate=0.001,     val_split=0.2,     save_best_only=False,  # Save checkpoints every 10 epochs     verbose=True,     plot_curves=True, ) print(\"Initial training completed!\") In\u00a0[\u00a0]: Copied! <pre># Check if checkpoint exists\ncheckpoint_path = f\"{out_folder}/initial_training/checkpoint_epoch_10.pth\"\nif os.path.exists(checkpoint_path):\n    print(f\"Found checkpoint: {checkpoint_path}\")\nelse:\n    print(f\"Checkpoint not found: {checkpoint_path}\")\n    print(\"Available files:\")\n    for f in os.listdir(f\"{out_folder}/initial_training\"):\n        print(f\"  {f}\")\n</pre> # Check if checkpoint exists checkpoint_path = f\"{out_folder}/initial_training/checkpoint_epoch_10.pth\" if os.path.exists(checkpoint_path):     print(f\"Found checkpoint: {checkpoint_path}\") else:     print(f\"Checkpoint not found: {checkpoint_path}\")     print(\"Available files:\")     for f in os.listdir(f\"{out_folder}/initial_training\"):         print(f\"  {f}\") In\u00a0[\u00a0]: Copied! <pre># Resume training from checkpoint\nprint(\"Resuming training from checkpoint...\")\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/resumed_training\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    batch_size=4,\n    num_epochs=20,  # Total epochs (will resume from epoch 10)\n    learning_rate=0.001,\n    val_split=0.2,\n    save_best_only=False,\n    verbose=True,\n    plot_curves=True,\n    checkpoint_path=checkpoint_path,\n    resume_training=True,  # Resume training state\n)\nprint(\"Resumed training completed!\")\n</pre> # Resume training from checkpoint print(\"Resuming training from checkpoint...\") geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/resumed_training\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     batch_size=4,     num_epochs=20,  # Total epochs (will resume from epoch 10)     learning_rate=0.001,     val_split=0.2,     save_best_only=False,     verbose=True,     plot_curves=True,     checkpoint_path=checkpoint_path,     resume_training=True,  # Resume training state ) print(\"Resumed training completed!\") In\u00a0[\u00a0]: Copied! <pre># Load weights only (no training state)\nprint(\"Loading model weights only (not resuming training state)...\")\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/weights_only_training\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    batch_size=4,\n    num_epochs=15,  # Train for 15 epochs starting from epoch 0\n    learning_rate=0.001,\n    val_split=0.2,\n    save_best_only=False,\n    verbose=True,\n    plot_curves=True,\n    checkpoint_path=checkpoint_path,\n    resume_training=False,  # Don't resume training state\n)\nprint(\"Weights-only training completed!\")\n</pre> # Load weights only (no training state) print(\"Loading model weights only (not resuming training state)...\") geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/weights_only_training\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     batch_size=4,     num_epochs=15,  # Train for 15 epochs starting from epoch 0     learning_rate=0.001,     val_split=0.2,     save_best_only=False,     verbose=True,     plot_curves=True,     checkpoint_path=checkpoint_path,     resume_training=False,  # Don't resume training state ) print(\"Weights-only training completed!\") In\u00a0[\u00a0]: Copied! <pre># Test inference with resumed model\nmasks_path = f\"{out_folder}/test_prediction.tif\"\nmodel_path = f\"{out_folder}/resumed_training/best_model.pth\"\n\nif os.path.exists(model_path):\n    print(f\"Running inference with model: {model_path}\")\n    geoai.semantic_segmentation(\n        input_path=test_raster_path,\n        output_path=masks_path,\n        model_path=model_path,\n        architecture=\"unet\",\n        encoder_name=\"resnet34\",\n        num_channels=3,\n        num_classes=2,\n        window_size=512,\n        overlap=256,\n        batch_size=4,\n    )\n    print(f\"Inference completed. Results saved to: {masks_path}\")\nelse:\n    print(f\"Model not found: {model_path}\")\n</pre> # Test inference with resumed model masks_path = f\"{out_folder}/test_prediction.tif\" model_path = f\"{out_folder}/resumed_training/best_model.pth\"  if os.path.exists(model_path):     print(f\"Running inference with model: {model_path}\")     geoai.semantic_segmentation(         input_path=test_raster_path,         output_path=masks_path,         model_path=model_path,         architecture=\"unet\",         encoder_name=\"resnet34\",         num_channels=3,         num_classes=2,         window_size=512,         overlap=256,         batch_size=4,     )     print(f\"Inference completed. Results saved to: {masks_path}\") else:     print(f\"Model not found: {model_path}\") In\u00a0[\u00a0]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n\n# Load training histories\ninitial_history_path = f\"{out_folder}/initial_training/training_history.pth\"\nresumed_history_path = f\"{out_folder}/resumed_training/training_history.pth\"\nweights_only_history_path = f\"{out_folder}/weights_only_training/training_history.pth\"\n\nhistories = {}\nfor name, path in [\n    (\"Initial (10 epochs)\", initial_history_path),\n    (\"Resumed (10\u219220 epochs)\", resumed_history_path),\n    (\"Weights Only (15 epochs)\", weights_only_history_path),\n]:\n    if os.path.exists(path):\n        histories[name] = torch.load(path)\n        print(f\"Loaded {name}: {len(histories[name]['train_losses'])} epochs\")\n    else:\n        print(f\"History not found: {path}\")\n</pre> import torch import matplotlib.pyplot as plt  # Load training histories initial_history_path = f\"{out_folder}/initial_training/training_history.pth\" resumed_history_path = f\"{out_folder}/resumed_training/training_history.pth\" weights_only_history_path = f\"{out_folder}/weights_only_training/training_history.pth\"  histories = {} for name, path in [     (\"Initial (10 epochs)\", initial_history_path),     (\"Resumed (10\u219220 epochs)\", resumed_history_path),     (\"Weights Only (15 epochs)\", weights_only_history_path), ]:     if os.path.exists(path):         histories[name] = torch.load(path)         print(f\"Loaded {name}: {len(histories[name]['train_losses'])} epochs\")     else:         print(f\"History not found: {path}\") In\u00a0[\u00a0]: Copied! <pre># Plot comparison - showing the continuation clearly\nif histories:\n    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n    # Get the individual histories\n    initial_history = histories[\"Initial (10 epochs)\"]\n    resumed_history = histories[\"Resumed (10\u219220 epochs)\"]\n    weights_only_history = histories[\"Weights Only (15 epochs)\"]\n\n    # Training Loss - show continuation clearly\n    initial_epochs = range(1, 11)\n    axes[0].plot(\n        initial_epochs,\n        initial_history[\"train_losses\"],\n        label=\"Initial (epochs 1-10)\",\n        marker=\"o\",\n        markersize=5,\n        color=\"blue\",\n        linewidth=3,\n        alpha=0.9,\n        zorder=3,\n    )\n\n    # Plot resumed training continuation (epochs 11-20 only)\n    resumed_epochs = range(11, 21)\n    resumed_continuation = resumed_history[\"train_losses\"][10:]  # epochs 11-20\n    axes[0].plot(\n        resumed_epochs,\n        resumed_continuation,\n        label=\"Resumed (epochs 11-20)\",\n        marker=\"s\",\n        markersize=5,\n        color=\"orange\",\n        linewidth=3,\n        alpha=0.9,\n        zorder=2,\n    )\n\n    # Plot weights only\n    weights_epochs = range(1, 16)\n    axes[0].plot(\n        weights_epochs,\n        weights_only_history[\"train_losses\"],\n        label=\"Weights Only (epochs 1-15)\",\n        marker=\"^\",\n        markersize=4,\n        color=\"green\",\n        linewidth=2,\n        alpha=0.7,\n        zorder=1,\n    )\n\n    # Add continuation line\n    axes[0].plot(\n        [10, 11],\n        [initial_history[\"train_losses\"][-1], resumed_continuation[0]],\n        color=\"red\",\n        linewidth=2,\n        linestyle=\"--\",\n        alpha=0.7,\n        label=\"Continuation\",\n    )\n\n    axes[0].set_title(\"Training Loss\", fontsize=14)\n    axes[0].set_xlabel(\"Epoch\")\n    axes[0].set_ylabel(\"Loss\")\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    # Validation IoU\n    axes[1].plot(\n        initial_epochs,\n        initial_history[\"val_ious\"],\n        label=\"Initial (epochs 1-10)\",\n        marker=\"o\",\n        markersize=5,\n        color=\"blue\",\n        linewidth=3,\n        alpha=0.9,\n        zorder=3,\n    )\n\n    resumed_iou_continuation = resumed_history[\"val_ious\"][10:]\n    axes[1].plot(\n        resumed_epochs,\n        resumed_iou_continuation,\n        label=\"Resumed (epochs 11-20)\",\n        marker=\"s\",\n        markersize=5,\n        color=\"orange\",\n        linewidth=3,\n        alpha=0.9,\n        zorder=2,\n    )\n\n    axes[1].plot(\n        weights_epochs,\n        weights_only_history[\"val_ious\"],\n        label=\"Weights Only (epochs 1-15)\",\n        marker=\"^\",\n        markersize=4,\n        color=\"green\",\n        linewidth=2,\n        alpha=0.7,\n        zorder=1,\n    )\n\n    axes[1].plot(\n        [10, 11],\n        [initial_history[\"val_ious\"][-1], resumed_iou_continuation[0]],\n        color=\"red\",\n        linewidth=2,\n        linestyle=\"--\",\n        alpha=0.7,\n        label=\"Continuation\",\n    )\n\n    axes[1].set_title(\"Validation IoU\", fontsize=14)\n    axes[1].set_xlabel(\"Epoch\")\n    axes[1].set_ylabel(\"IoU\")\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n\n    # Validation Dice\n    axes[2].plot(\n        initial_epochs,\n        initial_history[\"val_dices\"],\n        label=\"Initial (epochs 1-10)\",\n        marker=\"o\",\n        markersize=5,\n        color=\"blue\",\n        linewidth=3,\n        alpha=0.9,\n        zorder=3,\n    )\n\n    resumed_dice_continuation = resumed_history[\"val_dices\"][10:]\n    axes[2].plot(\n        resumed_epochs,\n        resumed_dice_continuation,\n        label=\"Resumed (epochs 11-20)\",\n        marker=\"s\",\n        markersize=5,\n        color=\"orange\",\n        linewidth=3,\n        alpha=0.9,\n        zorder=2,\n    )\n\n    axes[2].plot(\n        weights_epochs,\n        weights_only_history[\"val_dices\"],\n        label=\"Weights Only (epochs 1-15)\",\n        marker=\"^\",\n        markersize=4,\n        color=\"green\",\n        linewidth=2,\n        alpha=0.7,\n        zorder=1,\n    )\n\n    axes[2].plot(\n        [10, 11],\n        [initial_history[\"val_dices\"][-1], resumed_dice_continuation[0]],\n        color=\"red\",\n        linewidth=2,\n        linestyle=\"--\",\n        alpha=0.7,\n        label=\"Continuation\",\n    )\n\n    axes[2].set_title(\"Validation Dice\", fontsize=14)\n    axes[2].set_xlabel(\"Epoch\")\n    axes[2].set_ylabel(\"Dice\")\n    axes[2].legend()\n    axes[2].grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(\n        f\"{out_folder}/clear_training_comparison.png\", dpi=150, bbox_inches=\"tight\"\n    )\n    plt.show()\n\n    # Print explanation\n    print(\"\\n\" + \"=\" * 60)\n    print(\"CHECKPOINT RESUME VISUALIZATION EXPLANATION\")\n    print(\"=\" * 60)\n    print(\"\u2022 Blue line: Initial training (epochs 1-10)\")\n    print(\"\u2022 Orange line: Resumed training continuation (epochs 11-20)\")\n    print(\"\u2022 Green line: Weights-only training (fresh epochs 1-15)\")\n    print(\"\u2022 Red dashed line: Shows seamless continuation\")\n    print()\n    print(\"NOTE: In the original overlapping plot, the blue line was\")\n    print(\"completely covered by the orange line because resumed training\")\n    print(\"includes the exact same first 10 epochs plus 10 additional epochs.\")\n    print(\"=\" * 60)\n\n    # Print summary statistics\n    print(\"\\nTraining Summary:\")\n    for name, history in histories.items():\n        final_iou = history[\"val_ious\"][-1]\n        final_dice = history[\"val_dices\"][-1]\n        final_loss = history[\"train_losses\"][-1]\n        epochs_trained = len(history[\"train_losses\"])\n        print(f\"{name}: {epochs_trained} epochs\")\n        print(f\"  Final IoU: {final_iou:.4f}\")\n        print(f\"  Final Dice: {final_dice:.4f}\")\n        print(f\"  Final Loss: {final_loss:.4f}\")\n        print()\n\n    print(\"Training comparison plot saved to clear_training_comparison.png\")\nelse:\n    print(\"No training histories found for comparison\")\n</pre> # Plot comparison - showing the continuation clearly if histories:     fig, axes = plt.subplots(1, 3, figsize=(18, 5))      # Get the individual histories     initial_history = histories[\"Initial (10 epochs)\"]     resumed_history = histories[\"Resumed (10\u219220 epochs)\"]     weights_only_history = histories[\"Weights Only (15 epochs)\"]      # Training Loss - show continuation clearly     initial_epochs = range(1, 11)     axes[0].plot(         initial_epochs,         initial_history[\"train_losses\"],         label=\"Initial (epochs 1-10)\",         marker=\"o\",         markersize=5,         color=\"blue\",         linewidth=3,         alpha=0.9,         zorder=3,     )      # Plot resumed training continuation (epochs 11-20 only)     resumed_epochs = range(11, 21)     resumed_continuation = resumed_history[\"train_losses\"][10:]  # epochs 11-20     axes[0].plot(         resumed_epochs,         resumed_continuation,         label=\"Resumed (epochs 11-20)\",         marker=\"s\",         markersize=5,         color=\"orange\",         linewidth=3,         alpha=0.9,         zorder=2,     )      # Plot weights only     weights_epochs = range(1, 16)     axes[0].plot(         weights_epochs,         weights_only_history[\"train_losses\"],         label=\"Weights Only (epochs 1-15)\",         marker=\"^\",         markersize=4,         color=\"green\",         linewidth=2,         alpha=0.7,         zorder=1,     )      # Add continuation line     axes[0].plot(         [10, 11],         [initial_history[\"train_losses\"][-1], resumed_continuation[0]],         color=\"red\",         linewidth=2,         linestyle=\"--\",         alpha=0.7,         label=\"Continuation\",     )      axes[0].set_title(\"Training Loss\", fontsize=14)     axes[0].set_xlabel(\"Epoch\")     axes[0].set_ylabel(\"Loss\")     axes[0].legend()     axes[0].grid(True, alpha=0.3)      # Validation IoU     axes[1].plot(         initial_epochs,         initial_history[\"val_ious\"],         label=\"Initial (epochs 1-10)\",         marker=\"o\",         markersize=5,         color=\"blue\",         linewidth=3,         alpha=0.9,         zorder=3,     )      resumed_iou_continuation = resumed_history[\"val_ious\"][10:]     axes[1].plot(         resumed_epochs,         resumed_iou_continuation,         label=\"Resumed (epochs 11-20)\",         marker=\"s\",         markersize=5,         color=\"orange\",         linewidth=3,         alpha=0.9,         zorder=2,     )      axes[1].plot(         weights_epochs,         weights_only_history[\"val_ious\"],         label=\"Weights Only (epochs 1-15)\",         marker=\"^\",         markersize=4,         color=\"green\",         linewidth=2,         alpha=0.7,         zorder=1,     )      axes[1].plot(         [10, 11],         [initial_history[\"val_ious\"][-1], resumed_iou_continuation[0]],         color=\"red\",         linewidth=2,         linestyle=\"--\",         alpha=0.7,         label=\"Continuation\",     )      axes[1].set_title(\"Validation IoU\", fontsize=14)     axes[1].set_xlabel(\"Epoch\")     axes[1].set_ylabel(\"IoU\")     axes[1].legend()     axes[1].grid(True, alpha=0.3)      # Validation Dice     axes[2].plot(         initial_epochs,         initial_history[\"val_dices\"],         label=\"Initial (epochs 1-10)\",         marker=\"o\",         markersize=5,         color=\"blue\",         linewidth=3,         alpha=0.9,         zorder=3,     )      resumed_dice_continuation = resumed_history[\"val_dices\"][10:]     axes[2].plot(         resumed_epochs,         resumed_dice_continuation,         label=\"Resumed (epochs 11-20)\",         marker=\"s\",         markersize=5,         color=\"orange\",         linewidth=3,         alpha=0.9,         zorder=2,     )      axes[2].plot(         weights_epochs,         weights_only_history[\"val_dices\"],         label=\"Weights Only (epochs 1-15)\",         marker=\"^\",         markersize=4,         color=\"green\",         linewidth=2,         alpha=0.7,         zorder=1,     )      axes[2].plot(         [10, 11],         [initial_history[\"val_dices\"][-1], resumed_dice_continuation[0]],         color=\"red\",         linewidth=2,         linestyle=\"--\",         alpha=0.7,         label=\"Continuation\",     )      axes[2].set_title(\"Validation Dice\", fontsize=14)     axes[2].set_xlabel(\"Epoch\")     axes[2].set_ylabel(\"Dice\")     axes[2].legend()     axes[2].grid(True, alpha=0.3)      plt.tight_layout()     plt.savefig(         f\"{out_folder}/clear_training_comparison.png\", dpi=150, bbox_inches=\"tight\"     )     plt.show()      # Print explanation     print(\"\\n\" + \"=\" * 60)     print(\"CHECKPOINT RESUME VISUALIZATION EXPLANATION\")     print(\"=\" * 60)     print(\"\u2022 Blue line: Initial training (epochs 1-10)\")     print(\"\u2022 Orange line: Resumed training continuation (epochs 11-20)\")     print(\"\u2022 Green line: Weights-only training (fresh epochs 1-15)\")     print(\"\u2022 Red dashed line: Shows seamless continuation\")     print()     print(\"NOTE: In the original overlapping plot, the blue line was\")     print(\"completely covered by the orange line because resumed training\")     print(\"includes the exact same first 10 epochs plus 10 additional epochs.\")     print(\"=\" * 60)      # Print summary statistics     print(\"\\nTraining Summary:\")     for name, history in histories.items():         final_iou = history[\"val_ious\"][-1]         final_dice = history[\"val_dices\"][-1]         final_loss = history[\"train_losses\"][-1]         epochs_trained = len(history[\"train_losses\"])         print(f\"{name}: {epochs_trained} epochs\")         print(f\"  Final IoU: {final_iou:.4f}\")         print(f\"  Final Dice: {final_dice:.4f}\")         print(f\"  Final Loss: {final_loss:.4f}\")         print()      print(\"Training comparison plot saved to clear_training_comparison.png\") else:     print(\"No training histories found for comparison\")"},{"location":"examples/load_model_checkpoint/#load-checkpoint-and-resume-training-for-semantic-segmentation-model","title":"Load Checkpoint and Resume Training for Semantic Segmentation Model\u00b6","text":"<p>This notebook tests the new checkpoint loading and resume training functionality for the <code>train_segmentation_model</code> function. It demonstrates how to:</p> <ol> <li>Train a model for a few epochs</li> <li>Stop training and save a checkpoint</li> <li>Resume training from the checkpoint</li> <li>Load only model weights without resuming training state</li> </ol>"},{"location":"examples/load_model_checkpoint/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/load_model_checkpoint/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/load_model_checkpoint/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same dataset as the main segmentation example.</p>"},{"location":"examples/load_model_checkpoint/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/load_model_checkpoint/#initial-training-first-10-epochs","title":"Initial Training (First 10 epochs)\u00b6","text":"<p>First, we'll train a model for just 10 epochs and save checkpoints.</p>"},{"location":"examples/load_model_checkpoint/#resume-training-from-checkpoint","title":"Resume Training from Checkpoint\u00b6","text":"<p>Now we'll resume training from the checkpoint, continuing for another 10 epochs (total 20 epochs).</p>"},{"location":"examples/load_model_checkpoint/#load-model-weights-only-no-training-state-resume","title":"Load Model Weights Only (No Training State Resume)\u00b6","text":"<p>Finally, we'll test loading only the model weights without resuming the training state.</p>"},{"location":"examples/load_model_checkpoint/#load-best-model-for-inference","title":"Load Best Model for Inference\u00b6","text":"<p>Test loading the best model for inference.</p>"},{"location":"examples/load_model_checkpoint/#compare-training-histories","title":"Compare Training Histories\u00b6","text":"<p>Let's compare the training histories to verify that resuming worked correctly.</p>"},{"location":"examples/load_model_checkpoint/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrates the new checkpoint functionality:</p> <ol> <li>Initial Training: Trained for 10 epochs and saved checkpoints</li> <li>Resume Training: Successfully resumed from epoch 10 and continued to epoch 20</li> <li>Weights Only: Loaded model weights but started training from epoch 0</li> <li>Inference: Used the resumed model for inference</li> </ol> <p>The key features tested:</p> <ul> <li><code>checkpoint_path</code>: Path to the checkpoint file</li> <li><code>resume_training=True</code>: Resume training state (epoch, optimizer, scheduler, metrics)</li> <li><code>resume_training=False</code>: Load only model weights</li> </ul> <p>This functionality is useful for:</p> <ul> <li>Long training jobs that might be interrupted</li> <li>Experimenting with different training parameters after initial training</li> <li>Transfer learning from partially trained models</li> </ul>"},{"location":"examples/parking_spot_detection/","title":"Parking spot detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/parking_spots.tif\"\n)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/parking_spots.tif\" ) In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.ParkingSplotDetector()\n</pre> detector = geoai.ParkingSplotDetector() In\u00a0[\u00a0]: Copied! <pre>mask_path = detector.generate_masks(\n    raster_path=raster_path,\n    output_path=\"parking_masks.tif\",\n    confidence_threshold=0.5,\n    mask_threshold=0.5,\n    overlap=0.25,\n    chip_size=(256, 256),\n    min_object_area=10,\n    # max_object_area=5000,\n)\n</pre> mask_path = detector.generate_masks(     raster_path=raster_path,     output_path=\"parking_masks.tif\",     confidence_threshold=0.5,     mask_threshold=0.5,     overlap=0.25,     chip_size=(256, 256),     min_object_area=10,     # max_object_area=5000, ) <p>Convert the image masks to polygons and save the output GeoJSON file.</p> In\u00a0[\u00a0]: Copied! <pre>gdf = detector.vectorize_masks(\n    masks_path=\"parking_masks.tif\",\n    output_path=\"parking.geojson\",\n    min_object_area=300,\n    # max_object_area=5000,\n)\n</pre> gdf = detector.vectorize_masks(     masks_path=\"parking_masks.tif\",     output_path=\"parking.geojson\",     min_object_area=300,     # max_object_area=5000, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)"},{"location":"examples/parking_spot_detection/#parking-spot-detection","title":"Parking Spot Detection\u00b6","text":""},{"location":"examples/parking_spot_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/parking_spot_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/parking_spot_detection/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We will download a sample image from Hugging Face Hub to use for parking spot detection. You can find more high-resolution images from OpenAerialMap.</p>"},{"location":"examples/parking_spot_detection/#visualize-the-image","title":"Visualize the image\u00b6","text":""},{"location":"examples/parking_spot_detection/#initialize-the-model","title":"Initialize the model\u00b6","text":""},{"location":"examples/parking_spot_detection/#extract-parking-spots","title":"Extract parking spots\u00b6","text":"<p>Extract parking spots from the image using the model and save the output image.</p>"},{"location":"examples/parking_spot_detection/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/parking_spot_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/planetary_computer/","title":"Planetary computer","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>collections = geoai.pc_collection_list()\ncollections\n</pre> collections = geoai.pc_collection_list() collections In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area\n    time_range=\"2013-01-01/2014-12-31\",\n)\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area     time_range=\"2013-01-01/2014-12-31\", ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0])\n</pre> geoai.view_pc_item(item=items[0]) In\u00a0[\u00a0]: Copied! <pre>downloaded = geoai.pc_stac_download(\n    items, output_dir=\"data\", assets=[\"image\", \"thumbnail\"]\n)\n</pre> downloaded = geoai.pc_stac_download(     items, output_dir=\"data\", assets=[\"image\", \"thumbnail\"] ) In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"chesapeake-lc-13\",\n    bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area\n    time_range=\"2013-01-01/2014-12-31\",\n    max_items=10,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"chesapeake-lc-13\",     bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area     time_range=\"2013-01-01/2014-12-31\",     max_items=10, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], colormap_name=\"tab10\", basemap=\"SATELLITE\")\n</pre> geoai.view_pc_item(item=items[0], colormap_name=\"tab10\", basemap=\"SATELLITE\") In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(items[0], output_dir=\"data\", assets=[\"data\", \"rendered_preview\"])\n</pre> geoai.pc_stac_download(items[0], output_dir=\"data\", assets=[\"data\", \"rendered_preview\"]) In\u00a0[\u00a0]: Copied! <pre>ds = geoai.read_pc_item_asset(items[0], asset=\"data\")\n</pre> ds = geoai.read_pc_item_asset(items[0], asset=\"data\") In\u00a0[\u00a0]: Copied! <pre>ds\n</pre> ds In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"landsat-c2-l2\",\n    bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area\n    time_range=\"2024-10-27/2024-12-31\",\n    query={\"eo:cloud_cover\": {\"lt\": 1}},\n    max_items=10,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"landsat-c2-l2\",     bbox=[-76.6657, 39.2648, -76.6478, 39.2724],  # Baltimore area     time_range=\"2024-10-27/2024-12-31\",     query={\"eo:cloud_cover\": {\"lt\": 1}},     max_items=10, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], assets=[\"red\", \"green\", \"blue\"])\n</pre> geoai.view_pc_item(item=items[0], assets=[\"red\", \"green\", \"blue\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], assets=[\"nir08\", \"red\", \"green\"])\n</pre> geoai.view_pc_item(item=items[0], assets=[\"nir08\", \"red\", \"green\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(\n    item=items[0],\n    expression=\"(nir08-red)/(nir08+red)\",\n    rescale=\"-1,1\",\n    colormap_name=\"greens\",\n    name=\"NDVI Green\",\n)\n</pre> geoai.view_pc_item(     item=items[0],     expression=\"(nir08-red)/(nir08+red)\",     rescale=\"-1,1\",     colormap_name=\"greens\",     name=\"NDVI Green\", ) In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(\n    items[0], output_dir=\"data\", assets=[\"nir08\", \"red\", \"green\", \"blue\"]\n)\n</pre> geoai.pc_stac_download(     items[0], output_dir=\"data\", assets=[\"nir08\", \"red\", \"green\", \"blue\"] )"},{"location":"examples/planetary_computer/#download-data-from-planetary-computer","title":"Download Data from Planetary Computer\u00b6","text":""},{"location":"examples/planetary_computer/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/planetary_computer/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/planetary_computer/#retrieve-collections","title":"Retrieve collections\u00b6","text":""},{"location":"examples/planetary_computer/#search-naip-imagery","title":"Search NAIP imagery\u00b6","text":""},{"location":"examples/planetary_computer/#visualize-naip-imagery","title":"Visualize NAIP imagery\u00b6","text":""},{"location":"examples/planetary_computer/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/planetary_computer/#search-land-cover-data","title":"Search land cover data\u00b6","text":""},{"location":"examples/planetary_computer/#visualize-land-cover-data","title":"Visualize land cover data\u00b6","text":""},{"location":"examples/planetary_computer/#download-land-cover-data","title":"Download land cover data\u00b6","text":""},{"location":"examples/planetary_computer/#search-landsat-data","title":"Search Landsat data\u00b6","text":""},{"location":"examples/planetary_computer/#visualize-landsat-data","title":"Visualize Landsat data\u00b6","text":""},{"location":"examples/planetary_computer/#download-landsat-data","title":"Download Landsat data\u00b6","text":""},{"location":"examples/regularization/","title":"Regularization","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_original.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/buildings_original.geojson\" In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(vector_url, tiles=raster_url)\n</pre> geoai.view_vector_interactive(vector_url, tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.regularize(\n    data=vector_url,\n    simplify_tolerance=2.0,\n    allow_45_degree=True,\n    diagonal_threshold_reduction=30,\n    allow_circles=True,\n    circle_threshold=0.9,\n)\n</pre> gdf = geoai.regularize(     data=vector_url,     simplify_tolerance=2.0,     allow_45_degree=True,     diagonal_threshold_reduction=30,     allow_circles=True,     circle_threshold=0.9, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf,\n    right_layer=raster_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.3}},\n    basemap=raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf,     right_layer=raster_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.3}},     basemap=raster_url, ) In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(raster_url, name=\"NAIP\")\nm.add_vector(\n    vector_url, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Original\"\n)\nm.add_gdf(gdf, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\")\nlegend = {\n    \"Original\": \"yellow\",\n    \"Regularized\": \"red\",\n}\nm.add_legend(title=\"Building Footprints\", legend_dict=legend)\nm\n</pre> m = leafmap.Map() m.add_cog_layer(raster_url, name=\"NAIP\") m.add_vector(     vector_url, style={\"color\": \"yellow\", \"fillOpacity\": 0}, layer_name=\"Original\" ) m.add_gdf(gdf, style={\"color\": \"red\", \"fillOpacity\": 0}, layer_name=\"Regularized\") legend = {     \"Original\": \"yellow\",     \"Regularized\": \"red\", } m.add_legend(title=\"Building Footprints\", legend_dict=legend) m"},{"location":"examples/regularization/#regularization","title":"Regularization\u00b6","text":"<p>This example demonstrates how to regularize building footprints using the <code>regularize</code> function from the <code>geoai</code> package. The function is a wrapper around the <code>regularize_geodataframe</code> function from the buildingregulariser package. Credits to the original author, Nick Wright.</p> <p>It can be used to regularize features such as building footprints, solar panels, cars, and other objects that have a regular shape.</p>"},{"location":"examples/regularization/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/regularization/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/regularization/#use-sample-data","title":"Use sample data\u00b6","text":""},{"location":"examples/regularization/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/regularization/#regularize-buildings","title":"Regularize buildings\u00b6","text":""},{"location":"examples/regularization/#compare-results","title":"Compare results\u00b6","text":""},{"location":"examples/samgeo/","title":"Samgeo","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nfrom geoai.sam import SamGeo\n</pre> import geoai from geoai.sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/sam_demo_image.tif\"\n)\nimage = geoai.download_file(url)\n</pre> url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/sam_demo_image.tif\" ) image = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(url)\n</pre> geoai.view_raster(url) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(model=\"facebook/sam-vit-huge\", automatic=True)\n</pre> sam = SamGeo(model=\"facebook/sam-vit-huge\", automatic=True) In\u00a0[\u00a0]: Copied! <pre>sam.generate(source=image, output=\"masks.tif\", foreground=True)\n</pre> sam.generate(source=image, output=\"masks.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks()\n</pre> sam.show_masks() In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(alpha=0.6)\n</pre> sam.show_anns(alpha=0.6) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\"masks.tif\", colormap=\"tab20\", opacity=0.6, basemap=url)\n</pre> geoai.view_raster(\"masks.tif\", colormap=\"tab20\", opacity=0.6, basemap=url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=\"masks.tif\",\n    right_layer=url,\n    left_args={\n        \"colormap\": \"tab20\",\n        \"opacity\": 1,\n    },\n)\n</pre> geoai.create_split_map(     left_layer=\"masks.tif\",     right_layer=url,     left_args={         \"colormap\": \"tab20\",         \"opacity\": 1,     }, )"},{"location":"examples/samgeo/#semantic-segmentation-with-sam-and-geoai","title":"Semantic Segmentation with SAM and GeoAI\u00b6","text":""},{"location":"examples/samgeo/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/samgeo/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/samgeo/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/samgeo/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/samgeo/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"examples/samgeo/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/ship_detection/","title":"Ship detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_dubai.tif\"\n)\nraster_path = geoai.download_file(raster_url, \"ships_dubai.tif\")\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_dubai.tif\" ) raster_path = geoai.download_file(raster_url, \"ships_dubai.tif\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.ShipDetector()\n</pre> detector = geoai.ShipDetector() In\u00a0[\u00a0]: Copied! <pre>output_path = \"ships_dubai_masks.tif\"\n</pre> output_path = \"ships_dubai_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>masks_path = detector.generate_masks(\n    raster_path,\n    output_path=output_path,\n    confidence_threshold=0.9,\n    mask_threshold=0.7,\n    overlap=0.25,\n    chip_size=(256, 256),\n    batch_size=4,\n)\n</pre> masks_path = detector.generate_masks(     raster_path,     output_path=output_path,     confidence_threshold=0.9,     mask_threshold=0.7,     overlap=0.25,     chip_size=(256, 256),     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>gdf = detector.vectorize_masks(\n    output_path,\n    output_path=\"ships_dubai_masks.geojson\",\n    confidence_threshold=0.8,\n    min_object_area=100,\n    max_object_size=10000,\n)\n</pre> gdf = detector.vectorize_masks(     output_path,     output_path=\"ships_dubai_masks.geojson\",     confidence_threshold=0.8,     min_object_area=100,     max_object_size=10000, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\ngdf.head()\n</pre> gdf = geoai.add_geometric_properties(gdf) gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>m = geoai.view_raster(raster_url, backend=\"ipyleaflet\")\nm\n</pre> m = geoai.view_raster(raster_url, backend=\"ipyleaflet\") m <p>Use the drawing tool to select the area of interest.</p> In\u00a0[\u00a0]: Copied! <pre>aoi = m.user_rois\n\nif aoi is None:\n\n    aoi = {\n        \"type\": \"FeatureCollection\",\n        \"features\": [\n            {\n                \"type\": \"Feature\",\n                \"properties\": {},\n                \"geometry\": {\n                    \"type\": \"Polygon\",\n                    \"coordinates\": [\n                        [\n                            [55.133729, 25.110277],\n                            [55.134072, 25.11393],\n                            [55.134823, 25.115601],\n                            [55.136025, 25.117116],\n                            [55.137677, 25.118127],\n                            [55.140145, 25.118787],\n                            [55.142248, 25.11902],\n                            [55.142012, 25.118243],\n                            [55.140831, 25.116728],\n                            [55.13948, 25.116903],\n                            [55.137956, 25.116825],\n                            [55.136132, 25.115543],\n                            [55.13566, 25.114416],\n                            [55.135467, 25.1136],\n                            [55.135939, 25.112609],\n                            [55.136218, 25.111657],\n                            [55.13551, 25.110685],\n                            [55.134373, 25.110102],\n                            [55.133729, 25.110277],\n                        ]\n                    ],\n                },\n            }\n        ],\n    }\n</pre> aoi = m.user_rois  if aoi is None:      aoi = {         \"type\": \"FeatureCollection\",         \"features\": [             {                 \"type\": \"Feature\",                 \"properties\": {},                 \"geometry\": {                     \"type\": \"Polygon\",                     \"coordinates\": [                         [                             [55.133729, 25.110277],                             [55.134072, 25.11393],                             [55.134823, 25.115601],                             [55.136025, 25.117116],                             [55.137677, 25.118127],                             [55.140145, 25.118787],                             [55.142248, 25.11902],                             [55.142012, 25.118243],                             [55.140831, 25.116728],                             [55.13948, 25.116903],                             [55.137956, 25.116825],                             [55.136132, 25.115543],                             [55.13566, 25.114416],                             [55.135467, 25.1136],                             [55.135939, 25.112609],                             [55.136218, 25.111657],                             [55.13551, 25.110685],                             [55.134373, 25.110102],                             [55.133729, 25.110277],                         ]                     ],                 },             }         ],     } In\u00a0[\u00a0]: Copied! <pre>import geopandas as gpd\n</pre> import geopandas as gpd In\u00a0[\u00a0]: Copied! <pre>aoi_gdf = gpd.GeoDataFrame.from_features(aoi[\"features\"], crs=\"EPSG:4326\").to_crs(\n    gdf.crs\n)\n</pre> aoi_gdf = gpd.GeoDataFrame.from_features(aoi[\"features\"], crs=\"EPSG:4326\").to_crs(     gdf.crs ) <p>Intersect the selected area with the vectorized masks to filter the results.</p> In\u00a0[\u00a0]: Copied! <pre>gdf_filter = gdf[gdf.intersects(aoi_gdf.geometry[0])]\ngdf_filter.head()\n</pre> gdf_filter = gdf[gdf.intersects(aoi_gdf.geometry[0])] gdf_filter.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, column=\"confidence\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filter.to_file(\"ships_dubai.geojson\")\n</pre> gdf_filter.to_file(\"ships_dubai.geojson\") <p></p>"},{"location":"examples/ship_detection/#ship-detection","title":"Ship Detection\u00b6","text":"<p>This notebook demonstrates how to use the geoai package for ship detection using a pre-trained model.</p> <p></p>"},{"location":"examples/ship_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/ship_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/ship_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/ship_detection/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/ship_detection/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/ship_detection/#generate-masks","title":"Generate masks\u00b6","text":""},{"location":"examples/ship_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/ship_detection/#visualize-initial-results","title":"Visualize initial results\u00b6","text":""},{"location":"examples/ship_detection/#calculate-geometric-properties","title":"Calculate geometric properties\u00b6","text":""},{"location":"examples/ship_detection/#filter-results","title":"Filter results\u00b6","text":""},{"location":"examples/ship_detection/#visualize-final-results","title":"Visualize final results\u00b6","text":""},{"location":"examples/ship_detection/#save-results","title":"Save results\u00b6","text":""},{"location":"examples/solar_panel_detection/","title":"Solar panel detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\"\nraster_path = geoai.download_file(raster_url)\n</pre> raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\" raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(raster_path)\n</pre> geoai.print_raster_info(raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>detector = geoai.SolarPanelDetector()\n</pre> detector = geoai.SolarPanelDetector() In\u00a0[\u00a0]: Copied! <pre>output_path = \"solar_panel_masks.tif\"\n</pre> output_path = \"solar_panel_masks.tif\" In\u00a0[\u00a0]: Copied! <pre>masks_path = detector.generate_masks(\n    raster_path,\n    output_path=output_path,\n    confidence_threshold=0.4,\n    mask_threshold=0.5,\n    min_object_area=100,\n    overlap=0.25,\n    chip_size=(400, 400),\n    batch_size=4,\n    verbose=False,\n)\n</pre> masks_path = detector.generate_masks(     raster_path,     output_path=output_path,     confidence_threshold=0.4,     mask_threshold=0.5,     min_object_area=100,     overlap=0.25,     chip_size=(400, 400),     batch_size=4,     verbose=False, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    output_path,\n    indexes=[2],\n    colormap=\"autumn\",\n    layer_name=\"Solar Panels\",\n    basemap=raster_url,\n)\n</pre> geoai.view_raster(     output_path,     indexes=[2],     colormap=\"autumn\",     layer_name=\"Solar Panels\",     basemap=raster_url, ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.orthogonalize(\n    input_path=masks_path, output_path=\"solar_panel_masks.geojson\", epsilon=0.2\n)\n</pre> gdf = geoai.orthogonalize(     input_path=masks_path, output_path=\"solar_panel_masks.geojson\", epsilon=0.2 ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\ngdf.head()\n</pre> gdf = geoai.add_geometric_properties(gdf) gdf.head() In\u00a0[\u00a0]: Copied! <pre>print(len(gdf))\n</pre> print(len(gdf)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, column=\"elongation\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf, column=\"elongation\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filter = gdf[(gdf[\"elongation\"] &lt; 10) &amp; (gdf[\"area_m2\"] &gt; 5)]\nprint(len(gdf_filter))\n</pre> gdf_filter = gdf[(gdf[\"elongation\"] &lt; 10) &amp; (gdf[\"area_m2\"] &gt; 5)] print(len(gdf_filter)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filter, column=\"area_m2\", tiles=raster_url)\n</pre> geoai.view_vector_interactive(gdf_filter, column=\"area_m2\", tiles=raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    gdf_filter, style_kwds={\"color\": \"red\", \"fillOpacity\": 0}, tiles=raster_url\n)\n</pre> geoai.view_vector_interactive(     gdf_filter, style_kwds={\"color\": \"red\", \"fillOpacity\": 0}, tiles=raster_url ) In\u00a0[\u00a0]: Copied! <pre>gdf_filter[\"area_m2\"].hist()\n</pre> gdf_filter[\"area_m2\"].hist() In\u00a0[\u00a0]: Copied! <pre>gdf_filter[\"area_m2\"].describe()\n</pre> gdf_filter[\"area_m2\"].describe() In\u00a0[\u00a0]: Copied! <pre>gdf_filter[\"area_m2\"].sum()\n</pre> gdf_filter[\"area_m2\"].sum() In\u00a0[\u00a0]: Copied! <pre>gdf_filter.to_file(\"solar_panels.geojson\")\n</pre> gdf_filter.to_file(\"solar_panels.geojson\")"},{"location":"examples/solar_panel_detection/#solar-panel-detection","title":"Solar Panel Detection\u00b6","text":"<p>This notebook demonstrates how to use the geoai package for solar panel detection using a pre-trained model.</p> <p></p>"},{"location":"examples/solar_panel_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/solar_panel_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/solar_panel_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/solar_panel_detection/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/solar_panel_detection/#generate-masks","title":"Generate masks\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-masks","title":"Visualize masks\u00b6","text":""},{"location":"examples/solar_panel_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-initial-results","title":"Visualize initial results\u00b6","text":""},{"location":"examples/solar_panel_detection/#calculate-geometric-properties","title":"Calculate geometric properties\u00b6","text":""},{"location":"examples/solar_panel_detection/#filter-results","title":"Filter results\u00b6","text":""},{"location":"examples/solar_panel_detection/#visualize-final-results","title":"Visualize final results\u00b6","text":""},{"location":"examples/solar_panel_detection/#save-results","title":"Save results\u00b6","text":""},{"location":"examples/text_prompt_segmentation/","title":"Text prompt segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/trees_brazil.tif\"\n)\nraster_path = geoai.download_file(raster_url)\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/trees_brazil.tif\" ) raster_path = geoai.download_file(raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(raster_url)\n</pre> geoai.view_raster(raster_url) In\u00a0[\u00a0]: Copied! <pre>segmenter = geoai.CLIPSegmentation(tile_size=512, overlap=32)\n</pre> segmenter = geoai.CLIPSegmentation(tile_size=512, overlap=32) In\u00a0[\u00a0]: Copied! <pre>output_path = \"tree_masks.tif\"\ntext_prompt = \"trees\"\n</pre> output_path = \"tree_masks.tif\" text_prompt = \"trees\" In\u00a0[\u00a0]: Copied! <pre>segmenter.segment_image(\n    raster_path,\n    output_path=output_path,\n    text_prompt=text_prompt,\n    threshold=0.5,\n    smoothing_sigma=1.0,\n)\n</pre> segmenter.segment_image(     raster_path,     output_path=output_path,     text_prompt=text_prompt,     threshold=0.5,     smoothing_sigma=1.0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    output_path,\n    nodata=0,\n    opacity=0.8,\n    colormap=\"greens\",\n    layer_name=\"Trees\",\n    basemap=raster_url,\n)\n</pre> geoai.view_raster(     output_path,     nodata=0,     opacity=0.8,     colormap=\"greens\",     layer_name=\"Trees\",     basemap=raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=raster_url,\n    left_label=\"Trees\",\n    right_label=\"Satellite Image\",\n    left_args={\"nodata\": 0, \"opacity\": 0.8, \"colormap\": \"greens\"},\n    basemap=raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=raster_url,     left_label=\"Trees\",     right_label=\"Satellite Image\",     left_args={\"nodata\": 0, \"opacity\": 0.8, \"colormap\": \"greens\"},     basemap=raster_url, )"},{"location":"examples/text_prompt_segmentation/#text-prompt-segmentation","title":"Text Prompt Segmentation\u00b6","text":"<p>This example demonstrates how to use the <code>geoai-py</code> package for text prompt segmentation using CLIPSeg.</p> <p></p>"},{"location":"examples/text_prompt_segmentation/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/text_prompt_segmentation/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#visualize-data","title":"Visualize data\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#initialize-model","title":"Initialize model\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/text_prompt_segmentation/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_building_footprints_usa/","title":"Train building footprints usa","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/building_footprints_usa.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/building_footprints_usa.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"naip_test_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_building_footprints_usa/#train-a-building-footprints-detection-model-for-the-usa","title":"Train a Building Footprints Detection Model for the USA\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_building_footprints_usa/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_building_footprints_usa/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_car_detection/","title":"Train car detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/car_detection.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_test_7cm.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_7cm.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/car_detection.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_test_7cm.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"cars_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"cars_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"cars_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"cars_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_car_detection/#train-a-model-for-detecting-cars","title":"Train a Model for Detecting Cars\u00b6","text":""},{"location":"examples/train_car_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_car_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_car_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_car_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_car_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_car_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_car_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_car_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_car_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_instance_segmentation_model/","title":"Train instance segmentation model","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(train_raster_path)\n</pre> geoai.get_raster_info(train_raster_path) In\u00a0[\u00a0]: Copied! <pre>style_dict = {\n    \"color\": \"#ff0000\",\n    \"weight\": 2,\n    \"opacity\": 1,\n    # \"fill\": True,\n    # \"fillColor\": \"#ffffff\",\n    \"fillOpacity\": 0,\n    # \"dashArray\": \"9\"\n    # \"clickable\": True,\n}\nstyle_function = lambda x: style_dict\n\ngeoai.view_vector_interactive(\n    train_vector_path, tiles=train_raster_path, style_function=style_function\n)\n</pre> style_dict = {     \"color\": \"#ff0000\",     \"weight\": 2,     \"opacity\": 1,     # \"fill\": True,     # \"fillColor\": \"#ffffff\",     \"fillOpacity\": 0,     # \"dashArray\": \"9\"     # \"clickable\": True, } style_function = lambda x: style_dict  geoai.view_vector_interactive(     train_vector_path, tiles=train_raster_path, style_function=style_function ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_path)\n</pre> geoai.view_raster(test_raster_path) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"buildings_instance\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"buildings_instance\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train Mask R-CNN model\ngeoai.train_instance_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/instance_models\",\n    num_classes=2,  # background + building\n    num_channels=3,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n    visualize=True,\n    verbose=True,\n)\n</pre> # Train Mask R-CNN model geoai.train_instance_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/instance_models\",     num_classes=2,  # background + building     num_channels=3,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2,     visualize=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmasks_path = \"naip_test_instance_prediction.tif\"\nmodel_path = f\"{out_folder}/instance_models/best_model.pth\"\n</pre> # Define paths masks_path = \"naip_test_instance_prediction.tif\" model_path = f\"{out_folder}/instance_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run instance segmentation inference\ngeoai.instance_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    num_classes=2,\n    num_channels=3,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n)\n</pre> # Run instance segmentation inference geoai.instance_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     num_classes=2,     num_channels=3,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre># Run inference with higher confidence threshold\nmasks_path_high_conf = \"naip_test_instance_prediction_high_conf.tif\"\n\ngeoai.instance_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path_high_conf,\n    model_path=model_path,\n    num_classes=2,\n    num_channels=3,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.7,  # Higher threshold for more confident predictions\n    batch_size=4,\n)\n</pre> # Run inference with higher confidence threshold masks_path_high_conf = \"naip_test_instance_prediction_high_conf.tif\"  geoai.instance_segmentation(     input_path=test_raster_path,     output_path=masks_path_high_conf,     model_path=model_path,     num_classes=2,     num_channels=3,     window_size=512,     overlap=256,     confidence_threshold=0.7,  # Higher threshold for more confident predictions     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"naip_test_instance_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"naip_test_instance_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path, nodata=0, cmap=\"tab20\", basemap=test_raster_path, backend=\"ipyleaflet\"\n)\n</pre> geoai.view_raster(     masks_path, nodata=0, cmap=\"tab20\", basemap=test_raster_path, backend=\"ipyleaflet\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_path)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_path) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_path)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_path,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_path,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_path,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_path, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/instance_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/instance_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Uncomment to process multiple images\n# geoai.instance_segmentation_batch(\n#     input_dir=\"path/to/input/images\",\n#     output_dir=\"path/to/output/masks\",\n#     model_path=model_path,\n#     num_classes=2,\n#     num_channels=3,\n#     window_size=512,\n#     overlap=256,\n#     confidence_threshold=0.5,\n#     batch_size=4,\n# )\n</pre> # Uncomment to process multiple images # geoai.instance_segmentation_batch( #     input_dir=\"path/to/input/images\", #     output_dir=\"path/to/output/masks\", #     model_path=model_path, #     num_classes=2, #     num_channels=3, #     window_size=512, #     overlap=256, #     confidence_threshold=0.5, #     batch_size=4, # ) In\u00a0[\u00a0]: Copied! <pre># Example for 4-channel (RGBN) imagery\n# geoai.train_instance_segmentation_model(\n#     images_dir=f\"{out_folder}/images\",\n#     labels_dir=f\"{out_folder}/labels\",\n#     output_dir=f\"{out_folder}/instance_models_rgbn\",\n#     num_classes=2,\n#     num_channels=4,  # RGB + NIR\n#     batch_size=4,\n#     num_epochs=10,\n#     learning_rate=0.005,\n#     val_split=0.2,\n#     verbose=True,\n# )\n</pre> # Example for 4-channel (RGBN) imagery # geoai.train_instance_segmentation_model( #     images_dir=f\"{out_folder}/images\", #     labels_dir=f\"{out_folder}/labels\", #     output_dir=f\"{out_folder}/instance_models_rgbn\", #     num_classes=2, #     num_channels=4,  # RGB + NIR #     batch_size=4, #     num_epochs=10, #     learning_rate=0.005, #     val_split=0.2, #     verbose=True, # )"},{"location":"examples/train_instance_segmentation_model/#train-an-instance-segmentation-model-using-mask-r-cnn","title":"Train an Instance Segmentation Model using Mask R-CNN\u00b6","text":"<p>This notebook demonstrates how to train instance segmentation models for object detection (e.g., building detection) using Mask R-CNN. Unlike semantic segmentation, instance segmentation can distinguish between individual objects of the same class, providing separate masks for each instance.</p>"},{"location":"examples/train_instance_segmentation_model/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/train_instance_segmentation_model/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_instance_segmentation_model/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same dataset as the semantic segmentation example for consistency.</p>"},{"location":"examples/train_instance_segmentation_model/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_instance_segmentation_model/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create training tiles from the imagery and vector labels.</p>"},{"location":"examples/train_instance_segmentation_model/#train-instance-segmentation-model","title":"Train instance segmentation model\u00b6","text":"<p>Now we'll train an instance segmentation model using the <code>train_instance_segmentation_model</code> function. This function uses Mask R-CNN, which is specifically designed for instance segmentation tasks.</p>"},{"location":"examples/train_instance_segmentation_model/#key-differences-from-semantic-segmentation","title":"Key Differences from Semantic Segmentation:\u00b6","text":"<ul> <li>Instance Segmentation: Identifies and segments each individual object separately (e.g., distinguishes Building A from Building B)</li> <li>Semantic Segmentation: Only classifies pixels into categories (all buildings are treated as one class)</li> </ul>"},{"location":"examples/train_instance_segmentation_model/#model-architecture","title":"Model Architecture:\u00b6","text":"<p>Mask R-CNN combines:</p> <ul> <li>Faster R-CNN for object detection (bounding boxes)</li> <li>FCN for pixel-level segmentation (masks)</li> <li>ResNet-50 + FPN backbone for feature extraction</li> </ul>"},{"location":"examples/train_instance_segmentation_model/#training-parameters","title":"Training Parameters:\u00b6","text":"<ul> <li><code>num_classes</code>: Number of classes including background (default: 2 for background + buildings)</li> <li><code>num_channels</code>: Number of input channels (3 for RGB, 4 for RGBN)</li> <li><code>batch_size</code>: Typically smaller than semantic segmentation (4-8) due to model complexity</li> <li><code>num_epochs</code>: Number of training epochs</li> <li><code>learning_rate</code>: Initial learning rate (default: 0.005)</li> <li><code>val_split</code>: Fraction of data for validation (default: 0.2)</li> </ul>"},{"location":"examples/train_instance_segmentation_model/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image. The <code>instance_segmentation</code> function performs sliding window inference to handle large images.</p>"},{"location":"examples/train_instance_segmentation_model/#adjust-confidence-threshold-optional","title":"Adjust confidence threshold (optional)\u00b6","text":"<p>You can control which predictions to keep by adjusting the confidence threshold. Higher values (e.g., 0.7) will be more conservative and only keep high-confidence detections, while lower values (e.g., 0.3) will be more permissive.</p>"},{"location":"examples/train_instance_segmentation_model/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"examples/train_instance_segmentation_model/#add-geometric-properties","title":"Add geometric properties\u00b6","text":"<p>Calculate area, perimeter, and other geometric properties for each detected building.</p>"},{"location":"examples/train_instance_segmentation_model/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_instance_segmentation_model/#filter-by-area","title":"Filter by area\u00b6","text":"<p>Filter out small detections that might be noise or artifacts.</p>"},{"location":"examples/train_instance_segmentation_model/#compare-predictions-with-imagery","title":"Compare predictions with imagery\u00b6","text":""},{"location":"examples/train_instance_segmentation_model/#model-performance-analysis","title":"Model Performance Analysis\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/train_instance_segmentation_model/#instance-vs-semantic-segmentation-comparison","title":"Instance vs Semantic Segmentation Comparison\u00b6","text":""},{"location":"examples/train_instance_segmentation_model/#when-to-use-instance-segmentation","title":"When to use Instance Segmentation:\u00b6","text":"<ol> <li>Individual object analysis: When you need to count, measure, or analyze individual objects</li> <li>Overlapping objects: When objects of the same class may overlap or touch</li> <li>Object tracking: When tracking individual objects across frames or images</li> <li>Spatial relationships: When analyzing relationships between individual objects</li> </ol>"},{"location":"examples/train_instance_segmentation_model/#when-to-use-semantic-segmentation","title":"When to use Semantic Segmentation:\u00b6","text":"<ol> <li>Area coverage: When you only need to know what percentage of an image contains a certain class</li> <li>Land cover mapping: For continuous features like vegetation, water, roads</li> <li>Simpler models: When you want faster training and inference</li> <li>Pixel-level classification: When object boundaries are less important</li> </ol>"},{"location":"examples/train_instance_segmentation_model/#model-outputs","title":"Model Outputs:\u00b6","text":"<p>Instance Segmentation (Mask R-CNN):</p> <ul> <li>Bounding boxes for each object</li> <li>Confidence scores for each detection</li> <li>Binary mask for each individual object</li> <li>Class label for each object</li> </ul> <p>Semantic Segmentation:</p> <ul> <li>Single multi-class mask covering the entire image</li> <li>Probability map (optional)</li> <li>No distinction between individual objects</li> </ul>"},{"location":"examples/train_instance_segmentation_model/#performance-considerations","title":"Performance Considerations:\u00b6","text":"Aspect Instance Segmentation Semantic Segmentation Training Time Slower (more complex model) Faster Inference Time Slower Faster Memory Usage Higher Lower Accuracy Better for distinct objects Better for continuous classes Typical Batch Size 2-8 8-32"},{"location":"examples/train_instance_segmentation_model/#metrics","title":"Metrics:\u00b6","text":"<p>Instance Segmentation Metrics:</p> <ul> <li>AP (Average Precision): Precision at different IoU thresholds</li> <li>AP@0.5: Average Precision at IoU threshold of 0.5</li> <li>AP@0.75: Average Precision at IoU threshold of 0.75</li> <li>AR (Average Recall): Recall averaged across IoU thresholds</li> </ul> <p>Semantic Segmentation Metrics:</p> <ul> <li>IoU (Intersection over Union): Overlap between prediction and ground truth</li> <li>Dice Score: Similar to IoU but more sensitive to small objects</li> <li>Pixel Accuracy: Percentage of correctly classified pixels</li> </ul>"},{"location":"examples/train_instance_segmentation_model/#batch-processing-optional","title":"Batch Processing (Optional)\u00b6","text":"<p>If you have multiple images to process, you can use the batch inference function:</p>"},{"location":"examples/train_instance_segmentation_model/#advanced-multi-channel-input-rgbn","title":"Advanced: Multi-channel Input (RGBN)\u00b6","text":"<p>If your imagery includes a near-infrared (NIR) band, you can train with 4 channels:</p>"},{"location":"examples/train_landcover_classification/","title":"Train landcover classification","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/m_3807511_ne_18_060_20181104.tif\"\ntrain_landcover_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/m_3807511_ne_18_060_20181104_landcover.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/m_3807511_se_18_060_20181104.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/m_3807511_ne_18_060_20181104.tif\" train_landcover_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/m_3807511_ne_18_060_20181104_landcover.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/m_3807511_se_18_060_20181104.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_landcover_path = geoai.download_file(train_landcover_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_landcover_path = geoai.download_file(train_landcover_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_landcover_url, basemap=train_raster_url)\n</pre> geoai.view_raster(train_landcover_url, basemap=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"landcover\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_landcover_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"landcover\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_landcover_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=4,\n    num_classes=13,\n    batch_size=8,\n    num_epochs=50,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n    plot_curves=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=4,     num_classes=13,     batch_size=8,     num_epochs=50,     learning_rate=0.001,     val_split=0.2,     verbose=True,     plot_curves=True, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmasks_path = \"naip_test_semantic_prediction.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> # Define paths masks_path = \"naip_test_semantic_prediction.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run semantic segmentation inference\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=4,\n    num_classes=13,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> # Run semantic segmentation inference geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=4,     num_classes=13,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>geoai.write_colormap(masks_path, train_landcover_path, output=masks_path)\n</pre> geoai.write_colormap(masks_path, train_landcover_path, output=masks_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, basemap=test_raster_url)\n</pre> geoai.view_raster(masks_path, basemap=test_raster_url)"},{"location":"examples/train_landcover_classification/#train-a-land-cover-classification-model","title":"Train a Land Cover Classification Model\u00b6","text":""},{"location":"examples/train_landcover_classification/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/train_landcover_classification/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_landcover_classification/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_landcover_classification/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_landcover_classification/#create-training-data","title":"Create training data\u00b6","text":"<p>We will use the NAIP dataset for land cover classification. The classification scheme is adopted from the Chesapeake Land Cover project.</p> <p>Important Note for Land Cover Classification:</p> <ul> <li>Your label images should contain integer class values (0, 1, 2, ..., 13 for 13 classes)</li> <li>Do NOT use binary masks - the training code now properly handles multi-class labels</li> <li>Class 0 is typically background, classes 1-12 are your land cover types</li> </ul>"},{"location":"examples/train_landcover_classification/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_object_detection</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul>"},{"location":"examples/train_landcover_classification/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"examples/train_landcover_classification/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_object_detection_model/","title":"Train object detection model","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=4,     pretrained=True,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"naip_test_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_object_detection_model/#train-an-object-detection-model-with-geoai","title":"Train an Object Detection Model with GeoAI\u00b6","text":""},{"location":"examples/train_object_detection_model/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_object_detection_model/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_object_detection_model/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_object_detection_model/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_object_detection_model/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_object_detection_model/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_object_detection_model/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_object_detection_model/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_object_detection_model/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_segmentation_model/","title":"Train segmentation model","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(train_raster_path)\n</pre> geoai.get_raster_info(train_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_path)\n</pre> geoai.view_raster(test_raster_path) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"buildings\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"buildings\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=5,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and building     batch_size=8,     num_epochs=5,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/segformer_models\",\n    architecture=\"segformer\",\n    encoder_name=\"resnet152\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    batch_size=6,  # Smaller batch size for more complex model\n    num_epochs=5,\n    learning_rate=0.0005,\n    val_split=0.2,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/segformer_models\",     architecture=\"segformer\",     encoder_name=\"resnet152\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     batch_size=6,  # Smaller batch size for more complex model     num_epochs=5,     learning_rate=0.0005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmasks_path = \"naip_test_semantic_prediction.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> # Define paths masks_path = \"naip_test_semantic_prediction.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run semantic segmentation inference\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> # Run semantic segmentation inference geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre># Run inference with probability output\nprobability_path = \"naip_test_probability_map.tif\"\n\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n    probability_path=probability_path,  # Output probability map\n)\n</pre> # Run inference with probability output probability_path = \"naip_test_probability_map.tif\"  geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4,     probability_path=probability_path,  # Output probability map ) In\u00a0[\u00a0]: Copied! <pre># Visualize probability map for building class (band 2)\ngeoai.view_raster(\n    probability_path, indexes=[2], basemap=test_raster_path, backend=\"ipyleaflet\"\n)\n</pre> # Visualize probability map for building class (band 2) geoai.view_raster(     probability_path, indexes=[2], basemap=test_raster_path, backend=\"ipyleaflet\" ) <p>You can also control the classification threshold for binary segmentation. By default, argmax is used, but you can specify a custom threshold:</p> In\u00a0[\u00a0]: Copied! <pre># Run inference with custom probability threshold\nmasks_path_threshold = \"naip_test_semantic_prediction_threshold2.tif\"\n\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path_threshold,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n    probability_threshold=0.3,  # Only classify as building if probability &gt;= 0.7\n)\n</pre> # Run inference with custom probability threshold masks_path_threshold = \"naip_test_semantic_prediction_threshold2.tif\"  geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path_threshold,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4,     probability_threshold=0.3,  # Only classify as building if probability &gt;= 0.7 ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"naip_test_semantic_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"naip_test_semantic_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(masks_path, nodata=0, basemap=test_raster_path, backend=\"ipyleaflet\")\n</pre> geoai.view_raster(masks_path, nodata=0, basemap=test_raster_path, backend=\"ipyleaflet\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_path)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_path) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_path)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_path,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_path,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_path,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_path, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p>"},{"location":"examples/train_segmentation_model/#train-a-semantic-segmentation-model-using-segmentation-models-pytorch","title":"Train a Semantic Segmentation Model using Segmentation-Models-PyTorch\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models for object detection (e.g., building detection) using the segmentation-models-pytorch library. Unlike instance segmentation with Mask R-CNN, this approach treats the task as pixel-level binary classification.</p>"},{"location":"examples/train_segmentation_model/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/train_segmentation_model/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_segmentation_model/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the same dataset as the Mask R-CNN example for consistency.</p>"},{"location":"examples/train_segmentation_model/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_segmentation_model/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"examples/train_segmentation_model/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p>"},{"location":"examples/train_segmentation_model/#example-1-u-net-with-resnet34-encoder","title":"Example 1: U-Net with ResNet34 encoder\u00b6","text":""},{"location":"examples/train_segmentation_model/#example-2-segformer-with-resnet152-encoder","title":"Example 2: SegFormer with resnet152 encoder\u00b6","text":""},{"location":"examples/train_segmentation_model/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"examples/train_segmentation_model/#output-probability-map-optional","title":"Output probability map (optional)\u00b6","text":"<p>You can also output the probability map by providing the <code>probability_path</code> parameter. This will save a multi-band raster where each band represents the probability for each class (0-1 range).</p>"},{"location":"examples/train_segmentation_model/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"examples/train_segmentation_model/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"examples/train_segmentation_model/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_segmentation_model/#model-performance-analysis","title":"Model Performance Analysis\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/train_segmentation_model/#performance-metrics","title":"Performance Metrics\u00b6","text":"<p>IoU (Intersection over Union) and Dice score are both popular metrics used to evaluate the similarity between two binary masks\u2014often in image segmentation tasks. While they are related, they are not the same.</p>"},{"location":"examples/train_segmentation_model/#definitions","title":"\ud83d\udd38 Definitions\u00b6","text":""},{"location":"examples/train_segmentation_model/#iou-jaccard-index","title":"IoU (Jaccard Index)\u00b6","text":"<p>$$ \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} $$</p> <ul> <li>Measures the overlap between predicted region $A$ and ground truth region $B$ relative to their union.</li> <li>Ranges from 0 (no overlap) to 1 (perfect overlap).</li> </ul>"},{"location":"examples/train_segmentation_model/#dice-score-f1-score-for-sets","title":"Dice Score (F1 Score for Sets)\u00b6","text":"<p>$$ \\text{Dice} = \\frac{2|A \\cap B|}{|A| + |B|} $$</p> <ul> <li>Measures the overlap between $A$ and $B$, but gives more weight to the intersection.</li> <li>Also ranges from 0 to 1.</li> </ul>"},{"location":"examples/train_segmentation_model/#key-differences","title":"\ud83d\udd38 Key Differences\u00b6","text":"Metric Formula Penalizes Sensitivity IoU $\\frac{TP}{TP + FP + FN}$ FP and FN equally Less sensitive to small objects Dice $\\frac{2TP}{2TP + FP + FN}$ Less harsh on small mismatches More sensitive to small overlaps <p>TP: True Positive, FP: False Positive, FN: False Negative</p>"},{"location":"examples/train_segmentation_model/#relationship","title":"\ud83d\udd38 Relationship\u00b6","text":"<p>Dice and IoU are mathematically related:</p> <p>$$ \\text{Dice} = \\frac{2 \\cdot \\text{IoU}}{1 + \\text{IoU}} \\quad \\text{or} \\quad \\text{IoU} = \\frac{\\text{Dice}}{2 - \\text{Dice}} $$</p>"},{"location":"examples/train_segmentation_model/#when-to-use-what","title":"\ud83d\udd38 When to Use What\u00b6","text":"<ul> <li>IoU: Common in object detection and semantic segmentation benchmarks (e.g., COCO, Pascal VOC).</li> <li>Dice: Preferred in medical imaging and when class imbalance is an issue, due to its sensitivity to small regions.</li> </ul>"},{"location":"examples/train_ship_detection/","title":"Train ship detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_15cm.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ship_detection.geojson\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_test_15cm.tif\"\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_15cm.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ship_detection.geojson\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_test_15cm.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"ship_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"ship_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"ship_prediction.geojson\"\ngdf = geoai.raster_to_vector(masks_path, output_path)\n</pre> output_path = \"ship_prediction.geojson\" gdf = geoai.raster_to_vector(masks_path, output_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_ship_detection/#train-a-model-for-detecting-ships","title":"Train a Model for Detecting Ships\u00b6","text":""},{"location":"examples/train_ship_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_ship_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_ship_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_ship_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_ship_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_ship_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_ship_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_ship_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_ship_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_solar_panel_detection/","title":"Train solar panel detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\"\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.geojson\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_test_davis_ca.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\" train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.geojson\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_test_davis_ca.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=3,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=100,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=3,     pretrained=True,     batch_size=4,     num_epochs=100,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"solar_panels_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"solar_panels_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"solar_panels_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"solar_panels_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_solar_panel_detection/#train-a-model-for-detecting-solar-panels","title":"Train a Model for Detecting Solar Panels\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_solar_panel_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_solar_panel_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/train_timm_classifier/","title":"Train timm classifier","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py timm lightning datasets\n</pre> # %pip install geoai-py timm lightning datasets In\u00a0[\u00a0]: Copied! <pre>import os\nimport geoai\nfrom geoai.timm_train import (\n    list_timm_models,\n    get_timm_model,\n    RemoteSensingDataset,\n    train_timm_classifier,\n    predict_with_timm,\n)\n</pre> import os import geoai from geoai.timm_train import (     list_timm_models,     get_timm_model,     RemoteSensingDataset,     train_timm_classifier,     predict_with_timm, ) In\u00a0[\u00a0]: Copied! <pre># List ResNet models\nresnet_models = list_timm_models(filter=\"resnet\", limit=10)\nprint(\"ResNet models:\", resnet_models)\n</pre> # List ResNet models resnet_models = list_timm_models(filter=\"resnet\", limit=10) print(\"ResNet models:\", resnet_models) In\u00a0[\u00a0]: Copied! <pre># List EfficientNet models\nefficientnet_models = list_timm_models(filter=\"efficientnet\", limit=10)\nprint(\"EfficientNet models:\", efficientnet_models)\n</pre> # List EfficientNet models efficientnet_models = list_timm_models(filter=\"efficientnet\", limit=10) print(\"EfficientNet models:\", efficientnet_models) In\u00a0[\u00a0]: Copied! <pre># List Vision Transformer models\nvit_models = list_timm_models(filter=\"vit\", limit=10)\nprint(\"Vision Transformer models:\", vit_models)\n</pre> # List Vision Transformer models vit_models = list_timm_models(filter=\"vit\", limit=10) print(\"Vision Transformer models:\", vit_models) In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\nimport tempfile\nimport shutil\nfrom PIL import Image\n\n# Load EuroSAT RGB dataset from Hugging Face\nprint(\"Loading EuroSAT dataset from Hugging Face...\")\ndataset = load_dataset(\"timm/eurosat-rgb\", split=\"train\")\n\n# Create a temporary directory to save images\ntemp_dir = tempfile.mkdtemp(prefix=\"eurosat_\")\nprint(f\"Saving images to: {temp_dir}\")\n\n# Save images to disk organized by class\nclass_names = dataset.features[\"label\"].names\nprint(f\"Classes: {class_names}\")\n\nfor idx, sample in enumerate(dataset):\n    img = sample[\"image\"]\n    label = sample[\"label\"]\n    class_name = class_names[label]\n\n    # Create class directory\n    class_dir = os.path.join(temp_dir, class_name)\n    os.makedirs(class_dir, exist_ok=True)\n\n    # Save image as JPEG\n    img_path = os.path.join(class_dir, f\"{idx:05d}.jpg\")\n    img.save(img_path)\n\nprint(f\"Saved {len(dataset)} images to {temp_dir}\")\n</pre> from datasets import load_dataset import tempfile import shutil from PIL import Image  # Load EuroSAT RGB dataset from Hugging Face print(\"Loading EuroSAT dataset from Hugging Face...\") dataset = load_dataset(\"timm/eurosat-rgb\", split=\"train\")  # Create a temporary directory to save images temp_dir = tempfile.mkdtemp(prefix=\"eurosat_\") print(f\"Saving images to: {temp_dir}\")  # Save images to disk organized by class class_names = dataset.features[\"label\"].names print(f\"Classes: {class_names}\")  for idx, sample in enumerate(dataset):     img = sample[\"image\"]     label = sample[\"label\"]     class_name = class_names[label]      # Create class directory     class_dir = os.path.join(temp_dir, class_name)     os.makedirs(class_dir, exist_ok=True)      # Save image as JPEG     img_path = os.path.join(class_dir, f\"{idx:05d}.jpg\")     img.save(img_path)  print(f\"Saved {len(dataset)} images to {temp_dir}\") In\u00a0[\u00a0]: Copied! <pre>import glob\nfrom sklearn.model_selection import train_test_split\n\n# Get all image paths and labels\nimage_paths = []\nlabels = []\n\nfor class_idx, class_name in enumerate(class_names):\n    class_dir = os.path.join(temp_dir, class_name)\n    class_images = sorted(glob.glob(os.path.join(class_dir, \"*.jpg\")))\n\n    image_paths.extend(class_images)\n    labels.extend([class_idx] * len(class_images))\n\nprint(f\"Total images: {len(image_paths)}\")\nprint(f\"Number of classes: {len(class_names)}\")\nprint(f\"Class distribution:\")\nfor class_idx, class_name in enumerate(class_names):\n    count = labels.count(class_idx)\n    print(f\"  {class_name}: {count}\")\n</pre> import glob from sklearn.model_selection import train_test_split  # Get all image paths and labels image_paths = [] labels = []  for class_idx, class_name in enumerate(class_names):     class_dir = os.path.join(temp_dir, class_name)     class_images = sorted(glob.glob(os.path.join(class_dir, \"*.jpg\")))      image_paths.extend(class_images)     labels.extend([class_idx] * len(class_images))  print(f\"Total images: {len(image_paths)}\") print(f\"Number of classes: {len(class_names)}\") print(f\"Class distribution:\") for class_idx, class_name in enumerate(class_names):     count = labels.count(class_idx)     print(f\"  {class_name}: {count}\") In\u00a0[\u00a0]: Copied! <pre>train_paths, test_paths, train_labels, test_labels = train_test_split(\n    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n)\n\ntrain_paths, val_paths, train_labels, val_labels = train_test_split(\n    train_paths, train_labels, test_size=0.2, random_state=42, stratify=train_labels\n)\n\nprint(f\"Training samples: {len(train_paths)}\")\nprint(f\"Validation samples: {len(val_paths)}\")\nprint(f\"Test samples: {len(test_paths)}\")\n</pre> train_paths, test_paths, train_labels, test_labels = train_test_split(     image_paths, labels, test_size=0.2, random_state=42, stratify=labels )  train_paths, val_paths, train_labels, val_labels = train_test_split(     train_paths, train_labels, test_size=0.2, random_state=42, stratify=train_labels )  print(f\"Training samples: {len(train_paths)}\") print(f\"Validation samples: {len(val_paths)}\") print(f\"Test samples: {len(test_paths)}\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Show one sample from each class\nfig, axes = plt.subplots(2, 5, figsize=(20, 8))\n\nfor idx, class_name in enumerate(class_names):\n    ax = axes[idx // 5, idx % 5]\n\n    # Find first image of this class\n    img_idx = labels.index(idx)\n    img = Image.open(image_paths[img_idx])\n\n    ax.imshow(img)\n    ax.set_title(class_name, fontsize=12)\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt from PIL import Image  # Show one sample from each class fig, axes = plt.subplots(2, 5, figsize=(20, 8))  for idx, class_name in enumerate(class_names):     ax = axes[idx // 5, idx % 5]      # Find first image of this class     img_idx = labels.index(idx)     img = Image.open(image_paths[img_idx])      ax.imshow(img)     ax.set_title(class_name, fontsize=12)     ax.axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre># Create datasets\ntrain_dataset = RemoteSensingDataset(\n    image_paths=train_paths,\n    labels=train_labels,\n    num_channels=3,  # RGB images\n)\n\nval_dataset = RemoteSensingDataset(\n    image_paths=val_paths,\n    labels=val_labels,\n    num_channels=3,\n)\n\ntest_dataset = RemoteSensingDataset(\n    image_paths=test_paths,\n    labels=test_labels,\n    num_channels=3,\n)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n</pre> # Create datasets train_dataset = RemoteSensingDataset(     image_paths=train_paths,     labels=train_labels,     num_channels=3,  # RGB images )  val_dataset = RemoteSensingDataset(     image_paths=val_paths,     labels=val_labels,     num_channels=3, )  test_dataset = RemoteSensingDataset(     image_paths=test_paths,     labels=test_labels,     num_channels=3, )  print(f\"Train dataset size: {len(train_dataset)}\") print(f\"Validation dataset size: {len(val_dataset)}\") print(f\"Test dataset size: {len(test_dataset)}\") In\u00a0[\u00a0]: Copied! <pre># Train ResNet50 classifier\noutput_dir = \"timm_output/resnet50\"\n\nmodel = train_timm_classifier(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    test_dataset=test_dataset,\n    model_name=\"resnet50\",\n    num_classes=len(class_names),  # 10 classes\n    in_channels=3,\n    pretrained=True,\n    output_dir=output_dir,\n    batch_size=32,\n    num_epochs=20,\n    learning_rate=1e-3,\n    weight_decay=1e-4,\n    num_workers=4,\n    freeze_backbone=False,\n    monitor_metric=\"val_acc\",\n    mode=\"max\",\n    patience=5,\n    save_top_k=1,\n)\n</pre> # Train ResNet50 classifier output_dir = \"timm_output/resnet50\"  model = train_timm_classifier(     train_dataset=train_dataset,     val_dataset=val_dataset,     test_dataset=test_dataset,     model_name=\"resnet50\",     num_classes=len(class_names),  # 10 classes     in_channels=3,     pretrained=True,     output_dir=output_dir,     batch_size=32,     num_epochs=20,     learning_rate=1e-3,     weight_decay=1e-4,     num_workers=4,     freeze_backbone=False,     monitor_metric=\"val_acc\",     mode=\"max\",     patience=5,     save_top_k=1, ) In\u00a0[\u00a0]: Copied! <pre># Train EfficientNet-B0 classifier\noutput_dir = \"timm_output/efficientnet_b0\"\n\nmodel = train_timm_classifier(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    test_dataset=test_dataset,\n    model_name=\"efficientnet_b0\",\n    num_classes=len(class_names),\n    in_channels=3,\n    pretrained=True,\n    output_dir=output_dir,\n    batch_size=32,\n    num_epochs=20,\n    learning_rate=1e-3,\n    weight_decay=1e-4,\n    num_workers=4,\n    freeze_backbone=False,\n    monitor_metric=\"val_acc\",\n    mode=\"max\",\n    patience=5,\n    save_top_k=1,\n)\n</pre> # Train EfficientNet-B0 classifier output_dir = \"timm_output/efficientnet_b0\"  model = train_timm_classifier(     train_dataset=train_dataset,     val_dataset=val_dataset,     test_dataset=test_dataset,     model_name=\"efficientnet_b0\",     num_classes=len(class_names),     in_channels=3,     pretrained=True,     output_dir=output_dir,     batch_size=32,     num_epochs=20,     learning_rate=1e-3,     weight_decay=1e-4,     num_workers=4,     freeze_backbone=False,     monitor_metric=\"val_acc\",     mode=\"max\",     patience=5,     save_top_k=1, ) In\u00a0[\u00a0]: Copied! <pre># Fine-tune only the classifier head\noutput_dir = \"timm_output/resnet50_frozen\"\n\nmodel_frozen = train_timm_classifier(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    test_dataset=test_dataset,\n    model_name=\"resnet50\",\n    num_classes=len(class_names),\n    in_channels=3,\n    pretrained=True,\n    freeze_backbone=True,  # Freeze backbone weights\n    output_dir=output_dir,\n    batch_size=32,\n    num_epochs=10,  # Fewer epochs needed\n    learning_rate=1e-3,\n    monitor_metric=\"val_acc\",\n    mode=\"max\",\n)\n</pre> # Fine-tune only the classifier head output_dir = \"timm_output/resnet50_frozen\"  model_frozen = train_timm_classifier(     train_dataset=train_dataset,     val_dataset=val_dataset,     test_dataset=test_dataset,     model_name=\"resnet50\",     num_classes=len(class_names),     in_channels=3,     pretrained=True,     freeze_backbone=True,  # Freeze backbone weights     output_dir=output_dir,     batch_size=32,     num_epochs=10,  # Fewer epochs needed     learning_rate=1e-3,     monitor_metric=\"val_acc\",     mode=\"max\", ) In\u00a0[\u00a0]: Copied! <pre># Load the best model checkpoint\nfrom geoai.timm_train import TimmClassifier\nimport torch\n\n# Path to the best model checkpoint\ncheckpoint_path = \"timm_output/resnet50/models/last.ckpt\"\n\n# Load model\nmodel = TimmClassifier.load_from_checkpoint(checkpoint_path)\n\n# Make predictions\npredictions, probabilities = predict_with_timm(\n    model=model,\n    image_paths=test_paths[:20],  # Predict on first 20 test images\n    batch_size=8,\n    return_probabilities=True,\n)\n\nprint(f\"Predictions shape: {predictions.shape}\")\nprint(f\"Probabilities shape: {probabilities.shape}\")\nprint(f\"Sample predictions: {[class_names[p] for p in predictions[:5]]}\")\n</pre> # Load the best model checkpoint from geoai.timm_train import TimmClassifier import torch  # Path to the best model checkpoint checkpoint_path = \"timm_output/resnet50/models/last.ckpt\"  # Load model model = TimmClassifier.load_from_checkpoint(checkpoint_path)  # Make predictions predictions, probabilities = predict_with_timm(     model=model,     image_paths=test_paths[:20],  # Predict on first 20 test images     batch_size=8,     return_probabilities=True, )  print(f\"Predictions shape: {predictions.shape}\") print(f\"Probabilities shape: {probabilities.shape}\") print(f\"Sample predictions: {[class_names[p] for p in predictions[:5]]}\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Visualize predictions\nfig, axes = plt.subplots(4, 5, figsize=(20, 16))\n\nfor idx, ax in enumerate(axes.flat):\n    if idx &gt;= len(test_paths[:20]):\n        break\n\n    # Load and display image\n    img = Image.open(test_paths[idx])\n\n    ax.imshow(img)\n    pred_class = class_names[predictions[idx]]\n    true_class = class_names[test_labels[idx]]\n    confidence = probabilities[idx][predictions[idx]] * 100\n\n    color = \"green\" if predictions[idx] == test_labels[idx] else \"red\"\n    ax.set_title(\n        f\"Pred: {pred_class}\\nTrue: {true_class}\\n({confidence:.1f}%)\",\n        color=color,\n        fontsize=10,\n    )\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt from PIL import Image  # Visualize predictions fig, axes = plt.subplots(4, 5, figsize=(20, 16))  for idx, ax in enumerate(axes.flat):     if idx &gt;= len(test_paths[:20]):         break      # Load and display image     img = Image.open(test_paths[idx])      ax.imshow(img)     pred_class = class_names[predictions[idx]]     true_class = class_names[test_labels[idx]]     confidence = probabilities[idx][predictions[idx]] * 100      color = \"green\" if predictions[idx] == test_labels[idx] else \"red\"     ax.set_title(         f\"Pred: {pred_class}\\nTrue: {true_class}\\n({confidence:.1f}%)\",         color=color,         fontsize=10,     )     ax.axis(\"off\")  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels\n)\n\nprint(f\"Class weights: {class_weights}\")\n\n# Train with class weights\noutput_dir = \"timm_output/resnet50_weighted\"\n\nmodel_weighted = train_timm_classifier(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    model_name=\"resnet50\",\n    num_classes=len(class_names),\n    in_channels=3,\n    pretrained=True,\n    output_dir=output_dir,\n    batch_size=32,\n    num_epochs=20,\n    learning_rate=1e-3,\n    class_weights=class_weights.tolist(),  # Pass class weights\n    monitor_metric=\"val_acc\",\n    mode=\"max\",\n)\n</pre> from sklearn.utils.class_weight import compute_class_weight import numpy as np  # Compute class weights class_weights = compute_class_weight(     class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels )  print(f\"Class weights: {class_weights}\")  # Train with class weights output_dir = \"timm_output/resnet50_weighted\"  model_weighted = train_timm_classifier(     train_dataset=train_dataset,     val_dataset=val_dataset,     model_name=\"resnet50\",     num_classes=len(class_names),     in_channels=3,     pretrained=True,     output_dir=output_dir,     batch_size=32,     num_epochs=20,     learning_rate=1e-3,     class_weights=class_weights.tolist(),  # Pass class weights     monitor_metric=\"val_acc\",     mode=\"max\", ) In\u00a0[\u00a0]: Copied! <pre>from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Compute class weights\nclass_weights = compute_class_weight(\n    class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels\n)\n\nprint(f\"Class weights: {class_weights}\")\n\n# Train with class weights\noutput_dir = \"timm_output/resnet50_weighted\"\n\nmodel_weighted = train_timm_classifier(\n    train_dataset=train_dataset,\n    val_dataset=val_dataset,\n    model_name=\"resnet50\",\n    num_classes=len(class_names),\n    in_channels=3,\n    pretrained=True,\n    output_dir=output_dir,\n    batch_size=16,\n    num_epochs=20,\n    learning_rate=1e-3,\n    class_weights=class_weights.tolist(),  # Pass class weights\n    monitor_metric=\"val_acc\",\n    mode=\"max\",\n)\n</pre> from sklearn.utils.class_weight import compute_class_weight import numpy as np  # Compute class weights class_weights = compute_class_weight(     class_weight=\"balanced\", classes=np.unique(train_labels), y=train_labels )  print(f\"Class weights: {class_weights}\")  # Train with class weights output_dir = \"timm_output/resnet50_weighted\"  model_weighted = train_timm_classifier(     train_dataset=train_dataset,     val_dataset=val_dataset,     model_name=\"resnet50\",     num_classes=len(class_names),     in_channels=3,     pretrained=True,     output_dir=output_dir,     batch_size=16,     num_epochs=20,     learning_rate=1e-3,     class_weights=class_weights.tolist(),  # Pass class weights     monitor_metric=\"val_acc\",     mode=\"max\", )"},{"location":"examples/train_timm_classifier/#train-an-image-classifier-with-timm-models","title":"Train an Image Classifier with TIMM Models\u00b6","text":"<p>This notebook demonstrates how to train an image classification model using the PyTorch Image Models (timm) library. The <code>geoai.timm_train</code> module provides a high-level API for training state-of-the-art computer vision models on remote sensing imagery.</p>"},{"location":"examples/train_timm_classifier/#key-features","title":"Key Features\u00b6","text":"<ul> <li>1000+ Pre-trained Models: Access to ResNet, EfficientNet, Vision Transformers (ViT), ConvNeXt, and more</li> <li>Multi-channel Support: Train on RGB, RGBN (RGB + NIR), or any number of channels</li> <li>PyTorch Lightning Integration: Automatic training loops, checkpointing, and early stopping</li> <li>Transfer Learning: Fine-tune pretrained models or train from scratch</li> </ul>"},{"location":"examples/train_timm_classifier/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/train_timm_classifier/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_timm_classifier/#explore-available-models","title":"Explore Available Models\u00b6","text":"<p>The timm library provides over 1000 pretrained models. Let's explore some popular architectures:</p>"},{"location":"examples/train_timm_classifier/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>For this example, we'll use the EuroSAT RGB dataset from Hugging Face. This dataset contains Sentinel-2 satellite RGB images in 10 land use/land cover classes:</p> <ul> <li>AnnualCrop</li> <li>Forest</li> <li>HerbaceousVegetation</li> <li>Highway</li> <li>Industrial</li> <li>Pasture</li> <li>PermanentCrop</li> <li>Residential</li> <li>River</li> <li>SeaLake</li> </ul>"},{"location":"examples/train_timm_classifier/#prepare-training-data","title":"Prepare Training Data\u00b6","text":"<p>Now we'll load all image paths and create train/val/test splits.</p>"},{"location":"examples/train_timm_classifier/#split-data-into-train-validation-and-test-sets","title":"Split data into train, validation, and test sets\u00b6","text":""},{"location":"examples/train_timm_classifier/#create-datasets","title":"Create Datasets\u00b6","text":"<p>The <code>RemoteSensingDataset</code> class handles loading images with support for multi-channel imagery.</p>"},{"location":"examples/train_timm_classifier/#train-a-resnet50-classifier","title":"Train a ResNet50 Classifier\u00b6","text":"<p>Let's train a ResNet50 model with pretrained ImageNet weights for transfer learning on the 10-class EuroSAT dataset.</p>"},{"location":"examples/train_timm_classifier/#train-an-efficientnet-b0-classifier","title":"Train an EfficientNet-B0 Classifier\u00b6","text":"<p>EfficientNet models provide an excellent balance between accuracy and efficiency.</p>"},{"location":"examples/train_timm_classifier/#fine-tuning-with-frozen-backbone","title":"Fine-tuning with Frozen Backbone\u00b6","text":"<p>For faster training, you can freeze the backbone and only train the classification head:</p>"},{"location":"examples/train_timm_classifier/#make-predictions","title":"Make Predictions\u00b6","text":"<p>Use the trained model to make predictions on test images.</p>"},{"location":"examples/train_timm_classifier/#visualize-predictions","title":"Visualize Predictions\u00b6","text":""},{"location":"examples/train_timm_classifier/#using-class-weights-for-imbalanced-datasets","title":"Using Class Weights for Imbalanced Datasets\u00b6","text":"<p>When dealing with imbalanced datasets, you can provide class weights to the loss function:</p>"},{"location":"examples/train_timm_classifier/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated:</p> <ol> <li>Model Selection: Exploring 1000+ available timm models (ResNet, EfficientNet, ViT)</li> <li>Data Loading: Using the EuroSAT RGB dataset from Hugging Face</li> <li>Training: Training various architectures on 10-class land cover classification</li> <li>Transfer Learning: Fine-tuning pretrained models with frozen backbones</li> <li>Inference: Making predictions and visualizations</li> <li>Class Weighting: Handling imbalanced datasets</li> </ol>"},{"location":"examples/train_timm_classifier/#key-parameters","title":"Key Parameters\u00b6","text":"<ul> <li><code>model_name</code>: Choose from 1000+ timm models</li> <li><code>num_classes</code>: Number of output classes</li> <li><code>in_channels</code>: Number of input channels (3 for RGB, 4 for RGBN, etc.)</li> <li><code>pretrained</code>: Use ImageNet pretrained weights for transfer learning</li> <li><code>freeze_backbone</code>: Freeze backbone for faster fine-tuning</li> <li><code>class_weights</code>: Handle imbalanced datasets</li> <li><code>monitor_metric</code>: Track 'val_loss' or 'val_acc' for checkpointing</li> <li><code>patience</code>: Early stopping patience</li> </ul>"},{"location":"examples/train_timm_classifier/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Experiment with different model architectures (ConvNeXt, Swin Transformer, etc.)</li> <li>Try data augmentation for improved performance</li> <li>Use learning rate schedulers for better convergence</li> <li>Deploy models for inference on satellite imagery</li> </ul>"},{"location":"examples/train_timm_classifier/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated:</p> <ol> <li>Model Selection: Exploring 1000+ available timm models</li> <li>Data Preparation: Creating datasets for remote sensing imagery</li> <li>Training: Training various architectures (ResNet, EfficientNet, ViT)</li> <li>Multi-channel Support: Handling 4-band RGBN imagery</li> <li>Transfer Learning: Fine-tuning pretrained models with frozen backbones</li> <li>Inference: Making predictions on new images</li> <li>Class Weighting: Handling imbalanced datasets</li> </ol>"},{"location":"examples/train_timm_classifier/#key-parameters","title":"Key Parameters\u00b6","text":"<ul> <li><code>model_name</code>: Choose from 1000+ timm models</li> <li><code>num_classes</code>: Number of output classes</li> <li><code>in_channels</code>: Number of input channels (3 for RGB, 4 for RGBN, etc.)</li> <li><code>pretrained</code>: Use ImageNet pretrained weights for transfer learning</li> <li><code>freeze_backbone</code>: Freeze backbone for faster fine-tuning</li> <li><code>class_weights</code>: Handle imbalanced datasets</li> <li><code>monitor_metric</code>: Track 'val_loss' or 'val_acc' for checkpointing</li> <li><code>patience</code>: Early stopping patience</li> </ul>"},{"location":"examples/train_timm_classifier/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Experiment with different model architectures</li> <li>Try data augmentation for improved performance</li> <li>Use learning rate schedulers for better convergence</li> <li>Deploy models for inference on large raster datasets</li> </ul>"},{"location":"examples/train_timm_segmentation/","title":"Train timm segmentation","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py timm segmentation-models-pytorch lightning\n</pre> # %pip install geoai-py timm segmentation-models-pytorch lightning In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre># List some popular encoders\nprint(\"ResNet encoders:\", geoai.list_timm_models(filter=\"resnet\", limit=5))\nprint(\"EfficientNet encoders:\", geoai.list_timm_models(filter=\"efficientnet\", limit=5))\nprint(\"ConvNeXt encoders:\", geoai.list_timm_models(filter=\"convnext\", limit=5))\n</pre> # List some popular encoders print(\"ResNet encoders:\", geoai.list_timm_models(filter=\"resnet\", limit=5)) print(\"EfficientNet encoders:\", geoai.list_timm_models(filter=\"efficientnet\", limit=5)) print(\"ConvNeXt encoders:\", geoai.list_timm_models(filter=\"convnext\", limit=5)) In\u00a0[\u00a0]: Copied! <pre># Download NAIP aerial imagery and building footprints\ntrain_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n\ntrain_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> # Download NAIP aerial imagery and building footprints train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" )  train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"timm_buildings\"\n</pre> out_folder = \"timm_buildings\" In\u00a0[\u00a0]: Copied! <pre># Create training chips\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> # Create training chips tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net with ResNet50 encoder\ngeoai.train_timm_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_resnet50\",\n    encoder_name=\"resnet50\",\n    architecture=\"unet\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=20,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net with ResNet50 encoder geoai.train_timm_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_resnet50\",     encoder_name=\"resnet50\",     architecture=\"unet\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and building     batch_size=8,     num_epochs=20,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Train DeepLabV3+ with EfficientNet-B3\ngeoai.train_timm_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/deeplabv3plus_efficientnet_b3\",\n    encoder_name=\"efficientnet-b3\",  # Note: use dash for SMP encoders\n    architecture=\"deeplabv3plus\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    batch_size=8,\n    num_epochs=20,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train DeepLabV3+ with EfficientNet-B3 geoai.train_timm_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/deeplabv3plus_efficientnet_b3\",     encoder_name=\"efficientnet-b3\",  # Note: use dash for SMP encoders     architecture=\"deeplabv3plus\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     batch_size=8,     num_epochs=20,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_resnet50/models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_resnet50/models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Fine-tune with frozen encoder\ngeoai.train_timm_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_resnet50_frozen\",\n    encoder_name=\"resnet50\",\n    architecture=\"unet\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    freeze_encoder=True,  # Freeze encoder weights\n    batch_size=8,\n    num_epochs=10,  # Fewer epochs needed\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Fine-tune with frozen encoder geoai.train_timm_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_resnet50_frozen\",     encoder_name=\"resnet50\",     architecture=\"unet\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     freeze_encoder=True,  # Freeze encoder weights     batch_size=8,     num_epochs=10,  # Fewer epochs needed     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Run inference\nmasks_path = \"naip_test_timm_prediction.tif\"\nmodel_path = f\"{out_folder}/unet_resnet50/models/last.ckpt\"\n\ngeoai.timm_semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    encoder_name=\"resnet50\",\n    architecture=\"unet\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> # Run inference masks_path = \"naip_test_timm_prediction.tif\" model_path = f\"{out_folder}/unet_resnet50/models/last.ckpt\"  geoai.timm_semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     encoder_name=\"resnet50\",     architecture=\"unet\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre># Vectorize the mask\noutput_vector_path = \"naip_test_timm_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> # Vectorize the mask output_vector_path = \"naip_test_timm_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre># Add geometric properties\ngdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> # Add geometric properties gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre># Visualize results\ngeoai.view_raster(masks_path, nodata=0, basemap=test_raster_path, backend=\"ipyleaflet\")\n</pre> # Visualize results geoai.view_raster(masks_path, nodata=0, basemap=test_raster_path, backend=\"ipyleaflet\") In\u00a0[\u00a0]: Copied! <pre># Filter buildings by area and visualize\ngdf_filtered = gdf_props[gdf_props[\"area_m2\"] &gt; 50]\ngeoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_path)\n</pre> # Filter buildings by area and visualize gdf_filtered = gdf_props[gdf_props[\"area_m2\"] &gt; 50] geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_path) In\u00a0[\u00a0]: Copied! <pre># Create split map comparison\ngeoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_path,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_path,\n)\n</pre> # Create split map comparison geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_path,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_path, ) In\u00a0[\u00a0]: Copied! <pre># # Example: Load and fine-tune a model from HF Hub\n# geoai.train_timm_segmentation_model(\n#     images_dir=f\"{out_folder}/images\",\n#     labels_dir=f\"{out_folder}/labels\",\n#     output_dir=f\"{out_folder}/hf_hub_model\",\n#     use_timm_model=True,\n#     timm_model_name=\"hf-hub:username/model-name\",  # Replace with actual HF Hub model\n#     num_channels=3,\n#     num_classes=2,\n#     batch_size=8,\n#     num_epochs=10,\n#     learning_rate=0.0001,  # Lower LR for fine-tuning\n#     val_split=0.2,\n#     verbose=True,\n# )\n</pre> # # Example: Load and fine-tune a model from HF Hub # geoai.train_timm_segmentation_model( #     images_dir=f\"{out_folder}/images\", #     labels_dir=f\"{out_folder}/labels\", #     output_dir=f\"{out_folder}/hf_hub_model\", #     use_timm_model=True, #     timm_model_name=\"hf-hub:username/model-name\",  # Replace with actual HF Hub model #     num_channels=3, #     num_classes=2, #     batch_size=8, #     num_epochs=10, #     learning_rate=0.0001,  # Lower LR for fine-tuning #     val_split=0.2, #     verbose=True, # ) In\u00a0[\u00a0]: Copied! <pre># # Example: Load model from HF Hub and run inference\n# # First, download the model from HF Hub (this would happen automatically)\n# # Then run inference with the downloaded model\n# geoai.timm_semantic_segmentation(\n#     input_path=test_raster_path,\n#     output_path=\"naip_test_hf_hub_prediction.tif\",\n#     model_path=\"path/to/downloaded/model.pth\",  # Path to downloaded HF Hub model\n#     encoder_name=\"resnet50\",\n#     architecture=\"unet\",\n#     num_channels=3,\n#     num_classes=2,\n#     use_timm_model=False,  # Set to True if it's a pure timm model\n#     window_size=512,\n#     overlap=256,\n#     batch_size=4,\n# )\n</pre> # # Example: Load model from HF Hub and run inference # # First, download the model from HF Hub (this would happen automatically) # # Then run inference with the downloaded model # geoai.timm_semantic_segmentation( #     input_path=test_raster_path, #     output_path=\"naip_test_hf_hub_prediction.tif\", #     model_path=\"path/to/downloaded/model.pth\",  # Path to downloaded HF Hub model #     encoder_name=\"resnet50\", #     architecture=\"unet\", #     num_channels=3, #     num_classes=2, #     use_timm_model=False,  # Set to True if it's a pure timm model #     window_size=512, #     overlap=256, #     batch_size=4, # ) In\u00a0[\u00a0]: Copied! <pre># # Example: Push your trained model to HF Hub\n# url = geoai.push_timm_model_to_hub(\n#     model_path=f\"{out_folder}/unet_resnet50/models/last.ckpt\",\n#     repo_id=\"your-username/building-segmentation-resnet50-unet\",  # Replace with your repo\n#     encoder_name=\"resnet50\",\n#     architecture=\"unet\",\n#     num_channels=3,\n#     num_classes=2,\n#     commit_message=\"Upload building segmentation model trained on NAIP imagery\",\n#     private=False,  # Set to True for private repository\n# )\n# print(f\"Model uploaded to: {url}\")\n</pre> # # Example: Push your trained model to HF Hub # url = geoai.push_timm_model_to_hub( #     model_path=f\"{out_folder}/unet_resnet50/models/last.ckpt\", #     repo_id=\"your-username/building-segmentation-resnet50-unet\",  # Replace with your repo #     encoder_name=\"resnet50\", #     architecture=\"unet\", #     num_channels=3, #     num_classes=2, #     commit_message=\"Upload building segmentation model trained on NAIP imagery\", #     private=False,  # Set to True for private repository # ) # print(f\"Model uploaded to: {url}\")"},{"location":"examples/train_timm_segmentation/#train-a-semantic-segmentation-model-with-timm-encoders","title":"Train a Semantic Segmentation Model with TIMM Encoders\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models using PyTorch Image Models (timm) encoders. This approach combines:</p> <ul> <li>1000+ TIMM Encoders: State-of-the-art backbones (ResNet, EfficientNet, ViT, ConvNeXt, etc.)</li> <li>9 Architectures: U-Net, U-Net++, DeepLabV3+, FPN, PSPNet, LinkNet, MANet, PAN</li> <li>Multi-channel Support: RGB, RGBN, or any number of input channels</li> <li>Simplified API: Similar to <code>train_segmentation_model</code> for ease of use</li> </ul>"},{"location":"examples/train_timm_segmentation/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/train_timm_segmentation/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_timm_segmentation/#explore-available-encoders","title":"Explore Available Encoders\u00b6","text":"<p>The timm library provides 1000+ encoders that can be used with segmentation architectures:</p>"},{"location":"examples/train_timm_segmentation/#download-sample-data","title":"Download Sample Data\u00b6","text":"<p>We'll use the same NAIP building detection dataset as the <code>train_segmentation_model</code> example.</p>"},{"location":"examples/train_timm_segmentation/#visualize-sample-data","title":"Visualize Sample Data\u00b6","text":""},{"location":"examples/train_timm_segmentation/#create-training-data","title":"Create Training Data\u00b6","text":"<p>Generate image chips and corresponding segmentation masks:</p>"},{"location":"examples/train_timm_segmentation/#train-u-net-with-resnet50-encoder","title":"Train U-Net with ResNet50 Encoder\u00b6","text":"<p>The <code>train_timm_segmentation_model</code> function provides a simplified interface similar to <code>train_segmentation_model</code>:</p>"},{"location":"examples/train_timm_segmentation/#train-deeplabv3-with-efficientnet-b3-encoder","title":"Train DeepLabV3+ with EfficientNet-B3 Encoder\u00b6","text":"<p>EfficientNet encoders provide excellent performance with fewer parameters:</p>"},{"location":"examples/train_timm_segmentation/#model-performance-analysis","title":"Model Performance Analysis\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/train_timm_segmentation/#performance-metrics","title":"Performance Metrics\u00b6","text":"<p>IoU (Intersection over Union) is the primary metric used to evaluate semantic segmentation performance.</p>"},{"location":"examples/train_timm_segmentation/#iou-definition","title":"\ud83d\udd38 IoU Definition\u00b6","text":"<p>$$ \\text{IoU} = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{TP}{TP + FP + FN} $$</p> <ul> <li>Measures the overlap between predicted region $A$ and ground truth region $B$ relative to their union</li> <li>Ranges from 0 (no overlap) to 1 (perfect overlap)</li> <li>Common in object detection and semantic segmentation benchmarks (e.g., COCO, Pascal VOC)</li> </ul> <p>The training curves show:</p> <ul> <li>Training Loss: How well the model fits the training data</li> <li>Validation Loss: How well the model generalizes to unseen data</li> <li>Validation IoU: The overlap accuracy on validation data</li> </ul> <p>Note: Higher IoU is better (closer to 1.0), lower loss is better (closer to 0)</p>"},{"location":"examples/train_timm_segmentation/#run-inference-on-test-image","title":"Run Inference on Test Image\u00b6","text":"<p>Use the trained model to segment the test image using the <code>timm_semantic_segmentation</code> function:</p>"},{"location":"examples/train_timm_segmentation/#vectorize-and-visualize-results","title":"Vectorize and Visualize Results\u00b6","text":"<p>Convert the segmentation mask to vector format and visualize:</p>"},{"location":"examples/train_timm_segmentation/#hugging-face-hub-integration","title":"Hugging Face Hub Integration\u00b6","text":"<p>The geoai library now supports loading models from and pushing models to the Hugging Face Hub. This enables:</p> <ul> <li>Loading Pre-trained Models: Use state-of-the-art segmentation models from HF Hub</li> <li>Sharing Your Models: Upload trained models to share with the community</li> <li>Model Versioning: Leverage HF Hub's versioning and collaboration features</li> </ul>"},{"location":"examples/train_timm_segmentation/#option-1-load-a-pre-trained-model-from-hf-hub","title":"Option 1: Load a Pre-trained Model from HF Hub\u00b6","text":"<p>You can use complete segmentation models from Hugging Face Hub with the <code>use_timm_model=True</code> parameter:</p> <p>Note: This example is commented out as it requires a specific HF Hub model. Uncomment if you have a compatible model available.</p>"},{"location":"examples/train_timm_segmentation/#option-2-load-a-pushed-model-for-inference","title":"Option 2: Load a Pushed Model for Inference\u00b6","text":"<p>Once a model is pushed to HF Hub, you can load it for inference:</p>"},{"location":"examples/train_timm_segmentation/#option-3-push-your-trained-model-to-hf-hub","title":"Option 3: Push Your Trained Model to HF Hub\u00b6","text":"<p>After training a model, you can share it on Hugging Face Hub:</p> <p>Note: This requires you to be logged in to Hugging Face. Run <code>huggingface-cli login</code> in your terminal first.</p>"},{"location":"examples/train_timm_segmentation/#hugging-face-hub-integration","title":"Hugging Face Hub Integration\u00b6","text":"<p>The geoai library now supports loading models from and pushing models to the Hugging Face Hub. This enables:</p> <ul> <li>Loading Pre-trained Models: Use state-of-the-art segmentation models from HF Hub</li> <li>Sharing Your Models: Upload trained models to share with the community</li> <li>Model Versioning: Leverage HF Hub's versioning and collaboration features</li> </ul>"},{"location":"examples/train_timm_segmentation/#summary","title":"Summary\u00b6","text":"<p>This notebook demonstrated:</p> <ol> <li>Simplified API: Using <code>train_timm_segmentation_model()</code> similar to <code>train_segmentation_model()</code></li> <li>Multiple Encoders: Training with ResNet50, EfficientNet-B3, and ConvNeXt-Tiny</li> <li>Multiple Architectures: U-Net, DeepLabV3+, and FPN</li> <li>Transfer Learning: Fine-tuning with frozen encoders</li> <li>Inference: Using <code>timm_semantic_segmentation()</code> for predictions</li> <li>Post-processing: Vectorization and visualization</li> </ol>"},{"location":"examples/train_timm_segmentation/#supported-architectures","title":"Supported Architectures\u00b6","text":"<ul> <li>U-Net: Classic encoder-decoder architecture</li> <li>U-Net++: Nested U-Net with dense skip connections</li> <li>DeepLabV3: Atrous Spatial Pyramid Pooling (ASPP)</li> <li>DeepLabV3+: DeepLabV3 with decoder</li> <li>FPN: Feature Pyramid Network</li> <li>PSPNet: Pyramid Scene Parsing Network</li> <li>LinkNet: Efficient architecture with skip connections</li> <li>MANet: Multi-scale Attention Network</li> <li>PAN: Pyramid Attention Network</li> </ul>"},{"location":"examples/train_timm_segmentation/#popular-encoders","title":"Popular Encoders\u00b6","text":"<ul> <li>ResNet family: resnet18, resnet34, resnet50, resnet101, resnet152</li> <li>EfficientNet family: efficientnet_b0 to efficientnet_b7</li> <li>ConvNeXt family: convnext_tiny, convnext_small, convnext_base</li> <li>RegNet family: regnetx_002, regnetx_004, regnety_002, regnety_004</li> <li>MobileNet family: mobilenetv2_100, mobilenetv3_large_100</li> </ul>"},{"location":"examples/train_timm_segmentation/#key-advantages","title":"Key Advantages\u00b6","text":"<ol> <li>1000+ Encoders: Access to state-of-the-art backbones from timm</li> <li>Simple API: Functions match the existing <code>train_segmentation_model</code> interface</li> <li>Automatic Preprocessing: Handles data loading and splitting automatically</li> <li>Lightning Integration: Built-in checkpointing, early stopping, and logging</li> <li>IoU Monitoring: Track IoU metrics during training</li> </ol>"},{"location":"examples/train_timm_segmentation/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Experiment with different encoder-architecture combinations</li> <li>Try modern encoders like ConvNeXt or Swin Transformer</li> <li>Use data augmentation for better generalization</li> <li>Apply to multi-class segmentation tasks</li> </ul>"},{"location":"examples/train_water_detection/","title":"Train water detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\"\ntrain_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\" train_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_masks_path = geoai.download_file(train_masks_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_masks_path = geoai.download_file(train_masks_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(train_raster_path, show_preview=False)\n</pre> geoai.print_raster_info(train_raster_path, show_preview=False) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_masks_url, nodata=0, basemap=train_raster_url)\n</pre> geoai.view_raster(train_masks_url, nodata=0, basemap=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\n</pre> out_folder = \"output\" In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_masks_path,\n    tile_size=512,\n    stride=128,\n    buffer_radius=0,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_masks_path,     tile_size=512,     stride=128,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=4,     pretrained=True,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_water_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_water_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=128,\n    confidence_threshold=0.3,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=128,     confidence_threshold=0.3,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_water_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\n</pre> output_path = \"naip_water_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>len(gdf)\n</pre> len(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf[\"elongation\"].hist()\n</pre> gdf[\"elongation\"].hist() In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10]\n</pre> gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10] In\u00a0[\u00a0]: Copied! <pre>len(gdf_filtered)\n</pre> len(gdf_filtered) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, )"},{"location":"examples/train_water_detection/#train-a-model-for-detecting-surface-water","title":"Train a Model for Detecting Surface Water\u00b6","text":""},{"location":"examples/train_water_detection/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/train_water_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/train_water_detection/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"examples/train_water_detection/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"examples/train_water_detection/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/train_water_detection/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/train_water_detection/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/train_water_detection/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/train_water_detection/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/view_metadata/","title":"View metadata","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py <p>Import the package</p> In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai <p>Define URLs for sample datasets: a NAIP imagery raster file and a building footprints vector file</p> In\u00a0[\u00a0]: Copied! <pre>raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\nvector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\n</pre> raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" <p>Download the raster file (NAIP imagery) and save it locally</p> In\u00a0[\u00a0]: Copied! <pre>raster_path = geoai.download_file(raster_url)\n</pre> raster_path = geoai.download_file(raster_url) <p>Download the vector file (building footprints) and save it locally</p> In\u00a0[\u00a0]: Copied! <pre>vector_path = geoai.download_file(vector_url)\n</pre> vector_path = geoai.download_file(vector_url) <p>Display metadata about the raster file, including dimensions, resolution, projection, and bands</p> In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(raster_path)[\"band_stats\"]\n</pre> geoai.get_raster_info(raster_path)[\"band_stats\"] In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(raster_path, figsize=(18, 10))\n</pre> geoai.print_raster_info(raster_path, figsize=(18, 10)) <p>Display metadata about the vector file, including geometry type, feature count, extent, and attributes</p> In\u00a0[\u00a0]: Copied! <pre>geoai.print_vector_info(vector_path, figsize=(18, 10))\n</pre> geoai.print_vector_info(vector_path, figsize=(18, 10)) <p>Analyze the \"height\" attribute of buildings to obtain statistical information</p> In\u00a0[\u00a0]: Copied! <pre>geoai.analyze_vector_attributes(vector_path, \"height\")\n</pre> geoai.analyze_vector_attributes(vector_path, \"height\") <p>Create a visualization of building footprints colored by their height values</p> In\u00a0[\u00a0]: Copied! <pre>geoai.visualize_vector_by_attribute(vector_path, \"height\")\n</pre> geoai.visualize_vector_by_attribute(vector_path, \"height\") <p>Clip the raster file to a specified extent</p> In\u00a0[\u00a0]: Copied! <pre>clip_raster_path = \"naip_clip.tif\"\ngeoai.clip_raster_by_bbox(\n    raster_path,\n    clip_raster_path,\n    bbox=(0, 0, 500, 500),\n    bands=[1, 2, 3],\n    bbox_type=\"pixel\",\n)\n</pre> clip_raster_path = \"naip_clip.tif\" geoai.clip_raster_by_bbox(     raster_path,     clip_raster_path,     bbox=(0, 0, 500, 500),     bands=[1, 2, 3],     bbox_type=\"pixel\", ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_image(clip_raster_path)\n</pre> geoai.view_image(clip_raster_path)"},{"location":"examples/view_metadata/#view-metadata","title":"View Metadata\u00b6","text":"<p>Install Package</p> <p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/water_detection/","title":"Water detection","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\" In\u00a0[\u00a0]: Copied! <pre>out_folder = geoai.download_file(url)\n</pre> out_folder = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre># Test train_segmentation_model with automatic size detection\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/masks\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and water\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Test train_segmentation_model with automatic size detection geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/masks\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and water     batch_size=8,     num_epochs=3,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>index = 3\ntest_image_path = f\"{out_folder}/images/water_body_{index}.jpg\"\nground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\"\nprediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> index = 3 test_image_path = f\"{out_folder}/images/water_body_{index}.jpg\" ground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\" prediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre># Run semantic segmentation inference\ngeoai.semantic_segmentation(\n    input_path=test_image_path,\n    output_path=prediction_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> # Run semantic segmentation inference geoai.semantic_segmentation(     input_path=test_image_path,     output_path=prediction_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>fig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",\n    show_plot=True,\n)\n</pre> fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",     show_plot=True, )"},{"location":"examples/water_detection/#train-a-semantic-segmentation-model-for-water-detection","title":"Train a Semantic Segmentation Model for Water Detection\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models for water detection.</p>"},{"location":"examples/water_detection/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/water_detection/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/water_detection/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the waterbody dataset from Kaggle. You will need to create an account and download the dataset. I have already downloaded the dataset and saved a copy on Hugging Face. Let's download the dataset:</p>"},{"location":"examples/water_detection/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p> <p>Let's train the module using U-Net with ResNet34 encoder:</p>"},{"location":"examples/water_detection/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/water_detection/#run-inference","title":"Run inference\u00b6","text":"<p>You can run inference on a new image using the <code>semantic_segmentation</code> function. I don't have a new image to test on, so I'll use one of the training images. In reality, you would use your own images not used in training.</p>"},{"location":"examples/water_detection/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/water_detection_s2/","title":"Water detection s2","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/dset-s2.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/dset-s2.zip\" In\u00a0[\u00a0]: Copied! <pre>data_dir = geoai.download_file(url)\n</pre> data_dir = geoai.download_file(url) In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/tra_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/tra_truth\"\ntiles_dir = f\"{data_dir}/dset-s2/tiles\"\n</pre> images_dir = f\"{data_dir}/dset-s2/tra_scene\" masks_dir = f\"{data_dir}/dset-s2/tra_truth\" tiles_dir = f\"{data_dir}/dset-s2/tiles\" In\u00a0[\u00a0]: Copied! <pre>result = geoai.export_geotiff_tiles_batch(\n    images_folder=images_dir,\n    masks_folder=masks_dir,\n    output_folder=tiles_dir,\n    tile_size=512,\n    stride=128,\n    quiet=True,\n)\n</pre> result = geoai.export_geotiff_tiles_batch(     images_folder=images_dir,     masks_folder=masks_dir,     output_folder=tiles_dir,     tile_size=512,     stride=128,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre># Test train_segmentation_model with automatic size detection\ngeoai.train_segmentation_model(\n    images_dir=f\"{tiles_dir}/images\",\n    labels_dir=f\"{tiles_dir}/masks\",\n    output_dir=f\"{tiles_dir}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=6,\n    num_classes=2,  # background and water\n    batch_size=8,\n    num_epochs=50,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Test train_segmentation_model with automatic size detection geoai.train_segmentation_model(     images_dir=f\"{tiles_dir}/images\",     labels_dir=f\"{tiles_dir}/masks\",     output_dir=f\"{tiles_dir}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=6,     num_classes=2,  # background and water     batch_size=8,     num_epochs=50,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{tiles_dir}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{tiles_dir}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/val_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/val_truth\"\npredictions_dir = f\"{data_dir}/dset-s2/predictions\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> images_dir = f\"{data_dir}/dset-s2/val_scene\" masks_dir = f\"{data_dir}/dset-s2/val_truth\" predictions_dir = f\"{data_dir}/dset-s2/predictions\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=8,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=8,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>test_image_path = (\n    f\"{data_dir}/dset-s2/val_scene/S2A_L2A_20190318_N0211_R061_6Bands_S2.tif\"\n)\nground_truth_path = (\n    f\"{data_dir}/dset-s2/val_truth/S2A_L2A_20190318_N0211_R061_S2_Truth.tif\"\n)\nprediction_path = (\n    f\"{data_dir}/dset-s2/predictions/S2A_L2A_20190318_N0211_R061_6Bands_S2_mask.tif\"\n)\nsave_path = f\"{data_dir}/dset-s2/S2A_L2A_20190318_N0211_R061_6Bands_S2_comparison.png\"\n\nfig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=save_path,\n    show_plot=True,\n    indexes=[5, 4, 3],\n    divider=5000,\n)\n</pre> test_image_path = (     f\"{data_dir}/dset-s2/val_scene/S2A_L2A_20190318_N0211_R061_6Bands_S2.tif\" ) ground_truth_path = (     f\"{data_dir}/dset-s2/val_truth/S2A_L2A_20190318_N0211_R061_S2_Truth.tif\" ) prediction_path = (     f\"{data_dir}/dset-s2/predictions/S2A_L2A_20190318_N0211_R061_6Bands_S2_mask.tif\" ) save_path = f\"{data_dir}/dset-s2/S2A_L2A_20190318_N0211_R061_6Bands_S2_comparison.png\"  fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=save_path,     show_plot=True,     indexes=[5, 4, 3],     divider=5000, )"},{"location":"examples/water_detection_s2/#water-detection-with-sentinel-2-imagery","title":"Water Detection with Sentinel-2 Imagery\u00b6","text":"<p>This notebook demonstrates how to train semantic segmentation models for water detection using Sentinel-2 imagery.</p>"},{"location":"examples/water_detection_s2/#install-packages","title":"Install packages\u00b6","text":"<p>To use the new functionality, ensure the required packages are installed.</p>"},{"location":"examples/water_detection_s2/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/water_detection_s2/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the Earth Surface Water Dataset from Zenodo. Credits to the author (Xin Luo) of the dataset</p>"},{"location":"examples/water_detection_s2/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"examples/water_detection_s2/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p> <p>Let's train the module using U-Net with ResNet34 encoder:</p>"},{"location":"examples/water_detection_s2/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"examples/water_detection_s2/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/water_detection_s2/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/water_dynamics/","title":"Water dynamics","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap(center=[47.031260, -99.156360], zoom=14)\nm.add_basemap(\"Esri.WorldImagery\")\nm\n</pre> m = geoai.LeafMap(center=[47.031260, -99.156360], zoom=14) m.add_basemap(\"Esri.WorldImagery\") m <p>Use the drawing tool to select an area of interest (AOI) on the map. The selected area will be used to search for NAIP imagery.</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-99.1705, 47.0149, -99.1296, 47.0365]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-99.1705, 47.0149, -99.1296, 47.0365] In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=bbox,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=bbox, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_items(items=items)\n</pre> geoai.view_pc_items(items=items) In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"])\n</pre> geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection_batch(\n    input_paths=\"naip\",\n    output_dir=\"water\",\n    model_path=\"water_detection.pth\",\n    window_size=512,\n    overlap=128,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection_batch(     input_paths=\"naip\",     output_dir=\"water\",     model_path=\"water_detection.pth\",     window_size=512,     overlap=128,     confidence_threshold=0.5,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>geoai.raster_to_vector_batch(\n    input_dir=\"water\", output_dir=\"vector\", min_area=100, simplify_tolerance=1\n)\n</pre> geoai.raster_to_vector_batch(     input_dir=\"water\", output_dir=\"vector\", min_area=100, simplify_tolerance=1 )"},{"location":"examples/water_dynamics/#mapping-water-dynamics-with-naip-imagery-and-geoai","title":"Mapping Water Dynamics with NAIP Imagery and GeoAI\u00b6","text":""},{"location":"examples/water_dynamics/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/water_dynamics/#import-library","title":"Import library\u00b6","text":""},{"location":"examples/water_dynamics/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/water_dynamics/#search-for-naip-imagery","title":"Search for NAIP imagery\u00b6","text":""},{"location":"examples/water_dynamics/#visualize-the-search-results","title":"Visualize the search results\u00b6","text":""},{"location":"examples/water_dynamics/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"examples/water_dynamics/#object-detection","title":"Object detection\u00b6","text":""},{"location":"examples/water_dynamics/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/wetland_dynamics/","title":"Wetland dynamics","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport leafmap\n</pre> import geoai import leafmap In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap(center=[47.229011, -99.878662], zoom=13)\nm.add_basemap(\"Esri.WorldImagery\")\nm\n</pre> m = geoai.LeafMap(center=[47.229011, -99.878662], zoom=13) m.add_basemap(\"Esri.WorldImagery\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-99.9057, 47.2143, -99.8686, 47.2419]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-99.9057, 47.2143, -99.8686, 47.2419] In\u00a0[\u00a0]: Copied! <pre>nwi_year = leafmap.get_nwi_year(bbox=bbox, return_geometry=False)[0]\nnwi_year\n</pre> nwi_year = leafmap.get_nwi_year(bbox=bbox, return_geometry=False)[0] nwi_year In\u00a0[\u00a0]: Copied! <pre>nwi_year_geom = leafmap.get_nwi_year(bbox=bbox)\nnwi_year_geom\n</pre> nwi_year_geom = leafmap.get_nwi_year(bbox=bbox) nwi_year_geom In\u00a0[\u00a0]: Copied! <pre>m.add_gdf(nwi_year_geom, layer_name=\"NWI Year\")\n</pre> m.add_gdf(nwi_year_geom, layer_name=\"NWI Year\") In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=bbox,\n    time_range=f\"{nwi_year}-01-01/{nwi_year}-12-30\",\n)\nitems\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=bbox,     time_range=f\"{nwi_year}-01-01/{nwi_year}-12-30\", ) items In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_items(items=items)\n</pre> geoai.view_pc_items(items=items) In\u00a0[\u00a0]: Copied! <pre>images = geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"])\nimages\n</pre> images = geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"]) images In\u00a0[\u00a0]: Copied! <pre>first_image = list(images.values())[0][\"image\"]\nfirst_image\n</pre> first_image = list(images.values())[0][\"image\"] first_image In\u00a0[\u00a0]: Copied! <pre>second_image = list(images.values())[1][\"image\"]\nsecond_image\n</pre> second_image = list(images.values())[1][\"image\"] second_image In\u00a0[\u00a0]: Copied! <pre>image_bbox = leafmap.image_bbox(first_image, to_crs=\"EPSG:4326\")\nimage_bbox\n</pre> image_bbox = leafmap.image_bbox(first_image, to_crs=\"EPSG:4326\") image_bbox In\u00a0[\u00a0]: Copied! <pre>nwi_gdf = leafmap.get_nwi(\n    geometry=image_bbox, clip=True, add_class=True, output=\"wetlands.geojson\"\n)\nnwi_gdf\n</pre> nwi_gdf = leafmap.get_nwi(     geometry=image_bbox, clip=True, add_class=True, output=\"wetlands.geojson\" ) nwi_gdf In\u00a0[\u00a0]: Copied! <pre>wetlands_classes = nwi_gdf[\"WETLAND_TY\"].unique()\nwetlands_classes\n</pre> wetlands_classes = nwi_gdf[\"WETLAND_TY\"].unique() wetlands_classes In\u00a0[\u00a0]: Copied! <pre>num_classes = len(wetlands_classes) + 1\nnum_classes\n</pre> num_classes = len(wetlands_classes) + 1 num_classes In\u00a0[\u00a0]: Copied! <pre>m.remove_layer(m.layers[-1])\nm.add_raster(first_image, layer_name=\"NAIP\")\nm.add_nwi(nwi_gdf, layer_name=\"NWI\")\nm\n</pre> m.remove_layer(m.layers[-1]) m.add_raster(first_image, layer_name=\"NAIP\") m.add_nwi(nwi_gdf, layer_name=\"NWI\") m In\u00a0[\u00a0]: Copied! <pre>train_raster_path = first_image\ntest_raster_path = second_image\ntrain_vector_path = \"wetlands.geojson\"\n</pre> train_raster_path = first_image test_raster_path = second_image train_vector_path = \"wetlands.geojson\" In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    train_vector_path, column=\"WETLAND_TY\", tiles=train_raster_path\n)\n</pre> geoai.view_vector_interactive(     train_vector_path, column=\"WETLAND_TY\", tiles=train_raster_path ) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\n</pre> out_folder = \"output\" In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=1024,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=1024,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=4,\n    num_classes=num_classes,  # background and wetlands classes\n    batch_size=8,\n    num_epochs=50,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=4,     num_classes=num_classes,  # background and wetlands classes     batch_size=8,     num_epochs=50,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre># Define paths\nmasks_path = second_image.replace(\"naip\", \"prediction\")\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> # Define paths masks_path = second_image.replace(\"naip\", \"prediction\") model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=4,\n    num_classes=6,\n    window_size=1024,\n    overlap=256,\n    batch_size=4,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=4,     num_classes=6,     window_size=1024,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = masks_path.replace(\".tif\", \"_mask.geojson\")\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=300, simplify_tolerance=1\n)\n</pre> output_path = masks_path.replace(\".tif\", \"_mask.geojson\") gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=300, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_path)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_path) In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=bbox,\n)\nitems\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=bbox, ) items In\u00a0[\u00a0]: Copied! <pre>images = geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"])\nimages\n</pre> images = geoai.pc_stac_download(items, output_dir=\"naip\", assets=[\"image\"]) images In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=\"naip\",\n    output_dir=\"prediction\",\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=4,\n    num_classes=6,\n    window_size=1024,\n    overlap=256,\n    batch_size=4,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=\"naip\",     output_dir=\"prediction\",     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=4,     num_classes=6,     window_size=1024,     overlap=256,     batch_size=4, )"},{"location":"examples/wetland_dynamics/#wetland-mapping-with-geoai","title":"Wetland Mapping with GeoAI\u00b6","text":""},{"location":"examples/wetland_dynamics/#install-packages","title":"Install packages\u00b6","text":""},{"location":"examples/wetland_dynamics/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/wetland_dynamics/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/wetland_dynamics/#draw-an-area-of-interest","title":"Draw an area of interest\u00b6","text":""},{"location":"examples/wetland_dynamics/#get-the-year-of-nwi-data","title":"Get the year of NWI data\u00b6","text":""},{"location":"examples/wetland_dynamics/#search-for-naip-images-for-the-corresponding-nwi-year","title":"Search for NAIP images for the corresponding NWI year\u00b6","text":""},{"location":"examples/wetland_dynamics/#visualize-naip-images","title":"Visualize NAIP images\u00b6","text":""},{"location":"examples/wetland_dynamics/#download-naip-images","title":"Download NAIP images\u00b6","text":""},{"location":"examples/wetland_dynamics/#download-nwi-data","title":"Download NWI data\u00b6","text":""},{"location":"examples/wetland_dynamics/#visualize-nwi-data","title":"Visualize NWI data\u00b6","text":""},{"location":"examples/wetland_dynamics/#select-training-and-test-images","title":"Select training and test images\u00b6","text":""},{"location":"examples/wetland_dynamics/#create-image-chips-for-training","title":"Create image chips for training\u00b6","text":""},{"location":"examples/wetland_dynamics/#train-a-segmentation-model","title":"Train a segmentation model\u00b6","text":""},{"location":"examples/wetland_dynamics/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"examples/wetland_dynamics/#run-inference-on-a-single-image","title":"Run inference on a single image\u00b6","text":""},{"location":"examples/wetland_dynamics/#vectorize-the-results","title":"Vectorize the results\u00b6","text":""},{"location":"examples/wetland_dynamics/#download-more-images","title":"Download more images\u00b6","text":""},{"location":"examples/wetland_dynamics/#run-inference-on-multiple-images","title":"Run inference on multiple images\u00b6","text":""},{"location":"examples/wetland_mapping/","title":"Wetland mapping","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\nimport geoai\n</pre> import leafmap import geoai In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609931_ne_14_1_20100629.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609932_nw_14_1_20100629.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609931_ne_14_1_20100629.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609932_nw_14_1_20100629.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(test_raster_path, show_preview=False)\n</pre> geoai.print_raster_info(test_raster_path, show_preview=False) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_raster_url)\n</pre> geoai.view_raster(train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(train_raster_url, bidx=[4, 1, 2], name=\"NAIP Imagery\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(train_raster_url, bidx=[4, 1, 2], name=\"NAIP Imagery\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-99.192, 46.56, -99.1206, 46.6283]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-99.192, 46.56, -99.1206, 46.6283] In\u00a0[\u00a0]: Copied! <pre>bbox_geometry = leafmap.bbox_to_gdf(bbox)\n</pre> bbox_geometry = leafmap.bbox_to_gdf(bbox) In\u00a0[\u00a0]: Copied! <pre>nwi_gdf = leafmap.get_nwi(bbox_geometry)\nnwi_gdf.head()\n</pre> nwi_gdf = leafmap.get_nwi(bbox_geometry) nwi_gdf.head() In\u00a0[\u00a0]: Copied! <pre>m.add_nwi(nwi_gdf, col_name=\"Wetlands.WETLAND_TYPE\")\n</pre> m.add_nwi(nwi_gdf, col_name=\"Wetlands.WETLAND_TYPE\") In\u00a0[\u00a0]: Copied! <pre>nwi_geojson = \"nwi_wetlands.geojson\"\nnwi_gdf.to_file(nwi_geojson)\n</pre> nwi_geojson = \"nwi_wetlands.geojson\" nwi_gdf.to_file(nwi_geojson) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=nwi_geojson,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=nwi_geojson,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=4,     pretrained=True,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.3,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.3,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\n</pre> output_path = \"naip_test_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},     basemap=test_raster_url, )"},{"location":"examples/wetland_mapping/#wetland-mapping-with-geoai","title":"Wetland Mapping with GeoAI\u00b6","text":""},{"location":"examples/wetland_mapping/#install-package","title":"Install package\u00b6","text":"<p>To use the <code>geoai-py</code> package, ensure it is installed in your environment. Uncomment the command below if needed.</p>"},{"location":"examples/wetland_mapping/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/wetland_mapping/#download-sample-naip-imagery","title":"Download sample NAIP imagery\u00b6","text":"<p>Sample NAIP imagery: https://huggingface.co/datasets/giswqs/geospatial/tree/main/naip</p>"},{"location":"examples/wetland_mapping/#download-sample-wetland-data","title":"Download sample wetland data\u00b6","text":"<p>National Wetlands Inventory: https://www.fws.gov/program/national-wetlands-inventory/wetlands-mapper</p>"},{"location":"examples/wetland_mapping/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"examples/wetland_mapping/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"examples/wetland_mapping/#run-inference","title":"Run inference\u00b6","text":""},{"location":"examples/wetland_mapping/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"examples/wetland_mapping/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/","title":"Lidar viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[lidar]\" open3d\n</pre> # %pip install \"leafmap[lidar]\" open3d In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap <p>Download a sample LiDAR dataset from Google Drive. The zip file is 52.1 MB and the uncompressed LAS file is 109 MB.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.org/data/lidar/madison.zip\"\nfilename = \"madison.las\"\n</pre> url = \"https://opengeos.org/data/lidar/madison.zip\" filename = \"madison.las\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, \"madison.zip\", unzip=True)\n</pre> leafmap.download_file(url, \"madison.zip\", unzip=True) <p>Read the LiDAR data</p> In\u00a0[\u00a0]: Copied! <pre>las = leafmap.read_lidar(filename)\n</pre> las = leafmap.read_lidar(filename) <p>The LAS header.</p> In\u00a0[\u00a0]: Copied! <pre>las.header\n</pre> las.header <p>The number of points.</p> In\u00a0[\u00a0]: Copied! <pre>las.header.point_count\n</pre> las.header.point_count <p>The list of features.</p> In\u00a0[\u00a0]: Copied! <pre>list(las.point_format.dimension_names)\n</pre> list(las.point_format.dimension_names) <p>Inspect data.</p> In\u00a0[\u00a0]: Copied! <pre>las.X\n</pre> las.X In\u00a0[\u00a0]: Copied! <pre>las.Y\n</pre> las.Y In\u00a0[\u00a0]: Copied! <pre>las.Z\n</pre> las.Z In\u00a0[\u00a0]: Copied! <pre>las.intensity\n</pre> las.intensity In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"pyvista\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\")\n</pre> leafmap.view_lidar(filename, backend=\"ipygany\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\")\n</pre> leafmap.view_lidar(filename, cmap=\"terrain\", backend=\"panel\", background=\"white\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_lidar(filename, backend=\"open3d\")\n</pre> leafmap.view_lidar(filename, backend=\"open3d\") <p></p>"},{"location":"examples/dataviz/lidar_viz/#visualizing-lidar-data-with-leafmap","title":"Visualizing LiDAR Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize LiDAR data using leafmap.</p>"},{"location":"examples/dataviz/lidar_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment and run the following cell to install the required Python packages.</p>"},{"location":"examples/dataviz/lidar_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#download-data","title":"Download data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#metadata","title":"Metadata\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#read-data","title":"Read data\u00b6","text":""},{"location":"examples/dataviz/lidar_viz/#pyvista","title":"PyVista\u00b6","text":"<p>Visualize LiDAR data using the pyvista backend.</p>"},{"location":"examples/dataviz/lidar_viz/#ipygany","title":"ipygany\u00b6","text":"<p>Visualize LiDAR data using the ipygany backend.</p>"},{"location":"examples/dataviz/lidar_viz/#panel","title":"Panel\u00b6","text":"<p>Visualize LiDAR data using the panel backend.</p>"},{"location":"examples/dataviz/lidar_viz/#open3d","title":"Open3D\u00b6","text":"<p>Visualize LiDAR data using the open3d backend.</p>"},{"location":"examples/dataviz/raster_viz/","title":"Raster viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[raster]\"\n</pre> # %pip install \"leafmap[raster]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nurl = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\"\nm.add_cog_layer(url, name=\"Fire (pre-event)\")\nm\n</pre> m = leafmap.Map() url = \"https://opendata.digitalglobe.com/events/california-fire-2020/pre-event/2018-02-16/pine-gulch-fire20/1030010076004E00.tif\" m.add_cog_layer(url, name=\"Fire (pre-event)\") m <p>You can add multiple COGs to the map. Let's add another COG to the map.</p> In\u00a0[\u00a0]: Copied! <pre>url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\"\nm.add_cog_layer(url2, name=\"Fire (post-event)\")\nm\n</pre> url2 = \"https://opendata.digitalglobe.com/events/california-fire-2020/post-event/2020-08-14/pine-gulch-fire20/10300100AAC8DD00.tif\" m.add_cog_layer(url2, name=\"Fire (post-event)\") m <p></p> <p>Create a split map for comparing two COGs.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.split_map(left_layer=url, right_layer=url2)\nm\n</pre> m = leafmap.Map() m.split_map(left_layer=url, right_layer=url2) m <p></p> In\u00a0[\u00a0]: Copied! <pre>dem_url = \"https://opengeos.org/data/raster/srtm90.tif\"\nleafmap.download_file(dem_url, unzip=False)\n</pre> dem_url = \"https://opengeos.org/data/raster/srtm90.tif\" leafmap.download_file(dem_url, unzip=False) <p>Visualize a single-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"srtm90.tif\", cmap=\"terrain\", layer_name=\"DEM\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>landsat_url = \"https://opengeos.org/data/raster/cog.tif\"\nleafmap.download_file(landsat_url)\n</pre> landsat_url = \"https://opengeos.org/data/raster/cog.tif\" leafmap.download_file(landsat_url) <p>Visualize a multi-band raster.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"cog.tif\", bands=[4, 3, 2], layer_name=\"Landsat\") m <p></p> <p>Create an interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\"\nleafmap.stac_bands(url)\n</pre> url = \"https://canada-spot-ortho.s3.amazonaws.com/canada_spot_orthoimages/canada_spot5_orthoimages/S5_2007/S5_11055_6057_20070622/S5_11055_6057_20070622.json\" leafmap.stac_bands(url) <p>Add STAC layers to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\")\nm.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\")\nm\n</pre> m = leafmap.Map() m.add_stac_layer(url, bands=[\"pan\"], name=\"Panchromatic\") m.add_stac_layer(url, bands=[\"B3\", \"B2\", \"B1\"], name=\"False color\") m <p></p> <p>Provide custom STAC API endpoints as a dictionary in the format of <code>{\"name\": \"url\"}</code>. The name will show up in the dropdown menu, while the url is the STAC API endpoint that will be used to search for items.</p> In\u00a0[\u00a0]: Copied! <pre>catalogs = {\n    \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",\n    \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n}\n</pre> catalogs = {     \"Element84 Earth Search\": \"https://earth-search.aws.element84.com/v1\",     \"Microsoft Planetary Computer\": \"https://planetarycomputer.microsoft.com/api/stac/v1\", } In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4)\nm.set_catalog_source(catalogs)\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4) m.set_catalog_source(catalogs) m.add_stac_gui() m <p>Once the catalog panel is open, you can search for items from the custom STAC API endpoints. Simply draw a bounding box on the map or zoom to a location of interest. Click on the Collections button to retrieve the collections from the custom STAC API endpoints. Next, select a collection from the dropdown menu. Then, click on the Items button to retrieve the items from the selected collection. Finally, click on the Display button to add the selected item to the map.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>m.stac_gdf  # The GeoDataFrame of the STAC search results\n</pre> m.stac_gdf  # The GeoDataFrame of the STAC search results In\u00a0[\u00a0]: Copied! <pre>m.stac_dict  # The STAC search results as a dictionary\n</pre> m.stac_dict  # The STAC search results as a dictionary In\u00a0[\u00a0]: Copied! <pre>m.stac_item  # The selected STAC item of the search result\n</pre> m.stac_item  # The selected STAC item of the search result <p>To Be able to run this notebook you'll need to have AWS credential available as environment variables. Uncomment the following lines to set the environment variables.</p> In\u00a0[\u00a0]: Copied! <pre># os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\"\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\"\n</pre> # os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR AWS ACCESS ID HERE\" # os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR AWS ACCESS KEY HERE\" <p>In this example, we will use datasets from the Maxar Open Data Program on AWS.</p> In\u00a0[\u00a0]: Copied! <pre>BUCKET = \"maxar-opendata\"\nFOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\"\n</pre> BUCKET = \"maxar-opendata\" FOLDER = \"events/Kahramanmaras-turkey-earthquake-23/\" <p>List all the datasets in the bucket. Specify a file extension to filter the results if needed.</p> In\u00a0[\u00a0]: Copied! <pre>items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\")\nitems[:10]\n</pre> items = leafmap.s3_list_objects(BUCKET, FOLDER, ext=\".tif\") items[:10] <p>Visualize raster datasets from the bucket.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(items[2], name=\"Maxar\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(items[2], name=\"Maxar\") m <p></p>"},{"location":"examples/dataviz/raster_viz/#visualizing-raster-data-with-leafmap","title":"Visualizing Raster Data with Leafmap\u00b6","text":"<p>This notebook demonstrates how to visualize raster data using leafmap. Leafmap can visualize raster data (e.g., Cloud Optimized GeoTIFF) stored in a local file or on the cloud (e.g., AWS S3). It can also visualize raster data stored in a STAC catalog.</p>"},{"location":"examples/dataviz/raster_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install the required packages if needed.</p>"},{"location":"examples/dataviz/raster_viz/#import-packages","title":"Import packages\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#cog","title":"COG\u00b6","text":"<p>A Cloud Optimized GeoTIFF (COG) is a regular GeoTIFF file, aimed at being hosted on a HTTP file server, with an internal organization that enables more efficient workflows on the cloud. It does this by leveraging the ability of clients issuing HTTP GET range requests to ask for just the parts of a file they need. More information about COG can be found at https://www.cogeo.org/in-depth.html</p> <p>For this demo, we will use data from https://www.maxar.com/open-data/california-colorado-fires for mapping California and Colorado fires. Let's create an interactive map and add the COG to the map.</p>"},{"location":"examples/dataviz/raster_viz/#local-raster","title":"Local Raster\u00b6","text":"<p>Leafmap can also visualize local GeoTIFF files. Let's download some sample data</p>"},{"location":"examples/dataviz/raster_viz/#stac","title":"STAC\u00b6","text":"<p>The SpatioTemporal Asset Catalog (STAC) specification provides a common language to describe a range of geospatial information so that it can more easily be indexed and discovered. A SpatioTemporal Asset is any file that represents information about the earth captured in a certain space and time. STAC aims to enable that next generation of geospatial search engines, while also supporting web best practices so geospatial information is more easily surfaced in traditional search engines. More information about STAC can be found at the STAC website. In this example, we will use a STAC item from the SPOT Orthoimages of Canada available through the link below:</p>"},{"location":"examples/dataviz/raster_viz/#custom-stac-catalog","title":"Custom STAC Catalog\u00b6","text":""},{"location":"examples/dataviz/raster_viz/#aws-s3","title":"AWS S3\u00b6","text":""},{"location":"examples/dataviz/vector_viz/","title":"Vector viz","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install \"leafmap[vector]\"\n</pre> # %pip install \"leafmap[vector]\" In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[0, 0], zoom=2)\ndata = \"https://opengeos.org/data/vector/cables.geojson\"\nm.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\")\nm\n</pre> m = leafmap.Map(center=[0, 0], zoom=2) data = \"https://opengeos.org/data/vector/cables.geojson\" m.add_vector(data, layer_name=\"Cable lines\", info_mode=\"on_hover\") m <p>You can style the vector with custom style callback functions.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\ndata = \"https://opengeos.org/data/vector/cables.geojson\"\ncallback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1}\nm.add_vector(data, layer_name=\"Cable lines\", style_callback=callback)\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") data = \"https://opengeos.org/data/vector/cables.geojson\" callback = lambda feat: {\"color\": feat[\"properties\"][\"color\"], \"weight\": 1} m.add_vector(data, layer_name=\"Cable lines\", style_callback=callback) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\ndata = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\"\nm.add_data(\n    data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\"\n)\nm\n</pre> m = leafmap.Map() data = \"https://raw.githubusercontent.com/opengeos/leafmap/master/docs/examples/data/countries.geojson\" m.add_data(     data, column=\"POP_EST\", scheme=\"Quantiles\", cmap=\"Blues\", legend_title=\"Population\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_data(\n    data,\n    column=\"POP_EST\",\n    scheme=\"EqualInterval\",\n    cmap=\"Blues\",\n    legend_title=\"Population\",\n)\nm\n</pre> m = leafmap.Map() m.add_data(     data,     column=\"POP_EST\",     scheme=\"EqualInterval\",     cmap=\"Blues\",     legend_title=\"Population\", ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.org/data/duckdb/cities.parquet\"\ngdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\")\ngdf.head()\n</pre> url = \"https://opengeos.org/data/duckdb/cities.parquet\" gdf = leafmap.read_parquet(url, return_type=\"gdf\", src_crs=\"EPSG:4326\") gdf.head() <p>Visualize point data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(\n    gdf,\n    get_radius=20000,\n    get_fill_color=\"blue\",\n    zoom_to_layer=False,\n    map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500},\n)\n</pre> leafmap.view_vector(     gdf,     get_radius=20000,     get_fill_color=\"blue\",     zoom_to_layer=False,     map_args={\"center\": (40, -100), \"zoom\": 3, \"height\": 500}, ) <p></p> <p>Visualizing polygon data.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\"\ngdf = leafmap.read_parquet(\n    url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\"\n)\ngdf.head()\n</pre> url = \"https://data.source.coop/giswqs/nwi/wetlands/DC_Wetlands.parquet\" gdf = leafmap.read_parquet(     url, return_type=\"gdf\", src_crs=\"EPSG:5070\", dst_crs=\"EPSG:4326\" ) gdf.head() In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128])\n</pre> leafmap.view_vector(gdf, get_fill_color=[0, 0, 255, 128]) <p></p> <p>Alternatively, you can specify a color map to visualize the data.</p> In\u00a0[\u00a0]: Copied! <pre>color_map = {\n    \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),\n    \"Freshwater Emergent Wetland\": (127, 195, 28),\n    \"Freshwater Pond\": (104, 140, 192),\n    \"Estuarine and Marine Wetland\": (102, 194, 165),\n    \"Riverine\": (1, 144, 191),\n    \"Lake\": (19, 0, 124),\n    \"Estuarine and Marine Deepwater\": (0, 124, 136),\n    \"Other\": (178, 134, 86),\n}\n</pre> color_map = {     \"Freshwater Forested/Shrub Wetland\": (0, 136, 55),     \"Freshwater Emergent Wetland\": (127, 195, 28),     \"Freshwater Pond\": (104, 140, 192),     \"Estuarine and Marine Wetland\": (102, 194, 165),     \"Riverine\": (1, 144, 191),     \"Lake\": (19, 0, 124),     \"Estuarine and Marine Deepwater\": (0, 124, 136),     \"Other\": (178, 134, 86), } In\u00a0[\u00a0]: Copied! <pre>leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5)\n</pre> leafmap.view_vector(gdf, color_column=\"WETLAND_TYPE\", color_map=color_map, opacity=0.5) <p></p> <p>Display a legend for the data.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map)\n</pre> leafmap.Legend(title=\"Wetland Type\", legend_dict=color_map) <p></p> In\u00a0[\u00a0]: Copied! <pre>import leafmap.foliumap as leafmap\n</pre> import leafmap.foliumap as leafmap <p>Check the metadata of the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://storage.googleapis.com/ahp-research/overture/pmtiles/overture.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"CartoDB.DarkMatter\")\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"admins\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"admins\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},\n        },\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"places\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"places\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},\n        },\n        {\n            \"id\": \"roads\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"roads\",\n            \"type\": \"line\",\n            \"paint\": {\"line-color\": \"#FB8072\"},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True\n)\n\nlegend_dict = {\n    \"admins\": \"BDD3C7\",\n    \"buildings\": \"FFFFB3\",\n    \"places\": \"BEBADA\",\n    \"roads\": \"FB8072\",\n}\n\nm.add_legend(legend_dict=legend_dict)\nm\n</pre> m = leafmap.Map() m.add_basemap(\"CartoDB.DarkMatter\")  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"admins\",             \"source\": \"example_source\",             \"source-layer\": \"admins\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BDD3C7\", \"fill-opacity\": 0.1},         },         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#FFFFB3\", \"fill-opacity\": 0.5},         },         {             \"id\": \"places\",             \"source\": \"example_source\",             \"source-layer\": \"places\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#BEBADA\", \"fill-opacity\": 0.5},         },         {             \"id\": \"roads\",             \"source\": \"example_source\",             \"source-layer\": \"roads\",             \"type\": \"line\",             \"paint\": {\"line-color\": \"#FB8072\"},         },     ], }  m.add_pmtiles(     url, name=\"PMTiles\", style=style, overlay=True, show=True, zoom_to_layer=True )  legend_dict = {     \"admins\": \"BDD3C7\",     \"buildings\": \"FFFFB3\",     \"places\": \"BEBADA\",     \"roads\": \"FB8072\", }  m.add_legend(legend_dict=legend_dict) m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\"\nmetadata = leafmap.pmtiles_metadata(url)\nprint(f\"layer names: {metadata['layer_names']}\")\nprint(f\"bounds: {metadata['bounds']}\")\n</pre> url = \"https://data.source.coop/vida/google-microsoft-open-buildings/pmtiles/go_ms_building_footprints.pmtiles\" metadata = leafmap.pmtiles_metadata(url) print(f\"layer names: {metadata['layer_names']}\") print(f\"bounds: {metadata['bounds']}\") <p>Visualize the PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[20, 0], zoom=2)\nm.add_basemap(\"CartoDB.DarkMatter\")\nm.add_basemap(\"Esri.WorldImagery\", show=False)\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"building_footprints\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(\n    url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False\n)\n\nm\n</pre> m = leafmap.Map(center=[20, 0], zoom=2) m.add_basemap(\"CartoDB.DarkMatter\") m.add_basemap(\"Esri.WorldImagery\", show=False)  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"building_footprints\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(     url, name=\"Buildings\", style=style, overlay=True, show=True, zoom_to_layer=False )  m <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\"\nleafmap.download_file(url, \"buildings.geojson\")\n</pre> url = \"https://raw.githubusercontent.com/opengeos/open-data/main/datasets/libya/Derna_buildings.geojson\" leafmap.download_file(url, \"buildings.geojson\") <p>Convert vector to PMTiles.</p> In\u00a0[\u00a0]: Copied! <pre>pmtiles = \"buildings.pmtiles\"\nleafmap.geojson_to_pmtiles(\n    \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True\n)\n</pre> pmtiles = \"buildings.pmtiles\" leafmap.geojson_to_pmtiles(     \"buildings.geojson\", pmtiles, layer_name=\"buildings\", overwrite=True, quiet=True ) <p>Start a HTTP Sever</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.start_server(port=8000)\n</pre> leafmap.start_server(port=8000) In\u00a0[\u00a0]: Copied! <pre>url = f\"http://127.0.0.1:8000/{pmtiles}\"\nleafmap.pmtiles_metadata(url)\n</pre> url = f\"http://127.0.0.1:8000/{pmtiles}\" leafmap.pmtiles_metadata(url) <p>Display the PMTiles on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n\nstyle = {\n    \"version\": 8,\n    \"sources\": {\n        \"example_source\": {\n            \"type\": \"vector\",\n            \"url\": \"pmtiles://\" + url,\n            \"attribution\": \"PMTiles\",\n        }\n    },\n    \"layers\": [\n        {\n            \"id\": \"buildings\",\n            \"source\": \"example_source\",\n            \"source-layer\": \"buildings\",\n            \"type\": \"fill\",\n            \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},\n        },\n    ],\n}\n\nm.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style)\nm\n</pre> m = leafmap.Map()  style = {     \"version\": 8,     \"sources\": {         \"example_source\": {             \"type\": \"vector\",             \"url\": \"pmtiles://\" + url,             \"attribution\": \"PMTiles\",         }     },     \"layers\": [         {             \"id\": \"buildings\",             \"source\": \"example_source\",             \"source-layer\": \"buildings\",             \"type\": \"fill\",             \"paint\": {\"fill-color\": \"#3388ff\", \"fill-opacity\": 0.5},         },     ], }  m.add_pmtiles(url, name=\"Buildings\", show=True, zoom_to_layer=True, style=style) m <p></p>"},{"location":"examples/dataviz/vector_viz/#visualizing-vector-data-with-leafmap","title":"Visualizing Vector Data with Leafmap\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#installation","title":"Installation\u00b6","text":"<p>Uncomment the following line to install leafmap if needed.</p>"},{"location":"examples/dataviz/vector_viz/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#visualize-vector-data","title":"Visualize vector data\u00b6","text":"<p>You can visualize vector data using the <code>add_vector</code> function. It supports common vector data formats, including GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#choropleth-map","title":"Choropleth map\u00b6","text":"<p>You can create a choropleth map using the <code>add_data</code> function. It supports GeoJSON, Shapefile, GeoPackage, and any other formats supported by geopandas.</p>"},{"location":"examples/dataviz/vector_viz/#geoparquet","title":"GeoParquet\u00b6","text":"<p>Visualize GeoParquet data with leafmap and lonboard.</p>"},{"location":"examples/dataviz/vector_viz/#pmtiles","title":"PMTiles\u00b6","text":"<p>PMTiles is a single-file archive format for tiled data. A PMTiles archive can be hosted on a commodity storage platform such as S3, and enables low-cost, zero-maintenance map applications that are \"serverless\" - free of a custom tile backend or third party provider.</p>"},{"location":"examples/dataviz/vector_viz/#remote-pmtiles","title":"Remote PMTiles\u00b6","text":"<p>Leafmap can visualize PMTiles hosted locally or remotely.</p>"},{"location":"examples/dataviz/vector_viz/#overture-data","title":"Overture data\u00b6","text":""},{"location":"examples/dataviz/vector_viz/#source-cooperative","title":"Source Cooperative\u00b6","text":"<p>Visualize the Google-Microsoft Open Buildings data hosted on Source Cooperative.</p> <p>Check the metadata of the PMTiles.</p>"},{"location":"examples/dataviz/vector_viz/#local-pmtiles","title":"Local PMTiles\u00b6","text":"<p>tippecanoe is required to convert vector data to pmtiles. Install it with <code>conda install -c conda-forge tippecanoe</code>.</p> <p>Download building footprints of Derna, Libya.</p>"},{"location":"examples/rastervision/semantic_segmentation/","title":"Semantic segmentation","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport torch\nfrom matplotlib import pyplot as plt\n\nfrom rastervision.core.data import ClassConfig, SemanticSegmentationLabels\n\nimport albumentations as A\n\nfrom rastervision.pytorch_learner import (\n    SemanticSegmentationRandomWindowGeoDataset,\n    SemanticSegmentationSlidingWindowGeoDataset,\n    SemanticSegmentationVisualizer,\n    SemanticSegmentationGeoDataConfig,\n    SemanticSegmentationLearnerConfig,\n    SolverConfig,\n    SemanticSegmentationLearner,\n)\n</pre> import os import torch from matplotlib import pyplot as plt  from rastervision.core.data import ClassConfig, SemanticSegmentationLabels  import albumentations as A  from rastervision.pytorch_learner import (     SemanticSegmentationRandomWindowGeoDataset,     SemanticSegmentationSlidingWindowGeoDataset,     SemanticSegmentationVisualizer,     SemanticSegmentationGeoDataConfig,     SemanticSegmentationLearnerConfig,     SolverConfig,     SemanticSegmentationLearner, ) In\u00a0[\u00a0]: Copied! <pre>os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\"\n</pre> os.environ[\"AWS_NO_SIGN_REQUEST\"] = \"YES\" In\u00a0[\u00a0]: Copied! <pre>class_config = ClassConfig(\n    names=[\"background\", \"building\"],\n    colors=[\"lightgray\", \"darkred\"],\n    null_class=\"background\",\n)\n\nviz = SemanticSegmentationVisualizer(\n    class_names=class_config.names, class_colors=class_config.colors\n)\n</pre> class_config = ClassConfig(     names=[\"background\", \"building\"],     colors=[\"lightgray\", \"darkred\"],     null_class=\"background\", )  viz = SemanticSegmentationVisualizer(     class_names=class_config.names, class_colors=class_config.colors ) In\u00a0[\u00a0]: Copied! <pre>train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\"\ntrain_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"\n\nval_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\nval_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> train_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/images/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13.tif\" train_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0331E-1257N_1327_3160_13/labels/global_monthly_2018_01_mosaic_L15-0331E-1257N_1327_3160_13_Buildings.geojson\"  val_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" val_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2018_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\"\npred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\"\n</pre> pred_image_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/images/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13.tif\" pred_label_uri = \"s3://spacenet-dataset/spacenet/SN7_buildings/train/L15-0357E-1223N_1429_3296_13/labels/global_monthly_2020_01_mosaic_L15-0357E-1223N_1429_3296_13_Buildings.geojson\" In\u00a0[\u00a0]: Copied! <pre>data_augmentation_transform = A.Compose(\n    [\n        A.Flip(),\n        A.ShiftScaleRotate(),\n        A.OneOf(\n            [\n                A.HueSaturationValue(hue_shift_limit=10),\n                A.RGBShift(),\n                A.ToGray(),\n                A.ToSepia(),\n                A.RandomBrightness(),\n                A.RandomGamma(),\n            ]\n        ),\n        A.CoarseDropout(max_height=32, max_width=32, max_holes=5),\n    ]\n)\n</pre> data_augmentation_transform = A.Compose(     [         A.Flip(),         A.ShiftScaleRotate(),         A.OneOf(             [                 A.HueSaturationValue(hue_shift_limit=10),                 A.RGBShift(),                 A.ToGray(),                 A.ToSepia(),                 A.RandomBrightness(),                 A.RandomGamma(),             ]         ),         A.CoarseDropout(max_height=32, max_width=32, max_holes=5),     ] ) In\u00a0[\u00a0]: Copied! <pre>train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=train_image_uri,\n    label_vector_uri=train_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size_lims=(150, 200),\n    out_size=256,\n    max_windows=400,\n    transform=data_augmentation_transform,\n)\n\nlen(train_ds)\n</pre> train_ds = SemanticSegmentationRandomWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=train_image_uri,     label_vector_uri=train_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size_lims=(150, 200),     out_size=256,     max_windows=400,     transform=data_augmentation_transform, )  len(train_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(train_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(train_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=val_image_uri,\n    label_vector_uri=val_label_uri,\n    label_vector_default_class_id=class_config.get_class_id(\"building\"),\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(val_ds)\n</pre> val_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=val_image_uri,     label_vector_uri=val_label_uri,     label_vector_default_class_id=class_config.get_class_id(\"building\"),     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(val_ds) In\u00a0[\u00a0]: Copied! <pre>x, y = viz.get_batch(val_ds, 4)\nviz.plot_batch(x, y, show=True)\n</pre> x, y = viz.get_batch(val_ds, 4) viz.plot_batch(x, y, show=True) In\u00a0[\u00a0]: Copied! <pre>pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(\n    class_config=class_config,\n    image_uri=pred_image_uri,\n    size=200,\n    stride=100,\n    transform=A.Resize(256, 256),\n)\nlen(pred_ds)\n</pre> pred_ds = SemanticSegmentationSlidingWindowGeoDataset.from_uris(     class_config=class_config,     image_uri=pred_image_uri,     size=200,     stride=100,     transform=A.Resize(256, 256), ) len(pred_ds) In\u00a0[\u00a0]: Copied! <pre>model = torch.hub.load(\n    \"AdeelH/pytorch-fpn:0.3\",\n    \"make_fpn_resnet\",\n    name=\"resnet18\",\n    fpn_type=\"panoptic\",\n    num_classes=len(class_config),\n    fpn_channels=128,\n    in_channels=3,\n    out_size=(256, 256),\n    pretrained=True,\n)\n</pre> model = torch.hub.load(     \"AdeelH/pytorch-fpn:0.3\",     \"make_fpn_resnet\",     name=\"resnet18\",     fpn_type=\"panoptic\",     num_classes=len(class_config),     fpn_channels=128,     in_channels=3,     out_size=(256, 256),     pretrained=True, ) In\u00a0[\u00a0]: Copied! <pre>data_cfg = SemanticSegmentationGeoDataConfig(\n    class_names=class_config.names,\n    class_colors=class_config.colors,\n    num_workers=0,  # increase to use multi-processing\n)\n</pre> data_cfg = SemanticSegmentationGeoDataConfig(     class_names=class_config.names,     class_colors=class_config.colors,     num_workers=0,  # increase to use multi-processing ) In\u00a0[\u00a0]: Copied! <pre>solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0])\n</pre> solver_cfg = SolverConfig(batch_sz=8, lr=3e-2, class_loss_weights=[1.0, 10.0]) In\u00a0[\u00a0]: Copied! <pre>learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg)\n</pre> learner_cfg = SemanticSegmentationLearnerConfig(data=data_cfg, solver=solver_cfg) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner(\n    cfg=learner_cfg,\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n)\n</pre> learner = SemanticSegmentationLearner(     cfg=learner_cfg,     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds, ) In\u00a0[\u00a0]: Copied! <pre>learner.log_data_stats()\n</pre> learner.log_data_stats() In\u00a0[\u00a0]: Copied! <pre>%load_ext tensorboard\n</pre> %load_ext tensorboard In\u00a0[\u00a0]: Copied! <pre>%tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10\n</pre> %tensorboard --bind_all --logdir \"./train-demo/tb-logs\" --reload_interval 10 In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=3)\n</pre> learner.train(epochs=3) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>learner.save_model_bundle()\n</pre> learner.save_model_bundle() In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model, ) In\u00a0[\u00a0]: Copied! <pre>learner = SemanticSegmentationLearner.from_model_bundle(\n    model_bundle_uri=\"./train-demo/model-bundle.zip\",\n    output_dir=\"./train-demo/\",\n    model=model,\n    train_ds=train_ds,\n    valid_ds=val_ds,\n    training=True,\n)\n</pre> learner = SemanticSegmentationLearner.from_model_bundle(     model_bundle_uri=\"./train-demo/model-bundle.zip\",     output_dir=\"./train-demo/\",     model=model,     train_ds=train_ds,     valid_ds=val_ds,     training=True, ) In\u00a0[\u00a0]: Copied! <pre>learner.train(epochs=1)\n</pre> learner.train(epochs=1) In\u00a0[\u00a0]: Copied! <pre>learner.plot_predictions(split=\"valid\", show=True)\n</pre> learner.plot_predictions(split=\"valid\", show=True) In\u00a0[\u00a0]: Copied! <pre>predictions = learner.predict_dataset(\n    pred_ds,\n    raw_out=True,\n    numpy_out=True,\n    predict_kw=dict(out_shape=(325, 325)),\n    progress_bar=True,\n)\n</pre> predictions = learner.predict_dataset(     pred_ds,     raw_out=True,     numpy_out=True,     predict_kw=dict(out_shape=(325, 325)),     progress_bar=True, ) In\u00a0[\u00a0]: Copied! <pre>pred_labels = SemanticSegmentationLabels.from_predictions(\n    pred_ds.windows,\n    predictions,\n    smooth=True,\n    extent=pred_ds.scene.extent,\n    num_classes=len(class_config),\n)\n</pre> pred_labels = SemanticSegmentationLabels.from_predictions(     pred_ds.windows,     predictions,     smooth=True,     extent=pred_ds.scene.extent,     num_classes=len(class_config), ) In\u00a0[\u00a0]: Copied! <pre>scores = pred_labels.get_score_arr(pred_labels.extent)\n</pre> scores = pred_labels.get_score_arr(pred_labels.extent) In\u00a0[\u00a0]: Copied! <pre>pred_labels.save(\n    uri=f\"predict\",\n    crs_transformer=pred_ds.scene.raster_source.crs_transformer,\n    class_config=class_config,\n)\n</pre> pred_labels.save(     uri=f\"predict\",     crs_transformer=pred_ds.scene.raster_source.crs_transformer,     class_config=class_config, )"},{"location":"examples/samgeo/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/samgeo/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/samgeo/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Open Windows Registry Editor (<code>regedit.exe</code>) and navigate to <code>Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem</code>. Change the value of <code>LongPathsEnabled</code> to <code>1</code>. See this screenshot. This is a known issue with the deep learning libraries for ArcGIS Pro 3.1. A future release might fix this issue.</p> </li> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install mamba and Python 3.9.x from the Esri Anaconda channel. Mamba is a drop-in replacement for conda that is mach faster for installing Python packages and their dependencies.</p> <p><code>conda create conda-forge::mamba esri::python --name samgeo</code></p> </li> <li><p>Activate the new conda environment.</p> <p><code>conda activate samgeo</code></p> </li> <li><p>Install arcpy, deep-learning-essentials, segment-geospatial, and other dependencies (~4GB download).</p> <p><code>mamba install arcpy deep-learning-essentials leafmap localtileserver segment-geospatial -c esri -c conda-forge</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap samgeo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/samgeo/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/samgeo/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/samgeo/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/samgeo/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import (\n    SamGeo,\n    show_image,\n    download_file,\n    overlay_images,\n    tms_to_geotiff,\n)\n</pre> import os import leafmap from samgeo.hq_sam import (     SamGeo,     show_image,     download_file,     overlay_images,     tms_to_geotiff, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/samgeo/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/samgeo/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/samgeo/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/samgeo/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p> <p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p> <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/samgeo/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/samgeo/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/samgeo/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/samgeo/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/samgeo/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/samgeo/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.fast_sam import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.fast_sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/samgeo/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/samgeo/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo.hq_sam import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/samgeo/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/samgeo/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/samgeo/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/samgeo/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, raster_to_vector, overlay_images\n</pre> import os import leafmap from samgeo import SamGeo, raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = (\n    \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\"\n)\n</pre> url = (     \"https://drive.google.com/file/d/1jIIC5hvSPeJEC0fbDhtxVWk2XV9AxsQD/view?usp=sharing\" ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/samgeo/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/samgeo/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/samgeo/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/samgeo/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/samgeo/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/samgeo/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff, get_basemaps\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff, get_basemaps In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/samgeo/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/samgeo/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/samgeo/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/samgeo/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/samgeo/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/samgeo/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/","title":"Swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/samgeo/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/samgeo/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/samgeo/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/samgeo/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/samgeo/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/samgeo/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/samgeo/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/samgeo/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/samgeo/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"workshops/AGU_2025/","title":"AGU 2025","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\" In\u00a0[\u00a0]: Copied! <pre>out_folder = geoai.download_file(url)\nprint(f\"Downloaded dataset to {out_folder}\")\n</pre> out_folder = geoai.download_file(url) print(f\"Downloaded dataset to {out_folder}\") <p>The unzipped dataset contains two folders: <code>images</code> and <code>masks</code>. Each folder contains 2,841 images in JPG format. The <code>images</code> folder contains the original satellite imagery, and the <code>masks</code> folder contains the corresponding surface water masks in binary format (white pixels = water, black pixels = background).</p> <p>Dataset characteristics:</p> <ul> <li>Total image pairs: 2,841 training examples</li> <li>Image format: RGB satellite imagery (3 channels)</li> <li>Mask format: Binary masks where 255 = water, 0 = background</li> <li>Variable image sizes: Ranging from small 256x256 patches to larger 1024x1024+ images</li> <li>Global coverage: Samples from diverse geographic regions and water body types</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Test train_segmentation_model with automatic size detection\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/masks\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",  # The architecture to use for the model\n    encoder_name=\"resnet34\",  # The encoder to use for the model\n    encoder_weights=\"imagenet\",  # The weights to use for the encoder\n    num_channels=3,  # number of channels in the input image\n    num_classes=2,  # background and water\n    batch_size=16,  # The number of images to process in each batch\n    num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs\n    learning_rate=0.001,  # learning rate for the optimizer\n    val_split=0.2,  # 20% of the data for validation\n    target_size=(512, 512),  # target size of the input image\n    verbose=True,  # print progress\n)\n</pre> # Test train_segmentation_model with automatic size detection geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/masks\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",  # The architecture to use for the model     encoder_name=\"resnet34\",  # The encoder to use for the model     encoder_weights=\"imagenet\",  # The weights to use for the encoder     num_channels=3,  # number of channels in the input image     num_classes=2,  # background and water     batch_size=16,  # The number of images to process in each batch     num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs     learning_rate=0.001,  # learning rate for the optimizer     val_split=0.2,  # 20% of the data for validation     target_size=(512, 512),  # target size of the input image     verbose=True,  # print progress ) <p>In the model output folder <code>unet_models</code>, you will find the following files:</p> <ul> <li><code>best_model.pth</code>: The best model checkpoint (highest validation IoU)</li> <li><code>final_model.pth</code>: The last model checkpoint from the final epoch</li> <li><code>training_history.pth</code>: Complete training metrics for analysis and plotting</li> <li><code>training_summary.txt</code>: Human-readable summary of training configuration and results</li> </ul> In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>index = 3  # change it to other image index, e.g., 100\ntest_image_path = f\"{out_folder}/images/water_body_{index}.jpg\"\nground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\"\nprediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> index = 3  # change it to other image index, e.g., 100 test_image_path = f\"{out_folder}/images/water_body_{index}.jpg\" ground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\" prediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_image_path,\n    output_path=prediction_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_image_path,     output_path=prediction_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32, ) In\u00a0[\u00a0]: Copied! <pre>fig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",\n    show_plot=True,\n)\n</pre> fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",     show_plot=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\" In\u00a0[\u00a0]: Copied! <pre>data_dir = geoai.download_file(url)\nprint(f\"Downloaded dataset to {data_dir}\")\n</pre> data_dir = geoai.download_file(url) print(f\"Downloaded dataset to {data_dir}\") In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/images\"\nmasks_dir = f\"{data_dir}/masks\"\npredictions_dir = f\"{data_dir}/predictions\"\n</pre> images_dir = f\"{data_dir}/images\" masks_dir = f\"{data_dir}/masks\" predictions_dir = f\"{data_dir}/predictions\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/dset-s2.zip\"\ndata_dir = geoai.download_file(url, output_path=\"dset-s2.zip\")\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/dset-s2.zip\" data_dir = geoai.download_file(url, output_path=\"dset-s2.zip\") <p>Dataset structure:</p> <p>In the unzipped dataset, we have four folders:</p> <ul> <li><code>dset-s2/tra_scene</code>: Training images - Sentinel-2 scenes for model training</li> <li><code>dset-s2/tra_truth</code>: Training masks - Corresponding water truth masks</li> <li><code>dset-s2/val_scene</code>: Validation images - Independent Sentinel-2 scenes for testing</li> <li><code>dset-s2/val_truth</code>: Validation masks - Ground truth for performance evaluation</li> </ul> <p>We will use the training images and masks to train our model, then evaluate performance on the completely independent validation set.</p> In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/tra_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/tra_truth\"\ntiles_dir = f\"{data_dir}/dset-s2/tiles\"\n</pre> images_dir = f\"{data_dir}/dset-s2/tra_scene\" masks_dir = f\"{data_dir}/dset-s2/tra_truth\" tiles_dir = f\"{data_dir}/dset-s2/tiles\" In\u00a0[\u00a0]: Copied! <pre>result = geoai.export_geotiff_tiles_batch(\n    images_folder=images_dir,\n    masks_folder=masks_dir,\n    output_folder=tiles_dir,\n    tile_size=512,\n    stride=128,\n    quiet=True,\n)\n</pre> result = geoai.export_geotiff_tiles_batch(     images_folder=images_dir,     masks_folder=masks_dir,     output_folder=tiles_dir,     tile_size=512,     stride=128,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{tiles_dir}/images\",\n    labels_dir=f\"{tiles_dir}/masks\",\n    output_dir=f\"{tiles_dir}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=6,\n    num_classes=2,  # background and water\n    batch_size=32,\n    num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{tiles_dir}/images\",     labels_dir=f\"{tiles_dir}/masks\",     output_dir=f\"{tiles_dir}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=6,     num_classes=2,  # background and water     batch_size=32,     num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{tiles_dir}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{tiles_dir}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/val_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/val_truth\"\npredictions_dir = f\"{data_dir}/dset-s2/predictions\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> images_dir = f\"{data_dir}/dset-s2/val_scene\" masks_dir = f\"{data_dir}/dset-s2/val_truth\" predictions_dir = f\"{data_dir}/dset-s2/predictions\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>image_id = \"S2A_L2A_20190318_N0211_R061\"  # Change to other image id, e.g., S2B_L2A_20190620_N0212_R047\ntest_image_path = f\"{data_dir}/dset-s2/val_scene/{image_id}_6Bands_S2.tif\"\nground_truth_path = f\"{data_dir}/dset-s2/val_truth/{image_id}_S2_Truth.tif\"\nprediction_path = f\"{data_dir}/dset-s2/predictions/{image_id}_6Bands_S2_mask.tif\"\nsave_path = f\"{data_dir}/dset-s2/{image_id}_6Bands_S2_comparison.png\"\n\nfig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=save_path,\n    show_plot=True,\n    indexes=[5, 4, 3],\n    divider=5000,\n)\n</pre> image_id = \"S2A_L2A_20190318_N0211_R061\"  # Change to other image id, e.g., S2B_L2A_20190620_N0212_R047 test_image_path = f\"{data_dir}/dset-s2/val_scene/{image_id}_6Bands_S2.tif\" ground_truth_path = f\"{data_dir}/dset-s2/val_truth/{image_id}_S2_Truth.tif\" prediction_path = f\"{data_dir}/dset-s2/predictions/{image_id}_6Bands_S2_mask.tif\" save_path = f\"{data_dir}/dset-s2/{image_id}_6Bands_S2_comparison.png\"  fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=save_path,     show_plot=True,     indexes=[5, 4, 3],     divider=5000, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\n</pre> import os import leafmap <p>Set up the TiTiler endpoint for visualizing raster data.</p> In\u00a0[\u00a0]: Copied! <pre>os.environ[\"TITILER_ENDPOINT\"] = \"https://giswqs-titiler-endpoint.hf.space\"\n</pre> os.environ[\"TITILER_ENDPOINT\"] = \"https://giswqs-titiler-endpoint.hf.space\" <p>Create an interactive map to explore available Sentinel-2 data.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[46.693725, -95.925399], zoom=12)\nm.add_basemap(\"Esri.WorldImagery\")\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[46.693725, -95.925399], zoom=12) m.add_basemap(\"Esri.WorldImagery\") m.add_stac_gui() m In\u00a0[\u00a0]: Copied! <pre>try:\n    print(m.user_roi_bounds())\nexcept:\n    print(\"Please draw a rectangle on the map before running this cell\")\n</pre> try:     print(m.user_roi_bounds()) except:     print(\"Please draw a rectangle on the map before running this cell\") <p>Use the drawing tool to draw a rectangle on the map. Click on the Search button to search Sentinel-2 imagery intersecting the rectangle.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n    display(m.stac_gdf)\nexcept:\n    print(\"Click on the Search button before running this cell\")\n</pre> try:     display(m.stac_gdf) except:     print(\"Click on the Search button before running this cell\") In\u00a0[\u00a0]: Copied! <pre>try:\n    display(m.stac_item)\nexcept:\n    print(\"click on the Display button before running this cell\")\n</pre> try:     display(m.stac_item) except:     print(\"click on the Display button before running this cell\") <p>Search for Sentinel-2 data programmatically.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://earth-search.aws.element84.com/v1/\"\ncollection = \"sentinel-2-l2a\"\ntime_range = \"2025-08-15/2025-08-31\"\nbbox = [-95.9912, 46.6704, -95.834, 46.7469]\n</pre> url = \"https://earth-search.aws.element84.com/v1/\" collection = \"sentinel-2-l2a\" time_range = \"2025-08-15/2025-08-31\" bbox = [-95.9912, 46.6704, -95.834, 46.7469] In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=10,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_collection=True,\n)\nsearch\n</pre> search = leafmap.stac_search(     url=url,     max_items=10,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_collection=True, ) search In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=10,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_gdf=True,\n)\nsearch.head()\n</pre> search = leafmap.stac_search(     url=url,     max_items=10,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_gdf=True, ) search.head() In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=1,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_assets=True,\n)\nsearch\n</pre> search = leafmap.stac_search(     url=url,     max_items=1,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_assets=True, ) search In\u00a0[\u00a0]: Copied! <pre>bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\nassets = list(search.values())[0]\nlinks = [assets[band] for band in bands]\nfor link in links:\n    print(link)\n</pre> bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"] assets = list(search.values())[0] links = [assets[band] for band in bands] for link in links:     print(link) In\u00a0[\u00a0]: Copied! <pre>out_dir = \"s2\"\nleafmap.download_files(links, out_dir)\n</pre> out_dir = \"s2\" leafmap.download_files(links, out_dir) In\u00a0[\u00a0]: Copied! <pre># !apt-get install -y gdal-bin\n</pre> # !apt-get install -y gdal-bin In\u00a0[\u00a0]: Copied! <pre>s2_path = \"s2.tif\"\n\ntry:\n    if not os.path.exists(s2_path):\n        geoai.stack_bands(input_files=out_dir, output_file=s2_path)\nexcept Exception as e:\n    print(e)\n    url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/s2-minnesota-2025-08-31-subset.tif\"\n    geoai.download_file(url, output_path=s2_path)\n</pre> s2_path = \"s2.tif\"  try:     if not os.path.exists(s2_path):         geoai.stack_bands(input_files=out_dir, output_file=s2_path) except Exception as e:     print(e)     url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/s2-minnesota-2025-08-31-subset.tif\"     geoai.download_file(url, output_path=s2_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    s2_path, indexes=[4, 3, 2], vmin=0, vmax=5000, layer_name=\"Sentinel-2\"\n)\n</pre> geoai.view_raster(     s2_path, indexes=[4, 3, 2], vmin=0, vmax=5000, layer_name=\"Sentinel-2\" ) In\u00a0[\u00a0]: Copied! <pre>s2_mask = \"s2_mask.tif\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> s2_mask = \"s2_mask.tif\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=s2_path,\n    output_path=s2_mask,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n)\n</pre> geoai.semantic_segmentation(     input_path=s2_path,     output_path=s2_mask,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    s2_mask,\n    no_data=0,\n    colormap=\"binary\",\n    layer_name=\"Water\",\n    basemap=s2_path,\n    basemap_args={\"indexes\": [4, 3, 2], \"vmin\": 0, \"vmax\": 5000},\n)\n</pre> geoai.view_raster(     s2_mask,     no_data=0,     colormap=\"binary\",     layer_name=\"Water\",     basemap=s2_path,     basemap_args={\"indexes\": [4, 3, 2], \"vmin\": 0, \"vmax\": 5000}, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\"\ntrain_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\" train_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_masks_path = geoai.download_file(train_masks_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_masks_path = geoai.download_file(train_masks_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(train_raster_path, show_preview=False)\n</pre> geoai.print_raster_info(train_raster_path, show_preview=False) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_masks_url, nodata=0, opacity=0.5, basemap=train_raster_url)\n</pre> geoai.view_raster(train_masks_url, nodata=0, opacity=0.5, basemap=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"naip\"\n</pre> out_folder = \"naip\" In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_masks_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_masks_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=4,\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=4,     batch_size=8,     num_epochs=3,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_water_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_water_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    test_raster_path,\n    masks_path,\n    model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    window_size=512,\n    overlap=128,\n    batch_size=32,\n    num_channels=4,\n)\n</pre> geoai.semantic_segmentation(     test_raster_path,     masks_path,     model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     window_size=512,     overlap=128,     batch_size=32,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path,\n    nodata=0,\n    layer_name=\"Water\",\n    basemap=test_raster_url,\n)\n</pre> geoai.view_raster(     masks_path,     nodata=0,     layer_name=\"Water\",     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_water_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\n</pre> output_path = \"naip_water_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\nlen(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) len(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf[\"elongation\"].hist()\n</pre> gdf[\"elongation\"].hist() In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10]\n</pre> gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10] In\u00a0[\u00a0]: Copied! <pre>len(gdf_filtered)\n</pre> len(gdf_filtered) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() <p></p> In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(train_raster_path)\n</pre> geoai.get_raster_info(train_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"buildings\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"buildings\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and building     batch_size=8,     num_epochs=3,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_semantic_prediction.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> masks_path = \"naip_test_semantic_prediction.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path,\n    nodata=0,\n    colormap=\"binary\",\n    basemap=test_raster_url,\n)\n</pre> geoai.view_raster(     masks_path,     nodata=0,     colormap=\"binary\",     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"naip_test_semantic_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"naip_test_semantic_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 10)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 10)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache()"},{"location":"workshops/AGU_2025/#3-integrating-artificial-intelligence-with-geospatial-data-analysis-and-visualization","title":"\ud83e\udd16 3 - Integrating artificial intelligence with geospatial data analysis and visualization\u00b6","text":""},{"location":"workshops/AGU_2025/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is for the workshop (Open Source Geospatial Workflows in the Cloud) presented at the AGU Fall Meeting 2025.</p> <p>GeoAI represents the intersection of geospatial science and artificial intelligence, combining the power of machine learning with geographic information systems (GIS) to analyze, understand, and predict spatial patterns. This rapidly growing field enables us to extract meaningful insights from satellite imagery, aerial photos, and other geospatial datasets at unprecedented scales and accuracy levels.</p> <p>The GeoAI Python package (https://opengeoai.org) simplifies the application of deep learning models to geospatial data, making advanced AI techniques accessible to researchers, analysts, and practitioners in environmental science, urban planning, agriculture, and disaster management. The package provides a unified interface for:</p> <ul> <li>Data Preprocessing: Automated handling of various geospatial data formats, coordinate systems, and multi-spectral imagery</li> <li>Model Training: Pre-configured deep learning architectures optimized for geospatial tasks like semantic segmentation and object detection</li> <li>Feature Extraction: Automated extraction of geographic features from satellite and aerial imagery</li> <li>Visualization: Interactive mapping and analysis tools for exploring results</li> </ul> <p>In this workshop, you will:</p> <ul> <li>Discover the core capabilities of the GeoAI package, including data preprocessing, feature extraction, and geospatial deep learning workflows</li> <li>See live demonstrations on applying state-of-the-art AI models to satellite and aerial imagery</li> <li>Learn how to train custom segmentation models for surface water mapping using different data sources</li> <li>Explore real-world use cases in building footprint extraction and surface water mapping</li> </ul> <p>Additional Resources</p> <ul> <li>GitHub: GeoAI</li> <li>Book: Introduction to GIS Programming: A Practical Python Guide to Open Source Geospatial Tools</li> <li>YouTube: Open Geospatial Solution</li> </ul>"},{"location":"workshops/AGU_2025/#deep-learning-architectures-and-encoders","title":"Deep Learning Architectures and Encoders\u00b6","text":"<p>Before moving into the hands-on work, it\u2019s important to first understand the key ideas behind deep learning architectures and encoders.</p> <p>A deep learning architecture is like the blueprint of a factory. It defines how the network is organized, how data flows through different components, and how raw inputs are transformed into meaningful outputs. Just as a factory blueprint specifies where materials enter, how they are processed, and where finished products come out, a neural network architecture lays out the arrangement of layers (neurons) that progressively extract and refine patterns from data\u2014for example, detecting cats in images or translating between languages.</p> <p>Within this blueprint, an encoder functions as a specialized assembly line. Its role is to take messy raw materials (input data) and refine them into a compact, standardized representation that is easier for the rest of the system to work with. Some architectures also include a decoder assembly line, which reconstructs or generates the final output from the encoder\u2019s compressed representation\u2014for example, assembling a finished car from engine parts and panels.</p> <p>In short:</p> <ul> <li>Model architecture = the factory blueprint (overall design and flow)</li> <li>Encoder = the preprocessing line (condenses raw inputs into useful parts)</li> <li>Decoder = the finishing line (turns encoded parts into a final product)</li> </ul>"},{"location":"workshops/AGU_2025/#types-of-architectures","title":"Types of Architectures\u00b6","text":"<p>Different blueprints are suited for different tasks:</p> <ul> <li>Feedforward Neural Networks: simple, one-directional flow of data.</li> <li>Convolutional Neural Networks (CNNs): specialized for images, capturing spatial patterns like edges and textures.</li> <li>Recurrent Neural Networks (RNNs): designed for sequences, such as speech or time series.</li> <li>Transformers: powerful models for language and beyond, using attention mechanisms (e.g., ChatGPT).</li> </ul>"},{"location":"workshops/AGU_2025/#what-does-an-encoder-do","title":"What Does an Encoder Do?\u00b6","text":"<p>An encoder is the part of a neural network that takes an input (like an image or a sentence) and compresses it into a smaller, meaningful form called a feature representation or embedding. This process keeps the essential information while filtering out noise.</p> <p>For example, the sentence \u201cI love pizza\u201d might be converted by an encoder into a vector of numbers that still reflects its meaning, but in a way that is easier for a computer to analyze and use.</p> <p>Encoders appear in many contexts:</p> <ul> <li>Autoencoders: learn to compress and reconstruct data.</li> <li>Transformer Encoders: such as BERT, used for language understanding.</li> <li>Encoder\u2013Decoder Models: such as translation systems, where the encoder reads one language and the decoder generates another.</li> </ul>"},{"location":"workshops/AGU_2025/#encoders-and-architectures-in-practice","title":"Encoders and Architectures in Practice\u00b6","text":"<p>The pytorch segmentation models library provides a wide range of pre-trained models for semantic segmentation. It separates architectures (the blueprint) from encoders (the feature extractors):</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code>, <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code>.</li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, and many more.</li> </ul> <p>The GeoAI package builds on this library, offering a convenient wrapper so you can easily train your own segmentation models with a variety of architectures and encoders.</p>"},{"location":"workshops/AGU_2025/#environment-setup","title":"Environment Setup\u00b6","text":"<p>You can run this notebook locally or in Google Colab. You will need a GPU for training deep learning models. If you don't have a GPU, you can use the free GPU in Google Colab.</p> <p>To install the GeoAI package, it is recommended to use a virtual environment. Please refer to the GeoAI installation guide for more details.</p> <p>Here is a quick start guide to install the GeoAI package:</p> <pre>conda create -n geo python=3.12\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge geoai\n</pre> <p>If you have a GPU, you can install the package with GPU support:</p> <pre>mamba install -c conda-forge geoai \"pytorch=*=cuda*\"\n</pre> <p>You can install the package using pip:</p> <pre>pip install geoai-py\n</pre>"},{"location":"workshops/AGU_2025/#use-colab-gpu","title":"Use Colab GPU\u00b6","text":"<p>To use GPU, please click the \"Runtime\" menu and select \"Change runtime type\". Then select \"T4 GPU\" from the dropdown menu. GPU acceleration is highly recommended for training deep learning models, as it can reduce training time from hours to minutes.</p>"},{"location":"workshops/AGU_2025/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following cell to install the package. It may take a few minutes to install the package and its dependencies. Please be patient.</p>"},{"location":"workshops/AGU_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Let's import the GeoAI package.</p>"},{"location":"workshops/AGU_2025/#surface-water-mapping-with-non-georeferenced-satellite-imagery","title":"Surface water mapping with non-georeferenced satellite imagery\u00b6","text":"<p>Surface water mapping is one of the most important applications of GeoAI, as water resources are critical for ecosystem health, agriculture, urban planning, and climate monitoring. In this first demonstration, we'll work with non-georeferenced satellite imagery in standard image formats (JPG/PNG), which is often how satellite imagery is initially distributed or stored.</p> <p>Why start with non-georeferenced imagery?</p> <ul> <li>Many datasets and online sources provide satellite imagery without embedded geographic coordinates</li> <li>It demonstrates the core computer vision aspects of GeoAI before adding geospatial complexity</li> <li>The techniques learned here can be applied to any imagery, regardless of coordinate system</li> <li>It's often faster to iterate and experiment with standard image formats</li> </ul> <p>We'll use semantic segmentation, a deep learning technique that classifies every pixel in an image. Unlike object detection (which draws bounding boxes), semantic segmentation provides precise pixel-level predictions, making it ideal for mapping natural features like water bodies that have irregular shapes.</p>"},{"location":"workshops/AGU_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the waterbody dataset from Kaggle, which contains 2,841 satellite image pairs with corresponding water masks. This dataset is particularly valuable because:</p> <ul> <li>Diverse geographic coverage: Images from different continents and climate zones</li> <li>Varied water body types: Lakes, rivers, ponds, and coastal areas</li> <li>Multiple seasons and conditions: Different lighting conditions and seasonal variations</li> <li>High-quality annotations: Manually verified water body masks for training</li> </ul> <p>Credits to the author Francisco Escobar for providing this dataset. I downloaded the dataset from Kaggle and uploaded it to Hugging Face for easy access:</p>"},{"location":"workshops/AGU_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using U-Net architecture with a ResNet34 encoder and ImageNet pre-trained weights. Let's break down these important choices:</p> <p>Architecture: U-Net</p> <ul> <li>U-Net is a convolutional neural network architecture specifically designed for semantic segmentation</li> <li>It has a \"U\" shape with an encoder (downsampling) path and a decoder (upsampling) path</li> <li>Skip connections between encoder and decoder preserve fine-grained spatial details</li> <li>Originally designed for medical image segmentation, it works exceptionally well for geospatial applications</li> </ul> <p>Encoder: ResNet34</p> <ul> <li>ResNet34 is a 34-layer Residual Network that serves as the feature extraction backbone</li> <li>Residual connections allow training of very deep networks without vanishing gradient problems</li> <li>Balances model complexity with computational efficiency (deeper than ResNet18, more efficient than ResNet50)</li> <li>Well-suited for satellite imagery feature extraction</li> </ul> <p>Pre-trained weights: ImageNet</p> <ul> <li>Transfer learning from ImageNet provides a strong starting point for feature extraction</li> <li>ImageNet-trained models have learned to recognize edges, textures, and patterns relevant to natural imagery</li> <li>Significantly reduces training time and improves performance, especially with limited training data</li> <li>The encoder starts with knowledge of general image features, then specializes for water detection</li> </ul> <p>Key training parameters:</p> <ul> <li><code>num_channels=3</code>: RGB satellite imagery (red, green, blue bands)</li> <li><code>num_classes=2</code>: Binary classification (background vs. water)</li> <li><code>batch_size=32</code>: Process 32 images simultaneously for efficient GPU utilization</li> <li><code>num_epochs=3</code>: Training iterations (limited for demo; real-world would use 20-50+ epochs)</li> <li><code>learning_rate=0.001</code>: Controls optimization step size</li> <li><code>val_split=0.2</code>: Reserve 20% of data for validation to monitor overfitting</li> <li><code>target_size=(512, 512)</code>: Standardize all images to 512x512 pixels for consistent processing</li> </ul> <p>For more details on available architectures and encoders, please refer to https://smp.readthedocs.io/en/latest/encoders.html.</p>"},{"location":"workshops/AGU_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Model evaluation is crucial for understanding how well our trained model performs. We'll examine both the training curves and quantitative metrics to assess model quality and identify potential issues like overfitting or underfitting.</p> <p>Key evaluation metrics for semantic segmentation:</p> <ol> <li><p>Loss: Measures how far the model's predictions are from the ground truth</p> <ul> <li>Training loss: How well the model fits the training data</li> <li>Validation loss: How well the model generalizes to unseen data</li> <li>Ideal pattern: Both should decrease, with validation loss closely following training loss</li> </ul> </li> <li><p>IoU (Intersection over Union): The most important metric for segmentation tasks</p> <ul> <li>Definition: Area of overlap / Area of union between prediction and ground truth</li> <li>Range: 0.0 (no overlap) to 1.0 (perfect overlap)</li> <li>Interpretation: 0.69 IoU means ~69% accurate pixel-level water detection</li> <li>Industry standard: IoU &gt; 0.7 is generally considered good performance</li> </ul> </li> <li><p>Dice (F-1) Score: Alternative segmentation metric, closely related to IoU</p> <ul> <li>Definition: 2 \u00d7 (Area of overlap) / (Total pixels in both prediction and ground truth)</li> <li>Range: 0.0 to 1.0, similar to IoU but slightly more lenient</li> <li>Relationship: Dice = 2\u00d7IoU / (1+IoU)</li> </ul> </li> </ol> <p>IoU and Dice are monotonically related\u2014optimizing one generally optimizes the other. However, Dice tends to give slightly higher values than IoU for the same segmentation.</p> <ul> <li><p>IoU is stricter: it penalizes false positives and false negatives more heavily, making it less forgiving of small mismatches.</p> </li> <li><p>Dice is more sensitive to overlap and is often preferred in medical image segmentation, where the overlap between predicted and actual regions is more important than absolute boundaries.</p> </li> <li><p>IoU is often used in object detection and computer vision challenges (e.g., COCO benchmark), because it aligns with bounding box overlap evaluation.</p> </li> </ul> <p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/AGU_2025/#run-inference-on-a-single-image","title":"Run inference on a single image\u00b6","text":"<p>Inference is the process of using our trained model to make predictions on new, unseen images. This is where we see the practical application of our trained model.</p> <p>Note on testing approach: In this demo, we're using one of the training images for inference to demonstrate the workflow. In a real-world scenario, you should always test on completely independent images that were never seen during training to get an accurate assessment of model performance.</p> <p>You can run inference on a new image using the <code>semantic_segmentation</code> function:</p>"},{"location":"workshops/AGU_2025/#run-inference-on-multiple-images","title":"Run inference on multiple images\u00b6","text":"<p>Batch processing is essential for operational applications where you need to process many images efficiently. The GeoAI package provides <code>semantic_segmentation_batch</code> for processing entire directories of images with consistent parameters.</p> <p>First, let's download a sample set of test images that the model has never seen:</p>"},{"location":"workshops/AGU_2025/#surface-water-mapping-with-sentinel-2-imagery","title":"Surface water mapping with Sentinel-2 imagery\u00b6","text":"<p>In the second part of this notebook, we'll demonstrate surface water mapping using Sentinel-2 satellite imagery, which provides multispectral data with much richer information than standard RGB imagery.</p> <p>Why Sentinel-2 is ideal for water mapping:</p> <p>Sentinel-2 is a European Space Agency (ESA) satellite mission providing high-resolution optical imagery for land monitoring. Key advantages for water detection include:</p> <ul> <li>Multispectral capabilities: 13 spectral bands covering visible, near-infrared, and short-wave infrared</li> <li>High spatial resolution: 10-20m pixels for detailed water body mapping</li> <li>Frequent revisit time: 5-day global coverage for monitoring temporal changes</li> <li>Free and open access: Available through Copernicus Open Access Hub and other platforms</li> <li>Consistent quality: Calibrated, atmospherically corrected imagery (Level 2A)</li> </ul> <p>Spectral bands used in this analysis:</p> <ol> <li>Blue (490nm): Water absorption, atmospheric correction</li> <li>Green (560nm): Vegetation health, water clarity</li> <li>Red (665nm): Vegetation chlorophyll, land-water contrast</li> <li>Near-Infrared (842nm): Critical for water detection - water strongly absorbs NIR</li> <li>SWIR1 (1610nm): Excellent water discriminator - water has very low reflectance</li> <li>SWIR2 (2190nm): Additional water detection - separates water from wet soil/vegetation</li> </ol>"},{"location":"workshops/AGU_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the Earth Surface Water Dataset from Zenodo, which contains Sentinel-2 imagery with 6 spectral bands and corresponding water masks. Credits to Xin Luo for creating this high-quality dataset.</p> <p>Dataset characteristics:</p> <ul> <li>Sensor: Sentinel-2 Level 2A (atmospherically corrected)</li> <li>Bands: Blue, Green, Red, NIR, SWIR1, SWIR2 (6 channels total)</li> <li>Spatial resolution: 10-20 meters per pixel</li> <li>Geographic coverage: Multiple global locations with diverse water body types</li> <li>Ground truth: Expert-annotated water masks for training and validation</li> </ul>"},{"location":"workshops/AGU_2025/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create smaller training tiles from the large GeoTIFF images. Note that we have multiple Sentinel-2 scenes in the training and validation sets, we will use the <code>export_geotiff_tiles_batch</code> function to export tiles from each scene.</p>"},{"location":"workshops/AGU_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model specifically for 6-channel Sentinel-2 imagery. The key difference from our previous model is the input channel configuration.</p> <p>Important parameter changes:</p> <ul> <li><code>num_channels=6</code>: Accommodate the 6 Sentinel-2 spectral bands (Blue, Green, Red, NIR, SWIR1, SWIR2)</li> <li><code>num_epochs=5</code>: Slightly more training epochs to learn complex spectral relationships</li> <li>Architecture remains U-Net + ResNet34: Proven effective for multispectral imagery</li> </ul> <p>Let's train the model using the Sentinel-2 tiles:</p>"},{"location":"workshops/AGU_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/AGU_2025/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll run inference on the validation set to evaluate the model's performance. We will use the <code>semantic_segmentation_batch</code> function to process all the validation images at once.</p>"},{"location":"workshops/AGU_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/AGU_2025/#download-sentinel-2-imagery","title":"Download Sentinel-2 imagery\u00b6","text":"<p>Real-world data acquisition is a crucial skill for operational GeoAI applications. Here we'll demonstrate how to:</p> <ol> <li>Search for Sentinel-2 data using STAC (SpatioTemporal Asset Catalog) APIs</li> <li>Apply quality filters (cloud cover, date range, geographic bounds)</li> <li>Download specific spectral bands needed for our analysis</li> <li>Prepare data for inference with our trained model</li> </ol> <p>STAC catalogs provide a standardized way to search and access satellite imagery across different providers. The Earth Search STAC API aggregates Sentinel-2 data from AWS Open Data, making it easily accessible for analysis.</p> <p>Search parameters:</p> <ul> <li>Geographic bounds: Define area of interest (bbox)</li> <li>Temporal range: Specify date range for imagery</li> <li>Cloud cover filter: Limit to images with &lt;10% cloud cover</li> <li>Collection: Focus on Sentinel-2 Level 2A (atmospherically corrected)</li> <li>Sorting: Order by cloud cover (ascending) to get clearest images first</li> </ul> <p>Let's set up an interactive map to explore available Sentinel-2 data:</p>"},{"location":"workshops/AGU_2025/#stack-image-bands","title":"Stack image bands\u00b6","text":"<p>Uncomment the following cell to install GDAL on Colab.</p>"},{"location":"workshops/AGU_2025/#run-inference-on-a-sentinel-2-image","title":"Run inference on a Sentinel-2 image\u00b6","text":""},{"location":"workshops/AGU_2025/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"workshops/AGU_2025/#surface-water-mapping-with-aerial-imagery","title":"Surface water mapping with aerial imagery\u00b6","text":"<p>In this section, we'll demonstrate surface water mapping using aerial imagery from the USDA National Agriculture Imagery Program (NAIP). This represents the highest spatial resolution imagery commonly available for large-scale applications.</p> <p>NAIP imagery characteristics:</p> <p>What is NAIP?</p> <ul> <li>USDA Program: National Agriculture Imagery Program providing high-resolution aerial photography</li> <li>Coverage: Continental United States with comprehensive coverage</li> <li>Spatial resolution: 1-meter pixels (compared to 10-20m for Sentinel-2)</li> <li>Spectral bands: Red, Green, Blue, Near-Infrared (4 channels)</li> <li>Acquisition frequency: Updated every 2-3 years for each area</li> <li>Public availability: Free access through USGS and other data portals</li> </ul>"},{"location":"workshops/AGU_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>If you are interested in downloading NAIP imagery for your area of interest, check out this notebook here.</p> <p>To save time, we'll use a curated NAIP dataset with pre-processed training and testing imagery, including water body masks for model training and evaluation:</p>"},{"location":"workshops/AGU_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"workshops/AGU_2025/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"workshops/AGU_2025/#train-segmentation-model","title":"Train segmentation model\u00b6","text":"<p>Similar to the previous example, we'll train a U-Net model on the NAIP dataset.</p>"},{"location":"workshops/AGU_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"workshops/AGU_2025/#run-inference","title":"Run inference\u00b6","text":""},{"location":"workshops/AGU_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>We can convert the raster predictions to vector features for further analysis.</p>"},{"location":"workshops/AGU_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/AGU_2025/#building-detection-with-aerial-imagery","title":"Building detection with aerial imagery\u00b6","text":""},{"location":"workshops/AGU_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>If you are interested in downloading NAIP imagery and Overture Maps data for your area of interest, check out this notebook here.</p> <p>To save time, we'll use a curated NAIP dataset and building footprints for model training and evaluation:</p>"},{"location":"workshops/AGU_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"workshops/AGU_2025/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"workshops/AGU_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":""},{"location":"workshops/AGU_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/AGU_2025/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"workshops/AGU_2025/#visualize-raster-masks","title":"Visualize raster masks\u00b6","text":""},{"location":"workshops/AGU_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"workshops/AGU_2025/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"workshops/AGU_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/AGU_2025/#summary-and-next-steps","title":"Summary and Next Steps\u00b6","text":"<p>Congratulations! You've successfully completed a comprehensive introduction to the GeoAI package. Let's review what we accomplished and explore pathways for advancing your GeoAI skills.</p>"},{"location":"workshops/AGU_2025/#what-we-accomplished","title":"What We Accomplished\u00b6","text":"<p>1. Multi-scale Water Mapping Workflows:</p> <ul> <li>RGB Imagery: Trained models on standard satellite imagery (JPG format)</li> <li>Multispectral Sentinel-2: Leveraged 6 spectral bands for enhanced discrimination</li> <li>High-resolution NAIP: Utilized 1-meter aerial imagery for detailed mapping</li> </ul> <p>2. Deep Learning Fundamentals:</p> <ul> <li>U-Net Architecture: Applied state-of-the-art segmentation models</li> <li>Transfer Learning: Leveraged ImageNet pre-trained weights for faster convergence</li> <li>Multispectral Processing: Handled various spectral configurations (3, 4, and 6 channels)</li> </ul> <p>3. Operational Workflows:</p> <ul> <li>Data Acquisition: Downloaded and processed real satellite and aerial imagery</li> <li>Model Training: Trained custom models for different imagery types</li> <li>Performance Evaluation: Assessed model quality using IoU and Dice metrics</li> <li>Batch Processing: Applied models to multiple images efficiently</li> <li>Vector Conversion: Transformed predictions into GIS-ready polygon features</li> </ul> <p>4. Real-world Applications:</p> <ul> <li>Data Preprocessing: Handled various geospatial data formats and projections</li> <li>Quality Assessment: Filtered results based on geometric properties</li> <li>Interactive Visualization: Created interactive maps for exploring results</li> </ul>"},{"location":"workshops/AGU_2025/#thank-you","title":"Thank You!\u00b6","text":"<p>Thank you for participating in this GeoAI workshop! The techniques demonstrated here represent just the beginning of what's possible when combining artificial intelligence with geospatial analysis. The field of GeoAI is rapidly evolving, offering exciting opportunities to address real-world challenges in environmental monitoring, urban planning, agriculture, and climate science.</p> <p>Keep exploring, keep learning, and keep pushing the boundaries of what's possible with GeoAI!</p> <p>*For questions, feedback, or collaboration opportunities, please visit the GeoAI GitHub repository.</p>"},{"location":"workshops/AWS_2025/","title":"AWS 2025","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\" In\u00a0[\u00a0]: Copied! <pre>out_folder = geoai.download_file(url)\n</pre> out_folder = geoai.download_file(url) <p>The unzipped dataset contains two folders: <code>images</code> and <code>masks</code>. Each folder contains 2,841 images in jpg format. The <code>images</code> folder contains the original satellite imagery, and the <code>masks</code> folder contains the corresponding surface water masks. Note that the image size varies greatly.</p> <p>We will use these 2,841 image pairs to train a surface water mapping model.</p> <p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. This function supports various architectures from <code>segmentation-models-pytorch</code>:</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code> <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code></li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, etc.</li> </ul> <p>For more details, please refer to the segmentation-models-pytorch documentation.</p> <p>Let's train the module using U-Net with ResNet34 encoder:</p> In\u00a0[\u00a0]: Copied! <pre># Test train_segmentation_model with automatic size detection\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/masks\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,  # number of channels in the input image\n    num_classes=2,  # background and water\n    batch_size=32,  # The number of images to process in each batch\n    num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs\n    learning_rate=0.001,  # learning rate for the optimizer\n    val_split=0.2,  # 20% of the data for validation\n    target_size=(512, 512),  # target size of the input image\n    verbose=True,  # print progress\n)\n</pre> # Test train_segmentation_model with automatic size detection geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/masks\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,  # number of channels in the input image     num_classes=2,  # background and water     batch_size=32,  # The number of images to process in each batch     num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs     learning_rate=0.001,  # learning rate for the optimizer     val_split=0.2,  # 20% of the data for validation     target_size=(512, 512),  # target size of the input image     verbose=True,  # print progress ) <p>In the model output folder <code>unet_models</code>, you will find the following files:</p> <ul> <li><code>best_model.pth</code>: The best model checkpoint</li> <li><code>final_model.pth</code>: The last model checkpoint</li> <li><code>training_history.pth</code>: The training history</li> <li><code>training_summary.txt</code>: The training summary</li> </ul> In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>index = 3\ntest_image_path = f\"{out_folder}/images/water_body_{index}.jpg\"\nground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\"\nprediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> index = 3 test_image_path = f\"{out_folder}/images/water_body_{index}.jpg\" ground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\" prediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_image_path,\n    output_path=prediction_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_image_path,     output_path=prediction_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32, ) In\u00a0[\u00a0]: Copied! <pre>fig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",\n    show_plot=True,\n)\n</pre> fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",     show_plot=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\" In\u00a0[\u00a0]: Copied! <pre>data_dir = geoai.download_file(url)\nimages_dir = f\"{data_dir}/images\"\nmasks_dir = f\"{data_dir}/masks\"\npredictions_dir = f\"{data_dir}/predictions\"\n</pre> data_dir = geoai.download_file(url) images_dir = f\"{data_dir}/images\" masks_dir = f\"{data_dir}/masks\" predictions_dir = f\"{data_dir}/predictions\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>url = \"https://zenodo.org/records/5205674/files/dset-s2.zip?download=1\"\ndata_dir = geoai.download_file(url)\n</pre> url = \"https://zenodo.org/records/5205674/files/dset-s2.zip?download=1\" data_dir = geoai.download_file(url) <p>In the unzipped dataset, we have four folders:</p> <ul> <li><code>dset-s2/tra_scene</code>: training images</li> <li><code>dset-s2/tra_truth</code>: training masks</li> <li><code>dset-s2/val_scene</code>: validation images</li> <li><code>dset-s2/val_truth</code>: validation masks</li> </ul> <p>We will use the training images and masks to train a semantic segmentation model.</p> In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/tra_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/tra_truth\"\ntiles_dir = f\"{data_dir}/dset-s2/tiles\"\n</pre> images_dir = f\"{data_dir}/dset-s2/tra_scene\" masks_dir = f\"{data_dir}/dset-s2/tra_truth\" tiles_dir = f\"{data_dir}/dset-s2/tiles\" In\u00a0[\u00a0]: Copied! <pre>result = geoai.export_geotiff_tiles_batch(\n    images_folder=images_dir,\n    masks_folder=masks_dir,\n    output_folder=tiles_dir,\n    tile_size=512,\n    stride=128,\n    quiet=True,\n)\n</pre> result = geoai.export_geotiff_tiles_batch(     images_folder=images_dir,     masks_folder=masks_dir,     output_folder=tiles_dir,     tile_size=512,     stride=128,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{tiles_dir}/images\",\n    labels_dir=f\"{tiles_dir}/masks\",\n    output_dir=f\"{tiles_dir}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=6,\n    num_classes=2,  # background and water\n    batch_size=32,\n    num_epochs=5,  # training for 5 epochs to save time, in practice you should train for more epochs\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{tiles_dir}/images\",     labels_dir=f\"{tiles_dir}/masks\",     output_dir=f\"{tiles_dir}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=6,     num_classes=2,  # background and water     batch_size=32,     num_epochs=5,  # training for 5 epochs to save time, in practice you should train for more epochs     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{tiles_dir}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{tiles_dir}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/val_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/val_truth\"\npredictions_dir = f\"{data_dir}/dset-s2/predictions\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> images_dir = f\"{data_dir}/dset-s2/val_scene\" masks_dir = f\"{data_dir}/dset-s2/val_truth\" predictions_dir = f\"{data_dir}/dset-s2/predictions\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>test_image_path = (\n    f\"{data_dir}/dset-s2/val_scene/S2A_L2A_20190318_N0211_R061_6Bands_S2.tif\"\n)\nground_truth_path = (\n    f\"{data_dir}/dset-s2/val_truth/S2A_L2A_20190318_N0211_R061_S2_Truth.tif\"\n)\nprediction_path = (\n    f\"{data_dir}/dset-s2/predictions/S2A_L2A_20190318_N0211_R061_6Bands_S2_mask.tif\"\n)\nsave_path = f\"{data_dir}/dset-s2/S2A_L2A_20190318_N0211_R061_6Bands_S2_comparison.png\"\n\nfig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=save_path,\n    show_plot=True,\n    indexes=[5, 4, 3],\n    divider=5000,\n)\n</pre> test_image_path = (     f\"{data_dir}/dset-s2/val_scene/S2A_L2A_20190318_N0211_R061_6Bands_S2.tif\" ) ground_truth_path = (     f\"{data_dir}/dset-s2/val_truth/S2A_L2A_20190318_N0211_R061_S2_Truth.tif\" ) prediction_path = (     f\"{data_dir}/dset-s2/predictions/S2A_L2A_20190318_N0211_R061_6Bands_S2_mask.tif\" ) save_path = f\"{data_dir}/dset-s2/S2A_L2A_20190318_N0211_R061_6Bands_S2_comparison.png\"  fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=save_path,     show_plot=True,     indexes=[5, 4, 3],     divider=5000, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap(center=[-16.3043, 128.7412], zoom=10)\nm.add_basemap(\"Esri.WorldImagery\")\nm.add_stac_gui()\nm\n</pre> m = geoai.LeafMap(center=[-16.3043, 128.7412], zoom=10) m.add_basemap(\"Esri.WorldImagery\") m.add_stac_gui() m In\u00a0[\u00a0]: Copied! <pre># m.stac_gdf\n</pre> # m.stac_gdf In\u00a0[\u00a0]: Copied! <pre># m.stac_item\n</pre> # m.stac_item In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>url = \"https://earth-search.aws.element84.com/v1/\"\ncollection = \"sentinel-2-l2a\"\ntime_range = \"2025-01-01/2025-07-20\"\nbbox = [128.6735, -16.2466, 128.9577, -16.0962]\n</pre> url = \"https://earth-search.aws.element84.com/v1/\" collection = \"sentinel-2-l2a\" time_range = \"2025-01-01/2025-07-20\" bbox = [128.6735, -16.2466, 128.9577, -16.0962] In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=10,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_collection=True,\n)\nsearch\n</pre> search = leafmap.stac_search(     url=url,     max_items=10,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_collection=True, ) search In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=10,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_gdf=True,\n)\nsearch.head()\n</pre> search = leafmap.stac_search(     url=url,     max_items=10,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_gdf=True, ) search.head() In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=1,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_assets=True,\n)\nsearch\n</pre> search = leafmap.stac_search(     url=url,     max_items=1,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_assets=True, ) search In\u00a0[\u00a0]: Copied! <pre>bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\nassets = list(search.values())[0]\nlinks = [assets[band] for band in bands]\nfor link in links:\n    print(link)\n</pre> bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"] assets = list(search.values())[0] links = [assets[band] for band in bands] for link in links:     print(link) In\u00a0[\u00a0]: Copied! <pre>out_dir = \"s2\"\nleafmap.download_files(links, out_dir)\n</pre> out_dir = \"s2\" leafmap.download_files(links, out_dir) In\u00a0[\u00a0]: Copied! <pre># !apt-get install -y gdal-bin\n</pre> # !apt-get install -y gdal-bin In\u00a0[\u00a0]: Copied! <pre>s2_path = \"s2.tif\"\ngeoai.stack_bands(input_files=out_dir, output_file=s2_path)\n</pre> s2_path = \"s2.tif\" geoai.stack_bands(input_files=out_dir, output_file=s2_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(s2_path, indexes=[4, 3, 2])\n</pre> geoai.view_raster(s2_path, indexes=[4, 3, 2]) In\u00a0[\u00a0]: Copied! <pre>s2_mask = \"s2_mask.tif\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> s2_mask = \"s2_mask.tif\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=s2_path,\n    output_path=s2_mask,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n)\n</pre> geoai.semantic_segmentation(     input_path=s2_path,     output_path=s2_mask,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    s2_mask, no_data=0, colormap=\"viridis\", basemap=s2_path, backend=\"ipyleaflet\"\n)\n</pre> geoai.view_raster(     s2_mask, no_data=0, colormap=\"viridis\", basemap=s2_path, backend=\"ipyleaflet\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\"\ntrain_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\" train_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_masks_path = geoai.download_file(train_masks_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_masks_path = geoai.download_file(train_masks_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(train_raster_path, show_preview=False)\n</pre> geoai.print_raster_info(train_raster_path, show_preview=False) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_masks_url, nodata=0, basemap=train_raster_url)\n</pre> geoai.view_raster(train_masks_url, nodata=0, basemap=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"naip\"\n</pre> out_folder = \"naip\" In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_masks_path,\n    tile_size=512,\n    stride=128,\n    buffer_radius=0,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_masks_path,     tile_size=512,     stride=128,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=8,\n    num_epochs=5,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=4,     pretrained=True,     batch_size=8,     num_epochs=5,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_water_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_water_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    test_raster_path,\n    masks_path,\n    model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    window_size=512,\n    overlap=128,\n    confidence_threshold=0.3,\n    batch_size=32,\n    num_channels=4,\n)\n</pre> geoai.semantic_segmentation(     test_raster_path,     masks_path,     model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     window_size=512,     overlap=128,     confidence_threshold=0.3,     batch_size=32,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_water_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\n</pre> output_path = \"naip_water_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>len(gdf)\n</pre> len(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf[\"elongation\"].hist()\n</pre> gdf[\"elongation\"].hist() In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10]\n</pre> gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10] In\u00a0[\u00a0]: Copied! <pre>len(gdf_filtered)\n</pre> len(gdf_filtered) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) <p></p>"},{"location":"workshops/AWS_2025/#automating-surface-water-mapping-with-ai-tools","title":"Automating surface water mapping with AI tools\u00b6","text":"<p>This notebook is developed for the Australian Water School premium webinar on automating surface water mapping with AI tools on July 23, 2025. To register for the webinar, please visit https://awschool.com.au/training/ai-tools-for-mapping.</p> <p>More resources:</p> <ul> <li>GitHub: https://github.com/opengeos/geoai</li> <li>Documentation: https://opengeoai.org</li> <li>Notebook: https://opengeoai.org/workshops/GeoAI_Workshop_2025</li> <li>Web App: https://huggingface.co/spaces/giswqs/surface-water-app</li> <li>Web App Demo: https://youtu.be/LKySb6OYU7M</li> <li>YouTube Tutorials: https://tinyurl.com/GeoAI-Tutorials</li> </ul>"},{"location":"workshops/AWS_2025/#use-colab-gpu","title":"Use Colab GPU\u00b6","text":"<p>To use GPU, please click the \"Runtime\" menu and select \"Change runtime type\". Then select \"T4 GPU\" from the dropdown menu.</p>"},{"location":"workshops/AWS_2025/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following cell to install the package. It may take a few minutes to install the package. Please be patient.</p>"},{"location":"workshops/AWS_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Next, we need to install the <code>geoai</code> package.</p>"},{"location":"workshops/AWS_2025/#surface-water-mapping-with-non-georeferenced-satellite-imagery","title":"Surface water mapping with non-georeferenced satellite imagery\u00b6","text":"<p>In the first part of this notebook, we will demonstrate how to map surface water using non-georeferenced satellite imagery in jpg/png format.</p>"},{"location":"workshops/AWS_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the waterbody dataset from Kaggle. You will need to create an account and download the dataset. I have already downloaded the dataset and saved a copy on Hugging Face. Let's download the dataset:</p>"},{"location":"workshops/AWS_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Before diving into the training, let's understand the basic concepts of deep learning architectures and encoders.</p> <p>Deep learning architecture is like the blueprint for a neural network \u2014 it defines how the network is built and how data flows through it. It includes layers of nodes (neurons) that process the data step by step to learn patterns, like identifying cats in pictures or translating languages.</p> <p>There are many types of architectures:</p> <ul> <li>Feedforward Neural Networks (simple, goes one way)</li> <li>Convolutional Neural Networks (CNNs) (used for images)</li> <li>Recurrent Neural Networks (RNNs) (used for sequences like speech)</li> <li>Transformers (used for language tasks, like ChatGPT)</li> </ul> <p>Each one is designed for specific types of problems.</p> <p>An encoder is a part of the neural network that takes the input (like a sentence or image) and compresses it into a smaller, meaningful form called a feature representation or embedding. It captures the important information while throwing away the noise.</p> <p>For example, if you feed the sentence \u201cI love pizza\u201d into the encoder, it turns that sentence into a set of numbers that still represent its meaning but are easier for the computer to understand and work with.</p> <p>Encoders are used in models like:</p> <ul> <li>Autoencoders (for compressing and reconstructing data)</li> <li>Transformer Encoders (like BERT, for understanding language)</li> <li>Encoder-Decoder models (like translation systems)</li> </ul>"},{"location":"workshops/AWS_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/AWS_2025/#run-inference-on-a-single-image","title":"Run inference on a single image\u00b6","text":"<p>You can run inference on a new image using the <code>semantic_segmentation</code> function. I don't have a new image to test on, so I'll use one of the training images. In reality, you would use your own images not used in training.</p>"},{"location":"workshops/AWS_2025/#run-inference-on-multiple-images","title":"Run inference on multiple images\u00b6","text":"<p>First, let's download the test images and masks.</p>"},{"location":"workshops/AWS_2025/#surface-water-mapping-with-sentinel-2-imagery","title":"Surface water mapping with Sentinel-2 imagery\u00b6","text":"<p>In the second part of this notebook, we will demonstrate how to map surface water using Sentinel-2 imagery with six spectral bands, including blue, green, red, near-infrared, and short-wave infrared bands.</p>"},{"location":"workshops/AWS_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the Earth Surface Water Dataset from Zenodo. Credits to the author (Xin Luo) of the dataset</p>"},{"location":"workshops/AWS_2025/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"workshops/AWS_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using the new <code>train_segmentation_model</code> function. Let's train the module using U-Net with ResNet34 encoder:</p>"},{"location":"workshops/AWS_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/AWS_2025/#run-inference","title":"Run inference\u00b6","text":""},{"location":"workshops/AWS_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/AWS_2025/#download-sentinel-2-imagery","title":"Download Sentinel-2 imagery\u00b6","text":""},{"location":"workshops/AWS_2025/#stack-image-bands","title":"Stack image bands\u00b6","text":"<p>Uncomment the following cell to install GDAL on Colab.</p>"},{"location":"workshops/AWS_2025/#run-inference-on-a-sentinel-2-image","title":"Run inference on a Sentinel-2 image\u00b6","text":""},{"location":"workshops/AWS_2025/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"workshops/AWS_2025/#surface-water-mapping-with-aerial-imagery","title":"Surface water mapping with aerial imagery\u00b6","text":"<p>In the last part of this notebook, we will demonstrate how to map surface water using aerial imagery from the USDA National Agriculture Imagery Program (NAIP).</p>"},{"location":"workshops/AWS_2025/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/AWS_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"workshops/AWS_2025/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"workshops/AWS_2025/#train-segmentation-model","title":"Train segmentation model\u00b6","text":""},{"location":"workshops/AWS_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"workshops/AWS_2025/#run-inference","title":"Run inference\u00b6","text":""},{"location":"workshops/AWS_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"workshops/AWS_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/CANVAS_2025/","title":"CANVAS 2025","text":"<p>Open Source Pipeline to Integrate Drone and Satellite Geospatial Data Products for Agricultural Applications</p> <p>This notebook is designed for workshop presented at the CANAVS 2025 Conference on November 10, 2025.</p> <ul> <li>Registration: https://scisoc.confex.com/scisoc/2025am/meetingapp.cgi/Session/27796</li> <li>Notebook: https://opengeoai.org/workshops/CANVAS_2025</li> <li>Leafmap: https://leafmap.org</li> <li>Samgeo: https://samgeo.gishub.org</li> <li>GeoAI: https://opengeoai.org</li> <li>Data to Science (D2S): https://ps2.d2s.org</li> <li>D2S Python API: https://py.d2s.org</li> </ul> In\u00a0[\u00a0]: Copied! <pre>%pip install -U \"leafmap[raster]\" \"segment-geospatial[samgeo2]\" geoai-py\n</pre> %pip install -U \"leafmap[raster]\" \"segment-geospatial[samgeo2]\" geoai-py In\u00a0[\u00a0]: Copied! <pre>import leafmap\n</pre> import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\n</pre> m = leafmap.Map() <p>To display it in a Jupyter notebook, simply ask for the object representation:</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m <p>To customize the map, you can specify various keyword arguments, such as <code>center</code> ([lat, lon]), <code>zoom</code>, <code>width</code>, and <code>height</code>. The default <code>width</code> is <code>100%</code>, which takes up the entire cell width of the Jupyter notebook. The <code>height</code> argument accepts a number or a string. If a number is provided, it represents the height of the map in pixels. If a string is provided, the string must be in the format of a number followed by <code>px</code>, e.g., <code>600px</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40, -100], zoom=4, height=\"600px\")\nm\n</pre> m = leafmap.Map(center=[40, -100], zoom=4, height=\"600px\") m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(basemap=\"Esri.WorldImagery\")\nm\n</pre> m = leafmap.Map(basemap=\"Esri.WorldImagery\") m <p>You can add as many basemaps as you like to the map. For example, the following code adds the <code>OpenTopoMap</code> basemap to the map above:</p> In\u00a0[\u00a0]: Copied! <pre>m.add_basemap(\"OpenTopoMap\")\n</pre> m.add_basemap(\"OpenTopoMap\") <p>You can also add an XYZ tile layer to the map.</p> In\u00a0[\u00a0]: Copied! <pre>basemap_url = \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\"\nm.add_tile_layer(basemap_url, name=\"Hybrid\", attribution=\"Google\")\n</pre> basemap_url = \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\" m.add_tile_layer(basemap_url, name=\"Hybrid\", attribution=\"Google\") <p>You can also change basemaps interactively using the basemap GUI.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap_gui()\nm\n</pre> m = leafmap.Map() m.add_basemap_gui() m In\u00a0[\u00a0]: Copied! <pre>import os\nfrom datetime import date\nfrom d2spy.workspace import Workspace\n</pre> import os from datetime import date from d2spy.workspace import Workspace In\u00a0[\u00a0]: Copied! <pre>if os.environ.get(\"D2S_EMAIL\") is None:\n    os.environ[\"D2S_EMAIL\"] = \"qwu18@utk.edu\"  # Replace with your email address\n</pre> if os.environ.get(\"D2S_EMAIL\") is None:     os.environ[\"D2S_EMAIL\"] = \"qwu18@utk.edu\"  # Replace with your email address In\u00a0[\u00a0]: Copied! <pre># Connect to D2S platform and authenticate\nd2s_url = \"https://ps2.d2s.org\"\nworkspace = Workspace.connect(d2s_url)  # This will prompt for password\napi_key = workspace.api_key  # Get API key for data access\n</pre> # Connect to D2S platform and authenticate d2s_url = \"https://ps2.d2s.org\" workspace = Workspace.connect(d2s_url)  # This will prompt for password api_key = workspace.api_key  # Get API key for data access In\u00a0[\u00a0]: Copied! <pre>os.environ[\"D2S_API_KEY\"] = api_key\nos.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\"\n</pre> os.environ[\"D2S_API_KEY\"] = api_key os.environ[\"TITILER_ENDPOINT\"] = \"https://tt.d2s.org\" In\u00a0[\u00a0]: Copied! <pre># Get list of all your projects\nprojects = workspace.get_projects()\nfor project in projects:\n    print(project)\n</pre> # Get list of all your projects projects = workspace.get_projects() for project in projects:     print(project) <p>The <code>projects</code> variable is a <code>ProjectCollection</code>. The collection can be filtered by either the project descriptions or titles using the methods <code>filter_by_title</code> or <code>filter_by_name</code>.</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of only projects with the keyword \"Citrus Orchard\" in the title\nfiltered_projects = projects.filter_by_title(\"Citrus Orchard\")\nprint(filtered_projects)\n</pre> # Example of creating new collection of only projects with the keyword \"Citrus Orchard\" in the title filtered_projects = projects.filter_by_title(\"Citrus Orchard\") print(filtered_projects) <p>Now you can choose a specific project to work with. In this case, the filtered projects returned only one project, so we will use that project.</p> In\u00a0[\u00a0]: Copied! <pre>project = filtered_projects[0]\n</pre> project = filtered_projects[0] <p><code>get_project_boundary</code> method of the <code>Project</code> class will retrieve a GeoJSON object of the project boundary.</p> In\u00a0[\u00a0]: Copied! <pre># Get project boundary as Python dictionary in GeoJSON structure\nproject_boundary = project.get_project_boundary()\nproject_boundary\n</pre> # Get project boundary as Python dictionary in GeoJSON structure project_boundary = project.get_project_boundary() project_boundary In\u00a0[\u00a0]: Copied! <pre># Get list of all flights for a project\nflights = project.get_flights()\n# Print first flight object (if one exists)\nfor flight in flights:\n    print(flight)\n</pre> # Get list of all flights for a project flights = project.get_flights() # Print first flight object (if one exists) for flight in flights:     print(flight) In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of only flights from June 2022\nfiltered_flights = flights.filter_by_date(\n    start_date=date(2022, 6, 1), end_date=date(2022, 7, 1)\n)\nfor flight in filtered_flights:\n    print(flight)\n</pre> # Example of creating new collection of only flights from June 2022 filtered_flights = flights.filter_by_date(     start_date=date(2022, 6, 1), end_date=date(2022, 7, 1) ) for flight in filtered_flights:     print(flight) <p>Now, we can choose a flight from the filtered flight. Let's choose the flight on June 9, 2022.</p> In\u00a0[\u00a0]: Copied! <pre>flight = filtered_flights[0]\nflight\n</pre> flight = filtered_flights[0] flight In\u00a0[\u00a0]: Copied! <pre># Get list of data products from a flight\ndata_products = flight.get_data_products()\n\nfor data_product in data_products:\n    print(data_product)\n</pre> # Get list of data products from a flight data_products = flight.get_data_products()  for data_product in data_products:     print(data_product) <p>The <code>data_products</code> variable is a <code>DataProductCollection</code>. The collection can be filtered by data type using the method <code>filter_by_data_type</code>. This method will return all data products that match the requested data type.</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"ortho\" data type\northo_data_products = data_products.filter_by_data_type(\"ortho\")\nprint(ortho_data_products)\n</pre> # Example of creating new collection of data products with the \"ortho\" data type ortho_data_products = data_products.filter_by_data_type(\"ortho\") print(ortho_data_products) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_basemap(\"HYBRID\", show=False)\northo_data = ortho_data_products[0]\northo_url_202206 = ortho_data.url\northo_url_202206 = leafmap.d2s_tile(ortho_url_202206)\nm.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\")\nm\n</pre> m = leafmap.Map() m.add_basemap(\"HYBRID\", show=False) ortho_data = ortho_data_products[0] ortho_url_202206 = ortho_data.url ortho_url_202206 = leafmap.d2s_tile(ortho_url_202206) m.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\") m In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"dsm\" data type\ndsm_data_products = data_products.filter_by_data_type(\"dsm\")\nprint(dsm_data_products)\n</pre> # Example of creating new collection of data products with the \"dsm\" data type dsm_data_products = data_products.filter_by_data_type(\"dsm\") print(dsm_data_products) In\u00a0[\u00a0]: Copied! <pre>dsm_data = dsm_data_products[0]\ndsm_url_202206 = dsm_data.url\ndsm_url_202206 = leafmap.d2s_tile(dsm_url_202206)\nm.add_cog_layer(dsm_url_202206, colormap_name=\"terrain\", name=\"DSM 202206\")\n</pre> dsm_data = dsm_data_products[0] dsm_url_202206 = dsm_data.url dsm_url_202206 = leafmap.d2s_tile(dsm_url_202206) m.add_cog_layer(dsm_url_202206, colormap_name=\"terrain\", name=\"DSM 202206\") In\u00a0[\u00a0]: Copied! <pre>leafmap.cog_stats(dsm_url_202206)\n</pre> leafmap.cog_stats(dsm_url_202206) <p>Add a colorbar to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_colormap(cmap=\"terrain\", vmin=3, vmax=33, label=\"Elevation (m)\")\nm\n</pre> m.add_colormap(cmap=\"terrain\", vmin=3, vmax=33, label=\"Elevation (m)\") m In\u00a0[\u00a0]: Copied! <pre># Example of creating new collection of data products with the \"chm\" data type\nchm_data_products = data_products.filter_by_data_type(\"chm\")\nprint(chm_data_products)\n</pre> # Example of creating new collection of data products with the \"chm\" data type chm_data_products = data_products.filter_by_data_type(\"chm\") print(chm_data_products) In\u00a0[\u00a0]: Copied! <pre>chm_data = chm_data_products[0]\nchm_url_202206 = chm_data.url\nchm_url_202206 = leafmap.d2s_tile(chm_url_202206)\nm.add_cog_layer(chm_url_202206, colormap_name=\"jet\", name=\"CHM 202206\")\n</pre> chm_data = chm_data_products[0] chm_url_202206 = chm_data.url chm_url_202206 = leafmap.d2s_tile(chm_url_202206) m.add_cog_layer(chm_url_202206, colormap_name=\"jet\", name=\"CHM 202206\") In\u00a0[\u00a0]: Copied! <pre>leafmap.cog_stats(chm_url_202206)\n</pre> leafmap.cog_stats(chm_url_202206) In\u00a0[\u00a0]: Copied! <pre>m.add_colormap(cmap=\"jet\", vmin=0, vmax=13, label=\"Elevation (m)\")\nm\n</pre> m.add_colormap(cmap=\"jet\", vmin=0, vmax=13, label=\"Elevation (m)\") m <p>Add the project boundary to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_geojson(project_boundary, layer_name=\"Project Boundary\")\n</pre> m.add_geojson(project_boundary, layer_name=\"Project Boundary\") <p>Add tree boundaries to the map.</p> In\u00a0[\u00a0]: Copied! <pre>map_layers = project.get_map_layers()\ntree_boundaries = map_layers[0]\n</pre> map_layers = project.get_map_layers() tree_boundaries = map_layers[0] In\u00a0[\u00a0]: Copied! <pre>gdf = leafmap.geojson_to_gdf(tree_boundaries)\ngdf.to_file(\"tree_boundaries.geojson\")\n</pre> gdf = leafmap.geojson_to_gdf(tree_boundaries) gdf.to_file(\"tree_boundaries.geojson\") In\u00a0[\u00a0]: Copied! <pre>m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\")\n</pre> m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\") In\u00a0[\u00a0]: Copied! <pre>filtered_flights = flights.filter_by_date(\n    start_date=date(2022, 12, 1), end_date=date(2022, 12, 31)\n)\nfor flight in filtered_flights:\n    print(flight)\n</pre> filtered_flights = flights.filter_by_date(     start_date=date(2022, 12, 1), end_date=date(2022, 12, 31) ) for flight in filtered_flights:     print(flight) In\u00a0[\u00a0]: Copied! <pre>flight_202212 = filtered_flights[0]\ndata_products = flight_202212.get_data_products()\northo_data_products = data_products.filter_by_data_type(\"ortho\")\northo_data = ortho_data_products[0]\northo_url_202212 = ortho_data.url\northo_url_202212 = leafmap.d2s_tile(ortho_url_202212)\n</pre> flight_202212 = filtered_flights[0] data_products = flight_202212.get_data_products() ortho_data_products = data_products.filter_by_data_type(\"ortho\") ortho_data = ortho_data_products[0] ortho_url_202212 = ortho_data.url ortho_url_202212 = leafmap.d2s_tile(ortho_url_202212) In\u00a0[\u00a0]: Copied! <pre>from ipyleaflet import TileLayer\n\nm = leafmap.Map()\nleft_layer = TileLayer(\n    url=leafmap.cog_tile(ortho_url_202206), max_zoom=30, name=\"2022-06 Ortho\"\n)\nright_layer = TileLayer(\n    url=leafmap.cog_tile(ortho_url_202212), max_zoom=30, name=\"2022-12 Ortho\"\n)\nm.split_map(left_layer, right_layer, left_label=\"2022-06\", right_label=\"2022-12\")\nm.set_center(-97.955281, 26.165595, 18)\nm\n</pre> from ipyleaflet import TileLayer  m = leafmap.Map() left_layer = TileLayer(     url=leafmap.cog_tile(ortho_url_202206), max_zoom=30, name=\"2022-06 Ortho\" ) right_layer = TileLayer(     url=leafmap.cog_tile(ortho_url_202212), max_zoom=30, name=\"2022-12 Ortho\" ) m.split_map(left_layer, right_layer, left_label=\"2022-06\", right_label=\"2022-12\") m.set_center(-97.955281, 26.165595, 18) m In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\")\nm\n</pre> m = leafmap.Map() m.add_cog_layer(ortho_url_202206, name=\"Ortho Imagery 202206\") m In\u00a0[\u00a0]: Copied! <pre># Define bounding box for the area of interest\n# Format: [min_lon, min_lat, max_lon, max_lat]\nif m.user_roi is not None:\n    bbox = m.user_roi_bounds()  # Use drawn ROI if available\nelse:\n    bbox = [-97.956252, 26.165315, -97.954992, 26.165883]  # Default ROI\n</pre> # Define bounding box for the area of interest # Format: [min_lon, min_lat, max_lon, max_lat] if m.user_roi is not None:     bbox = m.user_roi_bounds()  # Use drawn ROI if available else:     bbox = [-97.956252, 26.165315, -97.954992, 26.165883]  # Default ROI In\u00a0[\u00a0]: Copied! <pre>ortho_image_202206 = \"ortho_image_202206.tif\"\nif not os.path.exists(ortho_image_202206):\n    leafmap.download_file(ortho_url_202206, output=ortho_image_202206)\n</pre> ortho_image_202206 = \"ortho_image_202206.tif\" if not os.path.exists(ortho_image_202206):     leafmap.download_file(ortho_url_202206, output=ortho_image_202206) In\u00a0[\u00a0]: Copied! <pre>ortho_image_202212 = \"ortho_image_202212.tif\"\nif not os.path.exists(ortho_image_202212):\n    leafmap.download_file(ortho_url_202212, output=ortho_image_202212)\n</pre> ortho_image_202212 = \"ortho_image_202212.tif\" if not os.path.exists(ortho_image_202212):     leafmap.download_file(ortho_url_202212, output=ortho_image_202212) <p>Draw an area of interest (AOI) on the map. If an AOI is not provided, a default AOI will be used.</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-97.956252, 26.165315, -97.954992, 26.165883]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-97.956252, 26.165315, -97.954992, 26.165883] In\u00a0[\u00a0]: Copied! <pre>gdf = leafmap.bbox_to_gdf(bbox)\nm.add_gdf(gdf, layer_name=\"AOI\", info_mode=None)\n</pre> gdf = leafmap.bbox_to_gdf(bbox) m.add_gdf(gdf, layer_name=\"AOI\", info_mode=None) In\u00a0[\u00a0]: Copied! <pre>ortho_image_10cm = \"ortho_image_202206_10cm.tif\"\nchm_image = \"chm_202206.tif\"\n</pre> ortho_image_10cm = \"ortho_image_202206_10cm.tif\" chm_image = \"chm_202206.tif\" In\u00a0[\u00a0]: Copied! <pre>clipped_ortho_data_10cm = leafmap.clip_raster(\n    ortho_image_202206,\n    geometry=bbox,\n    geom_crs=\"EPSG:4326\",\n    bands=[1, 2, 3],\n    resolution=0.1,\n    output=ortho_image_10cm,\n)\n</pre> clipped_ortho_data_10cm = leafmap.clip_raster(     ortho_image_202206,     geometry=bbox,     geom_crs=\"EPSG:4326\",     bands=[1, 2, 3],     resolution=0.1,     output=ortho_image_10cm, ) In\u00a0[\u00a0]: Copied! <pre>clipped_chm_data = leafmap.clip_raster(\n    chm_url_202206,\n    geometry=bbox,\n    geom_crs=\"EPSG:4326\",\n    match_raster=clipped_ortho_data_10cm,\n    output=chm_image,\n)\n</pre> clipped_chm_data = leafmap.clip_raster(     chm_url_202206,     geometry=bbox,     geom_crs=\"EPSG:4326\",     match_raster=clipped_ortho_data_10cm,     output=chm_image, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(ortho_image_10cm, layer_name=\"Ortho Image 202206\")\nm.add_raster(chm_image, colormap=\"terrain\", nodata=0, layer_name=\"CHM 202206\")\nm.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\")\nm\n</pre> m = leafmap.Map() m.add_raster(ortho_image_10cm, layer_name=\"Ortho Image 202206\") m.add_raster(chm_image, colormap=\"terrain\", nodata=0, layer_name=\"CHM 202206\") m.add_geojson(tree_boundaries, layer_name=\"Tree Boundaries\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo import SamGeo, SamGeo2\n</pre> from samgeo import SamGeo, SamGeo2 In\u00a0[\u00a0]: Copied! <pre># Initialize SAM2 model for automatic segmentation\nsam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",  # Use the large hierarchical SAM2 model\n    automatic=True,  # Enable automatic mask generation mode\n    stability_score_thresh=0.9,  # Higher = fewer, more stable masks\n    stability_score_offset=0.7,  # Adjusts stability calculation\n)\n</pre> # Initialize SAM2 model for automatic segmentation sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",  # Use the large hierarchical SAM2 model     automatic=True,  # Enable automatic mask generation mode     stability_score_thresh=0.9,  # Higher = fewer, more stable masks     stability_score_offset=0.7,  # Adjusts stability calculation ) In\u00a0[\u00a0]: Copied! <pre># Generate masks for all objects detected in the image\nsam2.generate(ortho_image_10cm)\n</pre> # Generate masks for all objects detected in the image sam2.generate(ortho_image_10cm) In\u00a0[\u00a0]: Copied! <pre># Save the generated masks as a GeoTIFF file\n# Each unique object gets a unique integer ID\nsam2.save_masks(output=\"masks.tif\")\n</pre> # Save the generated masks as a GeoTIFF file # Each unique object gets a unique integer ID sam2.save_masks(output=\"masks.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"binary_r\")\n</pre> sam2.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations.tif\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    ortho_image_10cm,\n    \"annotations.tif\",\n    label1=\"Drone Imagery\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     ortho_image_10cm,     \"annotations.tif\",     label1=\"Drone Imagery\",     label2=\"Image Segmentation\", ) <p>Add segmentation result to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(ortho_image_10cm, layer_name=\"Ortho Imagery 202206\")\nm.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7)\nm\n</pre> m = leafmap.Map() m.add_raster(ortho_image_10cm, layer_name=\"Ortho Imagery 202206\") m.add_raster(\"masks.tif\", colormap=\"jet\", layer_name=\"Masks\", nodata=0, opacity=0.7) m <p>Convert the object masks to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam2.raster_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.gpkg\", layer_name=\"Objects\")\n</pre> m.add_vector(\"masks.gpkg\", layer_name=\"Objects\") In\u00a0[\u00a0]: Copied! <pre>sam2 = SamGeo2(\n    model_id=\"sam2-hiera-large\",\n    apply_postprocessing=False,\n    points_per_side=64,\n    points_per_batch=128,\n    pred_iou_thresh=0.7,\n    stability_score_thresh=0.92,\n    stability_score_offset=0.7,\n    crop_n_layers=1,\n    box_nms_thresh=0.7,\n    crop_n_points_downscale_factor=2,\n    min_mask_region_area=25,\n    use_m2m=True,\n)\n</pre> sam2 = SamGeo2(     model_id=\"sam2-hiera-large\",     apply_postprocessing=False,     points_per_side=64,     points_per_batch=128,     pred_iou_thresh=0.7,     stability_score_thresh=0.92,     stability_score_offset=0.7,     crop_n_layers=1,     box_nms_thresh=0.7,     crop_n_points_downscale_factor=2,     min_mask_region_area=25,     use_m2m=True, ) In\u00a0[\u00a0]: Copied! <pre>sam2.generate(ortho_image_10cm, output=\"masks2.tif\")\n</pre> sam2.generate(ortho_image_10cm, output=\"masks2.tif\") In\u00a0[\u00a0]: Copied! <pre>sam2.show_masks(cmap=\"jet\")\n</pre> sam2.show_masks(cmap=\"jet\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\")\n</pre> sam2.show_anns(axis=\"off\", alpha=0.7, output=\"annotations2.tif\") <p></p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    ortho_image_10cm,\n    \"annotations2.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     ortho_image_10cm,     \"annotations2.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Remove small objects.</p> In\u00a0[\u00a0]: Copied! <pre>da, gdf = sam2.region_groups(\n    \"masks2.tif\",\n    connectivity=1,\n    min_size=10,\n    max_size=2000,\n    intensity_image=\"chm_202206.tif\",\n    out_image=\"objects.tif\",\n    out_csv=\"objects.csv\",\n    out_vector=\"objects.gpkg\",\n)\n</pre> da, gdf = sam2.region_groups(     \"masks2.tif\",     connectivity=1,     min_size=10,     max_size=2000,     intensity_image=\"chm_202206.tif\",     out_image=\"objects.tif\",     out_csv=\"objects.csv\",     out_vector=\"objects.gpkg\", ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(ortho_image_10cm, layer_name=\"Ortho Imagery 202206\")\nm.add_vector(\"objects.gpkg\", layer_name=\"Objects\")\nm\n</pre> m = leafmap.Map() m.add_raster(ortho_image_10cm, layer_name=\"Ortho Imagery 202206\") m.add_vector(\"objects.gpkg\", layer_name=\"Objects\") m In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>ortho_image_10cm = \"ortho_image_202206_10cm.tif\"\ngeojson = \"tree_boundaries.geojson\"\n</pre> ortho_image_10cm = \"ortho_image_202206_10cm.tif\" geojson = \"tree_boundaries.geojson\" In\u00a0[\u00a0]: Copied! <pre>gdf = leafmap.geojson_to_gdf(geojson)\ngdf.head()\n</pre> gdf = leafmap.geojson_to_gdf(geojson) gdf.head() In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(ortho_image_10cm, layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(ortho_image_10cm, layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.set_image(ortho_image_10cm)\n</pre> sam.set_image(ortho_image_10cm) In\u00a0[\u00a0]: Copied! <pre>sam.predict(\n    boxes=geojson, point_crs=\"EPSG:4326\", output=\"tree_masks.tif\", dtype=\"uint16\"\n)\n</pre> sam.predict(     boxes=geojson, point_crs=\"EPSG:4326\", output=\"tree_masks.tif\", dtype=\"uint16\" ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\n    \"tree_masks.tif\", cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Tree masks\"\n)\nm\n</pre> m.add_raster(     \"tree_masks.tif\", cmap=\"jet\", nodata=0, opacity=0.5, layer_name=\"Tree masks\" ) m <p></p> In\u00a0[\u00a0]: Copied! <pre>import geoai\nimport leafmap\n</pre> import geoai import leafmap In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"ortho_image_202206.tif\", layer_name=\"Ortho Imagery 202206\")\nm.add_vector(\"tree_boundaries.geojson\", layer_name=\"Tree Boundaries\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"ortho_image_202206.tif\", layer_name=\"Ortho Imagery 202206\") m.add_vector(\"tree_boundaries.geojson\", layer_name=\"Tree Boundaries\") m In\u00a0[\u00a0]: Copied! <pre># Define separate regions for training and testing\n# Training: Use June 2022 imagery with tree boundaries\n# Testing: Use December 2022 imagery to test model generalization\nbbox = [-97.956252, 26.165315, -97.954992, 26.165883]\n</pre> # Define separate regions for training and testing # Training: Use June 2022 imagery with tree boundaries # Testing: Use December 2022 imagery to test model generalization bbox = [-97.956252, 26.165315, -97.954992, 26.165883] In\u00a0[\u00a0]: Copied! <pre>ortho_image_202206 = \"ortho_image_202206.tif\"\northo_image_202212 = \"ortho_image_202212.tif\"\ntrain_raster_path = \"ortho_image_202206_train.tif\"\ntest_raster_path = \"ortho_image_202212_test.tif\"\ntrain_vector_path = \"tree_boundaries.geojson\"\n</pre> ortho_image_202206 = \"ortho_image_202206.tif\" ortho_image_202212 = \"ortho_image_202212.tif\" train_raster_path = \"ortho_image_202206_train.tif\" test_raster_path = \"ortho_image_202212_test.tif\" train_vector_path = \"tree_boundaries.geojson\" In\u00a0[\u00a0]: Copied! <pre># Clip training imagery to the training bbox\n# Resolution: 2cm (0.02m) provides good detail for tree segmentation\ntrain_image_array = leafmap.clip_raster(\n    ortho_image_202206,\n    geometry=bbox,\n    geom_crs=\"EPSG:4326\",\n    resolution=0.02,  # 2cm resolution\n    output=train_raster_path,\n)\n</pre> # Clip training imagery to the training bbox # Resolution: 2cm (0.02m) provides good detail for tree segmentation train_image_array = leafmap.clip_raster(     ortho_image_202206,     geometry=bbox,     geom_crs=\"EPSG:4326\",     resolution=0.02,  # 2cm resolution     output=train_raster_path, ) In\u00a0[\u00a0]: Copied! <pre>test_image_array = leafmap.clip_raster(\n    ortho_image_202212,\n    geometry=bbox,\n    geom_crs=\"EPSG:4326\",\n    resolution=0.02,\n    output=test_raster_path,\n)\n</pre> test_image_array = leafmap.clip_raster(     ortho_image_202212,     geometry=bbox,     geom_crs=\"EPSG:4326\",     resolution=0.02,     output=test_raster_path, ) In\u00a0[\u00a0]: Copied! <pre># Create training tiles from the imagery and tree boundaries\n# This will generate image/label pairs for U-Net training\nout_folder = \"trees\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,  # Input imagery\n    out_folder=out_folder,  # Output directory\n    in_class_data=train_vector_path,  # Tree boundary labels\n    tile_size=512,  # Size of each tile in pixels\n    stride=256,  # Step between tiles (50% overlap)\n    buffer_radius=0,  # No buffer around geometries\n)\n</pre> # Create training tiles from the imagery and tree boundaries # This will generate image/label pairs for U-Net training out_folder = \"trees\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,  # Input imagery     out_folder=out_folder,  # Output directory     in_class_data=train_vector_path,  # Tree boundary labels     tile_size=512,  # Size of each tile in pixels     stride=256,  # Step between tiles (50% overlap)     buffer_radius=0,  # No buffer around geometries ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=4,\n    num_classes=2,  # background and trees\n    batch_size=8,\n    num_epochs=10,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=4,     num_classes=2,  # background and trees     batch_size=8,     num_epochs=10,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> <pre>=== Performance Metrics Summary ===\nIoU     - Best: 0.9454 | Final: 0.9420\nF1      - Best: 0.9718 | Final: 0.9700\nPrecision - Best: 0.9715 | Final: 0.9700\nRecall  - Best: 0.9722 | Final: 0.9702\nVal Loss - Best: 0.0746 | Final: 0.0829\n===================================\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_path = f\"{out_folder}/unet_models/best_model.pth\"\nmasks_path = \"ortho_image_202212_masks.tif\"\nprobability_path = \"probabilities.tif\"\n</pre> model_path = f\"{out_folder}/unet_models/best_model.pth\" masks_path = \"ortho_image_202212_masks.tif\" probability_path = \"probabilities.tif\" In\u00a0[\u00a0]: Copied! <pre># Run inference on the test image using the trained model\ngeoai.semantic_segmentation(\n    input_path=test_raster_path,  # Input test imagery\n    output_path=masks_path,  # Output classification mask\n    model_path=model_path,  # Path to trained model weights\n    architecture=\"unet\",  # Same architecture as training\n    encoder_name=\"resnet34\",  # Same encoder as training\n    num_channels=4,  # RGB + NIR bands\n    num_classes=2,  # Background (0) and trees (1)\n    window_size=512,  # Process in 512x512 windows\n    overlap=256,  # 50% overlap for smooth boundaries\n    batch_size=8,  # Process 8 windows at a time\n    probability_threshold=0.5,  # Confidence threshold for classification\n    probability_path=probability_path,  # Save probability scores\n)\n</pre> # Run inference on the test image using the trained model geoai.semantic_segmentation(     input_path=test_raster_path,  # Input test imagery     output_path=masks_path,  # Output classification mask     model_path=model_path,  # Path to trained model weights     architecture=\"unet\",  # Same architecture as training     encoder_name=\"resnet34\",  # Same encoder as training     num_channels=4,  # RGB + NIR bands     num_classes=2,  # Background (0) and trees (1)     window_size=512,  # Process in 512x512 windows     overlap=256,  # 50% overlap for smooth boundaries     batch_size=8,  # Process 8 windows at a time     probability_threshold=0.5,  # Confidence threshold for classification     probability_path=probability_path,  # Save probability scores ) In\u00a0[\u00a0]: Copied! <pre># Create an interactive map to visualize the results\nm = leafmap.Map()\n\n# Add the original test imagery as the base layer\nm.add_raster(test_raster_path, layer_name=\"Ortho Imagery 202212\")\n\n# Add the segmentation mask (hidden by default)\nm.add_raster(\n    masks_path,\n    colormap=\"jet\",\n    layer_name=\"Segmentation\",\n    nodata=0,\n    visible=False,  # Hidden by default, can toggle on\n)\n\n# Add the probability map showing model confidence\n# Band 2 contains the probability for class 1 (trees)\nm.add_raster(\n    probability_path,\n    indexes=[2],  # Class 1 (tree) probability\n    colormap=\"jet\",  # Color scale: blue (low) to red (high confidence)\n    opacity=0.5,  # Semi-transparent overlay\n    layer_name=\"Tree Probability\",\n    nodata=0,\n)\n\nm\n</pre> # Create an interactive map to visualize the results m = leafmap.Map()  # Add the original test imagery as the base layer m.add_raster(test_raster_path, layer_name=\"Ortho Imagery 202212\")  # Add the segmentation mask (hidden by default) m.add_raster(     masks_path,     colormap=\"jet\",     layer_name=\"Segmentation\",     nodata=0,     visible=False,  # Hidden by default, can toggle on )  # Add the probability map showing model confidence # Band 2 contains the probability for class 1 (trees) m.add_raster(     probability_path,     indexes=[2],  # Class 1 (tree) probability     colormap=\"jet\",  # Color scale: blue (low) to red (high confidence)     opacity=0.5,  # Semi-transparent overlay     layer_name=\"Tree Probability\",     nodata=0, )  m <p></p>"},{"location":"workshops/CANVAS_2025/#introduction","title":"Introduction\u00b6","text":"<p>Recent advances in drone technology have revolutionized the remote sensing community by providing means to collect fine spatial and high temporal resolutions at affordable costs. As people are gaining access to increasingly larger volumes of drone and satellite geospatial data products, there is a growing need to extract relevant information from the vast amount of freely available geospatial data. However, the lack of specialized software packages tailored for processing such data makes it challenging to develop transdisciplinary research collaboration around them. This workshop aims to bridge the gap between big geospatial data and research scientists by providing training on an open-source online platform for managing big drone data known as Data to Science.</p>"},{"location":"workshops/CANVAS_2025/#agenda","title":"Agenda\u00b6","text":"<p>The main topics to be covered in this workshop include:</p> <ul> <li>Create interactive maps using leafmap</li> <li>Visualize drone imagery from D2S</li> <li>Automated segmentation of drone imagery using SAMGeo</li> <li>Training a U-Net model for tree segmentation</li> </ul>"},{"location":"workshops/CANVAS_2025/#environment-setup","title":"Environment setup\u00b6","text":""},{"location":"workshops/CANVAS_2025/#change-runtime-type-to-gpu","title":"Change runtime type to GPU\u00b6","text":"<p>To speed up the processing, you can change the Colab runtime type to GPU. Go to the \"Runtime\" menu, select \"Change runtime type\", and choose \"T4 GPU\" from the \"Hardware accelerator\" dropdown menu.</p> <p></p>"},{"location":"workshops/CANVAS_2025/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following code to install the required packages.</p>"},{"location":"workshops/CANVAS_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Import the necessary libraries for this workshop.</p>"},{"location":"workshops/CANVAS_2025/#creating-interactive-maps","title":"Creating interactive maps\u00b6","text":"<p>Let's create an interactive map using the <code>ipyleaflet</code> plotting backend. The <code>leafmap.Map</code> class inherits the <code>ipyleaflet.Map</code> class. Therefore, you can use the same syntax to create an interactive map as you would with <code>ipyleaflet.Map</code>.</p>"},{"location":"workshops/CANVAS_2025/#adding-basemaps","title":"Adding basemaps\u00b6","text":"<p>There are several ways to add basemaps to a map. You can specify the basemap to use in the <code>basemap</code> keyword argument when creating the map. Alternatively, you can add basemap layers to the map using the <code>add_basemap</code> method. leafmap has hundreds of built-in basemaps available that can be easily added to the map with only one line of code.</p> <p>Create a map by specifying the basemap to use as follows. For example, the <code>Esri.WorldImagery</code> basemap represents the Esri world imagery basemap.</p>"},{"location":"workshops/CANVAS_2025/#visualizing-drone-imagery-from-d2s","title":"Visualizing Drone Imagery from D2S\u00b6","text":"<p>The Data to Science (D2S) platform (https://ps2.d2s.org) hosts a large collection of drone imagery that can be accessed through the D2S API (https://py.d2s.org). To visualize drone imagery from D2S, you need to sign up for a free account on the D2S platform and obtain an API key.</p>"},{"location":"workshops/CANVAS_2025/#authenticate-with-d2s-platform","title":"Authenticate with D2S Platform\u00b6","text":"<p>Connect to your D2S workspace using your credentials. The workshop provides a demo account for testing.</p>"},{"location":"workshops/CANVAS_2025/#choose-a-project-to-work-with","title":"Choose a project to work with\u00b6","text":"<p>The Workspace <code>get_projects</code> method will retrieve a collection of the projects your account can currently access on the D2S instance.</p>"},{"location":"workshops/CANVAS_2025/#get-the-project-boundary","title":"Get the project boundary\u00b6","text":""},{"location":"workshops/CANVAS_2025/#get-project-flights","title":"Get project flights\u00b6","text":"<p>The <code>Project</code> <code>get_flights</code> method will retrieve a list of flights associated with the project.</p>"},{"location":"workshops/CANVAS_2025/#filter-flights-by-date","title":"Filter flights by date\u00b6","text":"<p>The <code>flights</code> variable is a <code>FlightCollection</code>. The collection can be filtered by the acquisition date using the method <code>filter_by_date</code>. This method will return all flights with an acquisition date between the provided start and end dates.</p>"},{"location":"workshops/CANVAS_2025/#get-data-products","title":"Get data products\u00b6","text":"<p>The Flight <code>get_data_products</code> method will retrieve a list of data products associated with the flight.</p>"},{"location":"workshops/CANVAS_2025/#visualize-ortho-imagery","title":"Visualize ortho imagery\u00b6","text":"<p>Now we can grab the ortho URL to display it using leafmap.</p>"},{"location":"workshops/CANVAS_2025/#visualize-dsm","title":"Visualize DSM\u00b6","text":"<p>Similarly, you can visualize the Digital Surface Model (DSM) from D2S using the code below.</p>"},{"location":"workshops/CANVAS_2025/#visualize-chm","title":"Visualize CHM\u00b6","text":"<p>Similarly, you can visualize the Canopy Height Model (CHM) from D2S using the code below.</p>"},{"location":"workshops/CANVAS_2025/#get-another-flight","title":"Get another flight\u00b6","text":"<p>Retrieve the Ortho data product for the December 2022 flight.</p>"},{"location":"workshops/CANVAS_2025/#compare-two-ortho-images","title":"Compare two ortho images\u00b6","text":"<p>Create a split map for comparing the June 2022 and December 2022 ortho images.</p>"},{"location":"workshops/CANVAS_2025/#download-and-prepare-data-for-analysis","title":"Download and Prepare Data for Analysis\u00b6","text":"<p>In this section, we'll download a subset of the drone imagery from D2S and prepare it for machine learning model training and inference.</p>"},{"location":"workshops/CANVAS_2025/#define-area-of-interest-aoi","title":"Define Area of Interest (AOI)\u00b6","text":"<p>Draw an area of interest (AOI) on the map using the drawing tools, or use the default bounding box provided below. This AOI will be used to clip the imagery for analysis.</p>"},{"location":"workshops/CANVAS_2025/#automated-segmentation-with-sam-segment-anything-model","title":"Automated Segmentation with SAM (Segment Anything Model)\u00b6","text":"<p>SAMGeo is a Python package that applies Meta's Segment Anything Model (SAM) to geospatial data. It can automatically segment objects in drone and satellite imagery without requiring training data.</p>"},{"location":"workshops/CANVAS_2025/#initialize-sam2-model","title":"Initialize SAM2 Model\u00b6","text":"<p>First, we'll initialize the SAM2 model with parameters that control segmentation quality and detail.</p>"},{"location":"workshops/CANVAS_2025/#generate-segmentation-masks","title":"Generate Segmentation Masks\u00b6","text":"<p>Run SAM2 on the ortho imagery to automatically detect and segment all objects. This process may take a few minutes depending on image size and computational resources.</p>"},{"location":"workshops/CANVAS_2025/#compare-original-and-segmented-images","title":"Compare Original and Segmented Images\u00b6","text":"<p>Use an interactive slider to compare the original drone imagery with the SAM segmentation results.</p>"},{"location":"workshops/CANVAS_2025/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/CANVAS_2025/#using-box-prompts","title":"Using box prompts\u00b6","text":"<p>Restart the Runtime to avoid the VRAM allocation issue.</p>"},{"location":"workshops/CANVAS_2025/#training-a-deep-learning-model-for-tree-segmentation","title":"Training a Deep Learning Model for Tree Segmentation\u00b6","text":"<p>While SAM provides zero-shot segmentation, sometimes you need a custom model trained on your specific data. In this section, we'll train a U-Net model to segment tree canopies using labeled training data.</p>"},{"location":"workshops/CANVAS_2025/#why-train-a-custom-model","title":"Why Train a Custom Model?\u00b6","text":"<ul> <li>Domain-specific accuracy: Custom models learn features specific to your task (e.g., tree canopies vs. general objects)</li> <li>Consistency: Produces consistent results across multiple images from the same sensor</li> <li>Speed: Once trained, inference is faster than running SAM on large datasets</li> <li>Control: You can tune the model to prioritize precision or recall based on your needs</li> </ul>"},{"location":"workshops/CANVAS_2025/#prepare-training-and-testing-datasets","title":"Prepare Training and Testing Datasets\u00b6","text":"<p>Deep learning models require labeled training data. We'll use tree boundary vectors from D2S to create training labels, then split our data into training and testing sets.</p>"},{"location":"workshops/CANVAS_2025/#generate-training-tiles","title":"Generate Training Tiles\u00b6","text":"<p>Neural networks typically work with fixed-size inputs. We'll split the large training image into 512x512 pixel tiles with 256-pixel overlap to ensure complete coverage.</p>"},{"location":"workshops/CANVAS_2025/#train-the-u-net-model","title":"Train the U-Net Model\u00b6","text":"<p>Now we'll train a U-Net semantic segmentation model with a ResNet34 encoder (pretrained on ImageNet). This process includes:</p> <ul> <li>Architecture: U-Net with ResNet34 backbone for feature extraction</li> <li>Training data: 288 tiles split into 80% train / 20% validation</li> <li>Optimization: Adam optimizer with learning rate 0.001</li> <li>Epochs: 30 training epochs with best model checkpointing</li> <li>Metrics: IoU (Intersection over Union), F1-score, Precision, and Recall</li> </ul> <p>Training on a GPU (T4 or better) typically takes 5-10 minutes for this dataset.</p>"},{"location":"workshops/CANVAS_2025/#apply-model-to-test-imagery","title":"Apply Model to Test Imagery\u00b6","text":"<p>Use the trained model to predict tree canopies on the December 2022 test imagery. We'll save both:</p> <ul> <li>Classification mask: Final tree/background predictions</li> <li>Probability map: Per-pixel confidence scores for each class</li> </ul>"},{"location":"workshops/CANVAS_2025/#visualize-results","title":"Visualize Results\u00b6","text":"<p>Display the test imagery, predicted segmentation masks, and probability maps on an interactive map. The probability layer shows the model's confidence for each pixel (higher values = more confident).</p>"},{"location":"workshops/CANVAS_2025/#summary-and-next-steps","title":"Summary and Next Steps\u00b6","text":"<p>In this workshop, you learned how to:</p> <ol> <li>Access and visualize drone imagery from the Data to Science (D2S) platform</li> <li>Perform zero-shot segmentation using SAM2 for automatic object detection</li> <li>Train custom deep learning models (U-Net) for domain-specific segmentation tasks</li> <li>Apply trained models to new imagery and evaluate results</li> </ol>"},{"location":"workshops/CANVAS_2025/#key-takeaways","title":"Key Takeaways\u00b6","text":"<ul> <li>SAM2 is excellent for exploratory analysis and when you don't have training labels</li> <li>Custom U-Net models provide better accuracy for specific tasks when training data is available</li> <li>The GeoAI package simplifies the entire pipeline from data preparation to model deployment</li> <li>Leafmap makes it easy to visualize and interact with geospatial AI results</li> </ul>"},{"location":"workshops/CANVAS_2025/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Apply these techniques to your own drone or satellite imagery</li> <li>Experiment with different model architectures (DeepLabV3+, FPN, etc.)</li> <li>Try transfer learning from pre-trained models on similar agricultural datasets</li> <li>Explore temporal analysis by comparing multiple flights over time</li> <li>Integrate with other geospatial analysis workflows (crop health, yield prediction, etc.)</li> </ul>"},{"location":"workshops/CANVAS_2025/#resources","title":"Resources\u00b6","text":"<ul> <li>GeoAI Documentation: https://opengeoai.org</li> <li>Leafmap Documentation: https://leafmap.org</li> <li>SAMGeo Documentation: https://samgeo.gishub.org</li> <li>D2S website: https://d2s.org</li> <li>D2S Platform: https://ps2.d2s.org</li> <li>D2S STAC: https://stac.d2s.org</li> <li>D2S GitHub: https://github.com/gdslab/data-to-science</li> </ul>"},{"location":"workshops/CANVAS_2025/#questions","title":"Questions?\u00b6","text":"<p>Feel free to reach out to the GeoAI community on GitHub.</p>"},{"location":"workshops/GeoAI_Workshop_2025/","title":"GeoAI Workshop 2025","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py overturemaps\n</pre> # %pip install geoai-py overturemaps In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>collections = geoai.pc_collection_list()\ncollections\n</pre> collections = geoai.pc_collection_list() collections In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap(center=[47.653010, -117.592167], zoom=16)\nm\n</pre> m = geoai.LeafMap(center=[47.653010, -117.592167], zoom=16) m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-117.6021, 47.6502, -117.5824, 47.6559]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-117.6021, 47.6502, -117.5824, 47.6559] In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"naip\",\n    bbox=bbox,\n    time_range=\"2013-01-01/2024-12-31\",\n)\n</pre> items = geoai.pc_stac_search(     collection=\"naip\",     bbox=bbox,     time_range=\"2013-01-01/2024-12-31\", ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>items[0]\n</pre> items[0] In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0])\n</pre> geoai.view_pc_item(item=items[0]) In\u00a0[\u00a0]: Copied! <pre>downloaded = geoai.pc_stac_download(\n    items[0], output_dir=\"data\", assets=[\"image\", \"thumbnail\"]\n)\n</pre> downloaded = geoai.pc_stac_download(     items[0], output_dir=\"data\", assets=[\"image\", \"thumbnail\"] ) In\u00a0[\u00a0]: Copied! <pre>items[0]\n</pre> items[0] In\u00a0[\u00a0]: Copied! <pre>items = geoai.pc_stac_search(\n    collection=\"landsat-c2-l2\",\n    bbox=bbox,\n    time_range=\"2023-07-01/2023-07-15\",\n    query={\"eo:cloud_cover\": {\"lt\": 1}},\n    max_items=10,\n)\n</pre> items = geoai.pc_stac_search(     collection=\"landsat-c2-l2\",     bbox=bbox,     time_range=\"2023-07-01/2023-07-15\",     query={\"eo:cloud_cover\": {\"lt\": 1}},     max_items=10, ) In\u00a0[\u00a0]: Copied! <pre>items\n</pre> items In\u00a0[\u00a0]: Copied! <pre>items[0]\n</pre> items[0] In\u00a0[\u00a0]: Copied! <pre>geoai.pc_item_asset_list(items[0])\n</pre> geoai.pc_item_asset_list(items[0]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], assets=[\"red\", \"green\", \"blue\"])\n</pre> geoai.view_pc_item(item=items[0], assets=[\"red\", \"green\", \"blue\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(item=items[0], assets=[\"nir08\", \"red\", \"green\"])\n</pre> geoai.view_pc_item(item=items[0], assets=[\"nir08\", \"red\", \"green\"]) In\u00a0[\u00a0]: Copied! <pre>geoai.view_pc_item(\n    item=items[0],\n    expression=\"(nir08-red)/(nir08+red)\",\n    rescale=\"-1,1\",\n    colormap_name=\"greens\",\n    name=\"NDVI Green\",\n)\n</pre> geoai.view_pc_item(     item=items[0],     expression=\"(nir08-red)/(nir08+red)\",     rescale=\"-1,1\",     colormap_name=\"greens\",     name=\"NDVI Green\", ) In\u00a0[\u00a0]: Copied! <pre>geoai.pc_stac_download(\n    items[0], output_dir=\"data\", assets=[\"nir08\", \"red\", \"green\", \"blue\"], max_workers=1\n)\n</pre> geoai.pc_stac_download(     items[0], output_dir=\"data\", assets=[\"nir08\", \"red\", \"green\", \"blue\"], max_workers=1 ) In\u00a0[\u00a0]: Copied! <pre>buildings_gdf = geoai.get_overture_data(\n    overture_type=\"building\",\n    bbox=bbox,\n    output=\"data/buildings.geojson\",\n)\n</pre> buildings_gdf = geoai.get_overture_data(     overture_type=\"building\",     bbox=bbox,     output=\"data/buildings.geojson\", ) In\u00a0[\u00a0]: Copied! <pre>buildings_gdf.head()\n</pre> buildings_gdf.head() In\u00a0[\u00a0]: Copied! <pre>stats = geoai.extract_building_stats(buildings_gdf)\nprint(stats)\n</pre> stats = geoai.extract_building_stats(buildings_gdf) print(stats) In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(train_raster_path, figsize=(18, 10))\n</pre> geoai.print_raster_info(train_raster_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.print_vector_info(train_vector_path, figsize=(18, 10))\n</pre> geoai.print_vector_info(train_vector_path, figsize=(18, 10)) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    train_vector_path,\n    style_kwds={\"color\": \"red\", \"fillOpacity\": 0},\n    tiles=train_raster_url,\n)\n</pre> geoai.view_vector_interactive(     train_vector_path,     style_kwds={\"color\": \"red\", \"fillOpacity\": 0},     tiles=train_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=\"Satellite\")\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=\"Satellite\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"output\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"output\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_MaskRCNN_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    num_channels=4,\n    pretrained=True,\n    batch_size=4,\n    num_epochs=10,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_MaskRCNN_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     num_channels=4,     pretrained=True,     batch_size=4,     num_epochs=10,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_test_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path,\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path,     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_test_prediction.geojson\"\nbuildings_gdf = geoai.raster_to_vector(masks_path, output_path)\nbuildings_gdf\n</pre> output_path = \"naip_test_prediction.geojson\" buildings_gdf = geoai.raster_to_vector(masks_path, output_path) buildings_gdf In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(buildings_gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(buildings_gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>regularized_gdf = geoai.regularize(\n    data=buildings_gdf,\n    simplify_tolerance=2.0,\n    allow_45_degree=True,\n    diagonal_threshold_reduction=30,\n    allow_circles=True,\n    circle_threshold=0.9,\n)\n</pre> regularized_gdf = geoai.regularize(     data=buildings_gdf,     simplify_tolerance=2.0,     allow_45_degree=True,     diagonal_threshold_reduction=30,     allow_circles=True,     circle_threshold=0.9, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(regularized_gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(regularized_gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=regularized_gdf,\n    right_layer=test_raster_url,\n    left_label=\"Regularized Buildings\",\n    right_label=\"NAIP Imagery\",\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.3}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=regularized_gdf,     right_layer=test_raster_url,     left_label=\"Regularized Buildings\",     right_label=\"NAIP Imagery\",     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.3}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>m = geoai.LeafMap()\nm.add_cog_layer(test_raster_url, name=\"NAIP\")\nm.add_gdf(\n    buildings_gdf,\n    style={\"color\": \"yellow\", \"fillOpacity\": 0},\n    layer_name=\"Original\",\n    info_mode=None,\n)\nm.add_gdf(\n    regularized_gdf,\n    style={\"color\": \"red\", \"fillOpacity\": 0},\n    layer_name=\"Regularized\",\n    info_mode=None,\n)\nlegend = {\n    \"Original\": \"#ffff00\",\n    \"Regularized\": \"#ff0000\",\n}\nm.add_legend(title=\"Building Footprints\", legend_dict=legend)\nm\n</pre> m = geoai.LeafMap() m.add_cog_layer(test_raster_url, name=\"NAIP\") m.add_gdf(     buildings_gdf,     style={\"color\": \"yellow\", \"fillOpacity\": 0},     layer_name=\"Original\",     info_mode=None, ) m.add_gdf(     regularized_gdf,     style={\"color\": \"red\", \"fillOpacity\": 0},     layer_name=\"Regularized\",     info_mode=None, ) legend = {     \"Original\": \"#ffff00\",     \"Regularized\": \"#ff0000\", } m.add_legend(title=\"Building Footprints\", legend_dict=legend) m In\u00a0[\u00a0]: Copied! <pre>props_gdf = geoai.add_geometric_properties(\n    regularized_gdf, area_unit=\"m2\", length_unit=\"m\"\n)\nprops_gdf.head()\n</pre> props_gdf = geoai.add_geometric_properties(     regularized_gdf, area_unit=\"m2\", length_unit=\"m\" ) props_gdf.head() In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(props_gdf, column=\"area_m2\", tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(props_gdf, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>props_gdf.to_file(\"naip_test_buildings.geojson\")\n</pre> props_gdf.to_file(\"naip_test_buildings.geojson\") In\u00a0[\u00a0]: Copied! <pre>test_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>test_raster_path = geoai.download_file(test_raster_url)\n</pre> test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"buildings_prediction.tif\"\ngeoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path=\"building_footprints_usa.pth\",\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> masks_path = \"buildings_prediction.tif\" geoai.object_detection(     test_raster_path,     masks_path,     model_path=\"building_footprints_usa.pth\",     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"buildings_prediction.geojson\"\nbuildings_gdf = geoai.raster_to_vector(masks_path, output_path)\nregularized_gdf = geoai.regularize(\n    data=buildings_gdf,\n    simplify_tolerance=2.0,\n    allow_45_degree=True,\n    diagonal_threshold_reduction=30,\n    allow_circles=True,\n    circle_threshold=0.9,\n)\nprops_gdf = geoai.add_geometric_properties(\n    regularized_gdf, area_unit=\"m2\", length_unit=\"m\"\n)\n</pre> output_path = \"buildings_prediction.geojson\" buildings_gdf = geoai.raster_to_vector(masks_path, output_path) regularized_gdf = geoai.regularize(     data=buildings_gdf,     simplify_tolerance=2.0,     allow_45_degree=True,     diagonal_threshold_reduction=30,     allow_circles=True,     circle_threshold=0.9, ) props_gdf = geoai.add_geometric_properties(     regularized_gdf, area_unit=\"m2\", length_unit=\"m\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(props_gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(props_gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\"\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\" test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"solar_panels_prediction.tif\"\ngeoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path=\"solar_panel_detection.pth\",\n    window_size=400,\n    overlap=100,\n    confidence_threshold=0.4,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> masks_path = \"solar_panels_prediction.tif\" geoai.object_detection(     test_raster_path,     masks_path,     model_path=\"solar_panel_detection.pth\",     window_size=400,     overlap=100,     confidence_threshold=0.4,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"solar_panels_prediction.geojson\"\nregularized_gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\nprops_gdf = geoai.add_geometric_properties(\n    regularized_gdf, area_unit=\"m2\", length_unit=\"m\"\n)\n</pre> output_path = \"solar_panels_prediction.geojson\" regularized_gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) props_gdf = geoai.add_geometric_properties(     regularized_gdf, area_unit=\"m2\", length_unit=\"m\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(props_gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(props_gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>test_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_test_7cm.tif\"\n)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/cars_test_7cm.tif\" ) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"cars_prediction.tif\"\ngeoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path=\"car_detection_usa.pth\",\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> masks_path = \"cars_prediction.tif\" geoai.object_detection(     test_raster_path,     masks_path,     model_path=\"car_detection_usa.pth\",     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"cars_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"cars_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_test_15cm.tif\"\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/ships_sfo_test_15cm.tif\" test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"ship_prediction.tif\"\ngeoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path=\"ship_detection.pth\",\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.5,\n    batch_size=4,\n    num_channels=3,\n)\n</pre> masks_path = \"ship_prediction.tif\" geoai.object_detection(     test_raster_path,     masks_path,     model_path=\"ship_detection.pth\",     window_size=512,     overlap=256,     confidence_threshold=0.5,     batch_size=4,     num_channels=3, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"ship_prediction.geojson\"\ngdf = geoai.raster_to_vector(masks_path, output_path)\n</pre> output_path = \"ship_prediction.geojson\" gdf = geoai.raster_to_vector(masks_path, output_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\"\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\" test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"water_prediction.tif\"\ngeoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path=\"water_detection.pth\",\n    window_size=512,\n    overlap=128,\n    confidence_threshold=0.3,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> masks_path = \"water_prediction.tif\" geoai.object_detection(     test_raster_path,     masks_path,     model_path=\"water_detection.pth\",     window_size=512,     overlap=128,     confidence_threshold=0.3,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"water_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\ngdf = geoai.add_geometric_properties(gdf)\n</pre> output_path = \"water_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609932_nw_14_1_20100629.tif\"\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/m_4609932_nw_14_1_20100629.tif\" test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.object_detection(\n    test_raster_path,\n    masks_path,\n    model_path=\"wetland_detection.pth\",\n    window_size=512,\n    overlap=256,\n    confidence_threshold=0.3,\n    batch_size=4,\n    num_channels=4,\n)\n</pre> geoai.object_detection(     test_raster_path,     masks_path,     model_path=\"wetland_detection.pth\",     window_size=512,     overlap=256,     confidence_threshold=0.3,     batch_size=4,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"wetland_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\ngdf = geoai.add_geometric_properties(gdf)\n</pre> output_path = \"wetland_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) gdf = geoai.add_geometric_properties(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.4}},     basemap=test_raster_url, )"},{"location":"workshops/GeoAI_Workshop_2025/#object-detection-from-remote-sensing-imagery-with-geoai","title":"Object Detection from Remote Sensing Imagery with GeoAI\u00b6","text":"<ul> <li>\ud83d\udcd3 Notebook: https://opengeoai.org/workshops/GeoAI_Workshop_2025</li> <li>\ud83d\udcbb GitHub: https://github.com/opengeos/geoai</li> </ul>"},{"location":"workshops/GeoAI_Workshop_2025/#introduction","title":"\ud83e\udded Introduction\u00b6","text":"<p>This notebook provides hands-on materials for using the GeoAI package for object detection in remote sensing imagery.</p>"},{"location":"workshops/GeoAI_Workshop_2025/#agenda","title":"\ud83d\uddc2\ufe0f Agenda\u00b6","text":"<p>The workshop will guide you through the full pipeline of GeoAI for object detection in remote sensing imagery, including:</p> <ul> <li>\ud83d\udce6 Package installation</li> <li>\u2b07\ufe0f Data download</li> <li>\ud83d\uddbc\ufe0f Data visualization</li> <li>\ud83e\udde0 Model training</li> <li>\ud83d\udd0d Model inference</li> <li>\ud83d\udee0\ufe0f Post-processing</li> <li>\ud83c\udf10 Real-world applications</li> </ul>"},{"location":"workshops/GeoAI_Workshop_2025/#prerequisites","title":"\u2699\ufe0f Prerequisites\u00b6","text":"<ul> <li>A Google Colab account (recommended for ease of setup)</li> <li>Basic familiarity with Python and geospatial data</li> </ul>"},{"location":"workshops/GeoAI_Workshop_2025/#package-installation","title":"\ud83d\udce6 Package installation\u00b6","text":"<p>You can install the required packages using either <code>conda</code> or <code>pip</code>:</p>"},{"location":"workshops/GeoAI_Workshop_2025/#option-1-using-conda-recommended-for-local-environments","title":"Option 1: Using Conda (recommended for local environments)\u00b6","text":"<pre>conda create -n geoai python=3.12\nconda activate geoai\nconda install -c conda-forge mamba\nmamba install -c conda-forge geoai\n</pre>"},{"location":"workshops/GeoAI_Workshop_2025/#option-2-using-pip-for-colab-or-quick-installation","title":"Option 2: Using pip (for Colab or quick installation)\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#data-download","title":"\u2b07\ufe0f Data download\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#import-library","title":"Import library\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#retrieve-collections","title":"Retrieve collections\u00b6","text":"<p>Get all STAC collections from Microsoft Planetary Computer.</p>"},{"location":"workshops/GeoAI_Workshop_2025/#search-naip-imagery","title":"Search NAIP imagery\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#visualize-naip-imagery","title":"Visualize NAIP imagery\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#download-naip-imagery","title":"Download NAIP imagery\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#search-landsat-data","title":"Search Landsat data\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#visualize-landsat-data","title":"Visualize Landsat data\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#download-landsat-data","title":"Download Landsat data\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#download-building-data","title":"Download building data\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#extract-building-statistics","title":"Extract building statistics\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#data-visualization","title":"\ud83d\uddbc\ufe0f Data Visualization\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#download-sample-datasets-from-hugging-face","title":"Download sample datasets from Hugging Face\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#view-metadata","title":"View metadata\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#interactive-visualization","title":"Interactive visualization\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#model-training","title":"\ud83e\udde0 Model training\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#train-object-detection-model","title":"Train object detection model\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#model-inference","title":"\ud83d\udd0d Model inference\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#post-processing","title":"\ud83d\udee0\ufe0f Post-processing\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#raster-to-vector-conversion","title":"Raster to vector conversion\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#building-regularization","title":"Building regularization\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#result-comparison","title":"Result comparison\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#calculate-geometric-properties","title":"Calculate geometric properties\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#save-results","title":"Save results\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#real-world-applications","title":"\ud83c\udf10 Real-world applications\u00b6","text":"<p>The section demonstrates how to apply pre-trained models to real-world scenarios.</p>"},{"location":"workshops/GeoAI_Workshop_2025/#building-footprint-extraction","title":"Building footprint extraction\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#solar-panel-detection","title":"Solar panel detection\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#car-detection","title":"Car detection\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#ship-detection","title":"Ship detection\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#surface-water-mapping","title":"Surface water mapping\u00b6","text":""},{"location":"workshops/GeoAI_Workshop_2025/#wetland-mapping","title":"Wetland mapping\u00b6","text":""},{"location":"workshops/TNView_2025/","title":"TNView 2025","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install geoai-py\n</pre> # %pip install geoai-py In\u00a0[\u00a0]: Copied! <pre>import geoai\n</pre> import geoai In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset.zip\" In\u00a0[\u00a0]: Copied! <pre>out_folder = geoai.download_file(url)\nprint(f\"Downloaded dataset to {out_folder}\")\n</pre> out_folder = geoai.download_file(url) print(f\"Downloaded dataset to {out_folder}\") <p>The unzipped dataset contains two folders: <code>images</code> and <code>masks</code>. Each folder contains 2,841 images in JPG format. The <code>images</code> folder contains the original satellite imagery, and the <code>masks</code> folder contains the corresponding surface water masks in binary format (white pixels = water, black pixels = background).</p> <p>Dataset characteristics:</p> <ul> <li>Total image pairs: 2,841 training examples</li> <li>Image format: RGB satellite imagery (3 channels)</li> <li>Mask format: Binary masks where 255 = water, 0 = background</li> <li>Variable image sizes: Ranging from small 256x256 patches to larger 1024x1024+ images</li> <li>Global coverage: Samples from diverse geographic regions and water body types</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Test train_segmentation_model with automatic size detection\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/masks\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",  # The architecture to use for the model\n    encoder_name=\"resnet34\",  # The encoder to use for the model\n    encoder_weights=\"imagenet\",  # The weights to use for the encoder\n    num_channels=3,  # number of channels in the input image\n    num_classes=2,  # background and water\n    batch_size=16,  # The number of images to process in each batch\n    num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs\n    learning_rate=0.001,  # learning rate for the optimizer\n    val_split=0.2,  # 20% of the data for validation\n    target_size=(512, 512),  # target size of the input image\n    verbose=True,  # print progress\n)\n</pre> # Test train_segmentation_model with automatic size detection geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/masks\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",  # The architecture to use for the model     encoder_name=\"resnet34\",  # The encoder to use for the model     encoder_weights=\"imagenet\",  # The weights to use for the encoder     num_channels=3,  # number of channels in the input image     num_classes=2,  # background and water     batch_size=16,  # The number of images to process in each batch     num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs     learning_rate=0.001,  # learning rate for the optimizer     val_split=0.2,  # 20% of the data for validation     target_size=(512, 512),  # target size of the input image     verbose=True,  # print progress ) <p>In the model output folder <code>unet_models</code>, you will find the following files:</p> <ul> <li><code>best_model.pth</code>: The best model checkpoint (highest validation IoU)</li> <li><code>final_model.pth</code>: The last model checkpoint from the final epoch</li> <li><code>training_history.pth</code>: Complete training metrics for analysis and plotting</li> <li><code>training_summary.txt</code>: Human-readable summary of training configuration and results</li> </ul> In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>index = 3  # change it to other image index, e.g., 100\ntest_image_path = f\"{out_folder}/images/water_body_{index}.jpg\"\nground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\"\nprediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> index = 3  # change it to other image index, e.g., 100 test_image_path = f\"{out_folder}/images/water_body_{index}.jpg\" ground_truth_path = f\"{out_folder}/masks/water_body_{index}.jpg\" prediction_path = f\"{out_folder}/prediction/water_body_{index}.png\"  # save as png to preserve exact values and avoid compression artifacts model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_image_path,\n    output_path=prediction_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_image_path,     output_path=prediction_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32, ) In\u00a0[\u00a0]: Copied! <pre>fig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",\n    show_plot=True,\n)\n</pre> fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=f\"{out_folder}/prediction/water_body_{index}_comparison.png\",     show_plot=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\"\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/waterbody-dataset-sample.zip\" In\u00a0[\u00a0]: Copied! <pre>data_dir = geoai.download_file(url)\nprint(f\"Downloaded dataset to {data_dir}\")\n</pre> data_dir = geoai.download_file(url) print(f\"Downloaded dataset to {data_dir}\") In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/images\"\nmasks_dir = f\"{data_dir}/masks\"\npredictions_dir = f\"{data_dir}/predictions\"\n</pre> images_dir = f\"{data_dir}/images\" masks_dir = f\"{data_dir}/masks\" predictions_dir = f\"{data_dir}/predictions\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() In\u00a0[\u00a0]: Copied! <pre>url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/dset-s2.zip\"\ndata_dir = geoai.download_file(url, output_path=\"dset-s2.zip\")\n</pre> url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/dset-s2.zip\" data_dir = geoai.download_file(url, output_path=\"dset-s2.zip\") <p>Dataset structure:</p> <p>In the unzipped dataset, we have four folders:</p> <ul> <li><code>dset-s2/tra_scene</code>: Training images - Sentinel-2 scenes for model training</li> <li><code>dset-s2/tra_truth</code>: Training masks - Corresponding water truth masks</li> <li><code>dset-s2/val_scene</code>: Validation images - Independent Sentinel-2 scenes for testing</li> <li><code>dset-s2/val_truth</code>: Validation masks - Ground truth for performance evaluation</li> </ul> <p>We will use the training images and masks to train our model, then evaluate performance on the completely independent validation set.</p> In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/tra_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/tra_truth\"\ntiles_dir = f\"{data_dir}/dset-s2/tiles\"\n</pre> images_dir = f\"{data_dir}/dset-s2/tra_scene\" masks_dir = f\"{data_dir}/dset-s2/tra_truth\" tiles_dir = f\"{data_dir}/dset-s2/tiles\" In\u00a0[\u00a0]: Copied! <pre>result = geoai.export_geotiff_tiles_batch(\n    images_folder=images_dir,\n    masks_folder=masks_dir,\n    output_folder=tiles_dir,\n    tile_size=512,\n    stride=128,\n    quiet=True,\n)\n</pre> result = geoai.export_geotiff_tiles_batch(     images_folder=images_dir,     masks_folder=masks_dir,     output_folder=tiles_dir,     tile_size=512,     stride=128,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{tiles_dir}/images\",\n    labels_dir=f\"{tiles_dir}/masks\",\n    output_dir=f\"{tiles_dir}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=6,\n    num_classes=2,  # background and water\n    batch_size=32,\n    num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{tiles_dir}/images\",     labels_dir=f\"{tiles_dir}/masks\",     output_dir=f\"{tiles_dir}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=6,     num_classes=2,  # background and water     batch_size=32,     num_epochs=3,  # training for 3 epochs to save time, in practice you should train for more epochs     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{tiles_dir}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{tiles_dir}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>images_dir = f\"{data_dir}/dset-s2/val_scene\"\nmasks_dir = f\"{data_dir}/dset-s2/val_truth\"\npredictions_dir = f\"{data_dir}/dset-s2/predictions\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> images_dir = f\"{data_dir}/dset-s2/val_scene\" masks_dir = f\"{data_dir}/dset-s2/val_truth\" predictions_dir = f\"{data_dir}/dset-s2/predictions\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation_batch(\n    input_dir=images_dir,\n    output_dir=predictions_dir,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n    quiet=True,\n)\n</pre> geoai.semantic_segmentation_batch(     input_dir=images_dir,     output_dir=predictions_dir,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32,     quiet=True, ) In\u00a0[\u00a0]: Copied! <pre>image_id = \"S2A_L2A_20190318_N0211_R061\"  # Change to other image id, e.g., S2B_L2A_20190620_N0212_R047\ntest_image_path = f\"{data_dir}/dset-s2/val_scene/{image_id}_6Bands_S2.tif\"\nground_truth_path = f\"{data_dir}/dset-s2/val_truth/{image_id}_S2_Truth.tif\"\nprediction_path = f\"{data_dir}/dset-s2/predictions/{image_id}_6Bands_S2_mask.tif\"\nsave_path = f\"{data_dir}/dset-s2/{image_id}_6Bands_S2_comparison.png\"\n\nfig = geoai.plot_prediction_comparison(\n    original_image=test_image_path,\n    prediction_image=prediction_path,\n    ground_truth_image=ground_truth_path,\n    titles=[\"Original\", \"Prediction\", \"Ground Truth\"],\n    figsize=(15, 5),\n    save_path=save_path,\n    show_plot=True,\n    indexes=[5, 4, 3],\n    divider=5000,\n)\n</pre> image_id = \"S2A_L2A_20190318_N0211_R061\"  # Change to other image id, e.g., S2B_L2A_20190620_N0212_R047 test_image_path = f\"{data_dir}/dset-s2/val_scene/{image_id}_6Bands_S2.tif\" ground_truth_path = f\"{data_dir}/dset-s2/val_truth/{image_id}_S2_Truth.tif\" prediction_path = f\"{data_dir}/dset-s2/predictions/{image_id}_6Bands_S2_mask.tif\" save_path = f\"{data_dir}/dset-s2/{image_id}_6Bands_S2_comparison.png\"  fig = geoai.plot_prediction_comparison(     original_image=test_image_path,     prediction_image=prediction_path,     ground_truth_image=ground_truth_path,     titles=[\"Original\", \"Prediction\", \"Ground Truth\"],     figsize=(15, 5),     save_path=save_path,     show_plot=True,     indexes=[5, 4, 3],     divider=5000, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\n</pre> import os import leafmap <p>Set up the TiTiler endpoint for visualizing raster data.</p> In\u00a0[\u00a0]: Copied! <pre>os.environ[\"TITILER_ENDPOINT\"] = \"https://giswqs-titiler-endpoint.hf.space\"\n</pre> os.environ[\"TITILER_ENDPOINT\"] = \"https://giswqs-titiler-endpoint.hf.space\" <p>Create an interactive map to explore available Sentinel-2 data.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[46.693725, -95.925399], zoom=12)\nm.add_basemap(\"Esri.WorldImagery\")\nm.add_stac_gui()\nm\n</pre> m = leafmap.Map(center=[46.693725, -95.925399], zoom=12) m.add_basemap(\"Esri.WorldImagery\") m.add_stac_gui() m In\u00a0[\u00a0]: Copied! <pre>try:\n    print(m.user_roi_bounds())\nexcept:\n    print(\"Please draw a rectangle on the map before running this cell\")\n</pre> try:     print(m.user_roi_bounds()) except:     print(\"Please draw a rectangle on the map before running this cell\") <p>Use the drawing tool to draw a rectangle on the map. Click on the Search button to search Sentinel-2 imagery intersecting the rectangle.</p> In\u00a0[\u00a0]: Copied! <pre>try:\n    display(m.stac_gdf)\nexcept:\n    print(\"Click on the Search button before running this cell\")\n</pre> try:     display(m.stac_gdf) except:     print(\"Click on the Search button before running this cell\") In\u00a0[\u00a0]: Copied! <pre>try:\n    display(m.stac_item)\nexcept:\n    print(\"click on the Display button before running this cell\")\n</pre> try:     display(m.stac_item) except:     print(\"click on the Display button before running this cell\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>Search for Sentinel-2 data programmatically.</p> In\u00a0[\u00a0]: Copied! <pre>url = \"https://earth-search.aws.element84.com/v1/\"\ncollection = \"sentinel-2-l2a\"\ntime_range = \"2025-08-15/2025-08-31\"\nbbox = [-95.9912, 46.6704, -95.834, 46.7469]\n</pre> url = \"https://earth-search.aws.element84.com/v1/\" collection = \"sentinel-2-l2a\" time_range = \"2025-08-15/2025-08-31\" bbox = [-95.9912, 46.6704, -95.834, 46.7469] In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=10,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_collection=True,\n)\nsearch\n</pre> search = leafmap.stac_search(     url=url,     max_items=10,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_collection=True, ) search In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=10,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_gdf=True,\n)\nsearch.head()\n</pre> search = leafmap.stac_search(     url=url,     max_items=10,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_gdf=True, ) search.head() In\u00a0[\u00a0]: Copied! <pre>search = leafmap.stac_search(\n    url=url,\n    max_items=1,\n    collections=[collection],\n    bbox=bbox,\n    datetime=time_range,\n    query={\"eo:cloud_cover\": {\"lt\": 10}},\n    sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],\n    get_assets=True,\n)\nsearch\n</pre> search = leafmap.stac_search(     url=url,     max_items=1,     collections=[collection],     bbox=bbox,     datetime=time_range,     query={\"eo:cloud_cover\": {\"lt\": 10}},     sortby=[{\"field\": \"properties.eo:cloud_cover\", \"direction\": \"asc\"}],     get_assets=True, ) search In\u00a0[\u00a0]: Copied! <pre>bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"]\nassets = list(search.values())[0]\nlinks = [assets[band] for band in bands]\nfor link in links:\n    print(link)\n</pre> bands = [\"blue\", \"green\", \"red\", \"nir\", \"swir16\", \"swir22\"] assets = list(search.values())[0] links = [assets[band] for band in bands] for link in links:     print(link) In\u00a0[\u00a0]: Copied! <pre>out_dir = \"s2\"\nleafmap.download_files(links, out_dir)\n</pre> out_dir = \"s2\" leafmap.download_files(links, out_dir) In\u00a0[\u00a0]: Copied! <pre># !apt-get install -y gdal-bin\n</pre> # !apt-get install -y gdal-bin In\u00a0[\u00a0]: Copied! <pre>s2_path = \"s2.tif\"\n\ntry:\n    if not os.path.exists(s2_path):\n        geoai.stack_bands(input_files=out_dir, output_file=s2_path)\nexcept Exception as e:\n    print(e)\n    url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/s2-minnesota-2025-08-31-subset.tif\"\n    geoai.download_file(url, output_path=s2_path)\n</pre> s2_path = \"s2.tif\"  try:     if not os.path.exists(s2_path):         geoai.stack_bands(input_files=out_dir, output_file=s2_path) except Exception as e:     print(e)     url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/s2-minnesota-2025-08-31-subset.tif\"     geoai.download_file(url, output_path=s2_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    s2_path, indexes=[4, 3, 2], vmin=0, vmax=5000, layer_name=\"Sentinel-2\"\n)\n</pre> geoai.view_raster(     s2_path, indexes=[4, 3, 2], vmin=0, vmax=5000, layer_name=\"Sentinel-2\" ) In\u00a0[\u00a0]: Copied! <pre>s2_mask = \"s2_mask.tif\"\nmodel_path = f\"{tiles_dir}/unet_models/best_model.pth\"\n</pre> s2_mask = \"s2_mask.tif\" model_path = f\"{tiles_dir}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=s2_path,\n    output_path=s2_mask,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=6,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=32,\n)\n</pre> geoai.semantic_segmentation(     input_path=s2_path,     output_path=s2_mask,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=6,     num_classes=2,     window_size=512,     overlap=256,     batch_size=32, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    s2_mask,\n    no_data=0,\n    colormap=\"binary\",\n    layer_name=\"Water\",\n    basemap=s2_path,\n    basemap_args={\"indexes\": [4, 3, 2], \"vmin\": 0, \"vmax\": 5000},\n    backend=\"ipyleaflet\",\n)\n</pre> geoai.view_raster(     s2_mask,     no_data=0,     colormap=\"binary\",     layer_name=\"Water\",     basemap=s2_path,     basemap_args={\"indexes\": [4, 3, 2], \"vmin\": 0, \"vmax\": 5000},     backend=\"ipyleaflet\", ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\"\ntrain_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_train.tif\" train_masks_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_masks.tif\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip/naip_water_test.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_masks_path = geoai.download_file(train_masks_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_masks_path = geoai.download_file(train_masks_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.print_raster_info(train_raster_path, show_preview=False)\n</pre> geoai.print_raster_info(train_raster_path, show_preview=False) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(train_masks_url, nodata=0, opacity=0.5, basemap=train_raster_url)\n</pre> geoai.view_raster(train_masks_url, nodata=0, opacity=0.5, basemap=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"naip\"\n</pre> out_folder = \"naip\" In\u00a0[\u00a0]: Copied! <pre>tiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_masks_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_masks_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=4,\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=4,     batch_size=8,     num_epochs=3,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_water_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"naip_water_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    test_raster_path,\n    masks_path,\n    model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    window_size=512,\n    overlap=128,\n    batch_size=32,\n    num_channels=4,\n)\n</pre> geoai.semantic_segmentation(     test_raster_path,     masks_path,     model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     window_size=512,     overlap=128,     batch_size=32,     num_channels=4, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path,\n    nodata=0,\n    layer_name=\"Water\",\n    basemap=test_raster_url,\n    backend=\"ipyleaflet\",\n)\n</pre> geoai.view_raster(     masks_path,     nodata=0,     layer_name=\"Water\",     basemap=test_raster_url,     backend=\"ipyleaflet\", ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"naip_water_prediction.geojson\"\ngdf = geoai.raster_to_vector(\n    masks_path, output_path, min_area=1000, simplify_tolerance=1\n)\n</pre> output_path = \"naip_water_prediction.geojson\" gdf = geoai.raster_to_vector(     masks_path, output_path, min_area=1000, simplify_tolerance=1 ) In\u00a0[\u00a0]: Copied! <pre>gdf = geoai.add_geometric_properties(gdf)\nlen(gdf)\n</pre> gdf = geoai.add_geometric_properties(gdf) len(gdf) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf[\"elongation\"].hist()\n</pre> gdf[\"elongation\"].hist() In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10]\n</pre> gdf_filtered = gdf[gdf[\"elongation\"] &lt; 10] In\u00a0[\u00a0]: Copied! <pre>len(gdf_filtered)\n</pre> len(gdf_filtered) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() <p></p> In\u00a0[\u00a0]: Copied! <pre>train_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n)\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\"\ntest_raster_url = (\n    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\"\n)\n</pre> train_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\" ) train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\" test_raster_url = (     \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_test.tif\" ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.get_raster_info(train_raster_path)\n</pre> geoai.get_raster_info(train_raster_path) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"buildings\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"buildings\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre># Train U-Net model\ngeoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> # Train U-Net model geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,  # background and building     batch_size=8,     num_epochs=3,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>masks_path = \"naip_test_semantic_prediction.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> masks_path = \"naip_test_semantic_prediction.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=4,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=4, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path,\n    nodata=0,\n    colormap=\"binary\",\n    basemap=test_raster_url,\n    backend=\"ipyleaflet\",\n)\n</pre> geoai.view_raster(     masks_path,     nodata=0,     colormap=\"binary\",     basemap=test_raster_url,     backend=\"ipyleaflet\", ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"naip_test_semantic_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"naip_test_semantic_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() In\u00a0[\u00a0]: Copied! <pre>train_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_naip.tif\"\ntrain_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_hag.tif\"\ntrain_building_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_buildings_train.geojson\"\ntest_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_naip.tif\"\ntest_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_hag.tif\"\n</pre> train_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_naip.tif\" train_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_train_hag.tif\" train_building_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_buildings_train.geojson\" test_aerial_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_naip.tif\" test_LiDAR_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/las_vegas_test_hag.tif\" In\u00a0[\u00a0]: Copied! <pre>train_aerial_path = geoai.download_file(train_aerial_url)\ntrain_LiDAR_path = geoai.download_file(train_LiDAR_url)\ntrain_building_path = geoai.download_file(train_building_url)\ntest_aerial_path = geoai.download_file(test_aerial_url)\ntest_LiDAR_path = geoai.download_file(test_LiDAR_url)\n</pre> train_aerial_path = geoai.download_file(train_aerial_url) train_LiDAR_path = geoai.download_file(train_LiDAR_url) train_building_path = geoai.download_file(train_building_url) test_aerial_path = geoai.download_file(test_aerial_url) test_LiDAR_path = geoai.download_file(test_LiDAR_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_building_path, tiles=train_aerial_url)\n</pre> geoai.view_vector_interactive(train_building_path, tiles=train_aerial_url) <p>Visualize the building footprints with the height above ground (HAG) data derived from LiDAR data.</p> In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(\n    train_building_path,\n    layer_name=\"Buildings\",\n    tiles=train_LiDAR_url,\n    tiles_args={\"palette\": \"terrain\"},\n)\n</pre> geoai.view_vector_interactive(     train_building_path,     layer_name=\"Buildings\",     tiles=train_LiDAR_url,     tiles_args={\"palette\": \"terrain\"}, ) In\u00a0[\u00a0]: Copied! <pre>train_raster_path = \"las_vegas_train_naip_hag.tif\"\ntest_raster_path = \"las_vegas_test_naip_hag.tif\"\n</pre> train_raster_path = \"las_vegas_train_naip_hag.tif\" test_raster_path = \"las_vegas_test_naip_hag.tif\" In\u00a0[\u00a0]: Copied! <pre># geoai.set_proj_lib_path()\n</pre> # geoai.set_proj_lib_path() In\u00a0[\u00a0]: Copied! <pre>geoai.stack_bands(\n    input_files=[train_aerial_path, train_LiDAR_path],\n    output_file=train_raster_path,\n    resolution=None,  # Automatically inferred from first image\n    overwrite=True,\n    dtype=\"Byte\",  # or \"UInt16\", \"Float32\"\n)\n</pre> geoai.stack_bands(     input_files=[train_aerial_path, train_LiDAR_path],     output_file=train_raster_path,     resolution=None,  # Automatically inferred from first image     overwrite=True,     dtype=\"Byte\",  # or \"UInt16\", \"Float32\" ) In\u00a0[\u00a0]: Copied! <pre>geoai.stack_bands(\n    input_files=[test_aerial_path, test_LiDAR_path],\n    output_file=test_raster_path,\n    resolution=None,  # Automatically inferred from first image\n    overwrite=True,\n    dtype=\"Byte\",  # or \"UInt16\", \"Float32\"\n)\n</pre> geoai.stack_bands(     input_files=[test_aerial_path, test_LiDAR_path],     output_file=test_raster_path,     resolution=None,  # Automatically inferred from first image     overwrite=True,     dtype=\"Byte\",  # or \"UInt16\", \"Float32\" ) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"lidar\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_building_path,\n    tile_size=1024,\n    stride=768,\n    buffer_radius=0,\n)\n</pre> out_folder = \"lidar\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_building_path,     tile_size=1024,     stride=768,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/unet_models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=5,\n    num_classes=2,  # background and building\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.001,\n    val_split=0.2,\n    verbose=True,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/unet_models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=5,     num_classes=2,  # background and building     batch_size=8,     num_epochs=3,     learning_rate=0.001,     val_split=0.2,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/unet_models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/unet_models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>masks_path = \"building_masks.tif\"\nmodel_path = f\"{out_folder}/unet_models/best_model.pth\"\n</pre> masks_path = \"building_masks.tif\" model_path = f\"{out_folder}/unet_models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=5,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=8,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=5,     num_classes=2,     window_size=512,     overlap=256,     batch_size=8, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path,\n    nodata=0,\n    colormap=\"binary\",\n    layer_name=\"Building\",\n    basemap=test_aerial_url,\n    backend=\"ipyleaflet\",\n)\n</pre> geoai.view_raster(     masks_path,     nodata=0,     colormap=\"binary\",     layer_name=\"Building\",     basemap=test_aerial_url,     backend=\"ipyleaflet\", ) In\u00a0[\u00a0]: Copied! <pre>output_vector_path = \"building_masks.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2)\n</pre> output_vector_path = \"building_masks.geojson\" gdf = geoai.orthogonalize(masks_path, output_vector_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\")\nprint(f\"Number of buildings: {len(gdf_props)}\")\n</pre> gdf_props = geoai.add_geometric_properties(gdf, area_unit=\"m2\", length_unit=\"m\") print(f\"Number of buildings: {len(gdf_props)}\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_aerial_url)\n</pre> geoai.view_vector_interactive(gdf_props, column=\"area_m2\", tiles=test_aerial_url) In\u00a0[\u00a0]: Copied! <pre>gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)]\nprint(f\"Number of buildings: {len(gdf_filtered)}\")\n</pre> gdf_filtered = gdf_props[(gdf_props[\"area_m2\"] &gt; 50)] print(f\"Number of buildings: {len(gdf_filtered)}\") In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_aerial_url)\n</pre> geoai.view_vector_interactive(gdf_filtered, column=\"area_m2\", tiles=test_aerial_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=gdf_filtered,\n    right_layer=test_aerial_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_aerial_url,\n)\n</pre> geoai.create_split_map(     left_layer=gdf_filtered,     right_layer=test_aerial_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_aerial_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() In\u00a0[\u00a0]: Copied! <pre>train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\"\ntrain_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.geojson\"\ntest_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_test_davis_ca.tif\"\n</pre> train_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.tif\" train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_davis_ca.geojson\" test_raster_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/solar_panels_test_davis_ca.tif\" In\u00a0[\u00a0]: Copied! <pre>train_raster_path = geoai.download_file(train_raster_url)\ntrain_vector_path = geoai.download_file(train_vector_url)\ntest_raster_path = geoai.download_file(test_raster_url)\n</pre> train_raster_path = geoai.download_file(train_raster_url) train_vector_path = geoai.download_file(train_vector_url) test_raster_path = geoai.download_file(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url)\n</pre> geoai.view_vector_interactive(train_vector_path, tiles=train_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(test_raster_url)\n</pre> geoai.view_raster(test_raster_url) In\u00a0[\u00a0]: Copied! <pre>out_folder = \"solar\"\ntiles = geoai.export_geotiff_tiles(\n    in_raster=train_raster_path,\n    out_folder=out_folder,\n    in_class_data=train_vector_path,\n    tile_size=512,\n    stride=256,\n    buffer_radius=0,\n)\n</pre> out_folder = \"solar\" tiles = geoai.export_geotiff_tiles(     in_raster=train_raster_path,     out_folder=out_folder,     in_class_data=train_vector_path,     tile_size=512,     stride=256,     buffer_radius=0, ) In\u00a0[\u00a0]: Copied! <pre>geoai.train_segmentation_model(\n    images_dir=f\"{out_folder}/images\",\n    labels_dir=f\"{out_folder}/labels\",\n    output_dir=f\"{out_folder}/models\",\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    encoder_weights=\"imagenet\",\n    num_channels=3,\n    num_classes=2,\n    batch_size=8,\n    num_epochs=3,\n    learning_rate=0.005,\n    val_split=0.2,\n)\n</pre> geoai.train_segmentation_model(     images_dir=f\"{out_folder}/images\",     labels_dir=f\"{out_folder}/labels\",     output_dir=f\"{out_folder}/models\",     architecture=\"unet\",     encoder_name=\"resnet34\",     encoder_weights=\"imagenet\",     num_channels=3,     num_classes=2,     batch_size=8,     num_epochs=3,     learning_rate=0.005,     val_split=0.2, ) In\u00a0[\u00a0]: Copied! <pre>geoai.plot_performance_metrics(\n    history_path=f\"{out_folder}/models/training_history.pth\",\n    figsize=(15, 5),\n    verbose=True,\n)\n</pre> geoai.plot_performance_metrics(     history_path=f\"{out_folder}/models/training_history.pth\",     figsize=(15, 5),     verbose=True, ) <p></p> In\u00a0[\u00a0]: Copied! <pre>masks_path = \"solar_panels_prediction.tif\"\nmodel_path = f\"{out_folder}/models/best_model.pth\"\n</pre> masks_path = \"solar_panels_prediction.tif\" model_path = f\"{out_folder}/models/best_model.pth\" In\u00a0[\u00a0]: Copied! <pre>geoai.semantic_segmentation(\n    input_path=test_raster_path,\n    output_path=masks_path,\n    model_path=model_path,\n    architecture=\"unet\",\n    encoder_name=\"resnet34\",\n    num_channels=3,\n    num_classes=2,\n    window_size=512,\n    overlap=256,\n    batch_size=8,\n)\n</pre> geoai.semantic_segmentation(     input_path=test_raster_path,     output_path=masks_path,     model_path=model_path,     architecture=\"unet\",     encoder_name=\"resnet34\",     num_channels=3,     num_classes=2,     window_size=512,     overlap=256,     batch_size=8, ) In\u00a0[\u00a0]: Copied! <pre>geoai.view_raster(\n    masks_path,\n    nodata=0,\n    layer_name=\"Solar Panels\",\n    basemap=test_raster_url,\n    backend=\"ipyleaflet\",\n)\n</pre> geoai.view_raster(     masks_path,     nodata=0,     layer_name=\"Solar Panels\",     basemap=test_raster_url,     backend=\"ipyleaflet\", ) In\u00a0[\u00a0]: Copied! <pre>output_path = \"solar_panels_prediction.geojson\"\ngdf = geoai.orthogonalize(masks_path, output_path, epsilon=2)\n</pre> output_path = \"solar_panels_prediction.geojson\" gdf = geoai.orthogonalize(masks_path, output_path, epsilon=2) In\u00a0[\u00a0]: Copied! <pre>geoai.view_vector_interactive(output_path, tiles=test_raster_url)\n</pre> geoai.view_vector_interactive(output_path, tiles=test_raster_url) In\u00a0[\u00a0]: Copied! <pre>geoai.create_split_map(\n    left_layer=output_path,\n    right_layer=test_raster_url,\n    left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},\n    basemap=test_raster_url,\n)\n</pre> geoai.create_split_map(     left_layer=output_path,     right_layer=test_raster_url,     left_args={\"style\": {\"color\": \"red\", \"fillOpacity\": 0.2}},     basemap=test_raster_url, ) In\u00a0[\u00a0]: Copied! <pre>geoai.empty_cache()\n</pre> geoai.empty_cache() <p></p>"},{"location":"workshops/TNView_2025/#unlocking-the-power-of-geoai-with-python","title":"Unlocking the Power of GeoAI with Python\u00b6","text":""},{"location":"workshops/TNView_2025/#introduction","title":"Introduction\u00b6","text":"<p>This notebook is developed for TNView webinar on Unlocking the Power of GeoAI with Python on September 12, 2025.</p> <p>GeoAI represents the intersection of geospatial science and artificial intelligence, combining the power of machine learning with geographic information systems (GIS) to analyze, understand, and predict spatial patterns. This rapidly growing field enables us to extract meaningful insights from satellite imagery, aerial photos, and other geospatial datasets at unprecedented scales and accuracy levels.</p> <p>The GeoAI Python package (https://opengeoai.org) simplifies the application of deep learning models to geospatial data, making advanced AI techniques accessible to researchers, analysts, and practitioners in environmental science, urban planning, agriculture, and disaster management. The package provides a unified interface for:</p> <ul> <li>Data Preprocessing: Automated handling of various geospatial data formats, coordinate systems, and multi-spectral imagery</li> <li>Model Training: Pre-configured deep learning architectures optimized for geospatial tasks like semantic segmentation and object detection</li> <li>Feature Extraction: Automated extraction of geographic features from satellite and aerial imagery</li> <li>Visualization: Interactive mapping and analysis tools for exploring results</li> </ul> <p>In this webinar, you will:</p> <ul> <li>Discover the core capabilities of the GeoAI package, including data preprocessing, feature extraction, and geospatial deep learning workflows</li> <li>See live demonstrations on applying state-of-the-art AI models to satellite and aerial imagery</li> <li>Learn how to train custom segmentation models for surface water mapping using different data sources</li> <li>Explore real-world use cases in building footprint extraction and surface water mapping</li> </ul> <p>Event Details</p> <ul> <li>Date: Friday, September 12, 2025</li> <li>Time: 10AM-12 PM ET</li> <li>Location: Online</li> <li>Registration: https://tiny.utk.edu/tnview-geoai-webinar</li> </ul> <p>Additional Resources</p> <ul> <li>GitHub: GeoAI</li> <li>Book: Introduction to GIS Programming: A Practical Python Guide to Open Source Geospatial Tools</li> <li>YouTube: Open Geospatial Solution</li> </ul>"},{"location":"workshops/TNView_2025/#deep-learning-architectures-and-encoders","title":"Deep Learning Architectures and Encoders\u00b6","text":"<p>Before moving into the hands-on work, it\u2019s important to first understand the key ideas behind deep learning architectures and encoders.</p> <p>A deep learning architecture is like the blueprint of a factory. It defines how the network is organized, how data flows through different components, and how raw inputs are transformed into meaningful outputs. Just as a factory blueprint specifies where materials enter, how they are processed, and where finished products come out, a neural network architecture lays out the arrangement of layers (neurons) that progressively extract and refine patterns from data\u2014for example, detecting cats in images or translating between languages.</p> <p>Within this blueprint, an encoder functions as a specialized assembly line. Its role is to take messy raw materials (input data) and refine them into a compact, standardized representation that is easier for the rest of the system to work with. Some architectures also include a decoder assembly line, which reconstructs or generates the final output from the encoder\u2019s compressed representation\u2014for example, assembling a finished car from engine parts and panels.</p> <p>In short:</p> <ul> <li>Model architecture = the factory blueprint (overall design and flow)</li> <li>Encoder = the preprocessing line (condenses raw inputs into useful parts)</li> <li>Decoder = the finishing line (turns encoded parts into a final product)</li> </ul>"},{"location":"workshops/TNView_2025/#types-of-architectures","title":"Types of Architectures\u00b6","text":"<p>Different blueprints are suited for different tasks:</p> <ul> <li>Feedforward Neural Networks: simple, one-directional flow of data.</li> <li>Convolutional Neural Networks (CNNs): specialized for images, capturing spatial patterns like edges and textures.</li> <li>Recurrent Neural Networks (RNNs): designed for sequences, such as speech or time series.</li> <li>Transformers: powerful models for language and beyond, using attention mechanisms (e.g., ChatGPT).</li> </ul>"},{"location":"workshops/TNView_2025/#what-does-an-encoder-do","title":"What Does an Encoder Do?\u00b6","text":"<p>An encoder is the part of a neural network that takes an input (like an image or a sentence) and compresses it into a smaller, meaningful form called a feature representation or embedding. This process keeps the essential information while filtering out noise.</p> <p>For example, the sentence \u201cI love pizza\u201d might be converted by an encoder into a vector of numbers that still reflects its meaning, but in a way that is easier for a computer to analyze and use.</p> <p>Encoders appear in many contexts:</p> <ul> <li>Autoencoders: learn to compress and reconstruct data.</li> <li>Transformer Encoders: such as BERT, used for language understanding.</li> <li>Encoder\u2013Decoder Models: such as translation systems, where the encoder reads one language and the decoder generates another.</li> </ul>"},{"location":"workshops/TNView_2025/#encoders-and-architectures-in-practice","title":"Encoders and Architectures in Practice\u00b6","text":"<p>The pytorch segmentation models library provides a wide range of pre-trained models for semantic segmentation. It separates architectures (the blueprint) from encoders (the feature extractors):</p> <ul> <li>Architectures: <code>unet</code>, <code>unetplusplus</code>, <code>deeplabv3</code>, <code>deeplabv3plus</code>, <code>fpn</code>, <code>pspnet</code>, <code>linknet</code>, <code>manet</code>.</li> <li>Encoders: <code>resnet34</code>, <code>resnet50</code>, <code>efficientnet-b0</code>, <code>mobilenet_v2</code>, and many more.</li> </ul> <p>The GeoAI package builds on this library, offering a convenient wrapper so you can easily train your own segmentation models with a variety of architectures and encoders.</p>"},{"location":"workshops/TNView_2025/#environment-setup","title":"Environment Setup\u00b6","text":"<p>You can run this notebook locally or in Google Colab. You will need a GPU for training deep learning models. If you don't have a GPU, you can use the free GPU in Google Colab.</p> <p>To install the GeoAI package, it is recommended to use a virtual environment. Please refer to the GeoAI installation guide for more details.</p> <p>Here is a quick start guide to install the GeoAI package:</p> <pre>conda create -n geo python=3.12\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge geoai\n</pre> <p>If you have a GPU, you can install the package with GPU support:</p> <pre>mamba install -c conda-forge geoai \"pytorch=*=cuda*\"\n</pre> <p>You can install the package using pip:</p> <pre>pip install geoai-py\n</pre>"},{"location":"workshops/TNView_2025/#use-colab-gpu","title":"Use Colab GPU\u00b6","text":"<p>To use GPU, please click the \"Runtime\" menu and select \"Change runtime type\". Then select \"T4 GPU\" from the dropdown menu. GPU acceleration is highly recommended for training deep learning models, as it can reduce training time from hours to minutes.</p>"},{"location":"workshops/TNView_2025/#install-packages","title":"Install packages\u00b6","text":"<p>Uncomment the following cell to install the package. It may take a few minutes to install the package and its dependencies. Please be patient.</p>"},{"location":"workshops/TNView_2025/#import-libraries","title":"Import libraries\u00b6","text":"<p>Let's import the GeoAI package.</p>"},{"location":"workshops/TNView_2025/#surface-water-mapping-with-non-georeferenced-satellite-imagery","title":"Surface water mapping with non-georeferenced satellite imagery\u00b6","text":"<p>Surface water mapping is one of the most important applications of GeoAI, as water resources are critical for ecosystem health, agriculture, urban planning, and climate monitoring. In this first demonstration, we'll work with non-georeferenced satellite imagery in standard image formats (JPG/PNG), which is often how satellite imagery is initially distributed or stored.</p> <p>Why start with non-georeferenced imagery?</p> <ul> <li>Many datasets and online sources provide satellite imagery without embedded geographic coordinates</li> <li>It demonstrates the core computer vision aspects of GeoAI before adding geospatial complexity</li> <li>The techniques learned here can be applied to any imagery, regardless of coordinate system</li> <li>It's often faster to iterate and experiment with standard image formats</li> </ul> <p>We'll use semantic segmentation, a deep learning technique that classifies every pixel in an image. Unlike object detection (which draws bounding boxes), semantic segmentation provides precise pixel-level predictions, making it ideal for mapping natural features like water bodies that have irregular shapes.</p>"},{"location":"workshops/TNView_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the waterbody dataset from Kaggle, which contains 2,841 satellite image pairs with corresponding water masks. This dataset is particularly valuable because:</p> <ul> <li>Diverse geographic coverage: Images from different continents and climate zones</li> <li>Varied water body types: Lakes, rivers, ponds, and coastal areas</li> <li>Multiple seasons and conditions: Different lighting conditions and seasonal variations</li> <li>High-quality annotations: Manually verified water body masks for training</li> </ul> <p>Credits to the author Francisco Escobar for providing this dataset. I downloaded the dataset from Kaggle and uploaded it to Hugging Face for easy access:</p>"},{"location":"workshops/TNView_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model using U-Net architecture with a ResNet34 encoder and ImageNet pre-trained weights. Let's break down these important choices:</p> <p>Architecture: U-Net</p> <ul> <li>U-Net is a convolutional neural network architecture specifically designed for semantic segmentation</li> <li>It has a \"U\" shape with an encoder (downsampling) path and a decoder (upsampling) path</li> <li>Skip connections between encoder and decoder preserve fine-grained spatial details</li> <li>Originally designed for medical image segmentation, it works exceptionally well for geospatial applications</li> </ul> <p>Encoder: ResNet34</p> <ul> <li>ResNet34 is a 34-layer Residual Network that serves as the feature extraction backbone</li> <li>Residual connections allow training of very deep networks without vanishing gradient problems</li> <li>Balances model complexity with computational efficiency (deeper than ResNet18, more efficient than ResNet50)</li> <li>Well-suited for satellite imagery feature extraction</li> </ul> <p>Pre-trained weights: ImageNet</p> <ul> <li>Transfer learning from ImageNet provides a strong starting point for feature extraction</li> <li>ImageNet-trained models have learned to recognize edges, textures, and patterns relevant to natural imagery</li> <li>Significantly reduces training time and improves performance, especially with limited training data</li> <li>The encoder starts with knowledge of general image features, then specializes for water detection</li> </ul> <p>Key training parameters:</p> <ul> <li><code>num_channels=3</code>: RGB satellite imagery (red, green, blue bands)</li> <li><code>num_classes=2</code>: Binary classification (background vs. water)</li> <li><code>batch_size=32</code>: Process 32 images simultaneously for efficient GPU utilization</li> <li><code>num_epochs=3</code>: Training iterations (limited for demo; real-world would use 20-50+ epochs)</li> <li><code>learning_rate=0.001</code>: Controls optimization step size</li> <li><code>val_split=0.2</code>: Reserve 20% of data for validation to monitor overfitting</li> <li><code>target_size=(512, 512)</code>: Standardize all images to 512x512 pixels for consistent processing</li> </ul> <p>For more details on available architectures and encoders, please refer to https://smp.readthedocs.io/en/latest/encoders.html.</p>"},{"location":"workshops/TNView_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Model evaluation is crucial for understanding how well our trained model performs. We'll examine both the training curves and quantitative metrics to assess model quality and identify potential issues like overfitting or underfitting.</p> <p>Key evaluation metrics for semantic segmentation:</p> <ol> <li><p>Loss: Measures how far the model's predictions are from the ground truth</p> <ul> <li>Training loss: How well the model fits the training data</li> <li>Validation loss: How well the model generalizes to unseen data</li> <li>Ideal pattern: Both should decrease, with validation loss closely following training loss</li> </ul> </li> <li><p>IoU (Intersection over Union): The most important metric for segmentation tasks</p> <ul> <li>Definition: Area of overlap / Area of union between prediction and ground truth</li> <li>Range: 0.0 (no overlap) to 1.0 (perfect overlap)</li> <li>Interpretation: 0.69 IoU means ~69% accurate pixel-level water detection</li> <li>Industry standard: IoU &gt; 0.7 is generally considered good performance</li> </ul> </li> <li><p>Dice (F-1) Score: Alternative segmentation metric, closely related to IoU</p> <ul> <li>Definition: 2 \u00d7 (Area of overlap) / (Total pixels in both prediction and ground truth)</li> <li>Range: 0.0 to 1.0, similar to IoU but slightly more lenient</li> <li>Relationship: Dice = 2\u00d7IoU / (1+IoU)</li> </ul> </li> </ol> <p>IoU and Dice are monotonically related\u2014optimizing one generally optimizes the other. However, Dice tends to give slightly higher values than IoU for the same segmentation.</p> <ul> <li><p>IoU is stricter: it penalizes false positives and false negatives more heavily, making it less forgiving of small mismatches.</p> </li> <li><p>Dice is more sensitive to overlap and is often preferred in medical image segmentation, where the overlap between predicted and actual regions is more important than absolute boundaries.</p> </li> <li><p>IoU is often used in object detection and computer vision challenges (e.g., COCO benchmark), because it aligns with bounding box overlap evaluation.</p> </li> </ul> <p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/TNView_2025/#run-inference-on-a-single-image","title":"Run inference on a single image\u00b6","text":"<p>Inference is the process of using our trained model to make predictions on new, unseen images. This is where we see the practical application of our trained model.</p> <p>Note on testing approach: In this demo, we're using one of the training images for inference to demonstrate the workflow. In a real-world scenario, you should always test on completely independent images that were never seen during training to get an accurate assessment of model performance.</p> <p>You can run inference on a new image using the <code>semantic_segmentation</code> function:</p>"},{"location":"workshops/TNView_2025/#run-inference-on-multiple-images","title":"Run inference on multiple images\u00b6","text":"<p>Batch processing is essential for operational applications where you need to process many images efficiently. The GeoAI package provides <code>semantic_segmentation_batch</code> for processing entire directories of images with consistent parameters.</p> <p>First, let's download a sample set of test images that the model has never seen:</p>"},{"location":"workshops/TNView_2025/#surface-water-mapping-with-sentinel-2-imagery","title":"Surface water mapping with Sentinel-2 imagery\u00b6","text":"<p>In the second part of this notebook, we'll demonstrate surface water mapping using Sentinel-2 satellite imagery, which provides multispectral data with much richer information than standard RGB imagery.</p> <p>Why Sentinel-2 is ideal for water mapping:</p> <p>Sentinel-2 is a European Space Agency (ESA) satellite mission providing high-resolution optical imagery for land monitoring. Key advantages for water detection include:</p> <ul> <li>Multispectral capabilities: 13 spectral bands covering visible, near-infrared, and short-wave infrared</li> <li>High spatial resolution: 10-20m pixels for detailed water body mapping</li> <li>Frequent revisit time: 5-day global coverage for monitoring temporal changes</li> <li>Free and open access: Available through Copernicus Open Access Hub and other platforms</li> <li>Consistent quality: Calibrated, atmospherically corrected imagery (Level 2A)</li> </ul> <p>Spectral bands used in this analysis:</p> <ol> <li>Blue (490nm): Water absorption, atmospheric correction</li> <li>Green (560nm): Vegetation health, water clarity</li> <li>Red (665nm): Vegetation chlorophyll, land-water contrast</li> <li>Near-Infrared (842nm): Critical for water detection - water strongly absorbs NIR</li> <li>SWIR1 (1610nm): Excellent water discriminator - water has very low reflectance</li> <li>SWIR2 (2190nm): Additional water detection - separates water from wet soil/vegetation</li> </ol>"},{"location":"workshops/TNView_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>We'll use the Earth Surface Water Dataset from Zenodo, which contains Sentinel-2 imagery with 6 spectral bands and corresponding water masks. Credits to Xin Luo for creating this high-quality dataset.</p> <p>Dataset characteristics:</p> <ul> <li>Sensor: Sentinel-2 Level 2A (atmospherically corrected)</li> <li>Bands: Blue, Green, Red, NIR, SWIR1, SWIR2 (6 channels total)</li> <li>Spatial resolution: 10-20 meters per pixel</li> <li>Geographic coverage: Multiple global locations with diverse water body types</li> <li>Ground truth: Expert-annotated water masks for training and validation</li> </ul>"},{"location":"workshops/TNView_2025/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create smaller training tiles from the large GeoTIFF images. Note that we have multiple Sentinel-2 scenes in the training and validation sets, we will use the <code>export_geotiff_tiles_batch</code> function to export tiles from each scene.</p>"},{"location":"workshops/TNView_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Now we'll train a semantic segmentation model specifically for 6-channel Sentinel-2 imagery. The key difference from our previous model is the input channel configuration.</p> <p>Important parameter changes:</p> <ul> <li><code>num_channels=6</code>: Accommodate the 6 Sentinel-2 spectral bands (Blue, Green, Red, NIR, SWIR1, SWIR2)</li> <li><code>num_epochs=5</code>: Slightly more training epochs to learn complex spectral relationships</li> <li>Architecture remains U-Net + ResNet34: Proven effective for multispectral imagery</li> </ul> <p>Let's train the model using the Sentinel-2 tiles:</p>"},{"location":"workshops/TNView_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/TNView_2025/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll run inference on the validation set to evaluate the model's performance. We will use the <code>semantic_segmentation_batch</code> function to process all the validation images at once.</p>"},{"location":"workshops/TNView_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/TNView_2025/#download-sentinel-2-imagery","title":"Download Sentinel-2 imagery\u00b6","text":"<p>Real-world data acquisition is a crucial skill for operational GeoAI applications. Here we'll demonstrate how to:</p> <ol> <li>Search for Sentinel-2 data using STAC (SpatioTemporal Asset Catalog) APIs</li> <li>Apply quality filters (cloud cover, date range, geographic bounds)</li> <li>Download specific spectral bands needed for our analysis</li> <li>Prepare data for inference with our trained model</li> </ol> <p>STAC catalogs provide a standardized way to search and access satellite imagery across different providers. The Earth Search STAC API aggregates Sentinel-2 data from AWS Open Data, making it easily accessible for analysis.</p> <p>Search parameters:</p> <ul> <li>Geographic bounds: Define area of interest (bbox)</li> <li>Temporal range: Specify date range for imagery</li> <li>Cloud cover filter: Limit to images with &lt;10% cloud cover</li> <li>Collection: Focus on Sentinel-2 Level 2A (atmospherically corrected)</li> <li>Sorting: Order by cloud cover (ascending) to get clearest images first</li> </ul> <p>Let's set up an interactive map to explore available Sentinel-2 data:</p>"},{"location":"workshops/TNView_2025/#stack-image-bands","title":"Stack image bands\u00b6","text":"<p>Uncomment the following cell to install GDAL on Colab.</p>"},{"location":"workshops/TNView_2025/#run-inference-on-a-sentinel-2-image","title":"Run inference on a Sentinel-2 image\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"workshops/TNView_2025/#surface-water-mapping-with-aerial-imagery","title":"Surface water mapping with aerial imagery\u00b6","text":"<p>In this section, we'll demonstrate surface water mapping using aerial imagery from the USDA National Agriculture Imagery Program (NAIP). This represents the highest spatial resolution imagery commonly available for large-scale applications.</p> <p>NAIP imagery characteristics:</p> <p>What is NAIP?</p> <ul> <li>USDA Program: National Agriculture Imagery Program providing high-resolution aerial photography</li> <li>Coverage: Continental United States with comprehensive coverage</li> <li>Spatial resolution: 1-meter pixels (compared to 10-20m for Sentinel-2)</li> <li>Spectral bands: Red, Green, Blue, Near-Infrared (4 channels)</li> <li>Acquisition frequency: Updated every 2-3 years for each area</li> <li>Public availability: Free access through USGS and other data portals</li> </ul>"},{"location":"workshops/TNView_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>If you are interested in downloading NAIP imagery for your area of interest, check out this notebook here.</p> <p>To save time, we'll use a curated NAIP dataset with pre-processed training and testing imagery, including water body masks for model training and evaluation:</p>"},{"location":"workshops/TNView_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"workshops/TNView_2025/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"workshops/TNView_2025/#train-segmentation-model","title":"Train segmentation model\u00b6","text":"<p>Similar to the previous example, we'll train a U-Net model on the NAIP dataset.</p>"},{"location":"workshops/TNView_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"workshops/TNView_2025/#run-inference","title":"Run inference\u00b6","text":""},{"location":"workshops/TNView_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>We can convert the raster predictions to vector features for further analysis.</p>"},{"location":"workshops/TNView_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/TNView_2025/#building-detection-with-aerial-imagery","title":"Building detection with aerial imagery\u00b6","text":""},{"location":"workshops/TNView_2025/#download-sample-data","title":"Download sample data\u00b6","text":"<p>If you are interested in downloading NAIP imagery and Overture Maps data for your area of interest, check out this notebook here.</p> <p>To save time, we'll use a curated NAIP dataset and building footprints for model training and evaluation:</p>"},{"location":"workshops/TNView_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"workshops/TNView_2025/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"workshops/TNView_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":""},{"location":"workshops/TNView_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":"<p>Let's examine the training curves and model performance:</p>"},{"location":"workshops/TNView_2025/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"workshops/TNView_2025/#visualize-raster-masks","title":"Visualize raster masks\u00b6","text":""},{"location":"workshops/TNView_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"workshops/TNView_2025/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/TNView_2025/#building-detection-with-aerial-imagery-and-lidar-data","title":"Building detection with aerial imagery and LiDAR data\u00b6","text":"<p>In this section, we'll demonstrate building detection using aerial imagery and LiDAR data. This approach leverages the complementary strengths of aerial imagery and LiDAR data to achieve accurate building detection.</p> <p>If you are interested in downloading NAIP imagery and LiDAR data for your area of interest, check out this notebook here.</p>"},{"location":"workshops/TNView_2025/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":"<p>Visualize the building footprints with the aerial imagery.</p>"},{"location":"workshops/TNView_2025/#stack-bands","title":"Stack bands\u00b6","text":"<p>Stack the NAIP and HAG bands into a single image.</p>"},{"location":"workshops/TNView_2025/#create-training-data","title":"Create training data\u00b6","text":"<p>We'll create the same training tiles as before.</p>"},{"location":"workshops/TNView_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":"<p>Let's train a U-Net with ResNet34 encoder</p>"},{"location":"workshops/TNView_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"workshops/TNView_2025/#run-inference","title":"Run inference\u00b6","text":"<p>Now we'll use the trained model to make predictions on the test image.</p>"},{"location":"workshops/TNView_2025/#visualize-raster-masks","title":"Visualize raster masks\u00b6","text":""},{"location":"workshops/TNView_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":"<p>Convert the predicted mask to vector format for better visualization and analysis.</p>"},{"location":"workshops/TNView_2025/#add-geometric-properties","title":"Add geometric properties\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/TNView_2025/#solar-panel-detection-with-aerial-imagery","title":"Solar panel detection with aerial imagery\u00b6","text":"<p>In this section, we'll demonstrate solar panel detection using aerial imagery. This approach leverages the power of deep learning to detect solar panels in aerial imagery. To find more free aerial imagery, check out OpenAerialMap.</p>"},{"location":"workshops/TNView_2025/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-sample-data","title":"Visualize sample data\u00b6","text":""},{"location":"workshops/TNView_2025/#create-training-data","title":"Create training data\u00b6","text":""},{"location":"workshops/TNView_2025/#train-semantic-segmentation-model","title":"Train semantic segmentation model\u00b6","text":""},{"location":"workshops/TNView_2025/#evaluate-the-model","title":"Evaluate the model\u00b6","text":""},{"location":"workshops/TNView_2025/#run-inference","title":"Run inference\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-raster-masks","title":"Visualize raster masks\u00b6","text":""},{"location":"workshops/TNView_2025/#vectorize-masks","title":"Vectorize masks\u00b6","text":""},{"location":"workshops/TNView_2025/#visualize-results","title":"Visualize results\u00b6","text":""},{"location":"workshops/TNView_2025/#summary-and-next-steps","title":"Summary and Next Steps\u00b6","text":"<p>Congratulations! You've successfully completed a comprehensive introduction to the GeoAI package. Let's review what we accomplished and explore pathways for advancing your GeoAI skills.</p>"},{"location":"workshops/TNView_2025/#what-we-accomplished","title":"What We Accomplished\u00b6","text":"<p>1. Multi-scale Water Mapping Workflows:</p> <ul> <li>RGB Imagery: Trained models on standard satellite imagery (JPG format)</li> <li>Multispectral Sentinel-2: Leveraged 6 spectral bands for enhanced discrimination</li> <li>High-resolution NAIP: Utilized 1-meter aerial imagery for detailed mapping</li> <li>LiDAR Data: Used LiDAR data for building detection</li> </ul> <p>2. Deep Learning Fundamentals:</p> <ul> <li>U-Net Architecture: Applied state-of-the-art segmentation models</li> <li>Transfer Learning: Leveraged ImageNet pre-trained weights for faster convergence</li> <li>Multispectral Processing: Handled various spectral configurations (3, 4, and 6 channels)</li> </ul> <p>3. Operational Workflows:</p> <ul> <li>Data Acquisition: Downloaded and processed real satellite and aerial imagery</li> <li>Model Training: Trained custom models for different imagery types</li> <li>Performance Evaluation: Assessed model quality using IoU and Dice metrics</li> <li>Batch Processing: Applied models to multiple images efficiently</li> <li>Vector Conversion: Transformed predictions into GIS-ready polygon features</li> </ul> <p>4. Real-world Applications:</p> <ul> <li>Data Preprocessing: Handled various geospatial data formats and projections</li> <li>Quality Assessment: Filtered results based on geometric properties</li> <li>Interactive Visualization: Created interactive maps for exploring results</li> </ul>"},{"location":"workshops/TNView_2025/#thank-you","title":"Thank You!\u00b6","text":"<p>Thank you for participating in this TNView GeoAI workshop! The techniques demonstrated here represent just the beginning of what's possible when combining artificial intelligence with geospatial analysis. The field of GeoAI is rapidly evolving, offering exciting opportunities to address real-world challenges in environmental monitoring, urban planning, agriculture, and climate science.</p> <p>Keep exploring, keep learning, and keep pushing the boundaries of what's possible with GeoAI!</p> <p>*For questions, feedback, or collaboration opportunities, please visit the GeoAI GitHub repository.</p>"}]}