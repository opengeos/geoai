{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Export Training Data in Multiple Formats (PASCAL VOC, COCO, YOLO)\n",
    "\n",
    "[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/examples/export_training_data_formats.ipynb)\n",
    "\n",
    "This notebook demonstrates how to export geospatial training data in three popular object detection formats:\n",
    "\n",
    "- **PASCAL VOC**: XML-based format, widely used in computer vision\n",
    "- **COCO**: JSON-based format, standard for object detection benchmarks\n",
    "- **YOLO**: Text-based format with normalized coordinates, optimized for YOLO models\n",
    "\n",
    "## Install packages\n",
    "\n",
    "Ensure the required packages are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install geoai-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geoai\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Download sample data\n",
    "\n",
    "We'll use the same building detection dataset from the segmentation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raster_url = (\n",
    "    \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_rgb_train.tif\"\n",
    ")\n",
    "train_vector_url = \"https://huggingface.co/datasets/giswqs/geospatial/resolve/main/naip_train_buildings.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raster_path = geoai.download_file(train_raster_url)\n",
    "train_vector_path = geoai.download_file(train_vector_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Visualize sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoai.get_raster_info(train_raster_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoai.view_vector_interactive(train_vector_path, tiles=train_raster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Format 1: PASCAL VOC (XML)\n",
    "\n",
    "PASCAL VOC format stores annotations in XML files with bounding boxes and class labels. This is the default format and is widely used in traditional object detection frameworks.\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "pascal_voc_output/\n",
    "├── images/          # GeoTIFF tiles\n",
    "├── labels/          # Label masks (GeoTIFF)\n",
    "└── annotations/     # XML annotation files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_output = \"buildings_pascal_voc\"\n",
    "\n",
    "stats = geoai.export_geotiff_tiles(\n",
    "    in_raster=train_raster_path,\n",
    "    out_folder=pascal_output,\n",
    "    in_class_data=train_vector_path,\n",
    "    tile_size=512,\n",
    "    stride=256,\n",
    "    buffer_radius=0,\n",
    "    max_tiles=10,  # Limit for demo purposes\n",
    "    metadata_format=\"PASCAL_VOC\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Examine PASCAL VOC output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List annotation files\n",
    "xml_files = list(Path(f\"{pascal_output}/annotations\").glob(\"*.xml\"))\n",
    "print(f\"Found {len(xml_files)} XML annotation files\")\n",
    "\n",
    "# Display first annotation file\n",
    "if xml_files:\n",
    "    with open(xml_files[0], \"r\") as f:\n",
    "        print(f\"\\nSample annotation ({xml_files[0].name}):\\n\")\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Format 2: COCO (JSON)\n",
    "\n",
    "COCO format uses a single JSON file containing all annotations, images, and categories. This is the standard format for modern object detection benchmarks.\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "coco_output/\n",
    "├── images/              # GeoTIFF tiles\n",
    "├── labels/              # Label masks (GeoTIFF)\n",
    "└── annotations/\n",
    "    └── instances.json   # COCO annotations\n",
    "```\n",
    "\n",
    "**COCO JSON structure:**\n",
    "```json\n",
    "{\n",
    "  \"images\": [{\"id\": 0, \"file_name\": \"tile_000000.tif\", \"width\": 512, \"height\": 512}],\n",
    "  \"annotations\": [{\"id\": 1, \"image_id\": 0, \"category_id\": 1, \"bbox\": [x, y, w, h]}],\n",
    "  \"categories\": [{\"id\": 1, \"name\": \"building\", \"supercategory\": \"object\"}]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_output = \"buildings_coco\"\n",
    "\n",
    "stats = geoai.export_geotiff_tiles(\n",
    "    in_raster=train_raster_path,\n",
    "    out_folder=coco_output,\n",
    "    in_class_data=train_vector_path,\n",
    "    tile_size=512,\n",
    "    stride=256,\n",
    "    buffer_radius=0,\n",
    "    max_tiles=10,\n",
    "    metadata_format=\"COCO\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Examine COCO output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO annotations\n",
    "coco_file = f\"{coco_output}/annotations/instances.json\"\n",
    "with open(coco_file, \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "print(f\"COCO Dataset Summary:\")\n",
    "print(f\"  Images: {len(coco_data['images'])}\")\n",
    "print(f\"  Annotations: {len(coco_data['annotations'])}\")\n",
    "print(f\"  Categories: {len(coco_data['categories'])}\")\n",
    "\n",
    "# Display categories\n",
    "print(f\"\\nCategories:\")\n",
    "for cat in coco_data[\"categories\"]:\n",
    "    print(f\"  {cat}\")\n",
    "\n",
    "# Display first image\n",
    "if coco_data[\"images\"]:\n",
    "    print(f\"\\nFirst image:\")\n",
    "    print(f\"  {coco_data['images'][0]}\")\n",
    "\n",
    "# Display first annotation\n",
    "if coco_data[\"annotations\"]:\n",
    "    print(f\"\\nFirst annotation:\")\n",
    "    print(f\"  {coco_data['annotations'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Format 3: YOLO (Text)\n",
    "\n",
    "YOLO format uses text files with normalized bounding box coordinates. Each image has a corresponding `.txt` file with one line per object.\n",
    "\n",
    "**Output structure:**\n",
    "```\n",
    "yolo_output/\n",
    "├── images/           # GeoTIFF tiles\n",
    "├── labels/           # Label masks (GeoTIFF) + YOLO .txt files\n",
    "└── classes.txt       # Class names (one per line)\n",
    "```\n",
    "\n",
    "**YOLO annotation format (normalized coordinates 0-1):**\n",
    "```\n",
    "<class_id> <x_center> <y_center> <width> <height>\n",
    "0 0.5 0.5 0.3 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_output = \"buildings_yolo\"\n",
    "\n",
    "stats = geoai.export_geotiff_tiles(\n",
    "    in_raster=train_raster_path,\n",
    "    out_folder=yolo_output,\n",
    "    in_class_data=train_vector_path,\n",
    "    tile_size=512,\n",
    "    stride=256,\n",
    "    buffer_radius=0,\n",
    "    max_tiles=10,\n",
    "    metadata_format=\"YOLO\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Examine YOLO output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classes\n",
    "classes_file = f\"{yolo_output}/classes.txt\"\n",
    "with open(classes_file, \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "print(f\"Classes ({len(classes)}):\")\n",
    "for i, cls in enumerate(classes):\n",
    "    print(f\"  {i}: {cls}\")\n",
    "\n",
    "# List annotation files\n",
    "txt_files = list(Path(f\"{yolo_output}/labels\").glob(\"*.txt\"))\n",
    "print(f\"\\nFound {len(txt_files)} YOLO annotation files\")\n",
    "\n",
    "# Display first annotation file\n",
    "if txt_files:\n",
    "    with open(txt_files[0], \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"\\nSample annotation ({txt_files[0].name}):\")\n",
    "    print(f\"  Format: <class_id> <x_center> <y_center> <width> <height>\")\n",
    "    for line in lines[:5]:  # Show first 5 objects\n",
    "        print(f\"  {line.strip()}\")\n",
    "    if len(lines) > 5:\n",
    "        print(f\"  ... and {len(lines) - 5} more objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Format Comparison\n",
    "\n",
    "### When to Use Each Format\n",
    "\n",
    "| Format | Best For | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| **PASCAL VOC** | Traditional CV frameworks, quick inspection | Human-readable XML, one file per image | Verbose, not ideal for large datasets |\n",
    "| **COCO** | Modern object detection, benchmarking, complex datasets | Efficient JSON, supports multiple annotations types | Single file can be large, requires parsing |\n",
    "| **YOLO** | YOLO models (v3-v8), real-time detection | Compact, fast to parse, normalized coordinates | Less human-readable, limited metadata |\n",
    "\n",
    "### Coordinate Systems\n",
    "\n",
    "- **PASCAL VOC**: Absolute pixel coordinates `[xmin, ymin, xmax, ymax]`\n",
    "- **COCO**: Absolute pixel coordinates `[x, y, width, height]` (top-left corner)\n",
    "- **YOLO**: Normalized coordinates `[x_center, y_center, width, height]` (0-1 range)\n",
    "\n",
    "### GeoAI Extensions\n",
    "\n",
    "All formats preserve geospatial information:\n",
    "- **PASCAL VOC**: CRS, transform, and bounds in `<georeference>` element\n",
    "- **COCO**: CRS and transform as custom fields in image metadata\n",
    "- **YOLO**: Georeferenced GeoTIFF tiles maintain spatial context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Multi-Class Example\n",
    "\n",
    "The formats also support multi-class datasets. Here's how class information is stored:\n",
    "\n",
    "**PASCAL VOC:**\n",
    "```xml\n",
    "<object>\n",
    "  <name>building</name>\n",
    "  <bndbox>...</bndbox>\n",
    "</object>\n",
    "```\n",
    "\n",
    "**COCO:**\n",
    "```json\n",
    "{\n",
    "  \"categories\": [\n",
    "    {\"id\": 1, \"name\": \"building\", \"supercategory\": \"object\"},\n",
    "    {\"id\": 2, \"name\": \"road\", \"supercategory\": \"object\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**YOLO:**\n",
    "```\n",
    "classes.txt:\n",
    "building\n",
    "road\n",
    "\n",
    "annotations:\n",
    "0 0.5 0.5 0.3 0.2  # class_id 0 = building\n",
    "1 0.7 0.3 0.2 0.1  # class_id 1 = road\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `export_geotiff_tiles` function now supports three popular annotation formats:\n",
    "\n",
    "- ✅ **PASCAL VOC** (XML) - Traditional, human-readable\n",
    "- ✅ **COCO** (JSON) - Modern benchmark standard\n",
    "- ✅ **YOLO** (TXT) - Lightweight, optimized for YOLO\n",
    "\n",
    "All formats maintain geospatial context through georeferenced GeoTIFF tiles, making them ideal for training object detection models on remote sensing imagery.\n",
    "\n",
    "Choose the format that best fits your model training framework:\n",
    "- Use **COCO** for detectron2, MMDetection, or benchmark comparisons\n",
    "- Use **YOLO** for YOLOv5, YOLOv8, or ultralytics\n",
    "- Use **PASCAL VOC** for TensorFlow Object Detection API or legacy frameworks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
