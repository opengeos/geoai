{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Object Detection with NWPU-VHR-10\n",
    "\n",
    "[![image](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/opengeos/geoai/blob/main/docs/examples/object_detection_nwpu.ipynb)\n",
    "\n",
    "This notebook demonstrates end-to-end multi-class object detection using the [NWPU-VHR-10](https://github.com/chaozhong2010/VHR-10_dataset_coco) dataset, a benchmark for object detection in very high resolution (VHR) remote sensing imagery.\n",
    "\n",
    "The dataset contains 800 images with 10 object classes:\n",
    "- airplane, ship, storage tank, baseball diamond, tennis court\n",
    "- basketball court, ground track field, harbor, bridge, vehicle\n",
    "\n",
    "## Install package\n",
    "To use the `geoai-py` package, ensure it is installed in your environment. Uncomment the command below if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install geoai-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import geoai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NWPU-VHR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = geoai.download_nwpu_vhr10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Dataset directory: {data_dir}\")\n",
    "print(f\"Contents: {os.listdir(data_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNWPU-VHR-10 Classes:\")\n",
    "for i, name in enumerate(geoai.NWPU_VHR10_CLASSES):\n",
    "    print(f\"  {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = geoai.prepare_nwpu_vhr10(data_dir, val_split=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Images directory: {splits['images_dir']}\")\n",
    "print(f\"Number of classes: {splits['num_classes']}\")\n",
    "print(f\"Class names: {splits['class_names']}\")\n",
    "print(f\"Training images: {len(splits['train_image_ids'])}\")\n",
    "print(f\"Validation images: {len(splits['val_image_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize sample annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load annotations\n",
    "with open(splits[\"annotations_path\"], \"r\") as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Get a few sample images\n",
    "sample_images = coco_data[\"images\"][:4]\n",
    "categories = {cat[\"id\"]: cat[\"name\"] for cat in coco_data[\"categories\"]}\n",
    "cmap = plt.cm.get_cmap(\"tab10\", 10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_idx, img_info in enumerate(sample_images):\n",
    "    img_path = os.path.join(splits[\"images_dir\"], img_info[\"file_name\"])\n",
    "    img = Image.open(img_path)\n",
    "    axes[ax_idx].imshow(img)\n",
    "    axes[ax_idx].set_title(img_info[\"file_name\"], fontsize=10)\n",
    "    axes[ax_idx].axis(\"off\")\n",
    "\n",
    "    # Draw annotations for this image\n",
    "    img_anns = [\n",
    "        ann for ann in coco_data[\"annotations\"] if ann[\"image_id\"] == img_info[\"id\"]\n",
    "    ]\n",
    "    for ann in img_anns:\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        cat_id = ann[\"category_id\"]\n",
    "        color = cmap(cat_id % 10)\n",
    "        rect = plt.Rectangle(\n",
    "            (x, y), w, h, linewidth=2, edgecolor=color, facecolor=\"none\"\n",
    "        )\n",
    "        axes[ax_idx].add_patch(rect)\n",
    "        axes[ax_idx].text(\n",
    "            x,\n",
    "            y - 3,\n",
    "            categories.get(cat_id, str(cat_id)),\n",
    "            color=\"white\",\n",
    "            fontsize=7,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=color, alpha=0.7),\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use pretrained model from HuggingFace\n",
    "\n",
    "A pretrained Mask R-CNN model for NWPU-VHR-10 is available on HuggingFace Hub. You can download it directly and run inference without training. If you prefer to train your own model, skip to the \"Train multi-class detection model\" section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = geoai.download_nwpu_vhr10_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on a sample image using the pretrained model. The `multiclass_detection` function will use the NWPU-VHR-10 class names automatically when using the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a sample image from the dataset\n",
    "sample_img_path = os.path.join(splits[\"images_dir\"], \"012.jpg\")\n",
    "output_raster = \"nwpu_pretrained_output.tif\"\n",
    "\n",
    "result_path, inference_time, detections = geoai.multiclass_detection(\n",
    "    input_path=sample_img_path,\n",
    "    output_path=output_raster,\n",
    "    model_path=model_path,\n",
    "    confidence_threshold=0.5,\n",
    ")\n",
    "\n",
    "print(f\"Inference time: {inference_time:.2f}s\")\n",
    "print(f\"Total detections: {len(detections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoai.visualize_multiclass_detections(\n",
    "    image_path=sample_img_path,\n",
    "    detections=detections,\n",
    "    confidence_threshold=0.5,\n",
    "    figsize=(12, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call `multiclass_detection` without specifying `model_path` at all. It will automatically download the pretrained model and use the NWPU-VHR-10 class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path, inference_time, detections = geoai.multiclass_detection(\n",
    "    input_path=sample_img_path,\n",
    "    output_path=\"nwpu_auto_output.tif\",\n",
    "    confidence_threshold=0.5,\n",
    ")\n",
    "\n",
    "print(f\"Inference time: {inference_time:.2f}s\")\n",
    "print(f\"Total detections: {len(detections)}\")\n",
    "\n",
    "# Clean up temporary output files\n",
    "for f in [\"nwpu_pretrained_output.tif\", \"nwpu_auto_output.tif\"]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train multi-class detection model (Optional)\n",
    "\n",
    "Alternatively, you can train your own Mask R-CNN model from scratch on the NWPU-VHR-10 dataset. This section is optional if you are using the pretrained model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"nwpu_output\"\n",
    "\n",
    "model_path = geoai.train_multiclass_detector(\n",
    "    images_dir=splits[\"images_dir\"],\n",
    "    annotations_path=splits[\"train_annotations\"],\n",
    "    output_dir=output_dir,\n",
    "    class_names=splits[\"class_names\"],\n",
    "    num_channels=3,\n",
    "    batch_size=4,\n",
    "    num_epochs=20,\n",
    "    learning_rate=0.005,\n",
    "    val_split=0.15,\n",
    "    seed=42,\n",
    "    pretrained=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "history_path = os.path.join(output_dir, \"training_history.pth\")\n",
    "if os.path.exists(history_path):\n",
    "    history = torch.load(history_path, weights_only=True)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    axes[0].plot(history[\"epochs\"], history[\"train_loss\"], label=\"Train Loss\")\n",
    "    axes[0].plot(history[\"epochs\"], history[\"val_loss\"], label=\"Val Loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Training & Validation Loss\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(history[\"epochs\"], history[\"val_iou\"], label=\"Val IoU\", color=\"green\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].set_ylabel(\"IoU\")\n",
    "    axes[1].set_title(\"Validation IoU\")\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(\n",
    "        history[\"epochs\"], history[\"lr\"], label=\"Learning Rate\", color=\"orange\"\n",
    "    )\n",
    "    axes[2].set_xlabel(\"Epoch\")\n",
    "    axes[2].set_ylabel(\"LR\")\n",
    "    axes[2].set_title(\"Learning Rate Schedule\")\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model with COCO metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = geoai.evaluate_multiclass_detector(\n",
    "    model_path=model_path,\n",
    "    images_dir=splits[\"images_dir\"],\n",
    "    annotations_path=splits[\"val_annotations\"],\n",
    "    num_classes=splits[\"num_classes\"],\n",
    "    class_names=splits[\"class_names\"][1:],  # Exclude background\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a validation image for inference\n",
    "with open(splits[\"val_annotations\"], \"r\") as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# Find an image with multiple object types\n",
    "test_img_info = val_data[\"images\"][0]\n",
    "test_img_path = os.path.join(splits[\"images_dir\"], test_img_info[\"file_name\"])\n",
    "print(f\"Test image: {test_img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_raster = \"nwpu_detection_output.tif\"\n",
    "\n",
    "result_path, inference_time, detections = geoai.multiclass_detection(\n",
    "    input_path=test_img_path,\n",
    "    output_path=output_raster,\n",
    "    model_path=model_path,\n",
    "    num_classes=splits[\"num_classes\"],\n",
    "    class_names=splits[\"class_names\"],\n",
    "    window_size=512,\n",
    "    overlap=256,\n",
    "    confidence_threshold=0.5,\n",
    "    batch_size=4,\n",
    "    num_channels=3,\n",
    ")\n",
    "\n",
    "print(f\"\\nInference time: {inference_time:.2f}s\")\n",
    "print(f\"Total detections: {len(detections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoai.visualize_multiclass_detections(\n",
    "    image_path=test_img_path,\n",
    "    detections=detections,\n",
    "    class_names=splits[\"class_names\"],\n",
    "    confidence_threshold=0.5,\n",
    "    figsize=(12, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch inference on multiple validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on a few validation images and display results\n",
    "num_samples = min(4, len(val_data[\"images\"]))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(num_samples):\n",
    "    img_info = val_data[\"images\"][idx]\n",
    "    img_path = os.path.join(splits[\"images_dir\"], img_info[\"file_name\"])\n",
    "    out_path = f\"nwpu_detection_{idx}.tif\"\n",
    "\n",
    "    _, _, dets = geoai.multiclass_detection(\n",
    "        input_path=img_path,\n",
    "        output_path=out_path,\n",
    "        model_path=model_path,\n",
    "        num_classes=splits[\"num_classes\"],\n",
    "        class_names=splits[\"class_names\"],\n",
    "        confidence_threshold=0.5,\n",
    "        num_channels=3,\n",
    "    )\n",
    "\n",
    "    # Display\n",
    "    img = Image.open(img_path)\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(\n",
    "        f\"{img_info['file_name']} ({len(dets)} detections)\", fontsize=10\n",
    "    )\n",
    "    axes[idx].axis(\"off\")\n",
    "\n",
    "    for det in dets:\n",
    "        box = det[\"box\"]\n",
    "        label = det[\"label\"]\n",
    "        score = det[\"score\"]\n",
    "        color = cmap(label % 10)\n",
    "        rect = plt.Rectangle(\n",
    "            (box[0], box[1]),\n",
    "            box[2] - box[0],\n",
    "            box[3] - box[1],\n",
    "            linewidth=2,\n",
    "            edgecolor=color,\n",
    "            facecolor=\"none\",\n",
    "        )\n",
    "        axes[idx].add_patch(rect)\n",
    "        name = (\n",
    "            splits[\"class_names\"][label]\n",
    "            if label < len(splits[\"class_names\"])\n",
    "            else str(label)\n",
    "        )\n",
    "        axes[idx].text(\n",
    "            box[0],\n",
    "            box[1] - 3,\n",
    "            f\"{name}: {score:.2f}\",\n",
    "            color=\"white\",\n",
    "            fontsize=7,\n",
    "            bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=color, alpha=0.7),\n",
    "        )\n",
    "\n",
    "    # Clean up temp file\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **Downloading** the NWPU-VHR-10 remote sensing object detection dataset\n",
    "2. **Preparing** train/validation splits from COCO-format annotations\n",
    "3. **Using a pretrained model** from HuggingFace Hub for instant inference\n",
    "4. **Training** a multi-class Mask R-CNN model for 10 object categories (optional)\n",
    "5. **Evaluating** the model using COCO-style mAP metrics\n",
    "6. **Running inference** on test images with multi-class detection\n",
    "7. **Visualizing** detection results with colored bounding boxes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
